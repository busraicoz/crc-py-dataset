[
  {
    "code": "@@ -20,7 +20,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        postgis-version: [latest, \"17-3.5-alpine\", \"17-master\"]\n+        postgis-version: [latest, \"17-3.6-alpine\", \"17-master\"]",
    "comment": "this is a good change! i think we should make this:",
    "line_number": 23,
    "enriched": "File: .github/workflows/postgis.yml\nCode: @@ -20,7 +20,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        postgis-version: [latest, \"17-3.5-alpine\", \"17-master\"]\n+        postgis-version: [latest, \"17-3.6-alpine\", \"17-master\"]\nComment: This is a good change! I think we should make this:\n\n```suggestion\n        postgis-version: [\"latest\", \"18-3.6-alpine\u2060\", \"17-3.6-alpine\", \"17-master\"]\n```",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": ".github/workflows/postgis.yml",
    "pr_number": 19928,
    "repo": "django",
    "owner": "django",
    "comment_id": 2411811987,
    "comment_created_at": "2025-10-07T20:24:26Z"
  },
  {
    "code": "@@ -29,8 +29,9 @@ def builtin_template_path(name):\n \n def set_language(request):\n     \"\"\"\n-    Redirect to a given URL while setting the chosen language in the session",
    "comment": "elsewhere (other than the other place in this pr), this is just \"the language cookie\", so i think this is sufficient here:",
    "line_number": 32,
    "enriched": "File: django/views/i18n.py\nCode: @@ -29,8 +29,9 @@ def builtin_template_path(name):\n \n def set_language(request):\n     \"\"\"\n-    Redirect to a given URL while setting the chosen language in the session\nComment: Elsewhere (other than the other place in this PR), this is just \"the language cookie\", so I think this is sufficient here:\n```suggestion\n    Redirect to a given URL while setting the chosen language in the language cookie.\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "django/views/i18n.py",
    "pr_number": 19917,
    "repo": "django",
    "owner": "django",
    "comment_id": 2402243330,
    "comment_created_at": "2025-10-03T14:50:01Z"
  },
  {
    "code": "@@ -4,6 +4,7 @@ import {\n   NG_VALUE_ACCESSOR,\n   ReactiveFormsModule,\n } from '@angular/forms'\n+import { NgxBootstrapIconsModule } from 'ngx-bootstrap-icons'",
    "comment": "This actually fixes a bug (did not search for related issues) where using a removable textarea would not show an `x` (on hover)",
    "line_number": 7,
    "enriched": "File: src-ui/src/app/components/common/input/textarea/textarea.component.ts\nCode: @@ -4,6 +4,7 @@ import {\n   NG_VALUE_ACCESSOR,\n   ReactiveFormsModule,\n } from '@angular/forms'\n+import { NgxBootstrapIconsModule } from 'ngx-bootstrap-icons'\nComment: This actually fixes a bug (did not search for related issues) where using a removable textarea would not show an `x` (on hover)",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "src-ui/src/app/components/common/input/textarea/textarea.component.ts",
    "pr_number": 10846,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2346891377,
    "comment_created_at": "2025-09-13T18:39:16Z"
  },
  {
    "code": "@@ -63,8 +63,8 @@ def build_attention_mask(\n         masked = torch.triu(minus_inf, diagonal=causal_diagonal)\n         # Apply sliding window mask if needed\n         if sliding_window > 1:\n-            sliding_diagonal = seqlen_k - seqlen_q + sliding_window\n-            masked = torch.tril(masked, diagonal=sliding_diagonal)\n+            sliding_diagonal = seqlen_k - seqlen_q - sliding_window\n+            masked += torch.tril(minus_inf, diagonal=sliding_diagonal)",
    "comment": "mmmm the sliding indeed has to be substracted but only if seqlen_q > sliding",
    "line_number": 67,
    "enriched": "File: src/transformers/generation/continuous_batching/continuous_api.py\nCode: @@ -63,8 +63,8 @@ def build_attention_mask(\n         masked = torch.triu(minus_inf, diagonal=causal_diagonal)\n         # Apply sliding window mask if needed\n         if sliding_window > 1:\n-            sliding_diagonal = seqlen_k - seqlen_q + sliding_window\n-            masked = torch.tril(masked, diagonal=sliding_diagonal)\n+            sliding_diagonal = seqlen_k - seqlen_q - sliding_window\n+            masked += torch.tril(minus_inf, diagonal=sliding_diagonal)\nComment: Mmmm the sliding indeed has to be substracted but only if seqlen_q > sliding ",
    "subcategory": "logical",
    "category": "functional",
    "llm_confidence": 0.9940773844718933,
    "file_path": "src/transformers/generation/continuous_batching/continuous_api.py",
    "pr_number": 41228,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2391533026,
    "comment_created_at": "2025-09-30T13:42:24Z"
  },
  {
    "code": "@@ -1302,8 +1302,8 @@ materialized view.\n     real column. If ``False``, the column acts as a virtual column and does\n     not occupy database storage space.\n \n-    PostgreSQL only supports persisted columns. Oracle only supports virtual\n-    columns.\n+    PostgreSQL supports persisted and virtual columns.\n+    Oracle only supports virtual columns.",
    "comment": "the comment should mention that virtual columns are only supported on 18+ on postgres.",
    "line_number": 1306,
    "enriched": "File: docs/ref/models/fields.txt\nCode: @@ -1302,8 +1302,8 @@ materialized view.\n     real column. If ``False``, the column acts as a virtual column and does\n     not occupy database storage space.\n \n-    PostgreSQL only supports persisted columns. Oracle only supports virtual\n-    columns.\n+    PostgreSQL supports persisted and virtual columns.\n+    Oracle only supports virtual columns.\nComment: The comment should mention that virtual columns are only supported on 18+ on Postgres.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "docs/ref/models/fields.txt",
    "pr_number": 19907,
    "repo": "django",
    "owner": "django",
    "comment_id": 2384293267,
    "comment_created_at": "2025-09-27T19:38:57Z"
  },
  {
    "code": "@@ -2445,8 +2445,8 @@ This has a number of caveats though:\n         Entry.objects.bulk_create(batch, batch_size)\n \n The ``batch_size`` parameter controls how many objects are created in a single\n-query. The default is to create all objects in one batch, except for SQLite\n-where the default is such that at most 999 variables per query are used.\n+query. The default is to create as many objects in one batch as the database\n+will allow. (SQLite and Oracle limit the number of parameters in a query.)",
    "comment": "i think you could combine punctuation:\n\n\n\ni'm not sure what the \"rules\" are though.",
    "line_number": 2449,
    "enriched": "File: docs/ref/models/querysets.txt\nCode: @@ -2445,8 +2445,8 @@ This has a number of caveats though:\n         Entry.objects.bulk_create(batch, batch_size)\n \n The ``batch_size`` parameter controls how many objects are created in a single\n-query. The default is to create all objects in one batch, except for SQLite\n-where the default is such that at most 999 variables per query are used.\n+query. The default is to create as many objects in one batch as the database\n+will allow. (SQLite and Oracle limit the number of parameters in a query.)\nComment: I think you could combine punctuation:\n\n```suggestion\nwill allow (SQLite and Oracle limit the number of parameters in a query).\n```\n\nI'm not sure what the \"rules\" are though.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/ref/models/querysets.txt",
    "pr_number": 19903,
    "repo": "django",
    "owner": "django",
    "comment_id": 2381267951,
    "comment_created_at": "2025-09-26T08:00:11Z"
  },
  {
    "code": "@@ -0,0 +1,65 @@\n+export const recommended = { ",
    "comment": "it would be nice for this to stay aligned with our version of eslint\r\ni believe this is coming from here: https://github.com/eslint/eslint/blob/main/packages/js/src/configs/eslint-recommended.js\r\nwe should have at least some comments in the file about where this comes from and ideally a script/commands on how to update the file. maybe a comment by the eslint version in package.json that this file should be updated",
    "line_number": 1,
    "enriched": "File: eslint.recommended.mjs\nCode: @@ -0,0 +1,65 @@\n+export const recommended = { \nComment: It would be nice for this to stay aligned with our version of eslint\r\nI believe this is coming from here: https://github.com/eslint/eslint/blob/main/packages/js/src/configs/eslint-recommended.js\r\nWe should have at least some comments in the file about where this comes from and ideally a script/commands on how to update the file. Maybe a comment by the eslint version in package.json that this file should be updated",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "eslint.recommended.mjs",
    "pr_number": 19899,
    "repo": "django",
    "owner": "django",
    "comment_id": 2375147454,
    "comment_created_at": "2025-09-24T09:20:38Z"
  },
  {
    "code": "@@ -36,7 +36,7 @@ jobs:\n       - name: Build docs\n         run: |\n           cd docs\n-          sphinx-build -b spelling -n -q -W -d _build/doctrees -D language=en_US -j auto . _build/spelling\n+          sphinx-build -b spelling -n -q -W -d _build/doctrees -D language=en_US -j auto -W . _build/spelling",
    "comment": ":thinking: but -w is already there",
    "line_number": 39,
    "enriched": "File: .github/workflows/docs.yml\nCode: @@ -36,7 +36,7 @@ jobs:\n       - name: Build docs\n         run: |\n           cd docs\n-          sphinx-build -b spelling -n -q -W -d _build/doctrees -D language=en_US -j auto . _build/spelling\n+          sphinx-build -b spelling -n -q -W -d _build/doctrees -D language=en_US -j auto -W . _build/spelling\nComment: :thinking: but `-W` is already there ",
    "subcategory": "false positive",
    "category": "false positive",
    "file_path": ".github/workflows/docs.yml",
    "pr_number": 19893,
    "repo": "django",
    "owner": "django",
    "comment_id": 2370356415,
    "comment_created_at": "2025-09-22T21:34:04Z"
  },
  {
    "code": "@@ -13,5 +13,5 @@ galaxy_info:\n     - name: Ubuntu\n       versions:\n         - focal\n-\n+  role_name: bare_metal_paperless",
    "comment": "How is this related?",
    "line_number": 16,
    "enriched": "File: ansible/meta/main.yml\nCode: @@ -13,5 +13,5 @@ galaxy_info:\n     - name: Ubuntu\n       versions:\n         - focal\n-\n+  role_name: bare_metal_paperless\nComment: How is this related?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "ansible/meta/main.yml",
    "pr_number": 11,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 806636317,
    "comment_created_at": "2022-02-15T09:41:36Z"
  },
  {
    "code": "@@ -12,22 +12,39 @@\n   </button>\n   <div class=\"dropdown-menu py-0 shadow\" ngbDropdownMenu attr.aria-labelledby=\"dropdown{{title}}\">\n     <div class=\"list-group list-group-flush\">\n+      <div *ngIf=\"!editing && multiple\" class=\"list-group-item d-flex\">\n+        <div class=\"btn-group btn-group-xs btn-group-toggle flex-fill\" ngbRadioGroup [(ngModel)]=\"selectionModel.logicalOperator\" (change)=\"selectionModel.toggleOperator()\">\n+          <label ngbButtonLabel class=\"btn btn-outline-primary\">\n+            <input ngbButton type=\"radio\" name=\"logicalOperator\" value=\"and\" [disabled]=\"!operatorToggleEnabled\"> All\n+          </label>\n+          <label ngbButtonLabel class=\"btn btn-outline-primary\">\n+            <input ngbButton type=\"radio\" name=\"logicalOperator\" value=\"or\" [disabled]=\"!operatorToggleEnabled\"> Any\n+          </label>\n+        </div>\n+      </div>\n       <div class=\"list-group-item\">\n         <div class=\"input-group input-group-sm\">\n           <input class=\"form-control\" type=\"text\" [(ngModel)]=\"filterText\" [placeholder]=\"filterPlaceholder\" (keyup.enter)=\"listFilterEnter()\" #listFilterTextInput>\n         </div>\n       </div>\n       <div *ngIf=\"selectionModel.items\" class=\"items\">\n         <ng-container *ngFor=\"let item of selectionModel.itemsSorted | filter: filterText\">\n-          <app-toggleable-dropdown-button *ngIf=\"allowSelectNone || item.id\" [item]=\"item\" [state]=\"selectionModel.get(item.id)\" (toggle)=\"selectionModel.toggle(item.id)\"></app-toggleable-dropdown-button>\n+          <app-toggleable-dropdown-button *ngIf=\"allowSelectNone || item.id\" [item]=\"item\" [state]=\"selectionModel.get(item.id)\" (toggle)=\"selectionModel.toggle(item.id)\" (exclude)=\"excludeClicked(item.id)\"></app-toggleable-dropdown-button>\n         </ng-container>\n       </div>\n       <button *ngIf=\"editing\" class=\"list-group-item list-group-item-action bg-light\" (click)=\"applyClicked()\" [disabled]=\"!selectionModel.isDirty()\">\n-        <small class=\"ml-1\" [ngClass]=\"{'font-weight-bold': selectionModel.isDirty()}\" i18n>Apply</small>\n+        <small class=\"ml-2\" [ngClass]=\"{'font-weight-bold': selectionModel.isDirty()}\" i18n>Apply</small>\n         <svg width=\"1.5em\" height=\"1em\" viewBox=\"0 0 16 16\" fill=\"currentColor\">\n           <use xlink:href=\"assets/bootstrap-icons.svg#arrow-right\" />\n         </svg>\n       </button>\n+      <div *ngIf=\"!editing\" class=\"list-group-item list-group-item-note pt-1 pb-2\">\n+        <small i18n>Use\n+          <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"currentColor\" class=\"bi bi-option\" viewBox=\"0 0 16 16\">\n+            <use xlink:href=\"assets/bootstrap-icons.svg#option\" />\n+          </svg>\n+           + click / reutrn to exclude items.</small>\n+      </div>",
    "comment": "Small spelling issue",
    "line_number": 47,
    "enriched": "File: src-ui/src/app/components/common/filterable-dropdown/filterable-dropdown.component.html\nCode: @@ -12,22 +12,39 @@\n   </button>\n   <div class=\"dropdown-menu py-0 shadow\" ngbDropdownMenu attr.aria-labelledby=\"dropdown{{title}}\">\n     <div class=\"list-group list-group-flush\">\n+      <div *ngIf=\"!editing && multiple\" class=\"list-group-item d-flex\">\n+        <div class=\"btn-group btn-group-xs btn-group-toggle flex-fill\" ngbRadioGroup [(ngModel)]=\"selectionModel.logicalOperator\" (change)=\"selectionModel.toggleOperator()\">\n+          <label ngbButtonLabel class=\"btn btn-outline-primary\">\n+            <input ngbButton type=\"radio\" name=\"logicalOperator\" value=\"and\" [disabled]=\"!operatorToggleEnabled\"> All\n+          </label>\n+          <label ngbButtonLabel class=\"btn btn-outline-primary\">\n+            <input ngbButton type=\"radio\" name=\"logicalOperator\" value=\"or\" [disabled]=\"!operatorToggleEnabled\"> Any\n+          </label>\n+        </div>\n+      </div>\n       <div class=\"list-group-item\">\n         <div class=\"input-group input-group-sm\">\n           <input class=\"form-control\" type=\"text\" [(ngModel)]=\"filterText\" [placeholder]=\"filterPlaceholder\" (keyup.enter)=\"listFilterEnter()\" #listFilterTextInput>\n         </div>\n       </div>\n       <div *ngIf=\"selectionModel.items\" class=\"items\">\n         <ng-container *ngFor=\"let item of selectionModel.itemsSorted | filter: filterText\">\n-          <app-toggleable-dropdown-button *ngIf=\"allowSelectNone || item.id\" [item]=\"item\" [state]=\"selectionModel.get(item.id)\" (toggle)=\"selectionModel.toggle(item.id)\"></app-toggleable-dropdown-button>\n+          <app-toggleable-dropdown-button *ngIf=\"allowSelectNone || item.id\" [item]=\"item\" [state]=\"selectionModel.get(item.id)\" (toggle)=\"selectionModel.toggle(item.id)\" (exclude)=\"excludeClicked(item.id)\"></app-toggleable-dropdown-button>\n         </ng-container>\n       </div>\n       <button *ngIf=\"editing\" class=\"list-group-item list-group-item-action bg-light\" (click)=\"applyClicked()\" [disabled]=\"!selectionModel.isDirty()\">\n-        <small class=\"ml-1\" [ngClass]=\"{'font-weight-bold': selectionModel.isDirty()}\" i18n>Apply</small>\n+        <small class=\"ml-2\" [ngClass]=\"{'font-weight-bold': selectionModel.isDirty()}\" i18n>Apply</small>\n         <svg width=\"1.5em\" height=\"1em\" viewBox=\"0 0 16 16\" fill=\"currentColor\">\n           <use xlink:href=\"assets/bootstrap-icons.svg#arrow-right\" />\n         </svg>\n       </button>\n+      <div *ngIf=\"!editing\" class=\"list-group-item list-group-item-note pt-1 pb-2\">\n+        <small i18n>Use\n+          <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"currentColor\" class=\"bi bi-option\" viewBox=\"0 0 16 16\">\n+            <use xlink:href=\"assets/bootstrap-icons.svg#option\" />\n+          </svg>\n+           + click / reutrn to exclude items.</small>\n+      </div>\nComment: Small spelling issue",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "src-ui/src/app/components/common/filterable-dropdown/filterable-dropdown.component.html",
    "pr_number": 10,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 807642715,
    "comment_created_at": "2022-02-16T07:59:21Z"
  },
  {
    "code": "@@ -412,6 +420,15 @@ For example, if we had a ``Book`` model with a ``ManyToManyField`` linking to\n             ),\n         ]\n \n+.. note::\n+   When switching from an implicit ``ManyToManyField`` to a custom through\n+   model, Django does not automatically preserve the unique index on the\n+   foreign keys. To maintain the same behavior, add a ``UniqueConstraint``\n+   in the migration. This ensures consistent behavior across database\n+   backends. It helps avoid subtle bugs that may arise from missing\n+   uniqueness enforcement. The older ``unique_together`` option is\n+   deprecated and should be avoided.\n+",
    "comment": "i don't think we need this\r\nin the manytomany.through docs it does say (https://docs.djangoproject.com/en/5.2/ref/models/fields/#django.db.models.manytomanyfield.through):\r\n\r\n> there is a unique constraint on the two foreign keys.\r\n\r\nwe are just making sure what is created here represents the auto created through model",
    "line_number": 431,
    "enriched": "File: docs/howto/writing-migrations.txt\nCode: @@ -412,6 +420,15 @@ For example, if we had a ``Book`` model with a ``ManyToManyField`` linking to\n             ),\n         ]\n \n+.. note::\n+   When switching from an implicit ``ManyToManyField`` to a custom through\n+   model, Django does not automatically preserve the unique index on the\n+   foreign keys. To maintain the same behavior, add a ``UniqueConstraint``\n+   in the migration. This ensures consistent behavior across database\n+   backends. It helps avoid subtle bugs that may arise from missing\n+   uniqueness enforcement. The older ``unique_together`` option is\n+   deprecated and should be avoided.\n+\nComment: I don't think we need this\r\nIn the ManyToMany.through docs it does say (https://docs.djangoproject.com/en/5.2/ref/models/fields/#django.db.models.ManyToManyField.through):\r\n\r\n> There is a unique constraint on the two foreign keys.\r\n\r\nWe are just making sure what is created here represents the auto created through model\r\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/howto/writing-migrations.txt",
    "pr_number": 19891,
    "repo": "django",
    "owner": "django",
    "comment_id": 2378877743,
    "comment_created_at": "2025-09-25T12:11:45Z"
  },
  {
    "code": "@@ -635,6 +636,16 @@ def technical_404_response(request, exception):\n         ):\n             return default_urlconf(request)\n \n+    for inner_tried in itertools.chain(*tried or []):\n+        if isinstance(inner_tried, URLResolver):\n+            inner_tried.debug_key_val = TriedPatternDebugInfo(\n+                key=\"namespace\", val=inner_tried.namespace\n+            )\n+        else:\n+            inner_tried.debug_key_val = TriedPatternDebugInfo(\n+                key=\"name\", val=inner_tried.name\n+            )",
    "comment": "i wouldn't add a namedtuple unless we had to",
    "line_number": 647,
    "enriched": "File: django/views/debug.py\nCode: @@ -635,6 +636,16 @@ def technical_404_response(request, exception):\n         ):\n             return default_urlconf(request)\n \n+    for inner_tried in itertools.chain(*tried or []):\n+        if isinstance(inner_tried, URLResolver):\n+            inner_tried.debug_key_val = TriedPatternDebugInfo(\n+                key=\"namespace\", val=inner_tried.namespace\n+            )\n+        else:\n+            inner_tried.debug_key_val = TriedPatternDebugInfo(\n+                key=\"name\", val=inner_tried.name\n+            )\nComment: ```suggestion\r\n        if isinstance(inner_tried, URLResolver):\r\n            inner_tried.debug_key_val = {\r\n                \"key\": \"namespace\",\r\n                \"val\": inner_tried.namespace,\r\n            }\r\n        else:\r\n            inner_tried.debug_key_val = {\"key\": \"name\", \"val\": inner_tried.name}\r\n```\r\n\r\nI wouldn't add a `namedtuple` unless we had to",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "django/views/debug.py",
    "pr_number": 19880,
    "repo": "django",
    "owner": "django",
    "comment_id": 2375043322,
    "comment_created_at": "2025-09-24T08:49:31Z"
  },
  {
    "code": "@@ -131,18 +131,18 @@ The code below is equivalent to the code above::\n \n .. _field-checking:\n \n-Field, constraint, model, manager, template engine, and database checks\n------------------------------------------------------------------------\n+Field, constraint, model, manager, template engine, task, and database checks",
    "comment": "good catch! though i'm not sure we should keep this given we are removing all inbuilt checks for tasks :thinking:",
    "line_number": 134,
    "enriched": "File: docs/topics/checks.txt\nCode: @@ -131,18 +131,18 @@ The code below is equivalent to the code above::\n \n .. _field-checking:\n \n-Field, constraint, model, manager, template engine, and database checks\n------------------------------------------------------------------------\n+Field, constraint, model, manager, template engine, task, and database checks\nComment: Good catch! Though I'm not sure we should keep this given we are removing all inbuilt checks for tasks :thinking: ",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "docs/topics/checks.txt",
    "pr_number": 19876,
    "repo": "django",
    "owner": "django",
    "comment_id": 2355590967,
    "comment_created_at": "2025-09-17T13:47:35Z"
  },
  {
    "code": "@@ -1423,6 +1423,14 @@ def values_list(self, *fields, flat=False, named=False):\n                     \"field.\"\n                 )\n             elif not fields:\n+                # RemovedInDjango70Warning: When the deprecation ends, raise\n+                # TypeError instead.",
    "comment": "given other typeerrors here have a specific message, i think it makes sense to define one here",
    "line_number": 1427,
    "enriched": "File: django/db/models/query.py\nCode: @@ -1423,6 +1423,14 @@ def values_list(self, *fields, flat=False, named=False):\n                     \"field.\"\n                 )\n             elif not fields:\n+                # RemovedInDjango70Warning: When the deprecation ends, raise\n+                # TypeError instead.\nComment: ```suggestion\r\n                # RemovedInDjango70Warning: When the deprecation ends, replace with:\r\n                # raise TypeError(\r\n                #     \"'flat' is not valid when values_list is called with no fields.\"\r\n                # )\r\n```\r\nGiven other `TypeError`s here have a specific message, I think it makes sense to define one here",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "django/db/models/query.py",
    "pr_number": 19874,
    "repo": "django",
    "owner": "django",
    "comment_id": 2362490328,
    "comment_created_at": "2025-09-19T10:38:00Z"
  },
  {
    "code": "@@ -18,13 +18,13 @@ project.\n Python compatibility\n ====================\n \n-Django 6.0 supports Python 3.12 and 3.13. We **highly recommend** and only\n-officially support the latest release of each series.\n+Django 6.0 supports Python 3.12 and 3.13. We **highly recommend**, and only\n+officially support, the latest release of each series.",
    "comment": "if we prefer this change, we should do it also in https://github.com/django/django/pull/19834 (cc @jacobtylerwalls )",
    "line_number": 22,
    "enriched": "File: docs/releases/6.0.txt\nCode: @@ -18,13 +18,13 @@ project.\n Python compatibility\n ====================\n \n-Django 6.0 supports Python 3.12 and 3.13. We **highly recommend** and only\n-officially support the latest release of each series.\n+Django 6.0 supports Python 3.12 and 3.13. We **highly recommend**, and only\n+officially support, the latest release of each series.\nComment: If we prefer this change, we should do it also in https://github.com/django/django/pull/19834 (cc @jacobtylerwalls )",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "docs/releases/6.0.txt",
    "pr_number": 19872,
    "repo": "django",
    "owner": "django",
    "comment_id": 2355508632,
    "comment_created_at": "2025-09-17T13:21:00Z"
  },
  {
    "code": "@@ -1142,23 +1142,22 @@ def _save_table(\n                         ),\n                     )[\"_order__max\"]\n                 )\n-            fields = [\n+            insert_fields = [",
    "comment": "i changed the variable name as it made it much easier for me to reason about the origin of the f.generated and (pk_set or f is not meta.auto_field) check.",
    "line_number": 1145,
    "enriched": "File: django/db/models/base.py\nCode: @@ -1142,23 +1142,22 @@ def _save_table(\n                         ),\n                     )[\"_order__max\"]\n                 )\n-            fields = [\n+            insert_fields = [\nComment: I changed the variable name as it made it much easier for me to reason about the origin of the `f.generated and (pk_set or f is not meta.auto_field)` check.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "django/db/models/base.py",
    "pr_number": 19871,
    "repo": "django",
    "owner": "django",
    "comment_id": 2353771065,
    "comment_created_at": "2025-09-16T22:14:50Z"
  },
  {
    "code": "@@ -1153,7 +1153,8 @@ def _save_table(\n                     getattr(self, field.attname) if raw else field.pre_save(self, False)\n                 )\n                 if hasattr(value, \"resolve_expression\"):\n-                    returning_fields.append(field)\n+                    if field not in returning_fields:",
    "comment": "in tandem with #19868 we can collapse this when the else is gone.",
    "line_number": 1156,
    "enriched": "File: django/db/models/base.py\nCode: @@ -1153,7 +1153,8 @@ def _save_table(\n                     getattr(self, field.attname) if raw else field.pre_save(self, False)\n                 )\n                 if hasattr(value, \"resolve_expression\"):\n-                    returning_fields.append(field)\n+                    if field not in returning_fields:\nComment: In tandem with #19868 we can collapse this when the else is gone.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "django/db/models/base.py",
    "pr_number": 19869,
    "repo": "django",
    "owner": "django",
    "comment_id": 2352984002,
    "comment_created_at": "2025-09-16T16:03:26Z"
  },
  {
    "code": "@@ -1154,8 +1154,6 @@ def _save_table(\n                 )\n                 if hasattr(value, \"resolve_expression\"):\n                     returning_fields.append(field)\n-                elif field.db_returning:",
    "comment": "i think we might want to tweak this check to compare against pk_fields and pk_set as well.\r\n\r\nan alternative would be have the pre_save value assigned back to the instance in _assign_returned_values without transiting through the database.\r\n\r\ni can work on proposed adjusted later today if you'd like.",
    "line_number": 1157,
    "enriched": "File: django/db/models/base.py\nCode: @@ -1154,8 +1154,6 @@ def _save_table(\n                 )\n                 if hasattr(value, \"resolve_expression\"):\n                     returning_fields.append(field)\n-                elif field.db_returning:\nComment: I think we might want to tweak this check to compare against `pk_fields` and `pk_set` as well.\r\n\r\nAn alternative would be have the `pre_save` value assigned back to the instance in `_assign_returned_values` without transiting through the database.\r\n\r\nI can work on proposed adjusted later today if you'd like.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "django/db/models/base.py",
    "pr_number": 19868,
    "repo": "django",
    "owner": "django",
    "comment_id": 2352980310,
    "comment_created_at": "2025-09-16T16:01:49Z"
  },
  {
    "code": "@@ -513,7 +513,7 @@ def test_rfc2231_wrong_title(self):\n             (\n                 \"Content-Type: application/x-stuff; \"\n                 \"title*='This%20is%20%2A%2A%2Afun%2A%2A%2A\",\n-                \"'This is ***fun***\",\n+                \"'This%20is%20%2A%2A%2Afun%2A%2A%2A\",",
    "comment": "in the docstring we have \"but stdlib email still decodes (#35440).\", shouldn't we revert that also, if we are making this change?",
    "line_number": 515,
    "enriched": "File: tests/utils_tests/test_http.py\nCode: @@ -513,7 +513,7 @@ def test_rfc2231_wrong_title(self):\n             (\n                 \"Content-Type: application/x-stuff; \"\n                 \"title*='This%20is%20%2A%2A%2Afun%2A%2A%2A\",\n-                \"'This is ***fun***\",\n+                \"'This%20is%20%2A%2A%2Afun%2A%2A%2A\",\nComment: In the docstring we have \"But stdlib email still decodes (#35440).\", shouldn't we revert that also, if we are making this change?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/utils_tests/test_http.py",
    "pr_number": 19867,
    "repo": "django",
    "owner": "django",
    "comment_id": 2349799935,
    "comment_created_at": "2025-09-15T18:32:36Z"
  },
  {
    "code": "@@ -226,8 +226,8 @@ both appearing in the \"To:\"::\n         [\"john@example.com\", \"jane@example.com\"],\n     )\n \n-This sends a message to john@example.com and jane@example.com, with them both\n-receiving a separate email::\n+This sends a message to ``john@example.com`` and ``jane@example.com``, with\n+them both receiving a separate email::",
    "comment": "unrelated, but this sounds off to me. perhaps \"both of them\" or \"each of them\"",
    "line_number": 230,
    "enriched": "File: docs/topics/email.txt\nCode: @@ -226,8 +226,8 @@ both appearing in the \"To:\"::\n         [\"john@example.com\", \"jane@example.com\"],\n     )\n \n-This sends a message to john@example.com and jane@example.com, with them both\n-receiving a separate email::\n+This sends a message to ``john@example.com`` and ``jane@example.com``, with\n+them both receiving a separate email::\nComment: Unrelated, but this sounds off to me. Perhaps \"both of them\" or \"each of them\"",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/topics/email.txt",
    "pr_number": 19866,
    "repo": "django",
    "owner": "django",
    "comment_id": 2348730208,
    "comment_created_at": "2025-09-15T11:41:43Z"
  },
  {
    "code": "@@ -150,6 +150,7 @@ def gettext_noop(s):\n     (\"vi\", gettext_noop(\"Vietnamese\")),\n     (\"zh-hans\", gettext_noop(\"Simplified Chinese\")),\n     (\"zh-hant\", gettext_noop(\"Traditional Chinese\")),\n+    (\"ht\", gettext_noop(\"Haitian Creole\")),",
    "comment": "this list should be alphabetized.",
    "line_number": 153,
    "enriched": "File: django/conf/global_settings.py\nCode: @@ -150,6 +150,7 @@ def gettext_noop(s):\n     (\"vi\", gettext_noop(\"Vietnamese\")),\n     (\"zh-hans\", gettext_noop(\"Simplified Chinese\")),\n     (\"zh-hant\", gettext_noop(\"Traditional Chinese\")),\n+    (\"ht\", gettext_noop(\"Haitian Creole\")),\nComment: This list should be alphabetized.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "django/conf/global_settings.py",
    "pr_number": 19858,
    "repo": "django",
    "owner": "django",
    "comment_id": 2347986200,
    "comment_created_at": "2025-09-15T06:24:07Z"
  },
  {
    "code": "@@ -565,6 +565,11 @@ def run(self, result):\n         ]\n         test_results = pool.imap_unordered(self.run_subsuite.__func__, args)\n \n+        # Disable buffering on the local test result that will accumulate\n+        # remote suites results as each process will take care of its own\n+        # buffering and there's nothing to capture on the main process.\n+        result.buffer = False",
    "comment": "i'd move this up, since this could _potentially_ happen before you start running the multiprocessing pool",
    "line_number": 567,
    "enriched": "File: django/test/runner.py\nCode: @@ -565,6 +565,11 @@ def run(self, result):\n         ]\n         test_results = pool.imap_unordered(self.run_subsuite.__func__, args)\n \n+        # Disable buffering on the local test result that will accumulate\n+        # remote suites results as each process will take care of its own\n+        # buffering and there's nothing to capture on the main process.\n+        result.buffer = False\nComment: I'd move this up, since this could _potentially_ happen before you start running the multiprocessing pool\r\n```suggestion\r\n        # Disable buffering on the local test result that will accumulate\r\n        # remote suites results as each process will take care of its own\r\n        # buffering and there's nothing to capture on the main process.\r\n        result.buffer = False\r\n\r\n        test_results = pool.imap_unordered(self.run_subsuite.__func__, args)\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "django/test/runner.py",
    "pr_number": 19853,
    "repo": "django",
    "owner": "django",
    "comment_id": 2345252663,
    "comment_created_at": "2025-09-12T19:36:17Z"
  },
  {
    "code": "@@ -138,10 +139,21 @@ def on_template_render(self, sender, signal, template, context, **kwargs):\n         self.rendered_templates.append(template)\n         self.context.append(copy(context))\n \n+    @property\n+    def rendered_template_names(self):\n+        rendered_names = []\n+        for t in self.rendered_templates:\n+            if t.name is None:\n+                continue\n+            if isinstance(t, PartialTemplate):\n+                rendered_names.append(f\"{t.origin.template_name}#{t.name}\")\n+            rendered_names.append(t.name)",
    "comment": "factored this property out once we needed it in two places. also nice is that this \"qualified\" form now appears first in the output before the \"short\" form.",
    "line_number": 150,
    "enriched": "File: django/test/testcases.py\nCode: @@ -138,10 +139,21 @@ def on_template_render(self, sender, signal, template, context, **kwargs):\n         self.rendered_templates.append(template)\n         self.context.append(copy(context))\n \n+    @property\n+    def rendered_template_names(self):\n+        rendered_names = []\n+        for t in self.rendered_templates:\n+            if t.name is None:\n+                continue\n+            if isinstance(t, PartialTemplate):\n+                rendered_names.append(f\"{t.origin.template_name}#{t.name}\")\n+            rendered_names.append(t.name)\nComment: Factored this property out once we needed it in two places. Also nice is that this \"qualified\" form now appears first in the output before the \"short\" form.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "django/test/testcases.py",
    "pr_number": 19847,
    "repo": "django",
    "owner": "django",
    "comment_id": 2349300078,
    "comment_created_at": "2025-09-15T15:00:50Z"
  },
  {
    "code": "@@ -1414,11 +1414,14 @@ def values(self, *fields, **expressions):\n     def values_list(self, *fields, flat=False, named=False):\n         if flat and named:\n             raise TypeError(\"'flat' and 'named' can't be used together.\")\n-        if flat and len(fields) > 1:\n-            raise TypeError(\n-                \"'flat' is not valid when values_list is called with more than one \"\n-                \"field.\"\n-            )\n+        if flat:\n+            if len(fields) > 1:\n+                raise TypeError(\n+                    \"'flat' is not valid when values_list is called with more than one \"\n+                    \"field.\"\n+                )\n+            elif not fields:\n+                fields = [self.model._meta.concrete_fields[0].attname]",
    "comment": "i think this is always pk, but i wasn't 100% sure, so i copied here what's eventually used in query.set_values() when not passing any fields.",
    "line_number": 1426,
    "enriched": "File: django/db/models/query.py\nCode: @@ -1414,11 +1414,14 @@ def values(self, *fields, **expressions):\n     def values_list(self, *fields, flat=False, named=False):\n         if flat and named:\n             raise TypeError(\"'flat' and 'named' can't be used together.\")\n-        if flat and len(fields) > 1:\n-            raise TypeError(\n-                \"'flat' is not valid when values_list is called with more than one \"\n-                \"field.\"\n-            )\n+        if flat:\n+            if len(fields) > 1:\n+                raise TypeError(\n+                    \"'flat' is not valid when values_list is called with more than one \"\n+                    \"field.\"\n+                )\n+            elif not fields:\n+                fields = [self.model._meta.concrete_fields[0].attname]\nComment: I think this is always `pk`, but I wasn't 100% sure, so I copied here what's eventually used in `Query.set_values()` when not passing any fields.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "django/db/models/query.py",
    "pr_number": 19844,
    "repo": "django",
    "owner": "django",
    "comment_id": 2342048189,
    "comment_created_at": "2025-09-11T18:47:30Z"
  },
  {
    "code": "@@ -1184,24 +1185,77 @@ def in_bulk(self, id_list=None, *, field_name=\"pk\"):\n                 \"in_bulk()'s field_name must be a unique field but %r isn't.\"\n                 % field_name\n             )\n+\n+        qs = self\n+\n+        def get_obj(obj):\n+            return obj",
    "comment": "kind of wish python had an identity function to avoid the call but oh well.",
    "line_number": 1191,
    "enriched": "File: django/db/models/query.py\nCode: @@ -1184,24 +1185,77 @@ def in_bulk(self, id_list=None, *, field_name=\"pk\"):\n                 \"in_bulk()'s field_name must be a unique field but %r isn't.\"\n                 % field_name\n             )\n+\n+        qs = self\n+\n+        def get_obj(obj):\n+            return obj\nComment: Kind of wish Python had an identity function to avoid the call but oh well.",
    "subcategory": "false positive",
    "category": "false positive",
    "file_path": "django/db/models/query.py",
    "pr_number": 19842,
    "repo": "django",
    "owner": "django",
    "comment_id": 2341185778,
    "comment_created_at": "2025-09-11T14:34:06Z"
  },
  {
    "code": "@@ -49,6 +49,7 @@\n     padding: 8px;\n }\n \n+.tabular .selector-chosen-title label,",
    "comment": "this visual regression was introduced in a0f50c2a483678d31bd1ad6f08fd3a0b8399e27b\r\nwe might want to do something like:\r\ndiff\r\n--- a/django/contrib/admin/static/admin/css/widgets.css\r\n+++ b/django/contrib/admin/static/admin/css/widgets.css\r\n@@ -49,8 +49,7 @@\r\n     padding: 8px;\r\n }\r\n \r\n-.tabular .selector-chosen-title label,\r\n-.aligned .selector-chosen-title label {\r\n+.selector-chosen-title label {\r\n     color: var(--header-link-color);\r\n     width: 100%;\r\n }\r\n@@ -61,7 +60,7 @@\r\n     padding: 8px;\r\n }\r\n \r\n-.aligned .selector-available-title label {\r\n+.selector-available-title label {\r\n     width: 100%;\r\n }",
    "line_number": 52,
    "enriched": "File: django/contrib/admin/static/admin/css/widgets.css\nCode: @@ -49,6 +49,7 @@\n     padding: 8px;\n }\n \n+.tabular .selector-chosen-title label,\nComment: This visual regression was introduced in a0f50c2a483678d31bd1ad6f08fd3a0b8399e27b\r\nWe might want to do something like:\r\n```diff\r\n--- a/django/contrib/admin/static/admin/css/widgets.css\r\n+++ b/django/contrib/admin/static/admin/css/widgets.css\r\n@@ -49,8 +49,7 @@\r\n     padding: 8px;\r\n }\r\n \r\n-.tabular .selector-chosen-title label,\r\n-.aligned .selector-chosen-title label {\r\n+.selector-chosen-title label {\r\n     color: var(--header-link-color);\r\n     width: 100%;\r\n }\r\n@@ -61,7 +60,7 @@\r\n     padding: 8px;\r\n }\r\n \r\n-.aligned .selector-available-title label {\r\n+.selector-available-title label {\r\n     width: 100%;\r\n }\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "django/contrib/admin/static/admin/css/widgets.css",
    "pr_number": 19837,
    "repo": "django",
    "owner": "django",
    "comment_id": 2348625941,
    "comment_created_at": "2025-09-15T10:59:16Z"
  },
  {
    "code": "@@ -536,7 +536,20 @@ async def __aiter__(self):\n             )\n             # sync iterator. Consume via sync_to_async and yield via async\n             # generator.\n-            for part in await sync_to_async(list)(self.streaming_content):\n+            _iterator = self.streaming_content\n+            _sentinel = object()\n+\n+            def _next_wrapper():\n+                try:\n+                    return _iterator.__next__()\n+                except StopIteration:\n+                    return _sentinel\n+\n+            _next = sync_to_async(_next_wrapper, thread_sensitive=False)",
    "comment": "thread_sensitive=false. and if the iterator was using the orm?",
    "line_number": 548,
    "enriched": "File: django/http/response.py\nCode: @@ -536,7 +536,20 @@ async def __aiter__(self):\n             )\n             # sync iterator. Consume via sync_to_async and yield via async\n             # generator.\n-            for part in await sync_to_async(list)(self.streaming_content):\n+            _iterator = self.streaming_content\n+            _sentinel = object()\n+\n+            def _next_wrapper():\n+                try:\n+                    return _iterator.__next__()\n+                except StopIteration:\n+                    return _sentinel\n+\n+            _next = sync_to_async(_next_wrapper, thread_sensitive=False)\nComment: `thread_sensitive=False`. And if the iterator was using the ORM? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "django/http/response.py",
    "pr_number": 19835,
    "repo": "django",
    "owner": "django",
    "comment_id": 2339953150,
    "comment_created_at": "2025-09-11T10:10:13Z"
  },
  {
    "code": "@@ -0,0 +1,264 @@\n+============================================\n+Django 6.1 release notes - UNDER DEVELOPMENT\n+============================================\n+\n+*Expected August 2026*\n+\n+Welcome to Django 6.1!\n+\n+These release notes cover the :ref:`new features <whats-new-6.1>`, as well as\n+some :ref:`backwards incompatible changes <backwards-incompatible-6.1>` you'll\n+want to be aware of when upgrading from Django 6.0 or earlier. We've\n+:ref:`begun the deprecation process for some features\n+<deprecated-features-6.1>`.\n+\n+See the :doc:`/howto/upgrade-version` guide if you're updating an existing\n+project.\n+\n+Python compatibility\n+====================\n+\n+Django 6.1 supports Python 3.12, 3.13, and 3.14. We **highly recommend** and\n+only officially support the latest release of each series.\n+\n+.. _whats-new-6.1:\n+\n+What's new in Django 6.1\n+========================\n+\n+Minor features\n+--------------\n+\n+:mod:`django.contrib.admin`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.admindocs`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.auth`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.contenttypes`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.gis`\n+~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.messages`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.postgres`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.redirects`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.sessions`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.sitemaps`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.sites`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.staticfiles`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.syndication`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Asynchronous views\n+~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Cache\n+~~~~~\n+\n+* ...\n+\n+CSRF\n+~~~~\n+\n+* ...\n+\n+Decorators\n+~~~~~~~~~~\n+\n+* ...\n+\n+Email\n+~~~~~\n+\n+* ...\n+\n+Error Reporting\n+~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+File Storage\n+~~~~~~~~~~~~\n+\n+* ...\n+\n+File Uploads\n+~~~~~~~~~~~~\n+\n+* ...\n+\n+Forms\n+~~~~~\n+\n+* ...\n+\n+Generic Views\n+~~~~~~~~~~~~~\n+\n+* ...\n+\n+Internationalization\n+~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Logging\n+~~~~~~~\n+\n+* ...\n+\n+Management Commands\n+~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Migrations\n+~~~~~~~~~~\n+\n+* ...\n+\n+Models\n+~~~~~~\n+\n+* ...\n+\n+Pagination\n+~~~~~~~~~~\n+\n+* ...",
    "comment": "in previous stub release notes (e.g. 52ad2ebc365b6806e417d05854d8005a18ee63fa and cdaf333498f4c1792c666f96d73a3a2d97c9ff27), we've not had a section for pagination but i think we can keep in the general template",
    "line_number": 181,
    "enriched": "File: docs/releases/6.1.txt\nCode: @@ -0,0 +1,264 @@\n+============================================\n+Django 6.1 release notes - UNDER DEVELOPMENT\n+============================================\n+\n+*Expected August 2026*\n+\n+Welcome to Django 6.1!\n+\n+These release notes cover the :ref:`new features <whats-new-6.1>`, as well as\n+some :ref:`backwards incompatible changes <backwards-incompatible-6.1>` you'll\n+want to be aware of when upgrading from Django 6.0 or earlier. We've\n+:ref:`begun the deprecation process for some features\n+<deprecated-features-6.1>`.\n+\n+See the :doc:`/howto/upgrade-version` guide if you're updating an existing\n+project.\n+\n+Python compatibility\n+====================\n+\n+Django 6.1 supports Python 3.12, 3.13, and 3.14. We **highly recommend** and\n+only officially support the latest release of each series.\n+\n+.. _whats-new-6.1:\n+\n+What's new in Django 6.1\n+========================\n+\n+Minor features\n+--------------\n+\n+:mod:`django.contrib.admin`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.admindocs`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.auth`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.contenttypes`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.gis`\n+~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.messages`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.postgres`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.redirects`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.sessions`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.sitemaps`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.sites`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.staticfiles`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.syndication`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Asynchronous views\n+~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Cache\n+~~~~~\n+\n+* ...\n+\n+CSRF\n+~~~~\n+\n+* ...\n+\n+Decorators\n+~~~~~~~~~~\n+\n+* ...\n+\n+Email\n+~~~~~\n+\n+* ...\n+\n+Error Reporting\n+~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+File Storage\n+~~~~~~~~~~~~\n+\n+* ...\n+\n+File Uploads\n+~~~~~~~~~~~~\n+\n+* ...\n+\n+Forms\n+~~~~~\n+\n+* ...\n+\n+Generic Views\n+~~~~~~~~~~~~~\n+\n+* ...\n+\n+Internationalization\n+~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Logging\n+~~~~~~~\n+\n+* ...\n+\n+Management Commands\n+~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Migrations\n+~~~~~~~~~~\n+\n+* ...\n+\n+Models\n+~~~~~~\n+\n+* ...\n+\n+Pagination\n+~~~~~~~~~~\n+\n+* ...\nComment: In previous stub release notes (e.g. 52ad2ebc365b6806e417d05854d8005a18ee63fa and cdaf333498f4c1792c666f96d73a3a2d97c9ff27), we've not had a section for pagination but I think we can keep in the general template",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/releases/6.1.txt",
    "pr_number": 19834,
    "repo": "django",
    "owner": "django",
    "comment_id": 2348858940,
    "comment_created_at": "2025-09-15T12:35:35Z"
  },
  {
    "code": "@@ -8,6 +8,8 @@\n \n import django\n \n+django_file_prefixes = (os.path.dirname(django.__file__),)",
    "comment": "what do you think about the proposal to use importlib.resources in ticket-30950?",
    "line_number": 11,
    "enriched": "File: django/utils/deprecation.py\nCode: @@ -8,6 +8,8 @@\n \n import django\n \n+django_file_prefixes = (os.path.dirname(django.__file__),)\nComment: What do you think about the proposal to use `importlib.resources` in ticket-30950?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "django/utils/deprecation.py",
    "pr_number": 19823,
    "repo": "django",
    "owner": "django",
    "comment_id": 2325304157,
    "comment_created_at": "2025-09-05T14:46:11Z"
  },
  {
    "code": "@@ -481,7 +481,7 @@ The class has the following methods:\n         message.attach(inline_image)\n         message.attach_alternative(f'\u2026 <img src=\"cid:${cid}\"> \u2026', \"text/html\")\n \n-    Python's :meth:`email.contentmanager.set_content` documentation describes\n+    Python's :func:`email.contentmanager.set_content` documentation describes",
    "comment": "set_content() and attach_alternative() are methods.",
    "line_number": 484,
    "enriched": "File: docs/topics/email.txt\nCode: @@ -481,7 +481,7 @@ The class has the following methods:\n         message.attach(inline_image)\n         message.attach_alternative(f'\u2026 <img src=\"cid:${cid}\"> \u2026', \"text/html\")\n \n-    Python's :meth:`email.contentmanager.set_content` documentation describes\n+    Python's :func:`email.contentmanager.set_content` documentation describes\nComment: `set_content()` and `attach_alternative()` are methods.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/topics/email.txt",
    "pr_number": 19817,
    "repo": "django",
    "owner": "django",
    "comment_id": 2338832727,
    "comment_created_at": "2025-09-11T06:27:42Z"
  },
  {
    "code": "@@ -323,185 +323,201 @@ email backend API :ref:`provides an alternative\n \n .. class:: EmailMessage\n \n-The :class:`~django.core.mail.EmailMessage` class is initialized with the\n-following parameters. All parameters are optional and can be set at any time\n-prior to calling the ``send()`` method.\n+    The :class:`!EmailMessage` class is initialized with the following\n+    parameters. All parameters are optional and can be set at any time prior\n+    to calling the :meth:`send` method.\n \n-The first four parameters can be passed as positional or keyword arguments,\n-but must be in the given order if positional arguments are used:\n+    The first four parameters can be passed as positional or keyword arguments,\n+    but must be in the given order if positional arguments are used:\n \n-* ``subject``: The subject line of the email.\n+    * ``subject``: The subject line of the email.\n \n-* ``body``: The body text. This should be a plain text message.\n+    * ``body``: The body text. This should be a plain text message.\n \n-* ``from_email``: The sender's address. Both ``fred@example.com`` and\n-  ``\"Fred\" <fred@example.com>`` forms are legal. If omitted, the\n-  :setting:`DEFAULT_FROM_EMAIL` setting is used.\n+    * ``from_email``: The sender's address. Both ``fred@example.com`` and\n+      ``\"Fred\" <fred@example.com>`` forms are legal. If omitted, the\n+      :setting:`DEFAULT_FROM_EMAIL` setting is used.\n \n-* ``to``: A list or tuple of recipient addresses.\n+    * ``to``: A list or tuple of recipient addresses.\n \n-The following parameters must be given as keyword arguments if used:\n+    The following parameters must be given as keyword arguments if used:\n \n-* ``cc``: A list or tuple of recipient addresses used in the \"Cc\" header\n-  when sending the email.\n+    * ``cc``: A list or tuple of recipient addresses used in the \"Cc\" header\n+      when sending the email.\n \n-* ``bcc``: A list or tuple of addresses used in the \"Bcc\" header when\n-  sending the email.\n+    * ``bcc``: A list or tuple of addresses used in the \"Bcc\" header when\n+      sending the email.\n \n-* ``reply_to``: A list or tuple of recipient addresses used in the \"Reply-To\"\n-  header when sending the email.\n+    * ``reply_to``: A list or tuple of recipient addresses used in the\n+      \"Reply-To\" header when sending the email.\n \n-* ``attachments``: A list of attachments to put on the message. Each can\n-  be an instance of :class:`~email.message.MIMEPart` or\n-  :class:`~django.core.mail.EmailAttachment`, or a tuple with attributes\n-  ``(filename, content, mimetype)``.\n+    * ``attachments``: A list of attachments to put on the message. Each can\n+      be an instance of :class:`~email.message.MIMEPart` or\n+      :class:`EmailAttachment`, or a tuple with attributes\n+      ``(filename, content, mimetype)``.\n \n-  .. versionchanged:: 5.2\n+      .. versionchanged:: 5.2\n \n-    Support for :class:`~django.core.mail.EmailAttachment` items of\n-    ``attachments`` was added.\n+          Support for :class:`EmailAttachment` items of ``attachments`` was\n+          added.\n \n-  .. versionchanged:: 6.0\n+      .. versionchanged:: 6.0\n \n-    Support for :class:`~email.message.MIMEPart` objects in the ``attachments``\n-    list was added.\n+          Support for :class:`~email.message.MIMEPart` objects in the\n+          ``attachments`` list was added.\n \n-  .. deprecated:: 6.0\n+      .. deprecated:: 6.0\n \n-    Support for Python's legacy :class:`~email.mime.base.MIMEBase` objects in\n-    ``attachments`` is deprecated. Use :class:`~email.message.MIMEPart`\n-    instead.\n+          Support for Python's legacy :class:`~email.mime.base.MIMEBase`\n+          objects in ``attachments`` is deprecated. Use\n+          :class:`~email.message.MIMEPart` instead.\n \n-* ``headers``: A dictionary of extra headers to put on the message. The\n-  keys are the header name, values are the header values. It's up to the\n-  caller to ensure header names and values are in the correct format for\n-  an email message. The corresponding attribute is ``extra_headers``.\n+    * ``headers``: A dictionary of extra headers to put on the message. The\n+      keys are the header name, values are the header values. It's up to the\n+      caller to ensure header names and values are in the correct format for\n+      an email message. The corresponding attribute is ``extra_headers``.\n \n-* ``connection``: An :ref:`email backend <topic-email-backends>` instance. Use\n-  this parameter if you are sending the ``EmailMessage`` via ``send()`` and you\n-  want to use the same connection for multiple messages. If omitted, a new\n-  connection is created when ``send()`` is called. This parameter is ignored\n-  when using :ref:`send_messages() <topics-sending-multiple-emails>`.\n+    * ``connection``: An :ref:`email backend <topic-email-backends>` instance.\n+      Use this parameter if you are sending the :class:`!EmailMessage` via\n+      :meth:`send` and you want to use the same connection for multiple\n+      messages. If omitted, a new connection is created when :meth:`send` is\n+      called. This parameter is ignored when using\n+      :ref:`send_messages() <topics-sending-multiple-emails>`.\n \n-.. deprecated:: 6.0\n+    .. deprecated:: 6.0\n \n-    Passing all except the first four parameters as positional arguments is\n-    deprecated.\n+        Passing all except the first four parameters as positional arguments is\n+        deprecated.\n \n-For example::\n+    For example::\n \n-    from django.core.mail import EmailMessage\n+        from django.core.mail import EmailMessage\n \n-    email = EmailMessage(\n-        subject=\"Hello\",\n-        body=\"Body goes here\",\n-        from_email=\"from@example.com\",\n-        to=[\"to1@example.com\", \"to2@example.com\"],\n-        bcc=[\"bcc@example.com\"],\n-        reply_to=[\"another@example.com\"],\n-        headers={\"Message-ID\": \"foo\"},\n-    )\n+        email = EmailMessage(\n+            subject=\"Hello\",\n+            body=\"Body goes here\",\n+            from_email=\"from@example.com\",\n+            to=[\"to1@example.com\", \"to2@example.com\"],\n+            bcc=[\"bcc@example.com\"],\n+            reply_to=[\"another@example.com\"],\n+            headers={\"Message-ID\": \"foo\"},\n+        )\n \n-The class has the following methods:\n-\n-* ``send(fail_silently=False)`` sends the message. If a connection was\n-  specified when the email was constructed, that connection will be used.\n-  Otherwise, an instance of the default backend will be instantiated and\n-  used. If the keyword argument ``fail_silently`` is ``True``, exceptions\n-  raised while sending the message will be quashed. An empty list of\n-  recipients will not raise an exception. It will return ``1`` if the message\n-  was sent successfully, otherwise ``0``.\n-\n-* ``message(policy=email.policy.default)`` constructs and returns a Python\n-  :class:`email.message.EmailMessage` object representing the message to be\n-  sent.\n-\n-  The keyword argument ``policy`` allows specifying the set of rules for\n-  updating and serializing the representation of the message. It must be an\n-  :mod:`email.policy.Policy <email.policy>` object. Defaults to\n-  :data:`email.policy.default`. In certain cases you may want to use\n-  :data:`~email.policy.SMTP`, :data:`~email.policy.SMTPUTF8` or a custom\n-  policy. For example, :class:`django.core.mail.backends.smtp.EmailBackend`\n-  uses the :data:`~email.policy.SMTP` policy to ensure ``\\r\\n`` line endings\n-  as required by the SMTP protocol.\n-\n-  If you ever need to extend Django's :class:`~django.core.mail.EmailMessage`\n-  class, you'll probably want to override this method to put the content you\n-  want into the Python EmailMessage object.\n-\n-  .. versionchanged:: 6.0\n-\n-      The ``policy`` keyword argument was added and the return type was updated\n-      to an instance of :class:`~email.message.EmailMessage`.\n-\n-* ``recipients()`` returns a list of all the recipients of the message,\n-  whether they're recorded in the ``to``, ``cc`` or ``bcc`` attributes. This\n-  is another method you might need to override when subclassing, because the\n-  SMTP server needs to be told the full list of recipients when the message\n-  is sent. If you add another way to specify recipients in your class, they\n-  need to be returned from this method as well.\n-\n-* ``attach()`` creates a new attachment and adds it to the message.\n-  There are two ways to call ``attach()``:\n-\n-  * You can pass it three arguments: ``filename``, ``content`` and\n-    ``mimetype``. ``filename`` is the name of the file attachment as it will\n-    appear in the email, ``content`` is the data that will be contained inside\n-    the attachment and ``mimetype`` is the optional MIME type for the\n-    attachment. If you omit ``mimetype``, the MIME content type will be guessed\n-    from the filename of the attachment.\n+    The class has the following methods:\n \n-    For example::\n+    .. method:: send(fail_silently=False)\n \n-       message.attach(\"design.png\", img_data, \"image/png\")\n-\n-    If you specify a ``mimetype`` of :mimetype:`message/rfc822`, ``content``\n-    can be a :class:`django.core.mail.EmailMessage` or Python's\n-    :class:`email.message.EmailMessage` or :class:`email.message.Message`.\n-\n-    For a ``mimetype`` starting with :mimetype:`text/`, content is expected to\n-    be a string. Binary data will be decoded using UTF-8, and if that fails,\n-    the MIME type will be changed to :mimetype:`application/octet-stream` and\n-    the data will be attached unchanged.\n-\n-  * Or for attachments requiring additional headers or parameters, you can pass\n-    ``attach()`` a single Python :class:`~email.message.MIMEPart` object.\n-    This will be attached directly to the resulting message. For example,\n-    to attach an inline image with a :mailheader:`Content-ID`::\n-\n-        cid = email.utils.make_msgid()\n-        inline_image = email.message.MIMEPart()\n-        inline_image.set_content(\n-            image_data_bytes,\n-            maintype=\"image\",\n-            subtype=\"png\",\n-            disposition=\"inline\",\n-            cid=f\"<{cid}>\",\n-        )\n-        message.attach(inline_image)\n-        message.attach_alternative(f'\u2026 <img src=\"cid:${cid}\"> \u2026', \"text/html\")\n+        Sends the message. If a connection was specified when the email was\n+        constructed, that connection will be used. Otherwise, an instance of\n+        the default backend will be instantiated and used. If the keyword\n+        argument ``fail_silently`` is ``True``, exceptions raised while sending\n+        the message will be quashed. An empty list of recipients will not raise\n+        an exception. It will return ``1`` if the message was sent\n+        successfully, otherwise ``0``.\n \n-    Python's :meth:`email.contentmanager.set_content` documentation describes\n-    the supported arguments for ``MIMEPart.set_content()``.\n+    .. method:: message(policy=email.policy.default)\n \n-    .. versionchanged:: 6.0\n+        Constructs and returns a Python :class:`email.message.EmailMessage`\n+        object representing the message to be sent.\n \n-        Support for :class:`~email.message.MIMEPart` attachments was added.\n+        The keyword argument ``policy`` allows specifying the set of rules for\n+        updating and serializing the representation of the message. It must be\n+        an :mod:`email.policy.Policy <email.policy>` object. Defaults to\n+        :data:`email.policy.default`. In certain cases you may want to use\n+        :data:`~email.policy.SMTP`, :data:`~email.policy.SMTPUTF8` or a custom\n+        policy. For example,\n+        :class:`django.core.mail.backends.smtp.EmailBackend` uses the\n+        :data:`~email.policy.SMTP` policy to ensure ``\\r\\n`` line endings as\n+        required by the SMTP protocol.\n \n-    .. deprecated:: 6.0\n+        If you ever need to extend Django's :class:`EmailMessage` class,\n+        you'll probably want to override this method to put the content you\n+        want into the Python EmailMessage object.\n+\n+        .. versionchanged:: 6.0\n+\n+            The ``policy`` keyword argument was added and the return type was\n+            updated to an instance of :class:`~email.message.EmailMessage`.\n+\n+    .. method:: recipients()\n+\n+        Returns a list of all the recipients of the message, whether they're\n+        recorded in the ``to``, ``cc`` or ``bcc`` attributes. This is another\n+        method you might need to override when subclassing, because the SMTP\n+        server needs to be told the full list of recipients when the message\n+        is sent. If you add another way to specify recipients in your class,\n+        they need to be returned from this method as well.\n+\n+    .. method:: attach(filename, content, mimetype)\n+                attach(mimepart)\n \n-        Support for :class:`email.mime.base.MIMEBase` attachments is\n-        deprecated. Use :class:`~email.message.MIMEPart` instead.\n+        Creates a new attachment and adds it to the message. There are two ways\n+        to call :meth:`!attach`:\n \n-* ``attach_file()`` creates a new attachment using a file from your\n-  filesystem. Call it with the path of the file to attach and, optionally,\n-  the MIME type to use for the attachment. If the MIME type is omitted, it\n-  will be guessed from the filename. You can use it like this::\n+        * You can pass it three arguments: ``filename``, ``content`` and\n+          ``mimetype``. ``filename`` is the name of the file attachment as it\n+          will appear in the email, ``content`` is the data that will be\n+          contained inside the attachment and ``mimetype`` is the optional MIME\n+          type for the attachment. If you omit ``mimetype``, the MIME content\n+          type will be guessed from the filename of the attachment.\n \n-    message.attach_file(\"/images/weather_map.png\")\n+          For example::\n \n-  For MIME types starting with :mimetype:`text/`, binary data is handled as in\n-  ``attach()``.\n+              message.attach(\"design.png\", img_data, \"image/png\")\n+\n+          If you specify a ``mimetype`` of :mimetype:`message/rfc822`,\n+          ``content`` can be a :class:`django.core.mail.EmailMessage` or\n+          Python's :class:`email.message.EmailMessage` or\n+          :class:`email.message.Message`.\n+\n+          For a ``mimetype`` starting with :mimetype:`text/`, content is\n+          expected to be a string. Binary data will be decoded using UTF-8,\n+          and if that fails, the MIME type will be changed to\n+          :mimetype:`application/octet-stream` and the data will be attached\n+          unchanged.\n+\n+        * Or for attachments requiring additional headers or parameters, you\n+          can pass :meth:`!attach` a single Python",
    "comment": "pre-existing issue, but this should probably be \"attach\", not \"pass attach\"? not a blocker here, happy to take a follow-up pr.",
    "line_number": 480,
    "enriched": "File: docs/topics/email.txt\nCode: @@ -323,185 +323,201 @@ email backend API :ref:`provides an alternative\n \n .. class:: EmailMessage\n \n-The :class:`~django.core.mail.EmailMessage` class is initialized with the\n-following parameters. All parameters are optional and can be set at any time\n-prior to calling the ``send()`` method.\n+    The :class:`!EmailMessage` class is initialized with the following\n+    parameters. All parameters are optional and can be set at any time prior\n+    to calling the :meth:`send` method.\n \n-The first four parameters can be passed as positional or keyword arguments,\n-but must be in the given order if positional arguments are used:\n+    The first four parameters can be passed as positional or keyword arguments,\n+    but must be in the given order if positional arguments are used:\n \n-* ``subject``: The subject line of the email.\n+    * ``subject``: The subject line of the email.\n \n-* ``body``: The body text. This should be a plain text message.\n+    * ``body``: The body text. This should be a plain text message.\n \n-* ``from_email``: The sender's address. Both ``fred@example.com`` and\n-  ``\"Fred\" <fred@example.com>`` forms are legal. If omitted, the\n-  :setting:`DEFAULT_FROM_EMAIL` setting is used.\n+    * ``from_email``: The sender's address. Both ``fred@example.com`` and\n+      ``\"Fred\" <fred@example.com>`` forms are legal. If omitted, the\n+      :setting:`DEFAULT_FROM_EMAIL` setting is used.\n \n-* ``to``: A list or tuple of recipient addresses.\n+    * ``to``: A list or tuple of recipient addresses.\n \n-The following parameters must be given as keyword arguments if used:\n+    The following parameters must be given as keyword arguments if used:\n \n-* ``cc``: A list or tuple of recipient addresses used in the \"Cc\" header\n-  when sending the email.\n+    * ``cc``: A list or tuple of recipient addresses used in the \"Cc\" header\n+      when sending the email.\n \n-* ``bcc``: A list or tuple of addresses used in the \"Bcc\" header when\n-  sending the email.\n+    * ``bcc``: A list or tuple of addresses used in the \"Bcc\" header when\n+      sending the email.\n \n-* ``reply_to``: A list or tuple of recipient addresses used in the \"Reply-To\"\n-  header when sending the email.\n+    * ``reply_to``: A list or tuple of recipient addresses used in the\n+      \"Reply-To\" header when sending the email.\n \n-* ``attachments``: A list of attachments to put on the message. Each can\n-  be an instance of :class:`~email.message.MIMEPart` or\n-  :class:`~django.core.mail.EmailAttachment`, or a tuple with attributes\n-  ``(filename, content, mimetype)``.\n+    * ``attachments``: A list of attachments to put on the message. Each can\n+      be an instance of :class:`~email.message.MIMEPart` or\n+      :class:`EmailAttachment`, or a tuple with attributes\n+      ``(filename, content, mimetype)``.\n \n-  .. versionchanged:: 5.2\n+      .. versionchanged:: 5.2\n \n-    Support for :class:`~django.core.mail.EmailAttachment` items of\n-    ``attachments`` was added.\n+          Support for :class:`EmailAttachment` items of ``attachments`` was\n+          added.\n \n-  .. versionchanged:: 6.0\n+      .. versionchanged:: 6.0\n \n-    Support for :class:`~email.message.MIMEPart` objects in the ``attachments``\n-    list was added.\n+          Support for :class:`~email.message.MIMEPart` objects in the\n+          ``attachments`` list was added.\n \n-  .. deprecated:: 6.0\n+      .. deprecated:: 6.0\n \n-    Support for Python's legacy :class:`~email.mime.base.MIMEBase` objects in\n-    ``attachments`` is deprecated. Use :class:`~email.message.MIMEPart`\n-    instead.\n+          Support for Python's legacy :class:`~email.mime.base.MIMEBase`\n+          objects in ``attachments`` is deprecated. Use\n+          :class:`~email.message.MIMEPart` instead.\n \n-* ``headers``: A dictionary of extra headers to put on the message. The\n-  keys are the header name, values are the header values. It's up to the\n-  caller to ensure header names and values are in the correct format for\n-  an email message. The corresponding attribute is ``extra_headers``.\n+    * ``headers``: A dictionary of extra headers to put on the message. The\n+      keys are the header name, values are the header values. It's up to the\n+      caller to ensure header names and values are in the correct format for\n+      an email message. The corresponding attribute is ``extra_headers``.\n \n-* ``connection``: An :ref:`email backend <topic-email-backends>` instance. Use\n-  this parameter if you are sending the ``EmailMessage`` via ``send()`` and you\n-  want to use the same connection for multiple messages. If omitted, a new\n-  connection is created when ``send()`` is called. This parameter is ignored\n-  when using :ref:`send_messages() <topics-sending-multiple-emails>`.\n+    * ``connection``: An :ref:`email backend <topic-email-backends>` instance.\n+      Use this parameter if you are sending the :class:`!EmailMessage` via\n+      :meth:`send` and you want to use the same connection for multiple\n+      messages. If omitted, a new connection is created when :meth:`send` is\n+      called. This parameter is ignored when using\n+      :ref:`send_messages() <topics-sending-multiple-emails>`.\n \n-.. deprecated:: 6.0\n+    .. deprecated:: 6.0\n \n-    Passing all except the first four parameters as positional arguments is\n-    deprecated.\n+        Passing all except the first four parameters as positional arguments is\n+        deprecated.\n \n-For example::\n+    For example::\n \n-    from django.core.mail import EmailMessage\n+        from django.core.mail import EmailMessage\n \n-    email = EmailMessage(\n-        subject=\"Hello\",\n-        body=\"Body goes here\",\n-        from_email=\"from@example.com\",\n-        to=[\"to1@example.com\", \"to2@example.com\"],\n-        bcc=[\"bcc@example.com\"],\n-        reply_to=[\"another@example.com\"],\n-        headers={\"Message-ID\": \"foo\"},\n-    )\n+        email = EmailMessage(\n+            subject=\"Hello\",\n+            body=\"Body goes here\",\n+            from_email=\"from@example.com\",\n+            to=[\"to1@example.com\", \"to2@example.com\"],\n+            bcc=[\"bcc@example.com\"],\n+            reply_to=[\"another@example.com\"],\n+            headers={\"Message-ID\": \"foo\"},\n+        )\n \n-The class has the following methods:\n-\n-* ``send(fail_silently=False)`` sends the message. If a connection was\n-  specified when the email was constructed, that connection will be used.\n-  Otherwise, an instance of the default backend will be instantiated and\n-  used. If the keyword argument ``fail_silently`` is ``True``, exceptions\n-  raised while sending the message will be quashed. An empty list of\n-  recipients will not raise an exception. It will return ``1`` if the message\n-  was sent successfully, otherwise ``0``.\n-\n-* ``message(policy=email.policy.default)`` constructs and returns a Python\n-  :class:`email.message.EmailMessage` object representing the message to be\n-  sent.\n-\n-  The keyword argument ``policy`` allows specifying the set of rules for\n-  updating and serializing the representation of the message. It must be an\n-  :mod:`email.policy.Policy <email.policy>` object. Defaults to\n-  :data:`email.policy.default`. In certain cases you may want to use\n-  :data:`~email.policy.SMTP`, :data:`~email.policy.SMTPUTF8` or a custom\n-  policy. For example, :class:`django.core.mail.backends.smtp.EmailBackend`\n-  uses the :data:`~email.policy.SMTP` policy to ensure ``\\r\\n`` line endings\n-  as required by the SMTP protocol.\n-\n-  If you ever need to extend Django's :class:`~django.core.mail.EmailMessage`\n-  class, you'll probably want to override this method to put the content you\n-  want into the Python EmailMessage object.\n-\n-  .. versionchanged:: 6.0\n-\n-      The ``policy`` keyword argument was added and the return type was updated\n-      to an instance of :class:`~email.message.EmailMessage`.\n-\n-* ``recipients()`` returns a list of all the recipients of the message,\n-  whether they're recorded in the ``to``, ``cc`` or ``bcc`` attributes. This\n-  is another method you might need to override when subclassing, because the\n-  SMTP server needs to be told the full list of recipients when the message\n-  is sent. If you add another way to specify recipients in your class, they\n-  need to be returned from this method as well.\n-\n-* ``attach()`` creates a new attachment and adds it to the message.\n-  There are two ways to call ``attach()``:\n-\n-  * You can pass it three arguments: ``filename``, ``content`` and\n-    ``mimetype``. ``filename`` is the name of the file attachment as it will\n-    appear in the email, ``content`` is the data that will be contained inside\n-    the attachment and ``mimetype`` is the optional MIME type for the\n-    attachment. If you omit ``mimetype``, the MIME content type will be guessed\n-    from the filename of the attachment.\n+    The class has the following methods:\n \n-    For example::\n+    .. method:: send(fail_silently=False)\n \n-       message.attach(\"design.png\", img_data, \"image/png\")\n-\n-    If you specify a ``mimetype`` of :mimetype:`message/rfc822`, ``content``\n-    can be a :class:`django.core.mail.EmailMessage` or Python's\n-    :class:`email.message.EmailMessage` or :class:`email.message.Message`.\n-\n-    For a ``mimetype`` starting with :mimetype:`text/`, content is expected to\n-    be a string. Binary data will be decoded using UTF-8, and if that fails,\n-    the MIME type will be changed to :mimetype:`application/octet-stream` and\n-    the data will be attached unchanged.\n-\n-  * Or for attachments requiring additional headers or parameters, you can pass\n-    ``attach()`` a single Python :class:`~email.message.MIMEPart` object.\n-    This will be attached directly to the resulting message. For example,\n-    to attach an inline image with a :mailheader:`Content-ID`::\n-\n-        cid = email.utils.make_msgid()\n-        inline_image = email.message.MIMEPart()\n-        inline_image.set_content(\n-            image_data_bytes,\n-            maintype=\"image\",\n-            subtype=\"png\",\n-            disposition=\"inline\",\n-            cid=f\"<{cid}>\",\n-        )\n-        message.attach(inline_image)\n-        message.attach_alternative(f'\u2026 <img src=\"cid:${cid}\"> \u2026', \"text/html\")\n+        Sends the message. If a connection was specified when the email was\n+        constructed, that connection will be used. Otherwise, an instance of\n+        the default backend will be instantiated and used. If the keyword\n+        argument ``fail_silently`` is ``True``, exceptions raised while sending\n+        the message will be quashed. An empty list of recipients will not raise\n+        an exception. It will return ``1`` if the message was sent\n+        successfully, otherwise ``0``.\n \n-    Python's :meth:`email.contentmanager.set_content` documentation describes\n-    the supported arguments for ``MIMEPart.set_content()``.\n+    .. method:: message(policy=email.policy.default)\n \n-    .. versionchanged:: 6.0\n+        Constructs and returns a Python :class:`email.message.EmailMessage`\n+        object representing the message to be sent.\n \n-        Support for :class:`~email.message.MIMEPart` attachments was added.\n+        The keyword argument ``policy`` allows specifying the set of rules for\n+        updating and serializing the representation of the message. It must be\n+        an :mod:`email.policy.Policy <email.policy>` object. Defaults to\n+        :data:`email.policy.default`. In certain cases you may want to use\n+        :data:`~email.policy.SMTP`, :data:`~email.policy.SMTPUTF8` or a custom\n+        policy. For example,\n+        :class:`django.core.mail.backends.smtp.EmailBackend` uses the\n+        :data:`~email.policy.SMTP` policy to ensure ``\\r\\n`` line endings as\n+        required by the SMTP protocol.\n \n-    .. deprecated:: 6.0\n+        If you ever need to extend Django's :class:`EmailMessage` class,\n+        you'll probably want to override this method to put the content you\n+        want into the Python EmailMessage object.\n+\n+        .. versionchanged:: 6.0\n+\n+            The ``policy`` keyword argument was added and the return type was\n+            updated to an instance of :class:`~email.message.EmailMessage`.\n+\n+    .. method:: recipients()\n+\n+        Returns a list of all the recipients of the message, whether they're\n+        recorded in the ``to``, ``cc`` or ``bcc`` attributes. This is another\n+        method you might need to override when subclassing, because the SMTP\n+        server needs to be told the full list of recipients when the message\n+        is sent. If you add another way to specify recipients in your class,\n+        they need to be returned from this method as well.\n+\n+    .. method:: attach(filename, content, mimetype)\n+                attach(mimepart)\n \n-        Support for :class:`email.mime.base.MIMEBase` attachments is\n-        deprecated. Use :class:`~email.message.MIMEPart` instead.\n+        Creates a new attachment and adds it to the message. There are two ways\n+        to call :meth:`!attach`:\n \n-* ``attach_file()`` creates a new attachment using a file from your\n-  filesystem. Call it with the path of the file to attach and, optionally,\n-  the MIME type to use for the attachment. If the MIME type is omitted, it\n-  will be guessed from the filename. You can use it like this::\n+        * You can pass it three arguments: ``filename``, ``content`` and\n+          ``mimetype``. ``filename`` is the name of the file attachment as it\n+          will appear in the email, ``content`` is the data that will be\n+          contained inside the attachment and ``mimetype`` is the optional MIME\n+          type for the attachment. If you omit ``mimetype``, the MIME content\n+          type will be guessed from the filename of the attachment.\n \n-    message.attach_file(\"/images/weather_map.png\")\n+          For example::\n \n-  For MIME types starting with :mimetype:`text/`, binary data is handled as in\n-  ``attach()``.\n+              message.attach(\"design.png\", img_data, \"image/png\")\n+\n+          If you specify a ``mimetype`` of :mimetype:`message/rfc822`,\n+          ``content`` can be a :class:`django.core.mail.EmailMessage` or\n+          Python's :class:`email.message.EmailMessage` or\n+          :class:`email.message.Message`.\n+\n+          For a ``mimetype`` starting with :mimetype:`text/`, content is\n+          expected to be a string. Binary data will be decoded using UTF-8,\n+          and if that fails, the MIME type will be changed to\n+          :mimetype:`application/octet-stream` and the data will be attached\n+          unchanged.\n+\n+        * Or for attachments requiring additional headers or parameters, you\n+          can pass :meth:`!attach` a single Python\nComment: Pre-existing issue, but this should probably be \"attach\", not \"pass attach\"? Not a blocker here, happy to take a follow-up PR.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/topics/email.txt",
    "pr_number": 19810,
    "repo": "django",
    "owner": "django",
    "comment_id": 2325910202,
    "comment_created_at": "2025-09-05T19:44:40Z"
  },
  {
    "code": "@@ -268,6 +268,13 @@ def trigger_reload(filename):\n \n def restart_with_reloader():\n     new_environ = {**os.environ, DJANGO_AUTORELOAD_ENV: \"true\"}\n+    orig = getattr(sys, \"orig_argv\", ())\n+    if any(\n+        (arg == \"-u\")\n+        or (arg.startswith(\"-\") and not arg.startswith((\"--\", \"-X\")) and \"u\" in arg[1:])",
    "comment": "i can remove this line without any test failures, can you add a test to cover this case?",
    "line_number": 274,
    "enriched": "File: django/utils/autoreload.py\nCode: @@ -268,6 +268,13 @@ def trigger_reload(filename):\n \n def restart_with_reloader():\n     new_environ = {**os.environ, DJANGO_AUTORELOAD_ENV: \"true\"}\n+    orig = getattr(sys, \"orig_argv\", ())\n+    if any(\n+        (arg == \"-u\")\n+        or (arg.startswith(\"-\") and not arg.startswith((\"--\", \"-X\")) and \"u\" in arg[1:])\nComment: ```suggestion\r\n```\r\nI can remove this line without any test failures, can you add a test to cover this case?",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "django/utils/autoreload.py",
    "pr_number": 19802,
    "repo": "django",
    "owner": "django",
    "comment_id": 2372081213,
    "comment_created_at": "2025-09-23T12:02:31Z"
  },
  {
    "code": "@@ -234,18 +234,32 @@ def test_name_with_import_error(self, modules_tmp_path):\n \n \n class TestStreaming:\n-    def test_streaming_with_context(self, app, client):",
    "comment": "why did you replace this test? it's still relevant as well.",
    "line_number": 237,
    "enriched": "File: tests/test_helpers.py\nCode: @@ -234,18 +234,32 @@ def test_name_with_import_error(self, modules_tmp_path):\n \n \n class TestStreaming:\n-    def test_streaming_with_context(self, app, client):\nComment: Why did you replace this test? It's still relevant as well.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/test_helpers.py",
    "pr_number": 5775,
    "repo": "flask",
    "owner": "pallets",
    "comment_id": 2210469867,
    "comment_created_at": "2025-07-16T13:37:47Z"
  },
  {
    "code": "@@ -9,6 +9,30 @@ will depend on each application's specific needs and threat model. Many hosting\n platforms may take care of certain types of problems without the need for the\n Flask application to handle them.\n \n+Host Header Injection and External URLs\n+---------------------------------------\n+\n+When generating external URLs using :func:`url_for` with the ``_external=True`` argument,\n+Flask constructs the URL using the requeust's ``Host`` header by default. If your application",
    "comment": "it always uses the host header, not only by default. that's why the config needs to be set.",
    "line_number": 16,
    "enriched": "File: docs/web-security.rst\nCode: @@ -9,6 +9,30 @@ will depend on each application's specific needs and threat model. Many hosting\n platforms may take care of certain types of problems without the need for the\n Flask application to handle them.\n \n+Host Header Injection and External URLs\n+---------------------------------------\n+\n+When generating external URLs using :func:`url_for` with the ``_external=True`` argument,\n+Flask constructs the URL using the requeust's ``Host`` header by default. If your application\nComment: It always uses the `Host` header, not only by default. That's why the config needs to be set.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "docs/web-security.rst",
    "pr_number": 5722,
    "repo": "flask",
    "owner": "pallets",
    "comment_id": 2283123541,
    "comment_created_at": "2025-08-18T18:18:14Z"
  },
  {
    "code": "@@ -71,14 +71,7 @@ export PYTORCH_BUILD_NUMBER=1\n \n # Set triton version as part of PYTORCH_EXTRA_INSTALL_REQUIREMENTS\n TRITON_VERSION=$(cat $PYTORCH_ROOT/.ci/docker/triton_version.txt)\n-\n-# Here PYTORCH_EXTRA_INSTALL_REQUIREMENTS is already set for the all the wheel builds hence append TRITON_CONSTRAINT\n-TRITON_CONSTRAINT=\"platform_system == 'Linux' and platform_machine == 'x86_64'\"\n-\n-# CUDA 12.9/13.0 builds have triton for Linux and Linux aarch64 binaries.\n-if [[ \"$DESIRED_CUDA\" == \"cu129\" ]] || [[ \"$DESIRED_CUDA\" == \"cu130\" ]]; then\n-  TRITON_CONSTRAINT=\"platform_system == 'Linux'\"\n-fi\n+TRITON_CONSTRAINT=\"platform_system == 'Linux'\"",
    "comment": "wouldn't that add triton constrain on cpu builds as well?",
    "line_number": 74,
    "enriched": "File: .circleci/scripts/binary_populate_env.sh\nCode: @@ -71,14 +71,7 @@ export PYTORCH_BUILD_NUMBER=1\n \n # Set triton version as part of PYTORCH_EXTRA_INSTALL_REQUIREMENTS\n TRITON_VERSION=$(cat $PYTORCH_ROOT/.ci/docker/triton_version.txt)\n-\n-# Here PYTORCH_EXTRA_INSTALL_REQUIREMENTS is already set for the all the wheel builds hence append TRITON_CONSTRAINT\n-TRITON_CONSTRAINT=\"platform_system == 'Linux' and platform_machine == 'x86_64'\"\n-\n-# CUDA 12.9/13.0 builds have triton for Linux and Linux aarch64 binaries.\n-if [[ \"$DESIRED_CUDA\" == \"cu129\" ]] || [[ \"$DESIRED_CUDA\" == \"cu130\" ]]; then\n-  TRITON_CONSTRAINT=\"platform_system == 'Linux'\"\n-fi\n+TRITON_CONSTRAINT=\"platform_system == 'Linux'\"\nComment: Wouldn't that add triton constrain on CPU builds as well?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".circleci/scripts/binary_populate_env.sh",
    "pr_number": 165013,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2415253484,
    "comment_created_at": "2025-10-08T23:52:43Z"
  },
  {
    "code": "@@ -281,11 +283,17 @@ def _get_slice_mesh_layout(self, device_mesh, mesh_dim_names) -> _MeshLayout:\n \n             layout_sliced = []\n             for name in mesh_dim_names:\n-                if name in device_mesh.mesh_dim_names:\n+                if name in not_none(device_mesh.mesh_dim_names):\n                     layout_sliced.append(\n-                        device_mesh._layout[device_mesh.mesh_dim_names.index(name)]\n+                        device_mesh._layout[\n+                            not_none(device_mesh.mesh_dim_names).index(name)\n+                        ]\n                     )\n                 elif name in flatten_name_to_root_layout:\n+                    warnings.warn(\n+                        \"Slicing a flattened dim from root mesh will be deprecated soon. \"\n+                        \"Users need to use and bookkeep the flattened mesh directly. \"",
    "comment": "the to use and bookkeep seems to be incorrect. are you trying to say users need to bookkeedp the flattened mesh directly.\"?",
    "line_number": 295,
    "enriched": "File: torch/distributed/device_mesh.py\nCode: @@ -281,11 +283,17 @@ def _get_slice_mesh_layout(self, device_mesh, mesh_dim_names) -> _MeshLayout:\n \n             layout_sliced = []\n             for name in mesh_dim_names:\n-                if name in device_mesh.mesh_dim_names:\n+                if name in not_none(device_mesh.mesh_dim_names):\n                     layout_sliced.append(\n-                        device_mesh._layout[device_mesh.mesh_dim_names.index(name)]\n+                        device_mesh._layout[\n+                            not_none(device_mesh.mesh_dim_names).index(name)\n+                        ]\n                     )\n                 elif name in flatten_name_to_root_layout:\n+                    warnings.warn(\n+                        \"Slicing a flattened dim from root mesh will be deprecated soon. \"\n+                        \"Users need to use and bookkeep the flattened mesh directly. \"\nComment: The `to use and bookkeep` seems to be incorrect. Are you trying to say `Users need to bookkeedp the flattened mesh directly.\"?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/distributed/device_mesh.py",
    "pr_number": 164993,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2415074977,
    "comment_created_at": "2025-10-08T21:28:08Z"
  },
  {
    "code": "@@ -4091,6 +4091,10 @@ such as `dist.all_reduce(tensor, async_op=True)`.\n             Stringified pickle work traces.\n             Default settings return everything - i.e. contains NCCL comm dumps and collective traces.\n       )\");\n+  module.def(\n+      \"_reset_fr_recording_nccl\",",
    "comment": "do we want to prefix the public api with _?",
    "line_number": 4095,
    "enriched": "File: torch/csrc/distributed/c10d/init.cpp\nCode: @@ -4091,6 +4091,10 @@ such as `dist.all_reduce(tensor, async_op=True)`.\n             Stringified pickle work traces.\n             Default settings return everything - i.e. contains NCCL comm dumps and collective traces.\n       )\");\n+  module.def(\n+      \"_reset_fr_recording_nccl\",\nComment: do we want to prefix the public api with _?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/csrc/distributed/c10d/init.cpp",
    "pr_number": 164988,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2414990929,
    "comment_created_at": "2025-10-08T20:42:33Z"
  },
  {
    "code": "@@ -290,7 +290,7 @@ def aot_stage2_inference(\n                 \"name\": \"torch._functorch.config\",\n                 \"encoding\": \"string\",\n             },\n-            payload_fn=lambda: torch._functorch.config.get_config_copy(),\n+            payload_fn=lambda: torch._functorch.config.get_serializable_config_copy(),",
    "comment": "what happened here?",
    "line_number": 293,
    "enriched": "File: torch/_functorch/_aot_autograd/graph_compile.py\nCode: @@ -290,7 +290,7 @@ def aot_stage2_inference(\n                 \"name\": \"torch._functorch.config\",\n                 \"encoding\": \"string\",\n             },\n-            payload_fn=lambda: torch._functorch.config.get_config_copy(),\n+            payload_fn=lambda: torch._functorch.config.get_serializable_config_copy(),\nComment: what happened here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/_functorch/_aot_autograd/graph_compile.py",
    "pr_number": 164981,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2414828827,
    "comment_created_at": "2025-10-08T19:31:58Z"
  },
  {
    "code": "@@ -121,7 +121,7 @@ def _snake_case(s: str) -> str:\n \n # starts with torch but does not start with torch._dynamo. or torch._inductor.\n _torch_but_not_dynamo = re.compile(\n-    r\"^torch(?:\\.(?!_dynamo\\.|_inductor\\.)[^.]+)*$\"\n+    r\"^torch(?:\\.(?!_dynamo\\.|_inductor\\.|_functorch\\.)[^.]+)*$\"",
    "comment": "what's the motivation?\r\ni thought for torch._dynamo /torch._inductor things, those shouldn't show up in the fx graph.\r\nit's actually desirable that torch._functorch things do show up in the fx graph.\r\n\r\nif you have a good enough motivation then feel free to ship it, i think the difference is just cosmetic (@guilhermeleobas can also confirm). it's just going to take you a while to get through all of the expecttests :p",
    "line_number": 124,
    "enriched": "File: torch/fx/graph.py\nCode: @@ -121,7 +121,7 @@ def _snake_case(s: str) -> str:\n \n # starts with torch but does not start with torch._dynamo. or torch._inductor.\n _torch_but_not_dynamo = re.compile(\n-    r\"^torch(?:\\.(?!_dynamo\\.|_inductor\\.)[^.]+)*$\"\n+    r\"^torch(?:\\.(?!_dynamo\\.|_inductor\\.|_functorch\\.)[^.]+)*$\"\nComment: What's the motivation?\r\nI thought for torch._dynamo /torch._inductor things, those shouldn't show up in the FX graph.\r\nIt's actually desirable that torch._functorch things do show up in the FX graph.\r\n\r\nIf you have a good enough motivation then feel free to ship it, I think the difference is just cosmetic (@guilhermeleobas can also confirm). It's just going to take you a while to get through all of the expecttests :P",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/fx/graph.py",
    "pr_number": 164900,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2414805928,
    "comment_created_at": "2025-10-08T19:20:54Z"
  },
  {
    "code": "@@ -604,6 +604,284 @@ def apply_same_precision(partial_model):\n \n         torch.distributed.destroy_process_group()\n \n+    @requires_accelerator_dist_backend([\"nccl\", \"xccl\"])\n+    @skip_if_lt_x_gpu(8)",
    "comment": "this test works with just 4 gpus",
    "line_number": 608,
    "enriched": "File: test/distributed/_composable/test_composability/test_pp_composability.py\nCode: @@ -604,6 +604,284 @@ def apply_same_precision(partial_model):\n \n         torch.distributed.destroy_process_group()\n \n+    @requires_accelerator_dist_backend([\"nccl\", \"xccl\"])\n+    @skip_if_lt_x_gpu(8)\nComment: this test works with just 4 GPUs",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "test/distributed/_composable/test_composability/test_pp_composability.py",
    "pr_number": 164890,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2414027431,
    "comment_created_at": "2025-10-08T14:17:34Z"
  },
  {
    "code": "@@ -370,11 +370,19 @@ def _load_global_deps() -> None:\n             \"cusparselt\": \"libcusparseLt.so.*[0-9]\",\n             \"cusolver\": \"libcusolver.so.*[0-9]\",\n             \"nccl\": \"libnccl.so.*[0-9]\",\n-            \"nvtx\": \"libnvToolsExt.so.*[0-9]\",",
    "comment": "hmm, do we have any binary that links against libnvtoolsext? i.e. may be easier solution is just to delete this line?",
    "line_number": 373,
    "enriched": "File: torch/__init__.py\nCode: @@ -370,11 +370,19 @@ def _load_global_deps() -> None:\n             \"cusparselt\": \"libcusparseLt.so.*[0-9]\",\n             \"cusolver\": \"libcusolver.so.*[0-9]\",\n             \"nccl\": \"libnccl.so.*[0-9]\",\n-            \"nvtx\": \"libnvToolsExt.so.*[0-9]\",\nComment: Hmm, do we have any binary that links against libnvToolsExt? I.e. may be easier solution is just to delete this line?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/__init__.py",
    "pr_number": 164870,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2411942967,
    "comment_created_at": "2025-10-07T21:12:21Z"
  },
  {
    "code": "@@ -1 +1 @@\n-e0dda9059d082537cee36be6c5e4fe3b18c880c0\n+deb42f2a8e48f5032b4a98ee781a15fa87a157cf",
    "comment": "intentional?",
    "line_number": 1,
    "enriched": "File: .ci/docker/ci_commit_pins/executorch.txt\nCode: @@ -1 +1 @@\n-e0dda9059d082537cee36be6c5e4fe3b18c880c0\n+deb42f2a8e48f5032b4a98ee781a15fa87a157cf\nComment: intentional?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".ci/docker/ci_commit_pins/executorch.txt",
    "pr_number": 164846,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2414074901,
    "comment_created_at": "2025-10-08T14:32:54Z"
  },
  {
    "code": "@@ -834,7 +834,7 @@ test_dynamo_benchmark() {\n     # TODO (huydhn): Just smoke test some sample models\n     if [[ \"${TEST_CONFIG}\" == *b200* ]]; then\n       if [[ \"${suite}\" == \"huggingface\" ]]; then\n-        export TORCHBENCH_ONLY_MODELS=\"DistillGPT2\"\n+        export TORCHBENCH_ONLY_MODELS=\"AlbertForMaskedLM\"",
    "comment": "distillgpt2 is deleted so run on albertformaskedlm instead.",
    "line_number": 837,
    "enriched": "File: .ci/pytorch/test.sh\nCode: @@ -834,7 +834,7 @@ test_dynamo_benchmark() {\n     # TODO (huydhn): Just smoke test some sample models\n     if [[ \"${TEST_CONFIG}\" == *b200* ]]; then\n       if [[ \"${suite}\" == \"huggingface\" ]]; then\n-        export TORCHBENCH_ONLY_MODELS=\"DistillGPT2\"\n+        export TORCHBENCH_ONLY_MODELS=\"AlbertForMaskedLM\"\nComment: `DistillGPT2` is deleted so run on `AlbertForMaskedLM` instead.",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": ".ci/pytorch/test.sh",
    "pr_number": 164815,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2409262833,
    "comment_created_at": "2025-10-07T03:45:30Z"
  },
  {
    "code": "@@ -2878,49 +2878,18 @@ def adapt_config_for_tiling(\n     )\n \n \n-class ReductionConfigKey:\n-    \"\"\"\n-    The part of reduction configs that affect determinism.\n-    \"\"\"\n-\n-    def __init__(self, config: Config):\n-        # persistent reduction does not have a RBLOCK, use -1 as a flag\n-        self.r0_block = config.kwargs.get(\"R0_BLOCK\", -1)\n-        self.r1_block = config.kwargs.get(\"R1_BLOCK\", -1)\n-        self.num_warps = config.num_warps\n-        self.num_ctas = config.num_ctas\n-\n-    def __hash__(self) -> int:\n-        return hash((self.r0_block, self.r1_block, self.num_warps, self.num_ctas))\n-\n-    def __eq__(self, other: object) -> bool:\n-        return (\n-            isinstance(other, ReductionConfigKey)\n-            and self.r0_block == other.r0_block\n-            and self.r1_block == other.r1_block\n-            and self.num_warps == other.num_warps\n-            and self.num_ctas == other.num_ctas\n-        )\n-\n-\n def filter_reduction_configs_for_determinism(\n     inductor_meta: dict[str, Any], configs: list[Config]\n ) -> list[Config]:\n     \"\"\"\n     Filter configs for reduction so the numerics can be deterministic.\n \n-    This function group configs by fields that affect determinism\n-    - rblock size\n-    - num warps\n-    - num ctas\n-    and return the most promising group based on heuristics.\n-\n     Heuristics:\n     - skip reduction configs with too small RBLOCK\n     - skip reduction configs with XBLOCK==1 if we are confident it will not perform well\n-    - pick the group with largest size: autotuning more configs may have more chance to give better perf\n-    - if there is a tie, pick the group with second largest RBLOCK\n-    - if there is still a tie, pick the group with second largest num_warps\n+    - if there is a tie, pick the config with second largest RBLOCK\n+    - if there is still a tie, pick the config with second largest num_warps",
    "comment": "why do we pick second largest in all of these cases? this for my understanding, everything else lgtm",
    "line_number": 2891,
    "enriched": "File: torch/_inductor/runtime/triton_heuristics.py\nCode: @@ -2878,49 +2878,18 @@ def adapt_config_for_tiling(\n     )\n \n \n-class ReductionConfigKey:\n-    \"\"\"\n-    The part of reduction configs that affect determinism.\n-    \"\"\"\n-\n-    def __init__(self, config: Config):\n-        # persistent reduction does not have a RBLOCK, use -1 as a flag\n-        self.r0_block = config.kwargs.get(\"R0_BLOCK\", -1)\n-        self.r1_block = config.kwargs.get(\"R1_BLOCK\", -1)\n-        self.num_warps = config.num_warps\n-        self.num_ctas = config.num_ctas\n-\n-    def __hash__(self) -> int:\n-        return hash((self.r0_block, self.r1_block, self.num_warps, self.num_ctas))\n-\n-    def __eq__(self, other: object) -> bool:\n-        return (\n-            isinstance(other, ReductionConfigKey)\n-            and self.r0_block == other.r0_block\n-            and self.r1_block == other.r1_block\n-            and self.num_warps == other.num_warps\n-            and self.num_ctas == other.num_ctas\n-        )\n-\n-\n def filter_reduction_configs_for_determinism(\n     inductor_meta: dict[str, Any], configs: list[Config]\n ) -> list[Config]:\n     \"\"\"\n     Filter configs for reduction so the numerics can be deterministic.\n \n-    This function group configs by fields that affect determinism\n-    - rblock size\n-    - num warps\n-    - num ctas\n-    and return the most promising group based on heuristics.\n-\n     Heuristics:\n     - skip reduction configs with too small RBLOCK\n     - skip reduction configs with XBLOCK==1 if we are confident it will not perform well\n-    - pick the group with largest size: autotuning more configs may have more chance to give better perf\n-    - if there is a tie, pick the group with second largest RBLOCK\n-    - if there is still a tie, pick the group with second largest num_warps\n+    - if there is a tie, pick the config with second largest RBLOCK\n+    - if there is still a tie, pick the config with second largest num_warps\nComment: Why do we pick second largest in all of these cases? this for my understanding, everything else lgtm",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/_inductor/runtime/triton_heuristics.py",
    "pr_number": 164801,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2409631648,
    "comment_created_at": "2025-10-07T07:15:47Z"
  },
  {
    "code": "@@ -459,6 +449,26 @@ def _is_backward_node_with_seq_nr(node):\n             node.meta[\"custom\"] = fwd_node.meta.get(\"custom\")\n \n \n+def copy_fwd_metadata_to_bw_nodes(fx_g):\n+    \"\"\"\n+    Input: `fx_g` which contains the joint fwd+bwd FX graph created by\n+    aot_autograd.\n+\n+    This function walks the graph and copies over metadata from forward nodes\n+    to backward nodes, using the `seq_nr` field as a one-to-many mapping\n+    from forward node to backward node. This metadata is useful for performance\n+    profiling and debugging.\n+    \"\"\"\n+\n+    # Copy the metadata recursively - useful for HOPs\n+    for node in fx_g.graph.nodes:\n+        if node.op == \"get_attr\":\n+            submod = getattr(fx_g, node.target)\n+            if isinstance(submod, torch.fx.GraphModule):\n+                copy_fwd_metadata_to_bw_nodes(submod)",
    "comment": "do we know for sure that the backward nodes and the forward nodes are always in the same subgraph? if so this would work. in the current implementation, the bw nodes would not be able to find the fw nodes in other subgraphs.",
    "line_number": 468,
    "enriched": "File: torch/_functorch/_aot_autograd/utils.py\nCode: @@ -459,6 +449,26 @@ def _is_backward_node_with_seq_nr(node):\n             node.meta[\"custom\"] = fwd_node.meta.get(\"custom\")\n \n \n+def copy_fwd_metadata_to_bw_nodes(fx_g):\n+    \"\"\"\n+    Input: `fx_g` which contains the joint fwd+bwd FX graph created by\n+    aot_autograd.\n+\n+    This function walks the graph and copies over metadata from forward nodes\n+    to backward nodes, using the `seq_nr` field as a one-to-many mapping\n+    from forward node to backward node. This metadata is useful for performance\n+    profiling and debugging.\n+    \"\"\"\n+\n+    # Copy the metadata recursively - useful for HOPs\n+    for node in fx_g.graph.nodes:\n+        if node.op == \"get_attr\":\n+            submod = getattr(fx_g, node.target)\n+            if isinstance(submod, torch.fx.GraphModule):\n+                copy_fwd_metadata_to_bw_nodes(submod)\nComment: do we know for sure that the backward nodes and the forward nodes are always in the same subgraph? If so this would work. In the current implementation, the bw nodes would not be able to find the fw nodes in other subgraphs.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/_functorch/_aot_autograd/utils.py",
    "pr_number": 164795,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2408969593,
    "comment_created_at": "2025-10-06T23:34:33Z"
  },
  {
    "code": "@@ -1,271 +1,221 @@\n+import itertools\n import logging\n-import operator\n-from typing import Any, Callable\n+from collections import defaultdict\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Union\n \n+import torch\n import torch.fx as fx\n-from torch._functorch.partitioners import _size_of, get_default_op_list\n+from torch.fx.experimental.symbolic_shapes import hint_int\n from torch.utils._ordered_set import OrderedSet\n+from torch.utils._pytree import tree_map_only\n \n \n log = logging.getLogger(__name__)\n \n \n+@dataclass(frozen=True)\n+class StorageKey:\n+    storage: torch.UntypedStorage\n+    device: torch.device\n+\n+    def __hash__(self) -> int:\n+        return self.storage._cdata\n+\n+    def __eq__(self, other: object) -> bool:\n+        if not isinstance(other, StorageKey):\n+            return False\n+        return (\n+            self.storage._cdata == other.storage._cdata and self.device == other.device\n+        )\n+\n+\n+class GraphAliasTracker:\n+    \"\"\"\n+    Tracks storage allocation and usage relationships in an FX graph.\n+\n+    Differentiates between:\n+    - Fresh allocations: nodes that allocate new storage (not views/aliases)\n+    - Uses: nodes that use a storage as input\n+    \"\"\"\n+\n+    def __init__(self, nodes: list[fx.Node]):\n+        # Map from node to the fresh storages it allocates (not views/aliases)\n+        self.node_to_fresh_allocations: dict[fx.Node, OrderedSet[StorageKey]] = {}\n+\n+        # Map from storage to the node that originally allocated it\n+        self.storage_to_allocator: dict[StorageKey, fx.Node] = {}\n+\n+        # Map from node to all storages it uses as inputs\n+        self.node_to_storage_uses: dict[fx.Node, OrderedSet[StorageKey]] = {}\n+\n+        # Map from storage to all nodes that use it\n+        self.storage_to_uses: dict[StorageKey, OrderedSet[fx.Node]] = defaultdict(\n+            OrderedSet\n+        )\n+\n+        # Map from storage to the last node that uses it\n+        self.storage_to_last_user: dict[StorageKey, fx.Node] = {}\n+\n+        # Map from node to storages that have their last use at that node\n+        self.node_to_storages_last_used: dict[fx.Node, OrderedSet[StorageKey]] = (\n+            defaultdict(OrderedSet)\n+        )\n+\n+        # Track all output storages for each node (for building usage graph)\n+        self.node_to_output_storages: dict[fx.Node, OrderedSet[StorageKey]] = {}\n+\n+        # First pass: build storage allocations and track uses\n+        for node in nodes:\n+            # Get output storages\n+            output_storages = self._get_output_storages(node)\n+            self.node_to_output_storages[node] = output_storages\n+\n+            # Track fresh allocations\n+            fresh_allocations: OrderedSet[StorageKey] = OrderedSet()\n+            for storage_key in output_storages:\n+                if storage_key not in self.storage_to_allocator:\n+                    self.storage_to_allocator[storage_key] = node\n+                    fresh_allocations.add(storage_key)\n+            self.node_to_fresh_allocations[node] = fresh_allocations\n+\n+            # Track input storage uses (safe because inputs were already processed)\n+            input_storages = self._get_input_storages(node)\n+            self.node_to_storage_uses[node] = input_storages\n+            for storage_key in input_storages:\n+                self.storage_to_uses[storage_key].add(node)\n+\n+        # Second pass: find last users (iterate in reverse)\n+        for node in reversed(nodes):\n+            input_storages = self.node_to_storage_uses[node]\n+            for storage_key in input_storages:\n+                if storage_key not in self.storage_to_last_user:\n+                    self.storage_to_last_user[storage_key] = node\n+                    self.node_to_storages_last_used[node].add(storage_key)\n+\n+    @staticmethod\n+    def _get_output_storages(node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"\n+        Get all storages from a node's outputs.\n+\n+        Uses pytree to handle arbitrary nested structures.\n+        \"\"\"\n+        val = node.meta.get(\"val\")\n+        if val is None:\n+            return OrderedSet()\n+\n+        storages: OrderedSet[StorageKey] = OrderedSet()\n+\n+        def collect_storage(tensor: torch._subclasses.FakeTensor) -> None:\n+            storages.add(StorageKey(tensor.untyped_storage(), tensor.device))\n+\n+        # Use tree_map_only to handle FakeTensors in nested structures\n+        tree_map_only(torch._subclasses.FakeTensor, collect_storage, val)\n+\n+        return storages\n+\n+    def _get_input_storages(self, node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"\n+        Get all storages from a node's inputs.\n+        \"\"\"\n+        input_storages: OrderedSet[StorageKey] = OrderedSet()\n+\n+        for input_node in node.all_input_nodes:\n+            input_storages.update(self.node_to_output_storages[input_node])\n+\n+        return input_storages\n+\n+    def get_fresh_allocations(self, node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"Get all fresh storage allocations by this node (not views/aliases).\"\"\"\n+        return self.node_to_fresh_allocations[node]\n+\n+    def get_storage_uses(self, node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"Get all storages that this node uses as inputs.\"\"\"\n+        return self.node_to_storage_uses[node]\n+\n+    def get_storages_last_used_at(\n+        self,\n+        node: fx.Node,\n+    ) -> OrderedSet[StorageKey]:\n+        \"\"\"\n+        Get storages whose last use is at this node.\n+\n+        Returns storages that are currently active and have their\n+        last use at this node.\n+        \"\"\"\n+        return self.node_to_storages_last_used[node]\n+\n+\n+def _size_of_default(num_bytes: Union[int, torch.SymInt]) -> int:\n+    return hint_int(num_bytes, fallback=torch._inductor.config.unbacked_symint_fallback)\n+\n+\n def build_memory_profile(\n     graph: fx.Graph,\n-    size_of: Callable[[fx.Node], int],\n     is_releasable: Callable[[fx.Node], bool],\n+    size_of: Optional[Callable[[Union[int, torch.SymInt]], int]] = None,\n ) -> list[int]:\n     \"\"\"\n     Function to estimate the memory profile of an input FX graph.\n \n     Args:\n     - graph (fx.Graph): The input FX graph for which the memory profile\n       is to be estimated.\n-    - size_of (Callable[[fx.Node], int]): A function that returns\n-      the size of a given node.\n     - is_releasable (Callable[[fx.Node], bool]): A function that\n       determines if a node's memory can be released (e.g. primal nodes\n       cannot be released).\n+    - size_of (Callable[[Union[int, torch.SymInt]], int]): A function that converts\n+      byte counts (possibly symbolic) to concrete integers.\n \n     Returns:\n     - List[int]: A list representing the memory profile over the execution\n       of the graph, where each entry corresponds to the memory usage at\n       a particular point in the execution.\n     \"\"\"\n \n+    size_of = size_of or _size_of_default\n     nodes = list(graph.nodes)\n-    op_types = get_default_op_list()\n-\n-    class AliasInfo:\n-        \"\"\"\n-        Class for storing and accessing alias information of a FX graph.\n-\n-        Attributes:\n-        - view_to_source: Maps view nodes to their source nodes\n-        - getitem_to_source: Maps getitem nodes to (source_node, key) tuples\n-        - source_to_getitems: Maps source nodes to dictionaries of\n-          {key: getitem_node, \"unclaimed\": None}\n-        - source_to_unclaimed_size: Maps source nodes to their storage size\n-          unclaimed by any getitem_nodes\n-        \"\"\"\n+    alias_info = GraphAliasTracker(nodes)\n \n-        def __init__(self, nodes: list[fx.Node]):\n-            \"\"\"\n-            Initialize the AliasInfo class with a list of FX graph nodes.\n-\n-            Args:\n-            - nodes (list[fx.Node]): A list of nodes from an FX graph,\n-              ordered in execution order.\n-\n-            The constructor analyzes the relationships between nodes in the FX graph\n-            to populate alias information. It identifies two types of alias nodes:\n-            getitem and view. For each view, it maps it to its source. For each\n-            getitem, it maps it to its source and key. It also populates mappings\n-            for source nodes to their getitems and calculates unclaimed storage sizes.\n-\n-            \"\"\"\n-            # For each view, we map it to its source.\n-            # Note that we treat getitems of a view (e.g. aten.split) as views.\n-            self.view_to_source: dict[fx.Node, fx.Node] = {}\n-\n-            # For each remaining getitem, we map it to its source and key.\n-            self.getitem_to_source: dict[fx.Node, tuple[fx.Node, Any]] = {}\n-\n-            # For each none-view source_node of getitems, we map it to a dictionary\n-            # in the form of {key: getitem_node, ..., \"unclaimed\": None}, where\n-            # \"unclaimed\" is a dummy key that represents all elements in the\n-            # source_node that is not claimed by any getitems.\n-            self.source_to_getitems: dict[fx.Node, dict[Any, fx.Node | None]] = {}\n-\n-            # For each none-view source_node of getitems with at least one unclaimed\n-            # elements, we map it to its unclaimed storage size.\n-            self.source_to_unclaimed_size: dict[fx.Node, int] = {}\n-\n-            for node in nodes:\n-                is_view = op_types.is_view(node)\n-                is_getitem = node.target is operator.getitem\n-                if not (is_view or is_getitem):\n-                    continue\n-                assert not (is_view and is_getitem)\n-                assert node.args and isinstance(node.args[0], fx.Node)\n-                source = node.args[0]\n-                if is_view:\n-                    assert not isinstance(source.meta[\"val\"], list | tuple | dict)\n-                    if source in self.view_to_source:\n-                        source = self.view_to_source[source]\n-                    self.view_to_source[node] = source\n-                if is_getitem:\n-                    assert isinstance(source.meta[\"val\"], list | tuple | dict)\n-                    # Source of getitem can be a view (e.g. aten.split).\n-                    if source in self.view_to_source:\n-                        if source in self.view_to_source:\n-                            source = self.view_to_source[source]\n-                        # In this case, the getitem node should be treated\n-                        # the same way as a regular view.\n-                        self.view_to_source[node] = source\n-                        continue\n-                    # Source of getitem cannot be a getitem.\n-                    assert source not in self.getitem_to_source\n-\n-                    # There must be a second argument that specifies the key.\n-                    assert len(node.args) >= 2\n-                    key = node.args[1]\n-                    self.getitem_to_source[node] = (source, key)\n-\n-                    # Populate source_to_getitems.\n-                    if source not in self.source_to_getitems:\n-                        self.source_to_getitems[source] = {\"unclaimed\": None}\n-                    assert key not in self.source_to_getitems[source]\n-                    self.source_to_getitems[source][key] = node  # type: ignore[index]\n-\n-            for source, getitem_map in self.source_to_getitems.items():\n-                unclaimed_source_size = size_of(source)\n-                for key, getitem_node in getitem_map.items():\n-                    if key != \"unclaimed\" and getitem_node is not None:\n-                        unclaimed_source_size -= size_of(getitem_node)\n-                assert unclaimed_source_size >= 0\n-                if unclaimed_source_size > 0:\n-                    self.source_to_unclaimed_size[source] = unclaimed_source_size\n-\n-        def is_view(self, node: fx.Node) -> bool:\n-            return node in self.view_to_source\n-\n-        def is_getitem(self, node: fx.Node) -> bool:\n-            return node in self.getitem_to_source\n-\n-        def get_source(self, node: fx.Node) -> fx.Node | tuple[fx.Node, Any]:\n-            if self.is_view(node):\n-                return self.view_to_source[node]\n-            if self.is_getitem(node):\n-                return self.getitem_to_source[node]\n-            return node\n-\n-        def is_source_of_getitems(self, node: fx.Node) -> bool:\n-            return node in self.source_to_getitems\n-\n-        def get_storage_keys(self, source_node: fx.Node) -> list[Any]:\n-            assert source_node in self.source_to_getitems\n-            return list(self.source_to_getitems[source_node].keys())\n-\n-        def get_unclaimed_storage_size(self, source_node: fx.Node) -> int:\n-            return self.source_to_unclaimed_size.get(source_node, 0)\n-\n-        def get_getitem_by_key(self, source: fx.Node, key: Any) -> fx.Node | None:\n-            assert source in self.source_to_getitems\n-            assert key in self.source_to_getitems[source]\n-            return self.source_to_getitems[source][key]\n-\n-    def _get_last_usage(\n-        nodes: list[fx.Node], alias_info: AliasInfo\n-    ) -> dict[fx.Node, list[tuple[fx.Node, Any]]]:\n-        \"\"\"\n-        Determine the last usage point of each storage. This information is used to\n-        identify when storages can be safely released.\n+    # Build memory profile\n+    current_memory = 0\n \n-        Args:\n-        - nodes (list[fx.Node]): A list of nodes from the FX graph, ordered\n-          in execution order.\n-        - alias_info (AliasInfo): An instance of AliasInfo containing aliasing\n-          relationships between nodes in the graph.\n+    for node in itertools.chain(\n+        graph.find_nodes(op=\"placeholder\"), graph.find_nodes(op=\"get_attr\")\n+    ):\n+        for storage_key in alias_info.get_fresh_allocations(node):\n+            if storage_key.device.type != \"cpu\":",
    "comment": "can we reuse device_filter instead of hardcode \"cpu\"?",
    "line_number": 187,
    "enriched": "File: torch/_inductor/fx_passes/memory_estimator.py\nCode: @@ -1,271 +1,221 @@\n+import itertools\n import logging\n-import operator\n-from typing import Any, Callable\n+from collections import defaultdict\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Union\n \n+import torch\n import torch.fx as fx\n-from torch._functorch.partitioners import _size_of, get_default_op_list\n+from torch.fx.experimental.symbolic_shapes import hint_int\n from torch.utils._ordered_set import OrderedSet\n+from torch.utils._pytree import tree_map_only\n \n \n log = logging.getLogger(__name__)\n \n \n+@dataclass(frozen=True)\n+class StorageKey:\n+    storage: torch.UntypedStorage\n+    device: torch.device\n+\n+    def __hash__(self) -> int:\n+        return self.storage._cdata\n+\n+    def __eq__(self, other: object) -> bool:\n+        if not isinstance(other, StorageKey):\n+            return False\n+        return (\n+            self.storage._cdata == other.storage._cdata and self.device == other.device\n+        )\n+\n+\n+class GraphAliasTracker:\n+    \"\"\"\n+    Tracks storage allocation and usage relationships in an FX graph.\n+\n+    Differentiates between:\n+    - Fresh allocations: nodes that allocate new storage (not views/aliases)\n+    - Uses: nodes that use a storage as input\n+    \"\"\"\n+\n+    def __init__(self, nodes: list[fx.Node]):\n+        # Map from node to the fresh storages it allocates (not views/aliases)\n+        self.node_to_fresh_allocations: dict[fx.Node, OrderedSet[StorageKey]] = {}\n+\n+        # Map from storage to the node that originally allocated it\n+        self.storage_to_allocator: dict[StorageKey, fx.Node] = {}\n+\n+        # Map from node to all storages it uses as inputs\n+        self.node_to_storage_uses: dict[fx.Node, OrderedSet[StorageKey]] = {}\n+\n+        # Map from storage to all nodes that use it\n+        self.storage_to_uses: dict[StorageKey, OrderedSet[fx.Node]] = defaultdict(\n+            OrderedSet\n+        )\n+\n+        # Map from storage to the last node that uses it\n+        self.storage_to_last_user: dict[StorageKey, fx.Node] = {}\n+\n+        # Map from node to storages that have their last use at that node\n+        self.node_to_storages_last_used: dict[fx.Node, OrderedSet[StorageKey]] = (\n+            defaultdict(OrderedSet)\n+        )\n+\n+        # Track all output storages for each node (for building usage graph)\n+        self.node_to_output_storages: dict[fx.Node, OrderedSet[StorageKey]] = {}\n+\n+        # First pass: build storage allocations and track uses\n+        for node in nodes:\n+            # Get output storages\n+            output_storages = self._get_output_storages(node)\n+            self.node_to_output_storages[node] = output_storages\n+\n+            # Track fresh allocations\n+            fresh_allocations: OrderedSet[StorageKey] = OrderedSet()\n+            for storage_key in output_storages:\n+                if storage_key not in self.storage_to_allocator:\n+                    self.storage_to_allocator[storage_key] = node\n+                    fresh_allocations.add(storage_key)\n+            self.node_to_fresh_allocations[node] = fresh_allocations\n+\n+            # Track input storage uses (safe because inputs were already processed)\n+            input_storages = self._get_input_storages(node)\n+            self.node_to_storage_uses[node] = input_storages\n+            for storage_key in input_storages:\n+                self.storage_to_uses[storage_key].add(node)\n+\n+        # Second pass: find last users (iterate in reverse)\n+        for node in reversed(nodes):\n+            input_storages = self.node_to_storage_uses[node]\n+            for storage_key in input_storages:\n+                if storage_key not in self.storage_to_last_user:\n+                    self.storage_to_last_user[storage_key] = node\n+                    self.node_to_storages_last_used[node].add(storage_key)\n+\n+    @staticmethod\n+    def _get_output_storages(node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"\n+        Get all storages from a node's outputs.\n+\n+        Uses pytree to handle arbitrary nested structures.\n+        \"\"\"\n+        val = node.meta.get(\"val\")\n+        if val is None:\n+            return OrderedSet()\n+\n+        storages: OrderedSet[StorageKey] = OrderedSet()\n+\n+        def collect_storage(tensor: torch._subclasses.FakeTensor) -> None:\n+            storages.add(StorageKey(tensor.untyped_storage(), tensor.device))\n+\n+        # Use tree_map_only to handle FakeTensors in nested structures\n+        tree_map_only(torch._subclasses.FakeTensor, collect_storage, val)\n+\n+        return storages\n+\n+    def _get_input_storages(self, node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"\n+        Get all storages from a node's inputs.\n+        \"\"\"\n+        input_storages: OrderedSet[StorageKey] = OrderedSet()\n+\n+        for input_node in node.all_input_nodes:\n+            input_storages.update(self.node_to_output_storages[input_node])\n+\n+        return input_storages\n+\n+    def get_fresh_allocations(self, node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"Get all fresh storage allocations by this node (not views/aliases).\"\"\"\n+        return self.node_to_fresh_allocations[node]\n+\n+    def get_storage_uses(self, node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"Get all storages that this node uses as inputs.\"\"\"\n+        return self.node_to_storage_uses[node]\n+\n+    def get_storages_last_used_at(\n+        self,\n+        node: fx.Node,\n+    ) -> OrderedSet[StorageKey]:\n+        \"\"\"\n+        Get storages whose last use is at this node.\n+\n+        Returns storages that are currently active and have their\n+        last use at this node.\n+        \"\"\"\n+        return self.node_to_storages_last_used[node]\n+\n+\n+def _size_of_default(num_bytes: Union[int, torch.SymInt]) -> int:\n+    return hint_int(num_bytes, fallback=torch._inductor.config.unbacked_symint_fallback)\n+\n+\n def build_memory_profile(\n     graph: fx.Graph,\n-    size_of: Callable[[fx.Node], int],\n     is_releasable: Callable[[fx.Node], bool],\n+    size_of: Optional[Callable[[Union[int, torch.SymInt]], int]] = None,\n ) -> list[int]:\n     \"\"\"\n     Function to estimate the memory profile of an input FX graph.\n \n     Args:\n     - graph (fx.Graph): The input FX graph for which the memory profile\n       is to be estimated.\n-    - size_of (Callable[[fx.Node], int]): A function that returns\n-      the size of a given node.\n     - is_releasable (Callable[[fx.Node], bool]): A function that\n       determines if a node's memory can be released (e.g. primal nodes\n       cannot be released).\n+    - size_of (Callable[[Union[int, torch.SymInt]], int]): A function that converts\n+      byte counts (possibly symbolic) to concrete integers.\n \n     Returns:\n     - List[int]: A list representing the memory profile over the execution\n       of the graph, where each entry corresponds to the memory usage at\n       a particular point in the execution.\n     \"\"\"\n \n+    size_of = size_of or _size_of_default\n     nodes = list(graph.nodes)\n-    op_types = get_default_op_list()\n-\n-    class AliasInfo:\n-        \"\"\"\n-        Class for storing and accessing alias information of a FX graph.\n-\n-        Attributes:\n-        - view_to_source: Maps view nodes to their source nodes\n-        - getitem_to_source: Maps getitem nodes to (source_node, key) tuples\n-        - source_to_getitems: Maps source nodes to dictionaries of\n-          {key: getitem_node, \"unclaimed\": None}\n-        - source_to_unclaimed_size: Maps source nodes to their storage size\n-          unclaimed by any getitem_nodes\n-        \"\"\"\n+    alias_info = GraphAliasTracker(nodes)\n \n-        def __init__(self, nodes: list[fx.Node]):\n-            \"\"\"\n-            Initialize the AliasInfo class with a list of FX graph nodes.\n-\n-            Args:\n-            - nodes (list[fx.Node]): A list of nodes from an FX graph,\n-              ordered in execution order.\n-\n-            The constructor analyzes the relationships between nodes in the FX graph\n-            to populate alias information. It identifies two types of alias nodes:\n-            getitem and view. For each view, it maps it to its source. For each\n-            getitem, it maps it to its source and key. It also populates mappings\n-            for source nodes to their getitems and calculates unclaimed storage sizes.\n-\n-            \"\"\"\n-            # For each view, we map it to its source.\n-            # Note that we treat getitems of a view (e.g. aten.split) as views.\n-            self.view_to_source: dict[fx.Node, fx.Node] = {}\n-\n-            # For each remaining getitem, we map it to its source and key.\n-            self.getitem_to_source: dict[fx.Node, tuple[fx.Node, Any]] = {}\n-\n-            # For each none-view source_node of getitems, we map it to a dictionary\n-            # in the form of {key: getitem_node, ..., \"unclaimed\": None}, where\n-            # \"unclaimed\" is a dummy key that represents all elements in the\n-            # source_node that is not claimed by any getitems.\n-            self.source_to_getitems: dict[fx.Node, dict[Any, fx.Node | None]] = {}\n-\n-            # For each none-view source_node of getitems with at least one unclaimed\n-            # elements, we map it to its unclaimed storage size.\n-            self.source_to_unclaimed_size: dict[fx.Node, int] = {}\n-\n-            for node in nodes:\n-                is_view = op_types.is_view(node)\n-                is_getitem = node.target is operator.getitem\n-                if not (is_view or is_getitem):\n-                    continue\n-                assert not (is_view and is_getitem)\n-                assert node.args and isinstance(node.args[0], fx.Node)\n-                source = node.args[0]\n-                if is_view:\n-                    assert not isinstance(source.meta[\"val\"], list | tuple | dict)\n-                    if source in self.view_to_source:\n-                        source = self.view_to_source[source]\n-                    self.view_to_source[node] = source\n-                if is_getitem:\n-                    assert isinstance(source.meta[\"val\"], list | tuple | dict)\n-                    # Source of getitem can be a view (e.g. aten.split).\n-                    if source in self.view_to_source:\n-                        if source in self.view_to_source:\n-                            source = self.view_to_source[source]\n-                        # In this case, the getitem node should be treated\n-                        # the same way as a regular view.\n-                        self.view_to_source[node] = source\n-                        continue\n-                    # Source of getitem cannot be a getitem.\n-                    assert source not in self.getitem_to_source\n-\n-                    # There must be a second argument that specifies the key.\n-                    assert len(node.args) >= 2\n-                    key = node.args[1]\n-                    self.getitem_to_source[node] = (source, key)\n-\n-                    # Populate source_to_getitems.\n-                    if source not in self.source_to_getitems:\n-                        self.source_to_getitems[source] = {\"unclaimed\": None}\n-                    assert key not in self.source_to_getitems[source]\n-                    self.source_to_getitems[source][key] = node  # type: ignore[index]\n-\n-            for source, getitem_map in self.source_to_getitems.items():\n-                unclaimed_source_size = size_of(source)\n-                for key, getitem_node in getitem_map.items():\n-                    if key != \"unclaimed\" and getitem_node is not None:\n-                        unclaimed_source_size -= size_of(getitem_node)\n-                assert unclaimed_source_size >= 0\n-                if unclaimed_source_size > 0:\n-                    self.source_to_unclaimed_size[source] = unclaimed_source_size\n-\n-        def is_view(self, node: fx.Node) -> bool:\n-            return node in self.view_to_source\n-\n-        def is_getitem(self, node: fx.Node) -> bool:\n-            return node in self.getitem_to_source\n-\n-        def get_source(self, node: fx.Node) -> fx.Node | tuple[fx.Node, Any]:\n-            if self.is_view(node):\n-                return self.view_to_source[node]\n-            if self.is_getitem(node):\n-                return self.getitem_to_source[node]\n-            return node\n-\n-        def is_source_of_getitems(self, node: fx.Node) -> bool:\n-            return node in self.source_to_getitems\n-\n-        def get_storage_keys(self, source_node: fx.Node) -> list[Any]:\n-            assert source_node in self.source_to_getitems\n-            return list(self.source_to_getitems[source_node].keys())\n-\n-        def get_unclaimed_storage_size(self, source_node: fx.Node) -> int:\n-            return self.source_to_unclaimed_size.get(source_node, 0)\n-\n-        def get_getitem_by_key(self, source: fx.Node, key: Any) -> fx.Node | None:\n-            assert source in self.source_to_getitems\n-            assert key in self.source_to_getitems[source]\n-            return self.source_to_getitems[source][key]\n-\n-    def _get_last_usage(\n-        nodes: list[fx.Node], alias_info: AliasInfo\n-    ) -> dict[fx.Node, list[tuple[fx.Node, Any]]]:\n-        \"\"\"\n-        Determine the last usage point of each storage. This information is used to\n-        identify when storages can be safely released.\n+    # Build memory profile\n+    current_memory = 0\n \n-        Args:\n-        - nodes (list[fx.Node]): A list of nodes from the FX graph, ordered\n-          in execution order.\n-        - alias_info (AliasInfo): An instance of AliasInfo containing aliasing\n-          relationships between nodes in the graph.\n+    for node in itertools.chain(\n+        graph.find_nodes(op=\"placeholder\"), graph.find_nodes(op=\"get_attr\")\n+    ):\n+        for storage_key in alias_info.get_fresh_allocations(node):\n+            if storage_key.device.type != \"cpu\":\nComment: can we reuse `device_filter` instead of hardcode \"cpu\"?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/_inductor/fx_passes/memory_estimator.py",
    "pr_number": 164783,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2408778957,
    "comment_created_at": "2025-10-06T21:57:12Z"
  },
  {
    "code": "@@ -375,8 +379,159 @@ def apply_dp(partial_model):\n                     p.grad.full_tensor(), ref_p.grad, atol=5e-5, rtol=2e-2\n                 )\n \n+    @requires_nccl()\n+    @skip_if_lt_x_gpu(4)\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"Test requires 4+ GPUs\")\n+    @parametrize(\"dp_type\", [\"FSDP\", \"FSDP_MP\"])\n+    def test_pp_fsdp_unshard_reshard_runtime(self, dp_type):\n+        \"\"\"Test FSDP UNSHARD/RESHARD functionality using _PipelineScheduleRuntime with custom schedules.\"\"\"\n+        if TEST_WITH_ROCM:\n+            return\n+\n+        torch.get_device_module(device_type).set_device(self.device)\n+        mesh_shape = (self.world_size, 1)\n+        mesh_dim_names = (\"dp\", \"pp\")\n+        device_mesh = init_device_mesh(\n+            \"cuda\", mesh_shape=mesh_shape, mesh_dim_names=mesh_dim_names\n+        )\n+        pp_group = device_mesh[\"pp\"].get_group()\n+        dp_mesh = device_mesh[\"dp\"]\n+\n+        # fsdp_mixed-precision dtype\n+        mp_dtype = torch.bfloat16 if dp_type == \"FSDP_MP\" else torch.float32\n+        total_layers = 4\n+        dim = 10\n+        full_model = nn.ModuleList([MLPModule(dim) for _ in range(total_layers)])\n+\n+        def apply_dp(partial_model):\n+            mp_policy = MixedPrecisionPolicy(\n+                param_dtype=mp_dtype,\n+                reduce_dtype=torch.float32,\n+            )\n+            fsdp_config = {\"mesh\": dp_mesh, \"mp_policy\": mp_policy}\n+            for layer in partial_model.children():\n+                fully_shard(\n+                    layer,\n+                    **fsdp_config,\n+                    reshard_after_forward=False,\n+                )\n+            return fully_shard(partial_model, **fsdp_config)\n+\n+        # Build pipeline stages\n+        num_stages = pp_group.size()\n+        layers_per_stage = total_layers // num_stages\n+        stage_idx = pp_group.rank()\n+        offset = stage_idx * layers_per_stage\n+\n+        partial_model = nn.Sequential(\n+            *full_model[offset : (stage_idx + 1) * layers_per_stage]\n+        )\n+        partial_model.to(self.device)\n+        fsdp_model = apply_dp(partial_model)\n+        distributed_state = fully_shard.state(fsdp_model)\n+        distributed_state._lazy_init()\n+\n+        stage = PipelineStage(\n+            fsdp_model,\n+            stage_idx,\n+            num_stages,\n+            self.device,\n+            group=pp_group,\n+        )\n+\n+        # Helper function to check FSDP sharding state\n+        def check_fsdp_unsharded_state(module, expected_unsharded=False):\n+            \"\"\"Check if FSDP parameters are in expected sharding state.\"\"\"\n+            distributed_state = fully_shard.state(module)\n+            unsharded_count = 0\n+            total_fsdp_params = 0\n+\n+            for state in distributed_state._state_ctx.all_states:\n+                if state._fsdp_param_group:\n+                    group = state._fsdp_param_group\n+                    for fsdp_param in group.fsdp_params:\n+                        total_fsdp_params += 1\n+                        if fsdp_param.sharded_state == ShardedState.UNSHARDED:\n+                            unsharded_count += 1\n+\n+            if expected_unsharded:\n+                self.assertEqual(\n+                    unsharded_count,\n+                    total_fsdp_params,\n+                    f\"Expected all {total_fsdp_params} FSDP parameters to be unsharded, \"\n+                    f\"but only {unsharded_count} are unsharded\",\n+                )\n+            else:\n+                self.assertEqual(\n+                    unsharded_count,\n+                    0,\n+                    f\"Expected all FSDP parameters to be sharded, \"\n+                    f\"but {unsharded_count} out of {total_fsdp_params} are unsharded\",\n+                )\n+\n+            return total_fsdp_params > 0  # Return whether we found any FSDP parameters\n+\n+        # Test initial state - should be sharded\n+        has_fsdp = check_fsdp_unsharded_state(stage.submod, expected_unsharded=False)\n+\n+        if not has_fsdp:\n+            self.skipTest(\"No FSDP parameters found in the model\")\n+\n+        def create_schedule(computation_types, microbatch_index=None):\n+            schedule = {\n+                0: [\n+                    _Action(\n+                        stage_index=0,  # stage 0 (the only stage)\n+                        computation_type=comp_type,\n+                        microbatch_index=microbatch_index\n+                        if comp_type == _ComputationType.FORWARD\n+                        else None,\n+                    )\n+                    for comp_type in computation_types\n+                ]\n+            }\n+            return schedule\n+\n+        unshard_schedule = create_schedule(\n+            [\n+                _ComputationType.UNSHARD,\n+                _ComputationType.FORWARD,\n+            ],\n+            microbatch_index=0,\n+        )\n+        unshard_reshard_schedule = create_schedule(\n+            [\n+                _ComputationType.UNSHARD,\n+                _ComputationType.FORWARD,\n+                _ComputationType.RESHARD,\n+            ],\n+            microbatch_index=0,\n+        )\n+\n+        # Test 1: Run UNSHARD + RESHARD schedule\n+        runtime = _PipelineScheduleRuntime(\n+            [stage], n_microbatches=1, loss_fn=None, scale_grads=False\n+        )\n+        runtime.pipeline_order_with_comms = unshard_reshard_schedule\n+        dummy_input = torch.randn(1, dim, device=self.device, dtype=mp_dtype)\n+        runtime.step(dummy_input)\n+\n+        # Verify parameters are now sharded again\n+        check_fsdp_unsharded_state(stage.submod, expected_unsharded=False)\n+\n+        # Test 2: Run UNSHARD only schedule\n+        runtime.pipeline_order_with_comms = unshard_schedule\n+        runtime.step(dummy_input)\n+\n+        # Verify parameters are now unsharded\n+        check_fsdp_unsharded_state(stage.submod, expected_unsharded=True)\n+\n \n instantiate_parametrized_tests(ComposabilityTest)\n \n+import fbvscode\n+\n+\n+fbvscode.attach_debugger()",
    "comment": "remove?",
    "line_number": 535,
    "enriched": "File: test/distributed/test_composability.py\nCode: @@ -375,8 +379,159 @@ def apply_dp(partial_model):\n                     p.grad.full_tensor(), ref_p.grad, atol=5e-5, rtol=2e-2\n                 )\n \n+    @requires_nccl()\n+    @skip_if_lt_x_gpu(4)\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"Test requires 4+ GPUs\")\n+    @parametrize(\"dp_type\", [\"FSDP\", \"FSDP_MP\"])\n+    def test_pp_fsdp_unshard_reshard_runtime(self, dp_type):\n+        \"\"\"Test FSDP UNSHARD/RESHARD functionality using _PipelineScheduleRuntime with custom schedules.\"\"\"\n+        if TEST_WITH_ROCM:\n+            return\n+\n+        torch.get_device_module(device_type).set_device(self.device)\n+        mesh_shape = (self.world_size, 1)\n+        mesh_dim_names = (\"dp\", \"pp\")\n+        device_mesh = init_device_mesh(\n+            \"cuda\", mesh_shape=mesh_shape, mesh_dim_names=mesh_dim_names\n+        )\n+        pp_group = device_mesh[\"pp\"].get_group()\n+        dp_mesh = device_mesh[\"dp\"]\n+\n+        # fsdp_mixed-precision dtype\n+        mp_dtype = torch.bfloat16 if dp_type == \"FSDP_MP\" else torch.float32\n+        total_layers = 4\n+        dim = 10\n+        full_model = nn.ModuleList([MLPModule(dim) for _ in range(total_layers)])\n+\n+        def apply_dp(partial_model):\n+            mp_policy = MixedPrecisionPolicy(\n+                param_dtype=mp_dtype,\n+                reduce_dtype=torch.float32,\n+            )\n+            fsdp_config = {\"mesh\": dp_mesh, \"mp_policy\": mp_policy}\n+            for layer in partial_model.children():\n+                fully_shard(\n+                    layer,\n+                    **fsdp_config,\n+                    reshard_after_forward=False,\n+                )\n+            return fully_shard(partial_model, **fsdp_config)\n+\n+        # Build pipeline stages\n+        num_stages = pp_group.size()\n+        layers_per_stage = total_layers // num_stages\n+        stage_idx = pp_group.rank()\n+        offset = stage_idx * layers_per_stage\n+\n+        partial_model = nn.Sequential(\n+            *full_model[offset : (stage_idx + 1) * layers_per_stage]\n+        )\n+        partial_model.to(self.device)\n+        fsdp_model = apply_dp(partial_model)\n+        distributed_state = fully_shard.state(fsdp_model)\n+        distributed_state._lazy_init()\n+\n+        stage = PipelineStage(\n+            fsdp_model,\n+            stage_idx,\n+            num_stages,\n+            self.device,\n+            group=pp_group,\n+        )\n+\n+        # Helper function to check FSDP sharding state\n+        def check_fsdp_unsharded_state(module, expected_unsharded=False):\n+            \"\"\"Check if FSDP parameters are in expected sharding state.\"\"\"\n+            distributed_state = fully_shard.state(module)\n+            unsharded_count = 0\n+            total_fsdp_params = 0\n+\n+            for state in distributed_state._state_ctx.all_states:\n+                if state._fsdp_param_group:\n+                    group = state._fsdp_param_group\n+                    for fsdp_param in group.fsdp_params:\n+                        total_fsdp_params += 1\n+                        if fsdp_param.sharded_state == ShardedState.UNSHARDED:\n+                            unsharded_count += 1\n+\n+            if expected_unsharded:\n+                self.assertEqual(\n+                    unsharded_count,\n+                    total_fsdp_params,\n+                    f\"Expected all {total_fsdp_params} FSDP parameters to be unsharded, \"\n+                    f\"but only {unsharded_count} are unsharded\",\n+                )\n+            else:\n+                self.assertEqual(\n+                    unsharded_count,\n+                    0,\n+                    f\"Expected all FSDP parameters to be sharded, \"\n+                    f\"but {unsharded_count} out of {total_fsdp_params} are unsharded\",\n+                )\n+\n+            return total_fsdp_params > 0  # Return whether we found any FSDP parameters\n+\n+        # Test initial state - should be sharded\n+        has_fsdp = check_fsdp_unsharded_state(stage.submod, expected_unsharded=False)\n+\n+        if not has_fsdp:\n+            self.skipTest(\"No FSDP parameters found in the model\")\n+\n+        def create_schedule(computation_types, microbatch_index=None):\n+            schedule = {\n+                0: [\n+                    _Action(\n+                        stage_index=0,  # stage 0 (the only stage)\n+                        computation_type=comp_type,\n+                        microbatch_index=microbatch_index\n+                        if comp_type == _ComputationType.FORWARD\n+                        else None,\n+                    )\n+                    for comp_type in computation_types\n+                ]\n+            }\n+            return schedule\n+\n+        unshard_schedule = create_schedule(\n+            [\n+                _ComputationType.UNSHARD,\n+                _ComputationType.FORWARD,\n+            ],\n+            microbatch_index=0,\n+        )\n+        unshard_reshard_schedule = create_schedule(\n+            [\n+                _ComputationType.UNSHARD,\n+                _ComputationType.FORWARD,\n+                _ComputationType.RESHARD,\n+            ],\n+            microbatch_index=0,\n+        )\n+\n+        # Test 1: Run UNSHARD + RESHARD schedule\n+        runtime = _PipelineScheduleRuntime(\n+            [stage], n_microbatches=1, loss_fn=None, scale_grads=False\n+        )\n+        runtime.pipeline_order_with_comms = unshard_reshard_schedule\n+        dummy_input = torch.randn(1, dim, device=self.device, dtype=mp_dtype)\n+        runtime.step(dummy_input)\n+\n+        # Verify parameters are now sharded again\n+        check_fsdp_unsharded_state(stage.submod, expected_unsharded=False)\n+\n+        # Test 2: Run UNSHARD only schedule\n+        runtime.pipeline_order_with_comms = unshard_schedule\n+        runtime.step(dummy_input)\n+\n+        # Verify parameters are now unsharded\n+        check_fsdp_unsharded_state(stage.submod, expected_unsharded=True)\n+\n \n instantiate_parametrized_tests(ComposabilityTest)\n \n+import fbvscode\n+\n+\n+fbvscode.attach_debugger()\nComment: remove?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "test/distributed/test_composability.py",
    "pr_number": 164775,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2409390205,
    "comment_created_at": "2025-10-07T05:15:53Z"
  },
  {
    "code": "@@ -1224,48 +1224,84 @@ static PyObject* THPModule_allowTF32CuBLAS(\n \n static PyObject* THPModule_setAllowFP16ReductionCuBLAS(\n     PyObject* _unused,\n-    PyObject* arg) {\n+    PyObject* args) {\n   HANDLE_TH_ERRORS\n+  PyObject* allow_reduction_obj = nullptr;\n+  PyObject* allow_splitk_obj = Py_None;\n+  if (!PyArg_ParseTuple(args, \"O|O\", &allow_reduction_obj, &allow_splitk_obj)) {\n+    return nullptr;\n+  }\n   TORCH_CHECK(\n-      PyBool_Check(arg),\n-      \"set_allow_fp16_reduction_cublas expects a bool, \"\n+      PyBool_Check(allow_reduction_obj),\n+      \"set_allow_fp16_reduction_cublas expects a bool for allow_reduced_precision, \"\n       \"but got \",\n-      THPUtils_typename(arg));\n-  at::globalContext().setAllowFP16ReductionCuBLAS(arg == Py_True);\n+      THPUtils_typename(allow_reduction_obj));\n+  bool allow_reduction = allow_reduction_obj == Py_True;\n+  bool allow_splitk = true;\n+  if (allow_splitk_obj != Py_None) {\n+    TORCH_CHECK(\n+        PyBool_Check(allow_splitk_obj),\n+        \"set_allow_fp16_reduction_cublas expects a bool for allow_splitk, \"\n+        \"but got \",\n+        THPUtils_typename(allow_splitk_obj));\n+    allow_splitk = allow_splitk_obj == Py_True;\n+  }\n+  at::globalContext().setAllowFP16ReductionCuBLAS(\n+      allow_reduction, allow_splitk);\n   Py_RETURN_NONE;\n   END_HANDLE_TH_ERRORS\n }\n \n static PyObject* THPModule_allowFP16ReductionCuBLAS(\n     PyObject* _unused,\n     PyObject* noargs) {\n-  if (at::globalContext().allowFP16ReductionCuBLAS()) {\n-    Py_RETURN_TRUE;\n-  }\n-  Py_RETURN_FALSE;\n+  auto option = at::globalContext().allowFP16ReductionCuBLAS();\n+  bool allow_reduced_precision =\n+      option == at::CuBLASReductionOption::AllowReducedPrecisionWithSplitK;\n+  bool allow_splitk = option !=\n+      at::CuBLASReductionOption::DisallowReducedPrecisionDisallowSplitK;\n+  return Py_BuildValue(\"(pp)\", allow_reduced_precision, allow_splitk);",
    "comment": "maybe it wants something like py_buildvalue(\"(oo)\", allow_reduced_precision ? py_true : py_false, allow_splitk ? py_true : py_false);",
    "line_number": 1263,
    "enriched": "File: torch/csrc/Module.cpp\nCode: @@ -1224,48 +1224,84 @@ static PyObject* THPModule_allowTF32CuBLAS(\n \n static PyObject* THPModule_setAllowFP16ReductionCuBLAS(\n     PyObject* _unused,\n-    PyObject* arg) {\n+    PyObject* args) {\n   HANDLE_TH_ERRORS\n+  PyObject* allow_reduction_obj = nullptr;\n+  PyObject* allow_splitk_obj = Py_None;\n+  if (!PyArg_ParseTuple(args, \"O|O\", &allow_reduction_obj, &allow_splitk_obj)) {\n+    return nullptr;\n+  }\n   TORCH_CHECK(\n-      PyBool_Check(arg),\n-      \"set_allow_fp16_reduction_cublas expects a bool, \"\n+      PyBool_Check(allow_reduction_obj),\n+      \"set_allow_fp16_reduction_cublas expects a bool for allow_reduced_precision, \"\n       \"but got \",\n-      THPUtils_typename(arg));\n-  at::globalContext().setAllowFP16ReductionCuBLAS(arg == Py_True);\n+      THPUtils_typename(allow_reduction_obj));\n+  bool allow_reduction = allow_reduction_obj == Py_True;\n+  bool allow_splitk = true;\n+  if (allow_splitk_obj != Py_None) {\n+    TORCH_CHECK(\n+        PyBool_Check(allow_splitk_obj),\n+        \"set_allow_fp16_reduction_cublas expects a bool for allow_splitk, \"\n+        \"but got \",\n+        THPUtils_typename(allow_splitk_obj));\n+    allow_splitk = allow_splitk_obj == Py_True;\n+  }\n+  at::globalContext().setAllowFP16ReductionCuBLAS(\n+      allow_reduction, allow_splitk);\n   Py_RETURN_NONE;\n   END_HANDLE_TH_ERRORS\n }\n \n static PyObject* THPModule_allowFP16ReductionCuBLAS(\n     PyObject* _unused,\n     PyObject* noargs) {\n-  if (at::globalContext().allowFP16ReductionCuBLAS()) {\n-    Py_RETURN_TRUE;\n-  }\n-  Py_RETURN_FALSE;\n+  auto option = at::globalContext().allowFP16ReductionCuBLAS();\n+  bool allow_reduced_precision =\n+      option == at::CuBLASReductionOption::AllowReducedPrecisionWithSplitK;\n+  bool allow_splitk = option !=\n+      at::CuBLASReductionOption::DisallowReducedPrecisionDisallowSplitK;\n+  return Py_BuildValue(\"(pp)\", allow_reduced_precision, allow_splitk);\nComment: maybe it wants something like `Py_BuildValue(\"(OO)\", allow_reduced_precision ? Py_True : Py_False, allow_splitk ? Py_True : Py_False);`",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "torch/csrc/Module.cpp",
    "pr_number": 164766,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2411252705,
    "comment_created_at": "2025-10-07T16:46:06Z"
  },
  {
    "code": "@@ -978,6 +978,89 @@ void tile_reduce(\n   });\n }\n \n+/* Multi-tile Communication */\n+\n+void multi_root_tile_reduce(\n+    at::ArrayRef<at::Tensor> in_tiles,\n+    at::Tensor& out_tile,\n+    at::ArrayRef<int64_t> roots,\n+    std::string group_name,\n+    std::string reduce_op) {\n+  /* Perform multiple tile reductions concurrently, with each tile reduced to a separate root.\n+   Args:\n+     - `in_tiles` is a list of input tensors.\n+     - `out_tile` is the output tensor.\n+     - `roots` is a list of root ranks corresponding to each input tile, in the same order. A rank cannot be a root more than once.\n+     - `group_name` is the name of the group to use for the collective operation.\n+     - `reduce_op` is the reduction operation to perform. Currently only \"sum\" is supported.\n+   */\n+  TORCH_CHECK(reduce_op == \"sum\", \"tile_reduce: only sum is supported for now\");\n+  TORCH_CHECK(out_tile.dtype() == at::kFloat, \"Only float is supported\");\n+  TORCH_CHECK(out_tile.dim() == 2, \"Only 2D tensors are supported\");\n+  TORCH_CHECK(roots.size() == in_tiles.size(), \"Number of roots must match number of tiles\");\n+\n+  // Get device and stream\n+  auto device = out_tile.device();\n+  c10::cuda::CUDAGuard guard(device);\n+  auto stream = at::cuda::getCurrentCUDAStream();\n+\n+  // Rendezvous all tensors, and find the tile \"I\" need to reduce\n+  auto hdl = c10d::symmetric_memory::rendezvous(out_tile, group_name);\n+  int rank = hdl->get_rank();\n+  int world_size = hdl->get_world_size();\n+  int i = 0, my_tile_idx = 0, root = world_size;\n+  // Note: if there is no tile for the current rank, my_tile_idx will remain\n+  // initial value 0, and root will remain `world_size`. This is OK. In\n+  // `nvshmemx::tile_sum_reduce_block`, this rank would skip the reduction\n+  // operation, but would still participate in the barrier.\n+  for (auto& in_tile : in_tiles) {\n+    TORCH_CHECK(in_tile.dtype() == at::kFloat, \"Only float is supported\");\n+    c10d::symmetric_memory::rendezvous(in_tile, group_name);\n+    if (roots[i] == rank) {\n+      TORCH_CHECK(root == world_size, \"Each rank can only be a root once\");\n+      my_tile_idx = i;\n+      root = rank;\n+    }\n+    i++;\n+  }",
    "comment": "should we check that root != world_size here?",
    "line_number": 1025,
    "enriched": "File: torch/csrc/distributed/c10d/symm_mem/nvshmem_extension.cu\nCode: @@ -978,6 +978,89 @@ void tile_reduce(\n   });\n }\n \n+/* Multi-tile Communication */\n+\n+void multi_root_tile_reduce(\n+    at::ArrayRef<at::Tensor> in_tiles,\n+    at::Tensor& out_tile,\n+    at::ArrayRef<int64_t> roots,\n+    std::string group_name,\n+    std::string reduce_op) {\n+  /* Perform multiple tile reductions concurrently, with each tile reduced to a separate root.\n+   Args:\n+     - `in_tiles` is a list of input tensors.\n+     - `out_tile` is the output tensor.\n+     - `roots` is a list of root ranks corresponding to each input tile, in the same order. A rank cannot be a root more than once.\n+     - `group_name` is the name of the group to use for the collective operation.\n+     - `reduce_op` is the reduction operation to perform. Currently only \"sum\" is supported.\n+   */\n+  TORCH_CHECK(reduce_op == \"sum\", \"tile_reduce: only sum is supported for now\");\n+  TORCH_CHECK(out_tile.dtype() == at::kFloat, \"Only float is supported\");\n+  TORCH_CHECK(out_tile.dim() == 2, \"Only 2D tensors are supported\");\n+  TORCH_CHECK(roots.size() == in_tiles.size(), \"Number of roots must match number of tiles\");\n+\n+  // Get device and stream\n+  auto device = out_tile.device();\n+  c10::cuda::CUDAGuard guard(device);\n+  auto stream = at::cuda::getCurrentCUDAStream();\n+\n+  // Rendezvous all tensors, and find the tile \"I\" need to reduce\n+  auto hdl = c10d::symmetric_memory::rendezvous(out_tile, group_name);\n+  int rank = hdl->get_rank();\n+  int world_size = hdl->get_world_size();\n+  int i = 0, my_tile_idx = 0, root = world_size;\n+  // Note: if there is no tile for the current rank, my_tile_idx will remain\n+  // initial value 0, and root will remain `world_size`. This is OK. In\n+  // `nvshmemx::tile_sum_reduce_block`, this rank would skip the reduction\n+  // operation, but would still participate in the barrier.\n+  for (auto& in_tile : in_tiles) {\n+    TORCH_CHECK(in_tile.dtype() == at::kFloat, \"Only float is supported\");\n+    c10d::symmetric_memory::rendezvous(in_tile, group_name);\n+    if (roots[i] == rank) {\n+      TORCH_CHECK(root == world_size, \"Each rank can only be a root once\");\n+      my_tile_idx = i;\n+      root = rank;\n+    }\n+    i++;\n+  }\nComment: Should we check that `root != world_size` here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/csrc/distributed/c10d/symm_mem/nvshmem_extension.cu",
    "pr_number": 164757,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2408093987,
    "comment_created_at": "2025-10-06T19:20:09Z"
  },
  {
    "code": "@@ -20,17 +21,28 @@ def normalize_graph(gm):\n     return normalize_gm(gm.print_readable(print_output=False))\n \n \n-@skipIf(not dist.is_available(), \"requires distributed\")\n-class TestFakeDistributed(DynamoTestCase):\n-    def setUp(self):\n-        # Use FakeProcessGroup to run tests on a single process\n-        dist.init_process_group(backend=\"fake\", rank=0, world_size=2)\n-        self.local_rank = 0\n-        self.world_size = 2\n+def with_fake_comms(func=None, local_rank=None, world_size=None):\n+    if func is None:\n+        return functools.partial(\n+            with_fake_comms, local_rank=local_rank, world_size=world_size\n+        )\n+\n+    @functools.wraps(func)\n+    def wrapper(self, *args, **kwargs):\n+        assert local_rank is not None\n+        assert world_size is not None\n+        dist.init_process_group(backend=\"fake\", rank=local_rank, world_size=world_size)\n+        try:\n+            return func(self, *args, **kwargs)\n+        finally:\n+            dist.destroy_process_group()\n \n-    def tearDown(self):\n-        dist.destroy_process_group()\n+    return wrapper\n \n+\n+@skipIf(not dist.is_available(), \"requires distributed\")\n+class TestFakeDistributed(DynamoTestCase):\n+    @with_fake_comms(local_rank=0, world_size=8)",
    "comment": "@bobrenjc93 if you change this world_size to something beyond 8 gpus",
    "line_number": 45,
    "enriched": "File: test/dynamo/test_fake_distributed.py\nCode: @@ -20,17 +21,28 @@ def normalize_graph(gm):\n     return normalize_gm(gm.print_readable(print_output=False))\n \n \n-@skipIf(not dist.is_available(), \"requires distributed\")\n-class TestFakeDistributed(DynamoTestCase):\n-    def setUp(self):\n-        # Use FakeProcessGroup to run tests on a single process\n-        dist.init_process_group(backend=\"fake\", rank=0, world_size=2)\n-        self.local_rank = 0\n-        self.world_size = 2\n+def with_fake_comms(func=None, local_rank=None, world_size=None):\n+    if func is None:\n+        return functools.partial(\n+            with_fake_comms, local_rank=local_rank, world_size=world_size\n+        )\n+\n+    @functools.wraps(func)\n+    def wrapper(self, *args, **kwargs):\n+        assert local_rank is not None\n+        assert world_size is not None\n+        dist.init_process_group(backend=\"fake\", rank=local_rank, world_size=world_size)\n+        try:\n+            return func(self, *args, **kwargs)\n+        finally:\n+            dist.destroy_process_group()\n \n-    def tearDown(self):\n-        dist.destroy_process_group()\n+    return wrapper\n \n+\n+@skipIf(not dist.is_available(), \"requires distributed\")\n+class TestFakeDistributed(DynamoTestCase):\n+    @with_fake_comms(local_rank=0, world_size=8)\nComment: @bobrenjc93 if you change this world_size to something beyond 8 gpus",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "test/dynamo/test_fake_distributed.py",
    "pr_number": 164754,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2408353338,
    "comment_created_at": "2025-10-06T20:18:03Z"
  },
  {
    "code": "@@ -8902,26 +8902,20 @@ def __init__(self) -> None:\n \n             def forward(self, start_pos: torch.Tensor):\n                 pos = start_pos.item()\n-                torch._check_is_size(pos)\n                 torch._check(pos >= 0)\n                 torch._check(pos <= 4)\n                 return self.freq[pos] * self.freq[pos]\n \n         ep = export(M(), (torch.tensor(1),))\n+        print(ep)",
    "comment": "debug cruft?",
    "line_number": 8910,
    "enriched": "File: test/export/test_export.py\nCode: @@ -8902,26 +8902,20 @@ def __init__(self) -> None:\n \n             def forward(self, start_pos: torch.Tensor):\n                 pos = start_pos.item()\n-                torch._check_is_size(pos)\n                 torch._check(pos >= 0)\n                 torch._check(pos <= 4)\n                 return self.freq[pos] * self.freq[pos]\n \n         ep = export(M(), (torch.tensor(1),))\n+        print(ep)\nComment: debug cruft?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "test/export/test_export.py",
    "pr_number": 164753,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2409064866,
    "comment_created_at": "2025-10-07T01:05:09Z"
  },
  {
    "code": "@@ -583,18 +578,7 @@ def _init_process_groups(\n \n                     # Respect dim group options specified via _MeshEnv.set_dim_group_options().",
    "comment": "nit: update this comment\n\nand check whether there are other ones?",
    "line_number": 579,
    "enriched": "File: torch/distributed/device_mesh.py\nCode: @@ -583,18 +578,7 @@ def _init_process_groups(\n \n                     # Respect dim group options specified via _MeshEnv.set_dim_group_options().\nComment: Nit: update this comment\n\nAnd check whether there are other ones?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "torch/distributed/device_mesh.py",
    "pr_number": 164750,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2409693269,
    "comment_created_at": "2025-10-07T07:40:27Z"
  },
  {
    "code": "@@ -30,6 +30,7 @@ class LinearReLU(nnqd.Linear):\n         torch.Size([128, 30])\n     \"\"\"\n \n+    # pyrefly: ignore  # bad-override\n     _FLOAT_MODULE = nni.LinearReLU",
    "comment": ",this is a legitimate error we should fix probably",
    "line_number": 34,
    "enriched": "File: torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\nCode: @@ -30,6 +30,7 @@ class LinearReLU(nnqd.Linear):\n         torch.Size([128, 30])\n     \"\"\"\n \n+    # pyrefly: ignore  # bad-override\n     _FLOAT_MODULE = nni.LinearReLU\nComment: ,This is a legitimate error we should fix probably",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py",
    "pr_number": 164748,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2407648019,
    "comment_created_at": "2025-10-06T17:41:14Z"
  },
  {
    "code": "@@ -1086,6 +1086,15 @@ def _is_make_fx_tracing():\n         return False\n \n \n+def _is_exporting():\n+    # More details at https://github.com/pytorch/pytorch/issues/164062\n+\n+    # The weird code is because of TorchScript not returning early on `and` conditions.\n+    if not torch.jit.is_scripting():",
    "comment": "maybe inline as not torch.jit.is_scripting() and torch.compiler.is_exporting()",
    "line_number": 1093,
    "enriched": "File: torch/nn/modules/activation.py\nCode: @@ -1086,6 +1086,15 @@ def _is_make_fx_tracing():\n         return False\n \n \n+def _is_exporting():\n+    # More details at https://github.com/pytorch/pytorch/issues/164062\n+\n+    # The weird code is because of TorchScript not returning early on `and` conditions.\n+    if not torch.jit.is_scripting():\nComment: maybe inline as `not torch.jit.is_scripting() and torch.compiler.is_exporting()`",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "torch/nn/modules/activation.py",
    "pr_number": 164721,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2407378818,
    "comment_created_at": "2025-10-06T16:40:17Z"
  },
  {
    "code": "@@ -74,13 +79,16 @@ jobs:\n           cache-dependency-glob: |\n             requirements**.txt\n             pyproject.toml\n+      - name: Preinstall Pydantic 2.12.0a1 for Python 3.14\n+        if: matrix.python-version == '3.14'\n+        run: uv pip install --pre \"pydantic==2.12.0a1\"",
    "comment": "this can be removed when a stable pydantic 2.12 release is available.",
    "line_number": 84,
    "enriched": "File: .github/workflows/test.yml\nCode: @@ -74,13 +79,16 @@ jobs:\n           cache-dependency-glob: |\n             requirements**.txt\n             pyproject.toml\n+      - name: Preinstall Pydantic 2.12.0a1 for Python 3.14\n+        if: matrix.python-version == '3.14'\n+        run: uv pip install --pre \"pydantic==2.12.0a1\"\nComment: This can be removed when a stable Pydantic 2.12 release is available.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": ".github/workflows/test.yml",
    "pr_number": 14110,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2376037999,
    "comment_created_at": "2025-09-24T14:39:21Z"
  },
  {
    "code": "@@ -10,7 +10,9 @@\n @app.exception_handler(RequestValidationError)\n async def validation_exception_handler(request: Request, exc: RequestValidationError):\n     return JSONResponse(\n-        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n+        # 422 = fastapi.status.HTTP_422_UNPROCESSABLE_CONTENT (RFC 9110,\n+        # Starlette >=0.48), previously HTTP_422_UNPROCESSABLE_ENTITY\n+        status_code=422,",
    "comment": "we have to be carefull here, this script is referenced in the docs [here](https://fastapi.tiangolo.com/tutorial/handling-errors/#use-the-requestvalidationerror-body), and by adding more lines, the wrong line will be highlighted in the docs (used to be l14 which is now l16).\r\n\r\nanyway, i don't think we need this comment here, or elsewhere where we're switching to 422. let's just edit to status_code=422 without the comments.",
    "line_number": 15,
    "enriched": "File: docs_src/handling_errors/tutorial005.py\nCode: @@ -10,7 +10,9 @@\n @app.exception_handler(RequestValidationError)\n async def validation_exception_handler(request: Request, exc: RequestValidationError):\n     return JSONResponse(\n-        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n+        # 422 = fastapi.status.HTTP_422_UNPROCESSABLE_CONTENT (RFC 9110,\n+        # Starlette >=0.48), previously HTTP_422_UNPROCESSABLE_ENTITY\n+        status_code=422,\nComment: We have to be carefull here, this script is referenced in the docs [here](https://fastapi.tiangolo.com/tutorial/handling-errors/#use-the-requestvalidationerror-body), and by adding more lines, the wrong line will be highlighted in the docs (used to be L14 which is now L16).\r\n\r\nAnyway, I don't think we need this comment here, or elsewhere where we're switching to 422. Let's just edit to `status_code=422` without the comments.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "docs_src/handling_errors/tutorial005.py",
    "pr_number": 14077,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2349101580,
    "comment_created_at": "2025-09-15T13:58:01Z"
  },
  {
    "code": "@@ -1,12 +1,52 @@\n-from typing import Any\n+from typing import Any, Optional\n+\n+from starlette.responses import FileResponse as StarletteFileResponse\n+from starlette.responses import HTMLResponse as StarletteHTMLResponse\n+from starlette.responses import JSONResponse as StarletteJSONResponse\n+from starlette.responses import PlainTextResponse as StarlettePlainTextResponse\n+from starlette.responses import RedirectResponse as StarletteRedirectResponse\n+from starlette.responses import Response as StarletteResponse\n+from starlette.responses import StreamingResponse as StarletteStreamingResponse\n+\n+\n+class Response(StarletteResponse):\n+    default_status_code: int = 200\n+\n+    def __init__(\n+        self,\n+        content: Any = None,\n+        *,",
    "comment": "looks like you are using [keyword-only arguments](https://peps.python.org/pep-3102/) feature in python,  it might be a breaking changes for people are currently not setting status_code as keyword argument. looking [starlette code ](https://github.com/kludex/starlette/blob/main/starlette/responses.py#l38), they also don't enforce it as keyword only argument. \r\n\r\ne.g. the response response(\"hello\", 404) is not going to work anymore.",
    "line_number": 18,
    "enriched": "File: fastapi/responses.py\nCode: @@ -1,12 +1,52 @@\n-from typing import Any\n+from typing import Any, Optional\n+\n+from starlette.responses import FileResponse as StarletteFileResponse\n+from starlette.responses import HTMLResponse as StarletteHTMLResponse\n+from starlette.responses import JSONResponse as StarletteJSONResponse\n+from starlette.responses import PlainTextResponse as StarlettePlainTextResponse\n+from starlette.responses import RedirectResponse as StarletteRedirectResponse\n+from starlette.responses import Response as StarletteResponse\n+from starlette.responses import StreamingResponse as StarletteStreamingResponse\n+\n+\n+class Response(StarletteResponse):\n+    default_status_code: int = 200\n+\n+    def __init__(\n+        self,\n+        content: Any = None,\n+        *,\nComment: Looks like you are using [Keyword-Only Arguments](https://peps.python.org/pep-3102/) feature in python,  It might be a breaking changes for people are currently not setting `status_code` as keyword argument. Looking [Starlette code ](https://github.com/Kludex/starlette/blob/main/starlette/responses.py#L38), they also don't enforce it as keyword only argument. \r\n\r\ne.g. the response `Response(\"Hello\", 404)` is not going to work anymore. ",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "fastapi/responses.py",
    "pr_number": 14075,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2370648463,
    "comment_created_at": "2025-09-22T23:38:57Z"
  },
  {
    "code": "@@ -19,13 +19,20 @@\n \n from fastapi.types import IncEx\n from pydantic import BaseModel\n-from pydantic.color import Color\n from pydantic.networks import AnyUrl, NameEmail\n from pydantic.types import SecretBytes, SecretStr\n+from pydantic_extra_types.color import Color  # \u2705 updated import",
    "comment": "you probably need to import this only when you need it for pydantic v2, as this will crash on pydantic v1.",
    "line_number": 24,
    "enriched": "File: fastapi/encoders.py\nCode: @@ -19,13 +19,20 @@\n \n from fastapi.types import IncEx\n from pydantic import BaseModel\n-from pydantic.color import Color\n from pydantic.networks import AnyUrl, NameEmail\n from pydantic.types import SecretBytes, SecretStr\n+from pydantic_extra_types.color import Color  # \u2705 updated import\nComment: You probably need to import this only when you need it for Pydantic v2, as this will crash on Pydantic v1.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "fastapi/encoders.py",
    "pr_number": 14060,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2343546755,
    "comment_created_at": "2025-09-12T09:09:30Z"
  },
  {
    "code": "@@ -44,6 +44,8 @@\n site_path = Path(\"site\").absolute()\n build_site_path = Path(\"site_build\").absolute()\n \n+header_with_permalink_pattern = re.compile(r\"^(#{1,6}) (.+?)(\\s*\\{\\s*#.*\\s*\\})\\s*$\")",
    "comment": "fancy regex! \ud83e\udd2f \ud83d\ude0e",
    "line_number": 47,
    "enriched": "File: scripts/docs.py\nCode: @@ -44,6 +44,8 @@\n site_path = Path(\"site\").absolute()\n build_site_path = Path(\"site_build\").absolute()\n \n+header_with_permalink_pattern = re.compile(r\"^(#{1,6}) (.+?)(\\s*\\{\\s*#.*\\s*\\})\\s*$\")\nComment: Fancy RegEx! \ud83e\udd2f \ud83d\ude0e ",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "scripts/docs.py",
    "pr_number": 14055,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2365598913,
    "comment_created_at": "2025-09-20T11:43:21Z"
  },
  {
    "code": "@@ -109,9 +110,24 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0a1 warns about this when building\n+                # TypeAdapters from field information that uses aliases.\n+                # The Pydantic team recommends ignoring this in this case:\n+                # https://github.com/fastapi/fastapi/pull/14036#issuecomment-3316045587",
    "comment": "suggestion\r\n                # pydantic >= 2.12.0 warns about field specific metadata that is unused\r\n                # (e.g. typeadapter(annotated[int, field(alias='b')])). in some cases, we\r\n                # end up building the type adapter from a model field annotation so we\r\n                # need to ignore the warning:",
    "line_number": 117,
    "enriched": "File: fastapi/_compat.py\nCode: @@ -109,9 +110,24 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0a1 warns about this when building\n+                # TypeAdapters from field information that uses aliases.\n+                # The Pydantic team recommends ignoring this in this case:\n+                # https://github.com/fastapi/fastapi/pull/14036#issuecomment-3316045587\nComment: ```suggestion\r\n                # Pydantic >= 2.12.0 warns about field specific metadata that is unused\r\n                # (e.g. `TypeAdapter(Annotated[int, Field(alias='b')])`). In some cases, we\r\n                # end up building the type adapter from a model field annotation so we\r\n                # need to ignore the warning:",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "fastapi/_compat.py",
    "pr_number": 14036,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2369192894,
    "comment_created_at": "2025-09-22T16:05:41Z"
  },
  {
    "code": "@@ -383,7 +383,7 @@ Starlette (\u0648 **FastAPI**) \u0628\u0631 \u067e\u0627\u06cc\u0647 <a href=\"https://anyio.readthedocs.io/e\n \n \u062a\u0648\u06cc \u0646\u0633\u062e\u0647\u200c\u0647\u0627\u06cc \u0642\u0628\u0644\u06cc \u067e\u0627\u06cc\u062a\u0648\u0646\u060c \u0645\u06cc\u200c\u062a\u0648\u0646\u0633\u062a\u06cc \u0627\u0632 \u0646\u062e\u200c\u0647\u0627 \u06cc\u0627 <a href=\"https://www.gevent.org/\" class=\"external-link\" target=\"_blank\">Gevent</a> \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u06a9\u0646\u06cc. \u0648\u0644\u06cc \u06a9\u062f \u062e\u06cc\u0644\u06cc \u067e\u06cc\u0686\u06cc\u062f\u0647\u200c\u062a\u0631 \u0645\u06cc\u200c\u0634\u0647 \u0628\u0631\u0627\u06cc \u0641\u0647\u0645\u06cc\u062f\u0646\u060c \u062f\u06cc\u0628\u0627\u06af \u06a9\u0631\u062f\u0646 \u0648 \u0641\u06a9\u0631 \u06a9\u0631\u062f\u0646 \u0628\u0647\u0634.\n \n-\u062a\u0648\u06cc \u0646\u0633\u062e\u0647\u200c\u0647\u0627\u06cc \u0642\u0628\u0644\u06cc NodeJS / \u062c\u0627\u0648\u0627\u0627\u0633\u06a9\u0631\u06cc\u067e\u062a \u0645\u0631\u0648\u0631\u06af\u0631\u060c \u0627\u0632 \"\u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627\" \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0645\u06cc\u200c\u06a9\u0631\u062f\u06cc. \u06a9\u0647 \u0645\u06cc\u200c\u0631\u0633\u06cc\u062f \u0628\u0647 <a href=\"http://callbackhell.com/\" class=\"external-link\" target=\"_blank\">\u062c\u0647\u0627\u0646 \u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627</a>.\n+\u062a\u0648\u06cc \u0646\u0633\u062e\u0647\u200c\u0647\u0627\u06cc \u0642\u0628\u0644\u06cc NodeJS / \u062c\u0627\u0648\u0627\u0627\u0633\u06a9\u0631\u06cc\u067e\u062a \u0645\u0631\u0648\u0631\u06af\u0631\u060c \u0627\u0632 \"\u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627\" \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0645\u06cc\u200c\u06a9\u0631\u062f\u06cc. \u06a9\u0647 \u0645\u06cc\u200c\u0631\u0633\u06cc\u062f \u0628\u0647 \"\u062c\u0647\u0627\u0646 \u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627\".",
    "comment": "this _looks_ incorrect when viewing the github diff (one of the quote marks seem to be placed incorrectly) but when viewing the actual doc page, it seems fine.",
    "line_number": 386,
    "enriched": "File: docs/fa/docs/async.md\nCode: @@ -383,7 +383,7 @@ Starlette (\u0648 **FastAPI**) \u0628\u0631 \u067e\u0627\u06cc\u0647 <a href=\"https://anyio.readthedocs.io/e\n \n \u062a\u0648\u06cc \u0646\u0633\u062e\u0647\u200c\u0647\u0627\u06cc \u0642\u0628\u0644\u06cc \u067e\u0627\u06cc\u062a\u0648\u0646\u060c \u0645\u06cc\u200c\u062a\u0648\u0646\u0633\u062a\u06cc \u0627\u0632 \u0646\u062e\u200c\u0647\u0627 \u06cc\u0627 <a href=\"https://www.gevent.org/\" class=\"external-link\" target=\"_blank\">Gevent</a> \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u06a9\u0646\u06cc. \u0648\u0644\u06cc \u06a9\u062f \u062e\u06cc\u0644\u06cc \u067e\u06cc\u0686\u06cc\u062f\u0647\u200c\u062a\u0631 \u0645\u06cc\u200c\u0634\u0647 \u0628\u0631\u0627\u06cc \u0641\u0647\u0645\u06cc\u062f\u0646\u060c \u062f\u06cc\u0628\u0627\u06af \u06a9\u0631\u062f\u0646 \u0648 \u0641\u06a9\u0631 \u06a9\u0631\u062f\u0646 \u0628\u0647\u0634.\n \n-\u062a\u0648\u06cc \u0646\u0633\u062e\u0647\u200c\u0647\u0627\u06cc \u0642\u0628\u0644\u06cc NodeJS / \u062c\u0627\u0648\u0627\u0627\u0633\u06a9\u0631\u06cc\u067e\u062a \u0645\u0631\u0648\u0631\u06af\u0631\u060c \u0627\u0632 \"\u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627\" \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0645\u06cc\u200c\u06a9\u0631\u062f\u06cc. \u06a9\u0647 \u0645\u06cc\u200c\u0631\u0633\u06cc\u062f \u0628\u0647 <a href=\"http://callbackhell.com/\" class=\"external-link\" target=\"_blank\">\u062c\u0647\u0627\u0646 \u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627</a>.\n+\u062a\u0648\u06cc \u0646\u0633\u062e\u0647\u200c\u0647\u0627\u06cc \u0642\u0628\u0644\u06cc NodeJS / \u062c\u0627\u0648\u0627\u0627\u0633\u06a9\u0631\u06cc\u067e\u062a \u0645\u0631\u0648\u0631\u06af\u0631\u060c \u0627\u0632 \"\u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627\" \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0645\u06cc\u200c\u06a9\u0631\u062f\u06cc. \u06a9\u0647 \u0645\u06cc\u200c\u0631\u0633\u06cc\u062f \u0628\u0647 \"\u062c\u0647\u0627\u0646 \u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627\".\nComment: This _looks_ incorrect when viewing the github diff (one of the quote marks seem to be placed incorrectly) but when viewing the actual doc page, it seems fine.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/fa/docs/async.md",
    "pr_number": 14006,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2297470814,
    "comment_created_at": "2025-08-25T08:34:47Z"
  },
  {
    "code": "@@ -106,7 +106,7 @@ See the similarities in `requests.get(...)` and `@app.get(...)`.\n \n ///\n \n-### <a href=\"https://swagger.io/\" class=\"external-link\" target=\"_blank\">Swagger</a> / <a href=\"https://github.com/OAI/OpenAPI-Specification/\" class=\"external-link\" target=\"_blank\">OpenAPI</a>\n+### <a href=\"https://swagger.io/\" class=\"external-link\" target=\"_blank\">Swagger</a> / <a href=\"https://github.com/OAI/OpenAPI-Specification/\" class=\"external-link\" target=\"_blank\">OpenAPI</a> { #swagger-openapi }",
    "comment": "this is an example of how the links inside headers are handled",
    "line_number": 109,
    "enriched": "File: docs/en/docs/alternatives.md\nCode: @@ -106,7 +106,7 @@ See the similarities in `requests.get(...)` and `@app.get(...)`.\n \n ///\n \n-### <a href=\"https://swagger.io/\" class=\"external-link\" target=\"_blank\">Swagger</a> / <a href=\"https://github.com/OAI/OpenAPI-Specification/\" class=\"external-link\" target=\"_blank\">OpenAPI</a>\n+### <a href=\"https://swagger.io/\" class=\"external-link\" target=\"_blank\">Swagger</a> / <a href=\"https://github.com/OAI/OpenAPI-Specification/\" class=\"external-link\" target=\"_blank\">OpenAPI</a> { #swagger-openapi }\nComment: This is an example of how the links inside headers are handled",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/en/docs/alternatives.md",
    "pr_number": 13993,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2280735255,
    "comment_created_at": "2025-08-17T05:58:47Z"
  },
  {
    "code": "@@ -0,0 +1,146 @@\n+\"\"\"\n+Tests for the dont_encode_url parameter in Request class.\n+Related to issue #7028 and #833.\n+\"\"\"\n+\n+import pytest\n+\n+from scrapy.http import Request\n+\n+\n+class TestRequestDontEncodeUrl:\n+    \"\"\"Tests for Request.dont_encode_url parameter\"\"\"\n+\n+    def test_url_encoding_default_behavior(self):\n+        \"\"\"Test that URL encoding is enabled by default\"\"\"\n+        # This URL should pass through safe_url_string\n+        url = \"http://example.com/page\"\n+        req = Request(url)\n+        # By default, safe_url_string is called\n+        assert req.url == url\n+        assert req.dont_encode_url is False\n+\n+    def test_url_with_slashes_in_query_params_default(self):\n+        \"\"\"Test that slashes in query params are preserved by default with safe_url_string\"\"\"",
    "comment": "nice, so why is this change needed?",
    "line_number": 24,
    "enriched": "File: tests/test_request_dont_encode_url.py\nCode: @@ -0,0 +1,146 @@\n+\"\"\"\n+Tests for the dont_encode_url parameter in Request class.\n+Related to issue #7028 and #833.\n+\"\"\"\n+\n+import pytest\n+\n+from scrapy.http import Request\n+\n+\n+class TestRequestDontEncodeUrl:\n+    \"\"\"Tests for Request.dont_encode_url parameter\"\"\"\n+\n+    def test_url_encoding_default_behavior(self):\n+        \"\"\"Test that URL encoding is enabled by default\"\"\"\n+        # This URL should pass through safe_url_string\n+        url = \"http://example.com/page\"\n+        req = Request(url)\n+        # By default, safe_url_string is called\n+        assert req.url == url\n+        assert req.dont_encode_url is False\n+\n+    def test_url_with_slashes_in_query_params_default(self):\n+        \"\"\"Test that slashes in query params are preserved by default with safe_url_string\"\"\"\nComment: Nice, so why is this change needed?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/test_request_dont_encode_url.py",
    "pr_number": 7063,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2393602669,
    "comment_created_at": "2025-10-01T06:44:36Z"
  },
  {
    "code": "@@ -59,4 +59,17 @@ And follow the documentation_ to learn how to use it.\n \n If you wish to contribute, see Contributing_.\n \n+\n .. _Contributing: https://docs.scrapy.org/en/master/contributing.html\n+\n+Community & Support",
    "comment": "this duplicates https://docs.scrapy.org/en/latest/#getting-help",
    "line_number": 65,
    "enriched": "File: README.rst\nCode: @@ -59,4 +59,17 @@ And follow the documentation_ to learn how to use it.\n \n If you wish to contribute, see Contributing_.\n \n+\n .. _Contributing: https://docs.scrapy.org/en/master/contributing.html\n+\n+Community & Support\nComment: This duplicates https://docs.scrapy.org/en/latest/#getting-help",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "README.rst",
    "pr_number": 7053,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2360041187,
    "comment_created_at": "2025-09-18T15:56:59Z"
  },
  {
    "code": "@@ -135,6 +135,9 @@ branch = true\n include = [\"scrapy/*\"]\n omit = [\"tests/*\"]\n disable_warnings = [\"include-ignored\"]\n+patch = [",
    "comment": "https://pytest-cov.readthedocs.io/en/latest/subprocess-support.html",
    "line_number": 138,
    "enriched": "File: pyproject.toml\nCode: @@ -135,6 +135,9 @@ branch = true\n include = [\"scrapy/*\"]\n omit = [\"tests/*\"]\n disable_warnings = [\"include-ignored\"]\n+patch = [\nComment: https://pytest-cov.readthedocs.io/en/latest/subprocess-support.html",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "pyproject.toml",
    "pr_number": 7050,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2346882000,
    "comment_created_at": "2025-09-13T18:15:56Z"
  },
  {
    "code": "@@ -414,11 +414,11 @@ def test_crawl_multiple(self):\n             mockserver=self.mockserver,\n         )\n \n-        with LogCapture() as log:",
    "comment": "it looked liked logcapture() changes the root handler.",
    "line_number": 417,
    "enriched": "File: tests/test_crawl.py\nCode: @@ -414,11 +414,11 @@ def test_crawl_multiple(self):\n             mockserver=self.mockserver,\n         )\n \n-        with LogCapture() as log:\nComment: It looked liked `LogCapture()` changes the root handler.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "tests/test_crawl.py",
    "pr_number": 7046,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2331143909,
    "comment_created_at": "2025-09-08T19:13:50Z"
  },
  {
    "code": "@@ -28,9 +28,15 @@ def __init__(self, crawler: Crawler):\n         self._crawler: Crawler = crawler\n \n     def __getattribute__(self, name):\n+        cached_name = f\"_cached_{name}\"\n+        try:\n+            return super().__getattribute__(cached_name)",
    "comment": "til hasattr() \"is implemented by calling getattr(object, name) and seeing whether it raises an attributeerror or not\"",
    "line_number": 33,
    "enriched": "File: scrapy/statscollectors.py\nCode: @@ -28,9 +28,15 @@ def __init__(self, crawler: Crawler):\n         self._crawler: Crawler = crawler\n \n     def __getattribute__(self, name):\n+        cached_name = f\"_cached_{name}\"\n+        try:\n+            return super().__getattribute__(cached_name)\nComment: TIL `hasattr()` \"is implemented by calling `getattr(object, name)` and seeing whether it raises an `AttributeError` or not\"",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "scrapy/statscollectors.py",
    "pr_number": 7045,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2330033772,
    "comment_created_at": "2025-09-08T12:02:32Z"
  },
  {
    "code": "@@ -21,29 +22,34 @@\n \n \n class OffsiteMiddleware:\n+    crawler: Crawler\n+\n+    def __init__(self, stats: StatsCollector):\n+        self.stats = stats\n+        self.domains_seen: set[str] = set()\n+\n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         assert crawler.stats\n         o = cls(crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.request_scheduled, signal=signals.request_scheduled)\n+        o.crawler = crawler\n         return o\n \n-    def __init__(self, stats: StatsCollector):\n-        self.stats = stats\n-        self.domains_seen: set[str] = set()",
    "comment": "for the record, i kind of like the __init__ after the from_crawler, in order of execution :slightly_smiling_face:",
    "line_number": 34,
    "enriched": "File: scrapy/downloadermiddlewares/offsite.py\nCode: @@ -21,29 +22,34 @@\n \n \n class OffsiteMiddleware:\n+    crawler: Crawler\n+\n+    def __init__(self, stats: StatsCollector):\n+        self.stats = stats\n+        self.domains_seen: set[str] = set()\n+\n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         assert crawler.stats\n         o = cls(crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.request_scheduled, signal=signals.request_scheduled)\n+        o.crawler = crawler\n         return o\n \n-    def __init__(self, stats: StatsCollector):\n-        self.stats = stats\n-        self.domains_seen: set[str] = set()\nComment: For the record, I kind of like the `__init__` after the `from_crawler`, in order of execution :slightly_smiling_face:",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "scrapy/downloadermiddlewares/offsite.py",
    "pr_number": 7037,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2314576819,
    "comment_created_at": "2025-09-01T21:16:59Z"
  },
  {
    "code": "@@ -890,17 +890,29 @@ cdef class BlockValuesRefs: \"\"\" cdef: public list referenced_blocks + public int dead_counter + object __weakref__ + object _weakref_cb def __cinit__(self, blk: Block | None = None) -> None: + def _weakref_cb(item: weakref.ref, selfref: weakref.ref = weakref.ref(self)) -> None: + self = selfref() + if self is not None: + self.dead_counter += 1",
    "comment": "is this ever reachable without the gil? this doesn't appear thread safe so thinking through what impacts that might have",
    "line_number": 900,
    "enriched": "File: pandas/_libs/internals.pyx\nCode: @@ -890,17 +890,29 @@ cdef class BlockValuesRefs:\n     \"\"\"\n     cdef:\n         public list referenced_blocks\n+        public int dead_counter\n+        object __weakref__\n+        object _weakref_cb\n \n     def __cinit__(self, blk: Block | None = None) -> None:\n+        def _weakref_cb(item: weakref.ref, selfref: weakref.ref = weakref.ref(self)) -> None:\n+            self = selfref()\n+            if self is not None:\n+                self.dead_counter += 1\nComment: Is this ever reachable without the GIL? This doesn't appear thread safe so thinking through what impacts that might have",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "pandas/_libs/internals.pyx",
    "pr_number": 55539,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1360553423,
    "comment_created_at": "2023-10-16T12:03:53Z"
  },
  {
    "code": "@@ -4,17 +4,14 @@ from scrapy.spiders import Spider from scrapy.utils.defer import deferred_from_coro from scrapy.utils.misc import arg_to_iter -try: - from scrapy.utils.py36 import collect_asyncgen -except SyntaxError: - collect_asyncgen = None +from scrapy.utils.python import collect_asyncgen logger = logging.getLogger(__name__) def iterate_spider_output(result): - if collect_asyncgen and hasattr(inspect, 'isasyncgen') and inspect.isasyncgen(result):",
    "comment": "[inspect.isasyncgen](https://docs.python.org/3.6/library/inspect.html#inspect.isasyncgen) was added in python 3.6",
    "line_number": 17,
    "enriched": "File: scrapy/utils/spider.py\nCode: @@ -4,17 +4,14 @@\n from scrapy.spiders import Spider\n from scrapy.utils.defer import deferred_from_coro\n from scrapy.utils.misc import arg_to_iter\n-try:\n-    from scrapy.utils.py36 import collect_asyncgen\n-except SyntaxError:\n-    collect_asyncgen = None\n+from scrapy.utils.python import collect_asyncgen\n \n \n logger = logging.getLogger(__name__)\n \n \n def iterate_spider_output(result):\n-    if collect_asyncgen and hasattr(inspect, 'isasyncgen') and inspect.isasyncgen(result):\nComment: [`inspect.isasyncgen`](https://docs.python.org/3.6/library/inspect.html#inspect.isasyncgen) was added in Python 3.6",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "scrapy/utils/spider.py",
    "pr_number": 4900,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 528767349,
    "comment_created_at": "2020-11-23T15:03:17Z"
  },
  {
    "code": "@@ -26,6 +26,20 @@ import setupFormatters from './setup/setupFormatters'; import setupDashboardComponents from './setup/setupDashboardComponents'; import { User } from './types/bootstrapTypes'; import getBootstrapData, { applicationRoot } from './utils/getBootstrapData'; +import 'dayjs/locale/en'; +import 'dayjs/locale/fr'; +import 'dayjs/locale/es'; +import 'dayjs/locale/it'; +import 'dayjs/locale/zh-cn'; +import 'dayjs/locale/ja'; +import 'dayjs/locale/de'; +import 'dayjs/locale/pt'; +import 'dayjs/locale/pt-br'; +import 'dayjs/locale/ru'; +import 'dayjs/locale/ko'; +import 'dayjs/locale/sk'; +import 'dayjs/locale/sl'; +import 'dayjs/locale/nl';",
    "comment": "### inefficient static locale imports <sub>![category design](https://img.shields.io/badge/design-0d9488)</sub>\n\n<details>\n  <summary>tell me more</summary>\n\n###### what is the issue?\nall locale imports are statically imported, leading to unnecessary bundle size increase even when locales aren't used.\n\n\n###### why this matters\nthis approach loads all locale data into the bundle regardless of which locales are actually needed by the application, impacting initial load performance.\n\n###### suggested change  *feature preview*\nimplement dynamic imports for locales based on the user's selected language:\ntypescript\nasync function loadlocale(locale: string) {\n  await import(dayjs/locale/${locale});\n  dayjs.locale(locale);\n}\n\n\n\n###### provide feedback to improve future suggestions\n[![nice catch](https://img.shields.io/badge/%20nice%20catch-71bc78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e7d7632-0faf-4958-8a0a-d4c3ecd4d162/upvote) [![incorrect](https://img.shields.io/badge/%20incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e7d7632-0faf-4958-8a0a-d4c3ecd4d162?what_not_true=true)  [![not in scope](https://img.shields.io/badge/%20out%20of%20pr%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e7d7632-0faf-4958-8a0a-d4c3ecd4d162?what_out_of_scope=true) [![not in coding standard](https://img.shields.io/badge/%20not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e7d7632-0faf-4958-8a0a-d4c3ecd4d162?what_not_in_standard=true) [![other](https://img.shields.io/badge/%20other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e7d7632-0faf-4958-8a0a-d4c3ecd4d162)\n</details>\n\n<sub>\n\n looking for more details? reply to this comment to chat with korbit.\n</sub>\n\n<!--- korbi internal id:09a98c94-191f-4e09-850c-0d5fc57c0cd6 -->\n\n\n[](09a98c94-191f-4e09-850c-0d5fc57c0cd6)",
    "line_number": 42,
    "enriched": "File: superset-frontend/src/preamble.ts\nCode: @@ -26,6 +26,20 @@ import setupFormatters from './setup/setupFormatters';\n import setupDashboardComponents from './setup/setupDashboardComponents';\n import { User } from './types/bootstrapTypes';\n import getBootstrapData, { applicationRoot } from './utils/getBootstrapData';\n+import 'dayjs/locale/en';\n+import 'dayjs/locale/fr';\n+import 'dayjs/locale/es';\n+import 'dayjs/locale/it';\n+import 'dayjs/locale/zh-cn';\n+import 'dayjs/locale/ja';\n+import 'dayjs/locale/de';\n+import 'dayjs/locale/pt';\n+import 'dayjs/locale/pt-br';\n+import 'dayjs/locale/ru';\n+import 'dayjs/locale/ko';\n+import 'dayjs/locale/sk';\n+import 'dayjs/locale/sl';\n+import 'dayjs/locale/nl';\nComment: ### Inefficient Static Locale Imports <sub>![category Design](https://img.shields.io/badge/Design-0d9488)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nAll locale imports are statically imported, leading to unnecessary bundle size increase even when locales aren't used.\n\n\n###### Why this matters\nThis approach loads all locale data into the bundle regardless of which locales are actually needed by the application, impacting initial load performance.\n\n###### Suggested change  *Feature Preview*\nImplement dynamic imports for locales based on the user's selected language:\n```typescript\nasync function loadLocale(locale: string) {\n  await import(`dayjs/locale/${locale}`);\n  dayjs.locale(locale);\n}\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e7d7632-0faf-4958-8a0a-d4c3ecd4d162/upvote) [![Incorrect](https://img.shields.io/badge/%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e7d7632-0faf-4958-8a0a-d4c3ecd4d162?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e7d7632-0faf-4958-8a0a-d4c3ecd4d162?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e7d7632-0faf-4958-8a0a-d4c3ecd4d162?what_not_in_standard=true) [![Other](https://img.shields.io/badge/%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e7d7632-0faf-4958-8a0a-d4c3ecd4d162)\n</details>\n\n<sub>\n\n Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:09a98c94-191f-4e09-850c-0d5fc57c0cd6 -->\n\n\n[](09a98c94-191f-4e09-850c-0d5fc57c0cd6)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "superset-frontend/src/preamble.ts",
    "pr_number": 34051,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 2182422513,
    "comment_created_at": "2025-07-03T10:21:51Z"
  },
   {
    "code": "@@ -52,40 +52,35 @@ repos: - id: trailing-whitespace exclude: ^.*\\.(snap) args: [\"--markdown-linebreak-ext=md\"] - - repo: local + - repo: https://github.com/pre-commit/mirrors-prettier + rev: v4.0.0-alpha.8 # Use the sha or tag you want to point at",
    "comment": "we're using 3.3.3 on the repo. i wonder if that'll lead to trouble.",
    "line_number": 56,
    "enriched": "File: .pre-commit-config.yaml\nCode: @@ -52,40 +52,35 @@ repos:\n       - id: trailing-whitespace\n         exclude: ^.*\\.(snap)\n         args: [\"--markdown-linebreak-ext=md\"]\n-  - repo: local\n+  - repo: https://github.com/pre-commit/mirrors-prettier\n+    rev: v4.0.0-alpha.8 # Use the sha or tag you want to point at\nComment: We're using 3.3.3 on the repo. I wonder if that'll lead to trouble.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": ".pre-commit-config.yaml",
    "pr_number": 32596,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1990056790,
    "comment_created_at": "2025-03-11T20:06:37Z"
  },
  {
    "code": "@@ -87,11 +90,9 @@ def get_cache_backend( if cache_type == \"RedisSentinelCache\": return RedisSentinelCacheBackend.from_config(cache_config) - # TODO: Deprecate hardcoded plain Redis code and expand cache backend options. - # Maintain backward compatibility with 'GLOBAL_ASYNC_QUERIES_REDIS_CONFIG' until it is deprecated. # noqa: E501 - return redis.Redis( - **config[\"GLOBAL_ASYNC_QUERIES_REDIS_CONFIG\"], decode_responses=True - ) + # TODO: Expand cache backend options. + # Removed support for deprecated 'GLOBAL_ASYNC_QUERIES_REDIS_CONFIG' as it is no longer needed. # noqa: E501",
    "comment": "this line is redundant",
    "line_number": 94,
    "enriched": "File: superset/async_events/async_query_manager.py\nCode: @@ -87,11 +90,9 @@ def get_cache_backend(\n     if cache_type == \"RedisSentinelCache\":\n         return RedisSentinelCacheBackend.from_config(cache_config)\n \n-    # TODO: Deprecate hardcoded plain Redis code and expand cache backend options.\n-    # Maintain backward compatibility with 'GLOBAL_ASYNC_QUERIES_REDIS_CONFIG' until it is deprecated.  # noqa: E501\n-    return redis.Redis(\n-        **config[\"GLOBAL_ASYNC_QUERIES_REDIS_CONFIG\"], decode_responses=True\n-    )\n+    # TODO: Expand cache backend options.\n+    # Removed support for deprecated 'GLOBAL_ASYNC_QUERIES_REDIS_CONFIG' as it is no longer needed.  # noqa: E501\nComment: This line is redundant\r\n```suggestion\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "superset/async_events/async_query_manager.py",
    "pr_number": 30284,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1920468675,
    "comment_created_at": "2025-01-17T16:46:50Z"
  },
  {
    "code": "@@ -0,0 +1,88 @@ +#define _UMATHMODULE +#define _MULTIARRAYMODULE +#define NPY_NO_DEPRECATED_API NPY_API_VERSION + +#define PY_SSIZE_T_CLEAN +#include <Python.h> + +#undef HWY_TARGET_INCLUDE +#define HWY_TARGET_INCLUDE \"absolute.cpp\" // this file +#include <hwy/foreach_target.h> // must come before highway.h +#include <hwy/highway.h> + +#include \"numpy/ndarraytypes.h\" +#include \"numpy/npy_common.h\" +#include \"numpy/npy_math.h\" +#include \"numpy/utils.h\" + +namespace numpy { +namespace HWY_NAMESPACE { // required: unique per target + +// Can skip hn:: prefixes if already inside hwy::HWY_NAMESPACE. +namespace hn = hwy::HWY_NAMESPACE; + +// Alternative to per-function HWY_ATTR: see HWY_BEFORE_NAMESPACE +template <typename T> +HWY_ATTR void SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) { + const T* HWY_RESTRICT input_array = (const T*) args[0]; + T* HWY_RESTRICT output_array = (T*) args[1]; + const size_t size = dimensions[0]; + const hn::ScalableTag<T> d; + + for (size_t i = 0; i < size; i += hn::Lanes(d)) { + const auto in = hn::Load(d, input_array + i); + auto x = hn::Abs(in); + hn::Store(x, d, output_array + i); + } +} + +HWY_ATTR void INT_SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) { + SuperAbsolute<npy_int>(args, dimensions, steps); +} + +HWY_ATTR void DOUBLE_SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) { + SuperAbsolute<npy_double>(args, dimensions, steps); +} + +HWY_ATTR void FLOAT_SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) { + SuperAbsolute<npy_float>(args, dimensions, steps); +} + +} +} + +#if HWY_ONCE +namespace numpy { + +HWY_EXPORT(INT_SuperAbsolute); +HWY_EXPORT(FLOAT_SuperAbsolute); +HWY_EXPORT(DOUBLE_SuperAbsolute); + +extern \"C\" { + +NPY_NO_EXPORT void +INT_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)) +{ + static auto dispatcher = HWY_DYNAMIC_DISPATCH(INT_SuperAbsolute); + return dispatcher(args, dimensions, steps); +} + +NPY_NO_EXPORT void +DOUBLE_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)) +{ + static auto dispatcher = HWY_DYNAMIC_DISPATCH(DOUBLE_SuperAbsolute); + return dispatcher(args, dimensions, steps); +} + +NPY_NO_EXPORT void +FLOAT_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)) +{ + static auto dispatcher = HWY_DYNAMIC_DISPATCH(FLOAT_SuperAbsolute);",
    "comment": "looks simple but i'm afraid of c++ evilness may involved since static initialization requires thread safety so this call usually will be warped in between a thread guard which make it actually slower than uses local variable if highway cached cpuid calls similar to what numpy does.\r\nhere a pesudo code:\r\npython\r\nstatic the_dudced_type dispatcher;\r\nstatic bool dispatcher_once = false\r\ncall cx_gaurd_lock\r\nif not dispatcher_once\r\n   dispatcher = hwy_dynamic_dispatch(float_superabsolute)\r\n   dispatcher_once = true\r\nendif\r\ncall cx_gaurd_unlock",
    "line_number": 80,
    "enriched": "File: numpy/core/src/umath/absolute.cpp\nCode: @@ -0,0 +1,88 @@\n+#define _UMATHMODULE\n+#define _MULTIARRAYMODULE\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+\n+#define PY_SSIZE_T_CLEAN\n+#include <Python.h>\n+\n+#undef HWY_TARGET_INCLUDE\n+#define HWY_TARGET_INCLUDE \"absolute.cpp\"  // this file\n+#include <hwy/foreach_target.h>  // must come before highway.h\n+#include <hwy/highway.h>\n+\n+#include \"numpy/ndarraytypes.h\"\n+#include \"numpy/npy_common.h\"\n+#include \"numpy/npy_math.h\"\n+#include \"numpy/utils.h\"\n+\n+namespace numpy {\n+namespace HWY_NAMESPACE {  // required: unique per target\n+\n+// Can skip hn:: prefixes if already inside hwy::HWY_NAMESPACE.\n+namespace hn = hwy::HWY_NAMESPACE;\n+\n+// Alternative to per-function HWY_ATTR: see HWY_BEFORE_NAMESPACE\n+template <typename T>\n+HWY_ATTR void SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) {\n+  const T* HWY_RESTRICT input_array = (const T*) args[0];\n+  T* HWY_RESTRICT output_array = (T*) args[1];\n+  const size_t size = dimensions[0];\n+  const hn::ScalableTag<T> d;\n+  \n+  for (size_t i = 0; i < size; i += hn::Lanes(d)) {\n+    const auto in = hn::Load(d, input_array + i);\n+    auto x = hn::Abs(in);\n+    hn::Store(x, d, output_array + i);\n+  }\n+}\n+\n+HWY_ATTR void INT_SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) {\n+  SuperAbsolute<npy_int>(args, dimensions, steps);\n+}\n+\n+HWY_ATTR void DOUBLE_SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) {\n+  SuperAbsolute<npy_double>(args, dimensions, steps);\n+}\n+\n+HWY_ATTR void FLOAT_SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) {\n+  SuperAbsolute<npy_float>(args, dimensions, steps);\n+}\n+\n+}\n+}\n+\n+#if HWY_ONCE\n+namespace numpy {\n+\n+HWY_EXPORT(INT_SuperAbsolute);\n+HWY_EXPORT(FLOAT_SuperAbsolute);\n+HWY_EXPORT(DOUBLE_SuperAbsolute);\n+\n+extern \"C\" {\n+\n+NPY_NO_EXPORT void\n+INT_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+  static auto dispatcher = HWY_DYNAMIC_DISPATCH(INT_SuperAbsolute);\n+  return dispatcher(args, dimensions, steps);\n+}\n+\n+NPY_NO_EXPORT void\n+DOUBLE_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+  static auto dispatcher = HWY_DYNAMIC_DISPATCH(DOUBLE_SuperAbsolute);\n+  return dispatcher(args, dimensions, steps);\n+}\n+\n+NPY_NO_EXPORT void\n+FLOAT_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+  static auto dispatcher = HWY_DYNAMIC_DISPATCH(FLOAT_SuperAbsolute);\nComment: ```suggestion\r\n  auto dispatcher = HWY_DYNAMIC_DISPATCH(FLOAT_SuperAbsolute);\r\n```\r\nLooks simple but I'm afraid of C++ evilness may involved since static initialization requires thread safety so this call usually will be warped in between a thread guard which make it actually slower than uses local variable if Highway cached CPUID calls similar to what NumPy does.\r\nHere a pesudo code:\r\n```Python\r\nstatic the_dudced_type dispatcher;\r\nstatic bool dispatcher_once = false\r\ncall cx_gaurd_lock\r\nif not dispatcher_once\r\n   dispatcher = HWY_DYNAMIC_DISPATCH(FLOAT_SuperAbsolute)\r\n   dispatcher_once = true\r\nendif\r\ncall cx_gaurd_unlock\r\n```\r\n",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "numpy/core/src/umath/absolute.cpp",
    "pr_number": 24384,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 1292330069,
    "comment_created_at": "2023-08-12T14:32:47Z"
  },
  {
    "code": "@@ -947,6 +944,13 @@ loaders that come with Django: information, see :ref:`template tag thread safety considerations <template_tag_thread_safety>`. +",
    "comment": "chop blank line",
    "line_number": 947,
    "enriched": "File: docs/ref/templates/api.txt\nCode: @@ -947,6 +944,13 @@ loaders that come with Django:\n         information, see :ref:`template tag thread safety considerations\n         <template_tag_thread_safety>`.\n \n+\nComment: Chop blank line\r\n```suggestion\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/ref/templates/api.txt",
    "pr_number": 15586,
    "repo": "django",
    "owner": "django",
    "comment_id": 849297772,
    "comment_created_at": "2022-04-13T09:51:06Z"
  },
  {
    "code": "@@ -65,3 +66,68 @@ def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n         return deferToThread(func, *a, **kw)\n \n     return wrapped\n+\n+\n+@overload\n+def _warn_spider_arg(\n+    func: Callable[_P, Coroutine[Any, Any, _T]],\n+) -> Callable[_P, Coroutine[Any, Any, _T]]: ...\n+\n+\n+@overload\n+def _warn_spider_arg(\n+    func: Callable[_P, AsyncGenerator[_T]],\n+) -> Callable[_P, AsyncGenerator[_T]]: ...\n+\n+\n+@overload\n+def _warn_spider_arg(func: Callable[_P, _T]) -> Callable[_P, _T]: ...\n+\n+\n+def _warn_spider_arg(\n+    func: Callable[_P, _T],\n+) -> (\n+    Callable[_P, _T]\n+    | Callable[_P, Coroutine[Any, Any, _T]]\n+    | Callable[_P, AsyncGenerator[_T]]\n+):\n+    \"\"\"Decorator to warn if a (non-None) ``spider`` argument is passed to a function.\"\"\"\n+\n+    def check_args(*args: _P.args, **kwargs: _P.kwargs) -> None:\n+        bound = inspect.signature(func).bind(*args, **kwargs)\n+        bound.apply_defaults()\n+        if bound.arguments.get(\"spider\"):\n+            warnings.warn(\n+                f\"Passing a 'spider' argument to {func.__qualname__}() is deprecated and \"\n+                \"the argument will be removed in a future Scrapy version.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=3,\n+            )\n+\n+    if inspect.iscoroutinefunction(func):",
    "comment": "ok, this is not used/covered because the only async functions that are decorated are process_spider_output_async() which are async generator functions.",
    "line_number": 107,
    "enriched": "File: scrapy/utils/decorators.py\nCode: @@ -65,3 +66,68 @@ def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n         return deferToThread(func, *a, **kw)\n \n     return wrapped\n+\n+\n+@overload\n+def _warn_spider_arg(\n+    func: Callable[_P, Coroutine[Any, Any, _T]],\n+) -> Callable[_P, Coroutine[Any, Any, _T]]: ...\n+\n+\n+@overload\n+def _warn_spider_arg(\n+    func: Callable[_P, AsyncGenerator[_T]],\n+) -> Callable[_P, AsyncGenerator[_T]]: ...\n+\n+\n+@overload\n+def _warn_spider_arg(func: Callable[_P, _T]) -> Callable[_P, _T]: ...\n+\n+\n+def _warn_spider_arg(\n+    func: Callable[_P, _T],\n+) -> (\n+    Callable[_P, _T]\n+    | Callable[_P, Coroutine[Any, Any, _T]]\n+    | Callable[_P, AsyncGenerator[_T]]\n+):\n+    \"\"\"Decorator to warn if a (non-None) ``spider`` argument is passed to a function.\"\"\"\n+\n+    def check_args(*args: _P.args, **kwargs: _P.kwargs) -> None:\n+        bound = inspect.signature(func).bind(*args, **kwargs)\n+        bound.apply_defaults()\n+        if bound.arguments.get(\"spider\"):\n+            warnings.warn(\n+                f\"Passing a 'spider' argument to {func.__qualname__}() is deprecated and \"\n+                \"the argument will be removed in a future Scrapy version.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=3,\n+            )\n+\n+    if inspect.iscoroutinefunction(func):\nComment: OK, this is not used/covered because the only async functions that are decorated are `process_spider_output_async()` which are async generator functions.",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "scrapy/utils/decorators.py",
    "pr_number": 7033,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2310684726,
    "comment_created_at": "2025-08-29T17:07:25Z"
  },
  {
    "code": "@@ -45,4 +55,38 @@ def process_item(self, item: Any, spider: Spider | None = None) -> Deferred[Any]\n         return deferred_from_coro(self.process_item_async(item))\n \n     async def process_item_async(self, item: Any) -> Any:\n-        return await self._process_chain(\"process_item\", item, self._spider)\n+        return await self._process_chain(\"process_item\", item, add_spider=True)\n+\n+    def _process_parallel(\n+        self, methodname: str, add_spider: bool = False",
    "comment": "what\u2019s the point of add_spider? it seems to be always true at the moment, and the method is private.",
    "line_number": 61,
    "enriched": "File: scrapy/pipelines/__init__.py\nCode: @@ -45,4 +55,38 @@ def process_item(self, item: Any, spider: Spider | None = None) -> Deferred[Any]\n         return deferred_from_coro(self.process_item_async(item))\n \n     async def process_item_async(self, item: Any) -> Any:\n-        return await self._process_chain(\"process_item\", item, self._spider)\n+        return await self._process_chain(\"process_item\", item, add_spider=True)\n+\n+    def _process_parallel(\n+        self, methodname: str, add_spider: bool = False\nComment: What\u2019s the point of `add_spider`? It seems to be always `True` at the moment, and the method is private.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "scrapy/pipelines/__init__.py",
    "pr_number": 7006,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2289042365,
    "comment_created_at": "2025-08-20T19:04:50Z"
  },
  {
    "code": "@@ -109,12 +110,38 @@ def __init__(self, crawler: Crawler) -> None:\n             crawler.settings[\"ITEM_PROCESSOR\"]\n         )\n         self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)\n-        self._itemproc_needs_spider: dict[str, bool] = {}\n-        for method in (\n+        itemproc_methods = [\n             \"open_spider\",\n             \"close_spider\",\n-            \"process_item\",\n+        ]\n+        if not hasattr(self.itemproc, \"process_item_async\"):\n+            warnings.warn(\n+                f\"{global_object_name(itemproc_cls)} doesn't define a process_item_async() method,\"\n+                f\" this is deprecated and the method will be required in the future Scrapy versions.\",\n+                ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+            itemproc_methods.append(\"process_item\")\n+            self._itemproc_has_process_async = False\n+        elif (\n+            issubclass(itemproc_cls, ItemPipelineManager)\n+            and method_is_overridden(itemproc_cls, ItemPipelineManager, \"process_item\")\n+            and not method_is_overridden(\n+                itemproc_cls, ItemPipelineManager, \"process_item_async\"\n+            )\n         ):\n+            warnings.warn(\n+                f\"{global_object_name(itemproc_cls)} overrides process_item() but doesn't override process_item_async().\"\n+                f\" This is deprecated. process_item() will be used, but in the future Scrapy versions process_item_async() will be used instead.\",",
    "comment": "(applies to similar strings as well)",
    "line_number": 135,
    "enriched": "File: scrapy/core/scraper.py\nCode: @@ -109,12 +110,38 @@ def __init__(self, crawler: Crawler) -> None:\n             crawler.settings[\"ITEM_PROCESSOR\"]\n         )\n         self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)\n-        self._itemproc_needs_spider: dict[str, bool] = {}\n-        for method in (\n+        itemproc_methods = [\n             \"open_spider\",\n             \"close_spider\",\n-            \"process_item\",\n+        ]\n+        if not hasattr(self.itemproc, \"process_item_async\"):\n+            warnings.warn(\n+                f\"{global_object_name(itemproc_cls)} doesn't define a process_item_async() method,\"\n+                f\" this is deprecated and the method will be required in the future Scrapy versions.\",\n+                ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+            itemproc_methods.append(\"process_item\")\n+            self._itemproc_has_process_async = False\n+        elif (\n+            issubclass(itemproc_cls, ItemPipelineManager)\n+            and method_is_overridden(itemproc_cls, ItemPipelineManager, \"process_item\")\n+            and not method_is_overridden(\n+                itemproc_cls, ItemPipelineManager, \"process_item_async\"\n+            )\n         ):\n+            warnings.warn(\n+                f\"{global_object_name(itemproc_cls)} overrides process_item() but doesn't override process_item_async().\"\n+                f\" This is deprecated. process_item() will be used, but in the future Scrapy versions process_item_async() will be used instead.\",\nComment: ```suggestion\r\n                f\" This is deprecated. process_item() will be used, but in future Scrapy versions process_item_async() will be used instead.\",\r\n```\r\n\r\n(applies to similar strings as well)",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "scrapy/core/scraper.py",
    "pr_number": 7005,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2269044848,
    "comment_created_at": "2025-08-12T08:06:32Z"
  },
  {
    "code": "@@ -84,7 +85,7 @@ def run(self, args: list[str], opts: Namespace) -> None:\n         crawler._apply_settings()\n         # The Shell class needs a persistent engine in the crawler\n         crawler.engine = crawler._create_engine()\n-        crawler.engine.start(_start_request_processing=False)\n+        deferred_from_coro(crawler.engine.start_async(_start_request_processing=False))",
    "comment": "deferred_from_coro() here and in some other places is used as a shortcut for \"if using asyncio, create a task for the coro\", as just calling the coro wouldn't schedule it; we could add a private helper function instead, or change all these places to keep the reference to the returned deferred so that the code doesn't look weird (and to add exception handling)",
    "line_number": 88,
    "enriched": "File: scrapy/commands/shell.py\nCode: @@ -84,7 +85,7 @@ def run(self, args: list[str], opts: Namespace) -> None:\n         crawler._apply_settings()\n         # The Shell class needs a persistent engine in the crawler\n         crawler.engine = crawler._create_engine()\n-        crawler.engine.start(_start_request_processing=False)\n+        deferred_from_coro(crawler.engine.start_async(_start_request_processing=False))\nComment: `deferred_from_coro()` here and in some other places is used as a shortcut for \"if using asyncio, create a task for the coro\", as just calling the coro wouldn't schedule it; we could add a private helper function instead, or change all these places to keep the reference to the returned deferred so that the code doesn't look weird (and to add exception handling)",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "scrapy/commands/shell.py",
    "pr_number": 6979,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2243709659,
    "comment_created_at": "2025-07-30T19:48:37Z"
  },
  {
    "code": "@@ -802,3 +804,20 @@ def from_crawler(cls, crawler):\n             assert len(w) == 0\n             assert pipe.store\n             assert pipe._from_crawler_called\n+\n+\n+@pytest.mark.parametrize(\"store\", [None, \"\"])\n+def test_files_pipeline_raises_notconfigured_when_files_store_invalid(\n+    monkeypatch, store\n+):\n+    settings = Settings()\n+\n+    if store is None:\n+        monkeypatch.delenv(\"IMAGES_STORE\", raising=False)\n+    else:\n+        monkeypatch.setenv(\"IMAGES_STORE\", store)",
    "comment": "why is this needed?",
    "line_number": 818,
    "enriched": "File: tests/test_pipeline_files.py\nCode: @@ -802,3 +804,20 @@ def from_crawler(cls, crawler):\n             assert len(w) == 0\n             assert pipe.store\n             assert pipe._from_crawler_called\n+\n+\n+@pytest.mark.parametrize(\"store\", [None, \"\"])\n+def test_files_pipeline_raises_notconfigured_when_files_store_invalid(\n+    monkeypatch, store\n+):\n+    settings = Settings()\n+\n+    if store is None:\n+        monkeypatch.delenv(\"IMAGES_STORE\", raising=False)\n+    else:\n+        monkeypatch.setenv(\"IMAGES_STORE\", store)\nComment: Why is this needed?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/test_pipeline_files.py",
    "pr_number": 6969,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2236173691,
    "comment_created_at": "2025-07-28T12:20:44Z"
  },
  {
    "code": "@@ -48,6 +55,12 @@ def _py_files(folder):\n     )\n \n \n+@pytest.fixture(scope=\"session\")\n+def mockserver() -> Generator[MockServer]:",
    "comment": "til you can skip the 2nd and 3rd param of generator :facepalm:",
    "line_number": 59,
    "enriched": "File: conftest.py\nCode: @@ -48,6 +55,12 @@ def _py_files(folder):\n     )\n \n \n+@pytest.fixture(scope=\"session\")\n+def mockserver() -> Generator[MockServer]:\nComment: TIL you can skip the 2nd and 3rd param of `Generator` :facepalm: ",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "conftest.py",
    "pr_number": 6960,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2230361719,
    "comment_created_at": "2025-07-25T07:22:47Z"
  },
  {
    "code": "@@ -91,12 +93,12 @@ def test_twistederrors(self):\n             ConnectError,\n             ConnectionDone,\n             ConnectionLost,\n-            ConnectionRefusedError,\n+            TxConnectionRefusedError,\n             defer.TimeoutError,\n             DNSLookupError,\n             ResponseFailed,\n             TCPTimedOutError,\n-            TimeoutError,\n+            TxTimeoutError,",
    "comment": "this checked builtin exceptions instead of twisted ones, the test was passing because the builtin exceptions are subclasses of oserror which is retried.",
    "line_number": 101,
    "enriched": "File: tests/test_downloadermiddleware_retry.py\nCode: @@ -91,12 +93,12 @@ def test_twistederrors(self):\n             ConnectError,\n             ConnectionDone,\n             ConnectionLost,\n-            ConnectionRefusedError,\n+            TxConnectionRefusedError,\n             defer.TimeoutError,\n             DNSLookupError,\n             ResponseFailed,\n             TCPTimedOutError,\n-            TimeoutError,\n+            TxTimeoutError,\nComment: This checked builtin exceptions instead of Twisted ones, the test was passing because the builtin exceptions are subclasses of OSError which is retried.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "tests/test_downloadermiddleware_retry.py",
    "pr_number": 6942,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2187298147,
    "comment_created_at": "2025-07-05T13:53:02Z"
  },
  {
    "code": "@@ -394,6 +390,9 @@ banned-module-level-imports = [\n     \"twisted.internet.reactor\",\n ]\n \n+[tool.ruff.lint.isort]\n+split-on-trailing-comma = false",
    "comment": "this forces short multiline imports to be folded back which i recently noticed to not be done by default. there is[ a similar option for all other comma-separated language things](https://docs.astral.sh/ruff/settings/#format_skip-magic-trailing-comma), but the diff without that one is much bigger and i think more controversial, i need to think on what do we want to do.",
    "line_number": 394,
    "enriched": "File: pyproject.toml\nCode: @@ -394,6 +390,9 @@ banned-module-level-imports = [\n     \"twisted.internet.reactor\",\n ]\n \n+[tool.ruff.lint.isort]\n+split-on-trailing-comma = false\nComment: This forces short multiline imports to be folded back which I recently noticed to not be done by default. There is[ a similar option for all other comma-separated language things](https://docs.astral.sh/ruff/settings/#format_skip-magic-trailing-comma), but the diff without that one is much bigger and I think more controversial, I need to think on what do we want to do.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "pyproject.toml",
    "pr_number": 6941,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2186943055,
    "comment_created_at": "2025-07-05T07:57:27Z"
  },
  {
    "code": "@@ -218,96 +214,98 @@ def setUp(self):\n         r.putChild(b\"payload\", PayloadResource())\n         r.putChild(b\"broken\", BrokenDownloadResource())\n         r.putChild(b\"encoding\", EncodingResource())\n-        self.site = server.Site(r, timeout=None)\n-        self.wrapper = WrappingFactory(self.site)\n-        self.port = self._listen(self.wrapper)\n-        self.portno = self.port.getHost().port\n+        site = server.Site(r, timeout=None)\n+        return WrappingFactory(site)\n \n-    @inlineCallbacks\n-    def tearDown(self):\n-        yield self.port.stopListening()\n-        shutil.rmtree(self.tmpname)\n+    @async_yield_fixture\n+    async def server_port(self, wrapper):\n+        port = self._listen(wrapper)\n+\n+        yield port.getHost().port\n+\n+        await port.stopListening()\n \n-    def getURL(self, path):\n-        return f\"http://127.0.0.1:{self.portno}/{path}\"\n+    @pytest.fixture\n+    def server_url(self, server_port):\n+        return f\"http://127.0.0.1:{server_port}/\"\n \n     @inlineCallbacks\n-    def testPayload(self):\n+    def testPayload(self, server_url):\n         s = \"0123456789\" * 10\n-        body = yield getPage(self.getURL(\"payload\"), body=s)\n+        body = yield getPage(server_url + \"payload\", body=s)\n         assert body == to_bytes(s)\n \n     @inlineCallbacks\n-    def testHostHeader(self):\n+    def testHostHeader(self, server_port, server_url):\n         # if we pass Host header explicitly, it should be used, otherwise\n         # it should extract from url\n-        body = yield getPage(self.getURL(\"host\"))\n-        assert body == to_bytes(f\"127.0.0.1:{self.portno}\")\n-        body = yield getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"})\n+        body = yield getPage(server_url + \"host\")\n+        assert body == to_bytes(f\"127.0.0.1:{server_port}\")\n+        body = yield getPage(server_url + \"host\", headers={\"Host\": \"www.example.com\"})\n         assert body == to_bytes(\"www.example.com\")\n \n     @inlineCallbacks\n-    def test_getPage(self):\n+    def test_getPage(self, server_url):\n         \"\"\"\n         L{client.getPage} returns a L{Deferred} which is called back with\n         the body of the response if the default method B{GET} is used.\n         \"\"\"\n-        body = yield getPage(self.getURL(\"file\"))\n+        body = yield getPage(server_url + \"file\")\n         assert body == b\"0123456789\"\n \n     @inlineCallbacks\n-    def test_getPageHead(self):\n+    def test_getPageHead(self, server_url):\n         \"\"\"\n         L{client.getPage} returns a L{Deferred} which is called back with\n         the empty string if the method is C{HEAD} and there is a successful\n         response code.\n         \"\"\"\n \n         def _getPage(method):\n-            return getPage(self.getURL(\"file\"), method=method)\n+            return getPage(server_url + \"file\", method=method)\n \n         body = yield _getPage(\"head\")\n         assert body == b\"\"\n         body = yield _getPage(\"HEAD\")\n         assert body == b\"\"\n \n     @inlineCallbacks\n-    def test_timeoutNotTriggering(self):\n+    def test_timeoutNotTriggering(self, server_port, server_url):\n         \"\"\"\n         When a non-zero timeout is passed to L{getPage} and the page is\n         retrieved before the timeout period elapses, the L{Deferred} is\n         called back with the contents of the page.\n         \"\"\"\n-        body = yield getPage(self.getURL(\"host\"), timeout=100)\n-        assert body == to_bytes(f\"127.0.0.1:{self.portno}\")\n+        body = yield getPage(server_url + \"host\", timeout=100)\n+        assert body == to_bytes(f\"127.0.0.1:{server_port}\")\n \n     @inlineCallbacks\n-    def test_timeoutTriggering(self):\n+    def test_timeoutTriggering(self, wrapper, server_url):\n         \"\"\"\n         When a non-zero timeout is passed to L{getPage} and that many\n         seconds elapse before the server responds to the request. the\n         L{Deferred} is errbacked with a L{error.TimeoutError}.\n         \"\"\"\n         with pytest.raises(defer.TimeoutError):\n-            yield getPage(self.getURL(\"wait\"), timeout=0.000001)\n+            yield getPage(server_url + \"wait\", timeout=0.000001)\n         # Clean up the server which is hanging around not doing\n         # anything.\n-        connected = list(self.wrapper.protocols.keys())\n+        connected = list(wrapper.protocols.keys())\n         # There might be nothing here if the server managed to already see\n         # that the connection was lost.\n         if connected:\n             connected[0].transport.loseConnection()\n \n     @inlineCallbacks",
    "comment": "this test uses yield getpage(...) but is not decorated with @inlinecallbacks, so the deferred won\u2019t be awaited. add @inlinecallbacks above the method or convert it to async def and use await.",
    "line_number": 299,
    "enriched": "File: tests/test_webclient.py\nCode: @@ -218,96 +214,98 @@ def setUp(self):\n         r.putChild(b\"payload\", PayloadResource())\n         r.putChild(b\"broken\", BrokenDownloadResource())\n         r.putChild(b\"encoding\", EncodingResource())\n-        self.site = server.Site(r, timeout=None)\n-        self.wrapper = WrappingFactory(self.site)\n-        self.port = self._listen(self.wrapper)\n-        self.portno = self.port.getHost().port\n+        site = server.Site(r, timeout=None)\n+        return WrappingFactory(site)\n \n-    @inlineCallbacks\n-    def tearDown(self):\n-        yield self.port.stopListening()\n-        shutil.rmtree(self.tmpname)\n+    @async_yield_fixture\n+    async def server_port(self, wrapper):\n+        port = self._listen(wrapper)\n+\n+        yield port.getHost().port\n+\n+        await port.stopListening()\n \n-    def getURL(self, path):\n-        return f\"http://127.0.0.1:{self.portno}/{path}\"\n+    @pytest.fixture\n+    def server_url(self, server_port):\n+        return f\"http://127.0.0.1:{server_port}/\"\n \n     @inlineCallbacks\n-    def testPayload(self):\n+    def testPayload(self, server_url):\n         s = \"0123456789\" * 10\n-        body = yield getPage(self.getURL(\"payload\"), body=s)\n+        body = yield getPage(server_url + \"payload\", body=s)\n         assert body == to_bytes(s)\n \n     @inlineCallbacks\n-    def testHostHeader(self):\n+    def testHostHeader(self, server_port, server_url):\n         # if we pass Host header explicitly, it should be used, otherwise\n         # it should extract from url\n-        body = yield getPage(self.getURL(\"host\"))\n-        assert body == to_bytes(f\"127.0.0.1:{self.portno}\")\n-        body = yield getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"})\n+        body = yield getPage(server_url + \"host\")\n+        assert body == to_bytes(f\"127.0.0.1:{server_port}\")\n+        body = yield getPage(server_url + \"host\", headers={\"Host\": \"www.example.com\"})\n         assert body == to_bytes(\"www.example.com\")\n \n     @inlineCallbacks\n-    def test_getPage(self):\n+    def test_getPage(self, server_url):\n         \"\"\"\n         L{client.getPage} returns a L{Deferred} which is called back with\n         the body of the response if the default method B{GET} is used.\n         \"\"\"\n-        body = yield getPage(self.getURL(\"file\"))\n+        body = yield getPage(server_url + \"file\")\n         assert body == b\"0123456789\"\n \n     @inlineCallbacks\n-    def test_getPageHead(self):\n+    def test_getPageHead(self, server_url):\n         \"\"\"\n         L{client.getPage} returns a L{Deferred} which is called back with\n         the empty string if the method is C{HEAD} and there is a successful\n         response code.\n         \"\"\"\n \n         def _getPage(method):\n-            return getPage(self.getURL(\"file\"), method=method)\n+            return getPage(server_url + \"file\", method=method)\n \n         body = yield _getPage(\"head\")\n         assert body == b\"\"\n         body = yield _getPage(\"HEAD\")\n         assert body == b\"\"\n \n     @inlineCallbacks\n-    def test_timeoutNotTriggering(self):\n+    def test_timeoutNotTriggering(self, server_port, server_url):\n         \"\"\"\n         When a non-zero timeout is passed to L{getPage} and the page is\n         retrieved before the timeout period elapses, the L{Deferred} is\n         called back with the contents of the page.\n         \"\"\"\n-        body = yield getPage(self.getURL(\"host\"), timeout=100)\n-        assert body == to_bytes(f\"127.0.0.1:{self.portno}\")\n+        body = yield getPage(server_url + \"host\", timeout=100)\n+        assert body == to_bytes(f\"127.0.0.1:{server_port}\")\n \n     @inlineCallbacks\n-    def test_timeoutTriggering(self):\n+    def test_timeoutTriggering(self, wrapper, server_url):\n         \"\"\"\n         When a non-zero timeout is passed to L{getPage} and that many\n         seconds elapse before the server responds to the request. the\n         L{Deferred} is errbacked with a L{error.TimeoutError}.\n         \"\"\"\n         with pytest.raises(defer.TimeoutError):\n-            yield getPage(self.getURL(\"wait\"), timeout=0.000001)\n+            yield getPage(server_url + \"wait\", timeout=0.000001)\n         # Clean up the server which is hanging around not doing\n         # anything.\n-        connected = list(self.wrapper.protocols.keys())\n+        connected = list(wrapper.protocols.keys())\n         # There might be nothing here if the server managed to already see\n         # that the connection was lost.\n         if connected:\n             connected[0].transport.loseConnection()\n \n     @inlineCallbacks\nComment: This test uses `yield getPage(...)` but is not decorated with `@inlineCallbacks`, so the Deferred won\u2019t be awaited. Add `@inlineCallbacks` above the method or convert it to `async def` and use `await`.\n```suggestion\n    @inlineCallbacks\n    @inlineCallbacks\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "tests/test_webclient.py",
    "pr_number": 6938,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2188233341,
    "comment_created_at": "2025-07-06T12:29:12Z"
  },
  {
    "code": "@@ -389,7 +390,27 @@ def _incompatible(self):\n         scheduler.open(spider)\n \n     def test_incompatibility(self):\n-        with pytest.raises(\n-            ValueError, match=\"does not support CONCURRENT_REQUESTS_PER_IP\"\n-        ):\n-            self._incompatible()\n+        with warnings.catch_warnings():\n+            warnings.filterwarnings(\"ignore\")",
    "comment": "i thought add the deprecate test here but having a separate test seems better, i just filtering the warning to not show on the test logs.",
    "line_number": 394,
    "enriched": "File: tests/test_scheduler.py\nCode: @@ -389,7 +390,27 @@ def _incompatible(self):\n         scheduler.open(spider)\n \n     def test_incompatibility(self):\n-        with pytest.raises(\n-            ValueError, match=\"does not support CONCURRENT_REQUESTS_PER_IP\"\n-        ):\n-            self._incompatible()\n+        with warnings.catch_warnings():\n+            warnings.filterwarnings(\"ignore\")\nComment: I thought add the deprecate test here but having a separate test seems better, I just filtering the warning to not show on the test logs.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "tests/test_scheduler.py",
    "pr_number": 6921,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2173110992,
    "comment_created_at": "2025-06-28T03:56:06Z"
  },
  {
    "code": "@@ -217,17 +224,30 @@ async def _start_request_processing(self) -> None:\n         # Starts the processing of scheduled requests, as well as a periodic\n         # call to that processing method for scenarios where the scheduler\n         # reports having pending requests but returns none.\n-        assert self._slot is not None  # typing\n-        self._slot.nextcall.schedule()\n-        self._slot.heartbeat.start(self._SLOT_HEARTBEAT_INTERVAL)\n-\n-        while self._start and self.spider:\n-            await self._process_start_next()\n-            if not self.needs_backout():\n-                # Give room for the outcome of self._process_start_next() to be\n-                # processed before continuing with the next iteration.\n-                self._slot.nextcall.schedule()\n-                await self._slot.nextcall.wait()\n+        try:\n+            assert self._slot is not None  # typing\n+            self._slot.nextcall.schedule()\n+            self._slot.heartbeat.start(self._SLOT_HEARTBEAT_INTERVAL)\n+\n+            while self._start and self.spider:\n+                await self._process_start_next()\n+                if not self.needs_backout():\n+                    # Give room for the outcome of self._process_start_next() to be\n+                    # processed before continuing with the next iteration.\n+                    self._slot.nextcall.schedule()\n+                    await self._slot.nextcall.wait()\n+        except (asyncio.exceptions.CancelledError, CancelledError):\n+            # self.stop() has cancelled us, nothing to do\n+            return\n+        except Exception:\n+            # an error happened, log it and stop the engine\n+            self._start_request_processing_dfd = None\n+            logger.error(\n+                \"Error while processing requests from start()\",\n+                exc_info=True,\n+                extra={\"spider\": self.spider},\n+            )\n+            await maybe_deferred_to_future(self.stop())",
    "comment": "self.stop() can theoretically cause another unhandled exception here, but in practice i don't see it being possible (and, consequently, couldn't design a failing test), as self.running should be true here, and self.close_spider() is designed to swallow any possible exceptions.",
    "line_number": 250,
    "enriched": "File: scrapy/core/engine.py\nCode: @@ -217,17 +224,30 @@ async def _start_request_processing(self) -> None:\n         # Starts the processing of scheduled requests, as well as a periodic\n         # call to that processing method for scenarios where the scheduler\n         # reports having pending requests but returns none.\n-        assert self._slot is not None  # typing\n-        self._slot.nextcall.schedule()\n-        self._slot.heartbeat.start(self._SLOT_HEARTBEAT_INTERVAL)\n-\n-        while self._start and self.spider:\n-            await self._process_start_next()\n-            if not self.needs_backout():\n-                # Give room for the outcome of self._process_start_next() to be\n-                # processed before continuing with the next iteration.\n-                self._slot.nextcall.schedule()\n-                await self._slot.nextcall.wait()\n+        try:\n+            assert self._slot is not None  # typing\n+            self._slot.nextcall.schedule()\n+            self._slot.heartbeat.start(self._SLOT_HEARTBEAT_INTERVAL)\n+\n+            while self._start and self.spider:\n+                await self._process_start_next()\n+                if not self.needs_backout():\n+                    # Give room for the outcome of self._process_start_next() to be\n+                    # processed before continuing with the next iteration.\n+                    self._slot.nextcall.schedule()\n+                    await self._slot.nextcall.wait()\n+        except (asyncio.exceptions.CancelledError, CancelledError):\n+            # self.stop() has cancelled us, nothing to do\n+            return\n+        except Exception:\n+            # an error happened, log it and stop the engine\n+            self._start_request_processing_dfd = None\n+            logger.error(\n+                \"Error while processing requests from start()\",\n+                exc_info=True,\n+                extra={\"spider\": self.spider},\n+            )\n+            await maybe_deferred_to_future(self.stop())\nComment: `self.stop()` can theoretically cause another unhandled exception here, but in practice I don't see it being possible (and, consequently, couldn't design a failing test), as `self.running` should be True here, and `self.close_spider()` is designed to swallow any possible exceptions.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "scrapy/core/engine.py",
    "pr_number": 6900,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2160135080,
    "comment_created_at": "2025-06-21T19:54:52Z"
  },
  {
    "code": "@@ -155,6 +155,12 @@ def _assert_stores(\n         finally:\n             path.unlink()\n \n+    def test_preserves_windows_path_without_file_scheme(self, tmp_path):",
    "comment": "the fixture is not used and can be dropped.",
    "line_number": 158,
    "enriched": "File: tests/test_feedexport.py\nCode: @@ -155,6 +155,12 @@ def _assert_stores(\n         finally:\n             path.unlink()\n \n+    def test_preserves_windows_path_without_file_scheme(self, tmp_path):\nComment: The fixture is not used and can be dropped.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "tests/test_feedexport.py",
    "pr_number": 6897,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2159917436,
    "comment_created_at": "2025-06-21T07:25:37Z"
  },
  {
    "code": "@@ -109,6 +113,24 @@ def _urllib3_request_context(\n     }\n     return host_params, pool_kwargs\n \n+def is_single_certificate(cert_path):",
    "comment": "it seems unlikely that special-casing single certs is the best approach \ud83e\udd14: we probably want to not care about the part of the chain that's above the trusted certificate.\r\ni wouldn't expect curl to special-case single certs either...\r\nprobably we want to match curl's behavior with regards to the openssl config here.",
    "line_number": 116,
    "enriched": "File: src/requests/adapters.py\nCode: @@ -109,6 +113,24 @@ def _urllib3_request_context(\n     }\n     return host_params, pool_kwargs\n \n+def is_single_certificate(cert_path):\nComment: It seems unlikely that special-casing single certs is the best approach \ud83e\udd14: we probably want to not care about the part of the chain that's above the trusted certificate.\r\nI wouldn't expect curl to special-case single certs either...\r\nProbably we want to match curl's behavior with regards to the openssl config here. ",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "src/requests/adapters.py",
    "pr_number": 7030,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 2347236444,
    "comment_created_at": "2025-09-14T10:16:36Z"
  },
  {
    "code": "@@ -663,10 +665,10 @@ def __init__(self):\n         #: Integer Code of responded HTTP Status, e.g. 404 or 200.\n         self.status_code = None\n \n-        #: Case-insensitive Dictionary of Response Headers.\n+        #: Case-insensitive Dictionary (email.EmailMessage) of Response Headers.",
    "comment": "this is incorrect for documentation linking (if that's what you're going for)",
    "line_number": 668,
    "enriched": "File: src/requests/models.py\nCode: @@ -663,10 +665,10 @@ def __init__(self):\n         #: Integer Code of responded HTTP Status, e.g. 404 or 200.\n         self.status_code = None\n \n-        #: Case-insensitive Dictionary of Response Headers.\n+        #: Case-insensitive Dictionary (email.EmailMessage) of Response Headers.\nComment: This is incorrect for documentation linking (if that's what you're going for)",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "src/requests/models.py",
    "pr_number": 7015,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 2291084175,
    "comment_created_at": "2025-08-21T13:34:06Z"
  },
  {
    "code": "@@ -236,16 +236,8 @@ def get_netrc_auth(url, raise_errors=False):\n             return\n \n         ri = urlparse(url)\n-\n-        # Strip port numbers from netloc. This weird `if...encode`` dance is\n-        # used for Python 3.2, which doesn't support unicode literals.\n-        splitstr = b\":\"\n-        if isinstance(url, str):\n-            splitstr = splitstr.decode(\"ascii\")\n-        host = ri.netloc.split(splitstr)[0]\n-\n         try:\n-            _netrc = netrc(netrc_path).authenticators(host)\n+            _netrc = netrc(netrc_path).authenticators(ri.hostname)",
    "comment": "this is fixed now in main: https://github.com/psf/requests/commit/96ba401c1296ab1dda74a2365ef36d88f7d144ef",
    "line_number": 240,
    "enriched": "File: src/requests/utils.py\nCode: @@ -236,16 +236,8 @@ def get_netrc_auth(url, raise_errors=False):\n             return\n \n         ri = urlparse(url)\n-\n-        # Strip port numbers from netloc. This weird `if...encode`` dance is\n-        # used for Python 3.2, which doesn't support unicode literals.\n-        splitstr = b\":\"\n-        if isinstance(url, str):\n-            splitstr = splitstr.decode(\"ascii\")\n-        host = ri.netloc.split(splitstr)[0]\n-\n         try:\n-            _netrc = netrc(netrc_path).authenticators(host)\n+            _netrc = netrc(netrc_path).authenticators(ri.hostname)\nComment: This is fixed now in main: https://github.com/psf/requests/commit/96ba401c1296ab1dda74a2365ef36d88f7d144ef",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "src/requests/utils.py",
    "pr_number": 6963,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 2128118428,
    "comment_created_at": "2025-06-05T07:12:17Z"
  },
  {
    "code": "@@ -956,3 +957,17 @@ def QueryValueEx(key, value_name):\n     monkeypatch.setattr(winreg, \"OpenKey\", OpenKey)\n     monkeypatch.setattr(winreg, \"QueryValueEx\", QueryValueEx)\n     assert should_bypass_proxies(\"http://example.com/\", None) is False\n+def test_super_len_stringio_multi_byte():\n+    s = \"\ud83d\udca9\"  # U+1F4A9 (4 bytes in UTF-8)\n+    sio = io.StringIO(s)\n+    assert super_len(sio) == 4, f\"Expected 4, got {super_len(sio)}\"\n+\n+def test_super_len_stringio_mixed_chars():\n+    s = \"A\ud83d\udca9B\"  # 1 byte + 4 bytes + 1 byte\n+    sio = io.StringIO(s)\n+    assert super_len(sio) == 6, f\"Expected 6, got {super_len(sio)}\"\n+\n+def test_super_len_stringio_empty():\n+    s = \"\"\n+    sio = io.StringIO(s)\n+    assert super_len(sio) == 0, f\"Expected 0, got {super_len(sio)}\"",
    "comment": "trailing space",
    "line_number": 973,
    "enriched": "File: tests/test_utils.py\nCode: @@ -956,3 +957,17 @@ def QueryValueEx(key, value_name):\n     monkeypatch.setattr(winreg, \"OpenKey\", OpenKey)\n     monkeypatch.setattr(winreg, \"QueryValueEx\", QueryValueEx)\n     assert should_bypass_proxies(\"http://example.com/\", None) is False\n+def test_super_len_stringio_multi_byte():\n+    s = \"\ud83d\udca9\"  # U+1F4A9 (4 bytes in UTF-8)\n+    sio = io.StringIO(s)\n+    assert super_len(sio) == 4, f\"Expected 4, got {super_len(sio)}\"\n+\n+def test_super_len_stringio_mixed_chars():\n+    s = \"A\ud83d\udca9B\"  # 1 byte + 4 bytes + 1 byte\n+    sio = io.StringIO(s)\n+    assert super_len(sio) == 6, f\"Expected 6, got {super_len(sio)}\"\n+\n+def test_super_len_stringio_empty():\n+    s = \"\"\n+    sio = io.StringIO(s)\n+    assert super_len(sio) == 0, f\"Expected 0, got {super_len(sio)}\"\nComment: Trailing space\r\n```suggestion\r\n    assert super_len(sio) == 0, f\"Expected 0, got {super_len(sio)}\"\r\n\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "tests/test_utils.py",
    "pr_number": 6955,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 2150138454,
    "comment_created_at": "2025-06-16T14:20:14Z"
  },
  {
    "code": "@@ -37,6 +37,8 @@ Exceptions\n .. autoexception:: requests.ReadTimeout\n .. autoexception:: requests.Timeout\n .. autoexception:: requests.JSONDecodeError\n+.. autoexception:: requests.ConnectTimeout",
    "comment": "not sure if i'm missing something. these already exist on lines 36 and 37. why are we adding them a second time?",
    "line_number": 40,
    "enriched": "File: docs/api.rst\nCode: @@ -37,6 +37,8 @@ Exceptions\n .. autoexception:: requests.ReadTimeout\n .. autoexception:: requests.Timeout\n .. autoexception:: requests.JSONDecodeError\n+.. autoexception:: requests.ConnectTimeout\nComment: Not sure if I'm missing something. These already exist on lines 36 and 37. Why are we adding them a second time?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/api.rst",
    "pr_number": 6879,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 1938333934,
    "comment_created_at": "2025-02-01T19:44:03Z"
  },
  {
    "code": "@@ -44,6 +44,9 @@ set with `headers=`.\n If credentials for the hostname are found, the request is sent with HTTP Basic\n Auth.\n \n+Requests will search for the netrc file at `~/.netrc`, `~/_netrc`, or at the path",
    "comment": "this doesn't list the windows paths or how to disable this altogether.",
    "line_number": 47,
    "enriched": "File: docs/user/authentication.rst\nCode: @@ -44,6 +44,9 @@ set with `headers=`.\n If credentials for the hostname are found, the request is sent with HTTP Basic\n Auth.\n \n+Requests will search for the netrc file at `~/.netrc`, `~/_netrc`, or at the path\nComment: This doesn't list the Windows paths or how to disable this altogether. ",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/user/authentication.rst",
    "pr_number": 6876,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 1931340274,
    "comment_created_at": "2025-01-27T23:56:20Z"
  },
  {
    "code": "@@ -128,11 +128,6 @@ jobs:\n       with:\n         python-version: ${{ matrix.version }}\n \n-    - uses: maxim-lobanov/setup-xcode@60606e260d2fc5762a71e64e74b2174e8ea3c8bd # v1.6.0",
    "comment": "this was probably required for a specific reason, now lost to time.",
    "line_number": 131,
    "enriched": "File: .github/workflows/macos.yml\nCode: @@ -128,11 +128,6 @@ jobs:\n       with:\n         python-version: ${{ matrix.version }}\n \n-    - uses: maxim-lobanov/setup-xcode@60606e260d2fc5762a71e64e74b2174e8ea3c8bd # v1.6.0\nComment: This was probably required for a specific reason, now lost to time.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": ".github/workflows/macos.yml",
    "pr_number": 29886,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2409385522,
    "comment_created_at": "2025-10-07T05:13:02Z"
  },
  {
    "code": "@@ -266,11 +266,6 @@ def test_scalar_coercion(self, scalar):\n             # Ensure we have a full-precision number if available\n             scalar = type(scalar)((scalar * 2)**0.5)\n \n-        if type(scalar) is rational:\n-            # Rational generally fails due to a missing cast. In the future\n-            # object casts should automatically be defined based on `setitem`.\n-            pytest.xfail(\"Rational to object cast is undefined currently.\")",
    "comment": "must have been working for many years...",
    "line_number": 272,
    "enriched": "File: numpy/_core/tests/test_array_coercion.py\nCode: @@ -266,11 +266,6 @@ def test_scalar_coercion(self, scalar):\n             # Ensure we have a full-precision number if available\n             scalar = type(scalar)((scalar * 2)**0.5)\n \n-        if type(scalar) is rational:\n-            # Rational generally fails due to a missing cast. In the future\n-            # object casts should automatically be defined based on `setitem`.\n-            pytest.xfail(\"Rational to object cast is undefined currently.\")\nComment: Must have been working for many years...",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "numpy/_core/tests/test_array_coercion.py",
    "pr_number": 29880,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2404625270,
    "comment_created_at": "2025-10-05T19:16:53Z"
  },
  {
    "code": "@@ -888,21 +891,20 @@ npy_uint64 _npy_halfbits_to_doublebits(npy_uint16 h){\n #    define _ROUND_TRIP(x) (x)\n #  elif @is_bool2@\n #    define _CONVERT_FN(x) ((npy_bool)!npy_half_iszero(x))\n-#    define _ROUND_TRIP(x) npy_float_to_half((float)(!npy_half_iszero(x)))\n+#    define _ROUND_TRIP(x) npy_float_to_half((!npy_half_iszero(x))\n #  else\n #    define _CONVERT_FN(x) ((_TYPE2)npy_half_to_float(x))\n #    define _ROUND_TRIP(x) npy_float_to_half((float)_CONVERT_FN(x))\n #  endif\n \n #elif @is_emu_half2@\n #  define _TO_RTYPE1(x) (@rtype1@)(x)\n-\n #  if @is_float1@\n #    define _CONVERT_FN(x) npy_floatbits_to_halfbits(x)\n-#    define _ROUND_TRIP(x) (@rtype1@)npy_halfbits_to_floatbits(_CONVERT_FN(x))\n+#    define _ROUND_TRIP(x) npy_half_to_float(npy_float_to_half(x))\n #  elif @is_double1@\n #    define _CONVERT_FN(x) npy_doublebits_to_halfbits(x)\n-#    define _ROUND_TRIP(x) (@rtype1@)_npy_halfbits_to_doublebits(_CONVERT_FN(x))\n+#    define _ROUND_TRIP(x) npy_half_to_double(npy_double_to_half(x))",
    "comment": "@mattip, it looks a bug to me, do we have test case covering it?",
    "line_number": 907,
    "enriched": "File: numpy/_core/src/multiarray/lowlevel_strided_loops.c.src\nCode: @@ -888,21 +891,20 @@ npy_uint64 _npy_halfbits_to_doublebits(npy_uint16 h){\n #    define _ROUND_TRIP(x) (x)\n #  elif @is_bool2@\n #    define _CONVERT_FN(x) ((npy_bool)!npy_half_iszero(x))\n-#    define _ROUND_TRIP(x) npy_float_to_half((float)(!npy_half_iszero(x)))\n+#    define _ROUND_TRIP(x) npy_float_to_half((!npy_half_iszero(x))\n #  else\n #    define _CONVERT_FN(x) ((_TYPE2)npy_half_to_float(x))\n #    define _ROUND_TRIP(x) npy_float_to_half((float)_CONVERT_FN(x))\n #  endif\n \n #elif @is_emu_half2@\n #  define _TO_RTYPE1(x) (@rtype1@)(x)\n-\n #  if @is_float1@\n #    define _CONVERT_FN(x) npy_floatbits_to_halfbits(x)\n-#    define _ROUND_TRIP(x) (@rtype1@)npy_halfbits_to_floatbits(_CONVERT_FN(x))\n+#    define _ROUND_TRIP(x) npy_half_to_float(npy_float_to_half(x))\n #  elif @is_double1@\n #    define _CONVERT_FN(x) npy_doublebits_to_halfbits(x)\n-#    define _ROUND_TRIP(x) (@rtype1@)_npy_halfbits_to_doublebits(_CONVERT_FN(x))\n+#    define _ROUND_TRIP(x) npy_half_to_double(npy_double_to_half(x))\nComment: @mattip, It looks a bug to me, do we have test case covering it?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "numpy/_core/src/multiarray/lowlevel_strided_loops.c.src",
    "pr_number": 29868,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2400472824,
    "comment_created_at": "2025-10-03T00:46:33Z"
  },
  {
    "code": "@@ -44,6 +56,10 @@ def run_lint(self, fix: bool) -> None:\n         retcode, c_API_errors = self.run_check_c_api()\n         c_API_errors and print(c_API_errors)\n ",
    "comment": "seems to miss an if retcode: sys.exit(retcode) now.",
    "line_number": 58,
    "enriched": "File: tools/linter.py\nCode: @@ -44,6 +56,10 @@ def run_lint(self, fix: bool) -> None:\n         retcode, c_API_errors = self.run_check_c_api()\n         c_API_errors and print(c_API_errors)\n \nComment: Seems to miss an `if retcode: sys.exit(retcode)` now.",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "tools/linter.py",
    "pr_number": 29861,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2401414580,
    "comment_created_at": "2025-10-03T09:55:53Z"
  },
  {
    "code": "@@ -78,7 +78,7 @@ __all__ = [\n     \"vecdot\",\n ]\n \n-_ArrayT = TypeVar(\"_ArrayT\", bound=NDArray[Any])",
    "comment": "this one was unused?",
    "line_number": 81,
    "enriched": "File: numpy/linalg/_linalg.pyi\nCode: @@ -78,7 +78,7 @@ __all__ = [\n     \"vecdot\",\n ]\n \n-_ArrayT = TypeVar(\"_ArrayT\", bound=NDArray[Any])\nComment: This one was unused?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "numpy/linalg/_linalg.pyi",
    "pr_number": 29846,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2393060390,
    "comment_created_at": "2025-09-30T23:34:23Z"
  },
  {
    "code": "@@ -9260,9 +9250,24 @@ def __int__(self):\n                     raise NotImplementedError\n             assert_raises(NotImplementedError,\n                 int_func, np.array(NotConvertible()))\n-            with pytest.warns(DeprecationWarning):\n-                assert_raises(NotImplementedError,\n-                    int_func, np.array([NotConvertible()]))\n+            assert_raises(TypeError,\n+                int_func, np.array([NotConvertible()]))\n+\n+    def test_to_float_scalar(self):\n+        float_funcs = (float, lambda x: x.__float__())\n+        for float_func in float_funcs:\n+            assert_equal(float_func(np.array(0)), 0.0)\n+            assert_equal(float_func(np.array(1.0, np.float64)), 1.0)\n+            assert_raises(TypeError, float_func, np.array([2]))\n+            assert_raises(TypeError, float_func, np.array([3.14]))\n+            assert_raises(TypeError, float_func, np.array([[4.0]]))\n+\n+            assert_equal(5.0, float_func(np.array('5')))\n+            assert_equal(5.1, float_func(np.array('5.1')))\n+            assert_equal(6.0, float_func(np.bytes_(b'6')))\n+            assert_equal(6.1, float_func(np.bytes_(b'6.1')))\n+            assert_equal(7.0, float_func(np.str_('7')))\n+            assert_equal(7.1, float_func(np.str_('7.1')))",
    "comment": "new tests?  i don't say no to new tests ever :).  (i think float and __float__ is a bit double, since float just calls __float__, but ok.)",
    "line_number": 9270,
    "enriched": "File: numpy/_core/tests/test_multiarray.py\nCode: @@ -9260,9 +9250,24 @@ def __int__(self):\n                     raise NotImplementedError\n             assert_raises(NotImplementedError,\n                 int_func, np.array(NotConvertible()))\n-            with pytest.warns(DeprecationWarning):\n-                assert_raises(NotImplementedError,\n-                    int_func, np.array([NotConvertible()]))\n+            assert_raises(TypeError,\n+                int_func, np.array([NotConvertible()]))\n+\n+    def test_to_float_scalar(self):\n+        float_funcs = (float, lambda x: x.__float__())\n+        for float_func in float_funcs:\n+            assert_equal(float_func(np.array(0)), 0.0)\n+            assert_equal(float_func(np.array(1.0, np.float64)), 1.0)\n+            assert_raises(TypeError, float_func, np.array([2]))\n+            assert_raises(TypeError, float_func, np.array([3.14]))\n+            assert_raises(TypeError, float_func, np.array([[4.0]]))\n+\n+            assert_equal(5.0, float_func(np.array('5')))\n+            assert_equal(5.1, float_func(np.array('5.1')))\n+            assert_equal(6.0, float_func(np.bytes_(b'6')))\n+            assert_equal(6.1, float_func(np.bytes_(b'6.1')))\n+            assert_equal(7.0, float_func(np.str_('7')))\n+            assert_equal(7.1, float_func(np.str_('7.1')))\nComment: New tests?  I don't say no to new tests ever :).  (I think `float` and `__float__` is a bit double, since `float` just calls `__float__`, but OK.)",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "numpy/_core/tests/test_multiarray.py",
    "pr_number": 29841,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2394058314,
    "comment_created_at": "2025-10-01T10:04:49Z"
  },
  {
    "code": "@@ -1687,9 +1687,9 @@ def test_integer_alias_names(self, int_, size):\n \n     @pytest.mark.parametrize(\"name\",\n             [\"Half\", \"Float\", \"Double\", \"CFloat\", \"CDouble\"])\n-    def test_float_alias_names(self, name):\n+    def test_float_alias_names_not_present(self, name):\n         with pytest.raises(AttributeError):\n-            getattr(numpy.dtypes, name + \"DType\") is numpy.dtypes.Float16DType\n+            getattr(numpy.dtypes, name + \"DType\")",
    "comment": "or just assert not hasattr(numpy.dtypes, f\"{name}dtype\") \ud83e\udd37\ud83c\udffb",
    "line_number": 1692,
    "enriched": "File: numpy/_core/tests/test_dtype.py\nCode: @@ -1687,9 +1687,9 @@ def test_integer_alias_names(self, int_, size):\n \n     @pytest.mark.parametrize(\"name\",\n             [\"Half\", \"Float\", \"Double\", \"CFloat\", \"CDouble\"])\n-    def test_float_alias_names(self, name):\n+    def test_float_alias_names_not_present(self, name):\n         with pytest.raises(AttributeError):\n-            getattr(numpy.dtypes, name + \"DType\") is numpy.dtypes.Float16DType\n+            getattr(numpy.dtypes, name + \"DType\")\nComment: or just `assert not hasattr(numpy.dtypes, f\"{name}DType\")` \ud83e\udd37\ud83c\udffb ",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "numpy/_core/tests/test_dtype.py",
    "pr_number": 29796,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2373551442,
    "comment_created_at": "2025-09-23T22:01:23Z"
  },
  {
    "code": "@@ -187,6 +187,9 @@ def setbufsize(size):\n     8192\n \n     \"\"\"\n+    if size < 0:\n+        raise ValueError(\"buffer size must be non-negative\")\n+        ",
    "comment": "linting failure",
    "line_number": 192,
    "enriched": "File: numpy/_core/_ufunc_config.py\nCode: @@ -187,6 +187,9 @@ def setbufsize(size):\n     8192\n \n     \"\"\"\n+    if size < 0:\n+        raise ValueError(\"buffer size must be non-negative\")\n+        \nComment: Linting failure\r\n```suggestion\r\n\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "numpy/_core/_ufunc_config.py",
    "pr_number": 29774,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2366942627,
    "comment_created_at": "2025-09-22T07:16:45Z"
  },
  {
    "code": "@@ -572,6 +577,33 @@ def _init(self, dtype):\n         self._str_smallest_subnormal = machar._str_smallest_subnormal.strip()\n         return self\n \n+    def _init_from_finfo_obj(self, dtype, finfo_obj):\n+        self.dtype = numeric.dtype(dtype)\n+        self.precision = finfo_obj.get('precision')\n+        self.iexp = finfo_obj.get('iexp')\n+        self.maxexp = finfo_obj.get('maxexp')\n+        self.minexp = finfo_obj.get('minexp')\n+        self.negep = finfo_obj.get('negep')\n+        self.machep = finfo_obj.get('machep')\n+        self.resolution = finfo_obj.get('resolution')\n+        self.epsneg = finfo_obj.get('epsneg')\n+        self.smallest_subnormal = finfo_obj.get('smallest_subnormal')\n+        self.bits = self.dtype.itemsize * 8\n+        self.max = finfo_obj.get('max')\n+        self.min = finfo_obj.get('min')\n+        self.eps = finfo_obj.get('eps')\n+        self.nexp = finfo_obj.get('nexp')\n+        self.nmant = finfo_obj.get('nmant')\n+        self._machar = finfo_obj.get('machar')\n+        self._str_tiny = str(finfo_obj.get(\"smallest_normal\"))\n+        self._str_max = str(self.max)\n+        self._str_epsneg = str(self.epsneg)\n+        self._str_eps = str(self.eps)\n+        self._str_resolution = str(self.resolution)\n+        self._str_smallest_normal = str(finfo_obj.get(\"smallest_normal\"))\n+        self._str_smallest_subnormal = str(self.smallest_subnormal)",
    "comment": "these will be the string \"none\" if not present. i don't think that's what we want here.",
    "line_number": 604,
    "enriched": "File: numpy/_core/getlimits.py\nCode: @@ -572,6 +577,33 @@ def _init(self, dtype):\n         self._str_smallest_subnormal = machar._str_smallest_subnormal.strip()\n         return self\n \n+    def _init_from_finfo_obj(self, dtype, finfo_obj):\n+        self.dtype = numeric.dtype(dtype)\n+        self.precision = finfo_obj.get('precision')\n+        self.iexp = finfo_obj.get('iexp')\n+        self.maxexp = finfo_obj.get('maxexp')\n+        self.minexp = finfo_obj.get('minexp')\n+        self.negep = finfo_obj.get('negep')\n+        self.machep = finfo_obj.get('machep')\n+        self.resolution = finfo_obj.get('resolution')\n+        self.epsneg = finfo_obj.get('epsneg')\n+        self.smallest_subnormal = finfo_obj.get('smallest_subnormal')\n+        self.bits = self.dtype.itemsize * 8\n+        self.max = finfo_obj.get('max')\n+        self.min = finfo_obj.get('min')\n+        self.eps = finfo_obj.get('eps')\n+        self.nexp = finfo_obj.get('nexp')\n+        self.nmant = finfo_obj.get('nmant')\n+        self._machar = finfo_obj.get('machar')\n+        self._str_tiny = str(finfo_obj.get(\"smallest_normal\"))\n+        self._str_max = str(self.max)\n+        self._str_epsneg = str(self.epsneg)\n+        self._str_eps = str(self.eps)\n+        self._str_resolution = str(self.resolution)\n+        self._str_smallest_normal = str(finfo_obj.get(\"smallest_normal\"))\n+        self._str_smallest_subnormal = str(self.smallest_subnormal)\nComment: These will be the string `\"None\"` if not present. I don't think that's what we want here.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "numpy/_core/getlimits.py",
    "pr_number": 29771,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2364781388,
    "comment_created_at": "2025-09-19T23:13:54Z"
  },
  {
    "code": "@@ -616,6 +616,35 @@ def test_names_are_undersood_by_dtype(self, t):\n         assert np.dtype(t.__name__).type is t\n \n \n+class TestScalarTypeOrder:\n+    @pytest.mark.parametrize(('a', 'b'), [\n+        # signedinteger\n+        (np.byte, np.short),\n+        (np.short, np.intc),\n+        (np.intc, np.long),\n+        (np.long, np.longlong),\n+        # unsignedinteger\n+        (np.ubyte, np.ushort),\n+        (np.ushort, np.uintc),\n+        (np.uintc, np.ulong),\n+        (np.ulong, np.ulonglong),\n+        # floating\n+        (np.half, np.single),\n+        (np.single, np.double),\n+        (np.double, np.longdouble),\n+        # complexfloating\n+        (np.csingle, np.cdouble),\n+        (np.cdouble, np.clongdouble),\n+        # flexible\n+        (np.bytes_, np.str_),\n+        (np.str_, np.void),\n+        # bouncy castles\n+        (np.datetime64, np.timedelta64),",
    "comment": "total quibble but i think you can write this with just a single static list and then generate these tuples with e.g. itertools.pairwise.",
    "line_number": 642,
    "enriched": "File: numpy/_core/tests/test_numerictypes.py\nCode: @@ -616,6 +616,35 @@ def test_names_are_undersood_by_dtype(self, t):\n         assert np.dtype(t.__name__).type is t\n \n \n+class TestScalarTypeOrder:\n+    @pytest.mark.parametrize(('a', 'b'), [\n+        # signedinteger\n+        (np.byte, np.short),\n+        (np.short, np.intc),\n+        (np.intc, np.long),\n+        (np.long, np.longlong),\n+        # unsignedinteger\n+        (np.ubyte, np.ushort),\n+        (np.ushort, np.uintc),\n+        (np.uintc, np.ulong),\n+        (np.ulong, np.ulonglong),\n+        # floating\n+        (np.half, np.single),\n+        (np.single, np.double),\n+        (np.double, np.longdouble),\n+        # complexfloating\n+        (np.csingle, np.cdouble),\n+        (np.cdouble, np.clongdouble),\n+        # flexible\n+        (np.bytes_, np.str_),\n+        (np.str_, np.void),\n+        # bouncy castles\n+        (np.datetime64, np.timedelta64),\nComment: Total quibble but I think you can write this with just a single static list and then generate these tuples with e.g. `itertools.pairwise`.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "numpy/_core/tests/test_numerictypes.py",
    "pr_number": 29761,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2360522870,
    "comment_created_at": "2025-09-18T17:52:44Z"
  },
  {
    "code": "@@ -32,8 +32,17 @@ if [[ \"$INSTALL_OPENBLAS\" = \"true\" ]] ; then\n     echo pkgconf_path is $pkgconf_path, OPENBLAS is ${OPENBLAS}\n     rm -rf $pkgconf_path\n     mkdir -p $pkgconf_path\n-    python -m pip install -r $PROJECT_DIR/requirements/ci_requirements.txt\n-    python -c \"import scipy_${OPENBLAS}; print(scipy_${OPENBLAS}.get_pkg_config())\" > $pkgconf_path/scipy-openblas.pc\n+\n+    if [[ $CIBW_ARCHS_MACOS == \"x86_64\" ]]; then\n+        # We're cross compiling; the before-build hook isn't using Rosetta (see cibuildwheel#2592)\n+        mkdir host-env\n+        python -m pip install -r $PROJECT_DIR/requirements/ci_requirements.txt --platform macosx_10_13_x86_64 --only-binary :all: -U --target ./host-env\n+\t# Use a handwritten .pc file, because we can't run the cross Python to generate it",
    "comment": "tab instead of spaces - needs fixing",
    "line_number": 40,
    "enriched": "File: tools/wheels/cibw_before_build.sh\nCode: @@ -32,8 +32,17 @@ if [[ \"$INSTALL_OPENBLAS\" = \"true\" ]] ; then\n     echo pkgconf_path is $pkgconf_path, OPENBLAS is ${OPENBLAS}\n     rm -rf $pkgconf_path\n     mkdir -p $pkgconf_path\n-    python -m pip install -r $PROJECT_DIR/requirements/ci_requirements.txt\n-    python -c \"import scipy_${OPENBLAS}; print(scipy_${OPENBLAS}.get_pkg_config())\" > $pkgconf_path/scipy-openblas.pc\n+\n+    if [[ $CIBW_ARCHS_MACOS == \"x86_64\" ]]; then\n+        # We're cross compiling; the before-build hook isn't using Rosetta (see cibuildwheel#2592)\n+        mkdir host-env\n+        python -m pip install -r $PROJECT_DIR/requirements/ci_requirements.txt --platform macosx_10_13_x86_64 --only-binary :all: -U --target ./host-env\n+\t# Use a handwritten .pc file, because we can't run the cross Python to generate it\nComment: tab instead of spaces - needs fixing",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "tools/wheels/cibw_before_build.sh",
    "pr_number": 29756,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2347570239,
    "comment_created_at": "2025-09-14T20:48:11Z"
  },
  {
    "code": "@@ -864,11 +864,11 @@ result of multiplying the elements together, ``std`` to get the standard\n deviation, and more. ::\n \n   >>> data.max()",
    "comment": "on line 838, in the section about broadcasting, data is redefined data = np.array([1.0, 2.0]). so the text is correct, the image is wrong. i think the easiest thing would be to redefine data here to match the image (which will restore it to the same values as in the \"indexing and slicing\" section):\r\n\r\n\r\n\r\nthen at some point someone could regenerate the image in the \"broadcasting\" section and change the text so that the value of data remains consistent throughout this page. when they do that, it would be nice to add a .. note how they created the image.",
    "line_number": 867,
    "enriched": "File: doc/source/user/absolute_beginners.rst\nCode: @@ -864,11 +864,11 @@ result of multiplying the elements together, ``std`` to get the standard\n deviation, and more. ::\n \n   >>> data.max()\nComment: On line 838, in the section about Broadcasting, data is redefined `data = np.array([1.0, 2.0])`. So the text is correct, the image is wrong. I think the easiest thing would be to redefine `data` here to match the image (which will restore it to the same values as in the \"Indexing and slicing\" section):\r\n\r\n```suggestion\r\n  >>> data = np.array([1, 2, 3])\r\n  \r\n  >>> data.max()\r\n```\r\n\r\nThen at some point someone could regenerate the image in the \"Broadcasting\" section and change the text so that the value of `data` remains consistent throughout this page. When they do that, it would be nice to add a `.. note` how they created the image.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "doc/source/user/absolute_beginners.rst",
    "pr_number": 29753,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2347382996,
    "comment_created_at": "2025-09-14T15:08:45Z"
  },
  {
    "code": "@@ -74,8 +74,7 @@ def __bool__(self):\n         raise TypeError(\"boolean value of NA is ambiguous\")\n \n     def __hash__(self):\n-        exponent = 31 if is_32bit else 61\n-        return 2**exponent - 1\n+        return 2**61 - 1",
    "comment": "i assume this is string specific? might be good to have \"string\" in the file name somewhere.",
    "line_number": 77,
    "enriched": "File: numpy/_core/tests/_natype.py\nCode: @@ -74,8 +74,7 @@ def __bool__(self):\n         raise TypeError(\"boolean value of NA is ambiguous\")\n \n     def __hash__(self):\n-        exponent = 31 if is_32bit else 61\n-        return 2**exponent - 1\n+        return 2**61 - 1\nComment: I assume this is string specific? Might be good to have \"string\" in the file name somewhere.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "numpy/_core/tests/_natype.py",
    "pr_number": 29744,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2350346334,
    "comment_created_at": "2025-09-15T23:53:43Z"
  },
  {
    "code": "@@ -489,4 +491,9 @@ typedef PyArray_Descr *(PyArrayDTypeMeta_FinalizeDescriptor)(PyArray_Descr *dtyp\n typedef int(PyArrayDTypeMeta_SetItem)(PyArray_Descr *, PyObject *, char *);\n typedef PyObject *(PyArrayDTypeMeta_GetItem)(PyArray_Descr *, char *);\n \n+typedef struct {",
    "comment": "shouldn't we be able to just pass on the flags that were defined directly? i.e., wouldn't you'd want something like\r\n\r\ntypedef struct {\r\n    npy_sortkind flags;\r\n} pyarraymethod_sortparameters;\r\n\r\nthat avoids having to set .stable and .descending attributes in the code.\r\n\r\np.s. not 100% the indirection via a struct is even needed, but maybe it will become useful to pass on other information later on, without having to extend context with yet another slot.",
    "line_number": 494,
    "enriched": "File: numpy/_core/include/numpy/dtype_api.h\nCode: @@ -489,4 +491,9 @@ typedef PyArray_Descr *(PyArrayDTypeMeta_FinalizeDescriptor)(PyArray_Descr *dtyp\n typedef int(PyArrayDTypeMeta_SetItem)(PyArray_Descr *, PyObject *, char *);\n typedef PyObject *(PyArrayDTypeMeta_GetItem)(PyArray_Descr *, char *);\n \n+typedef struct {\nComment: Shouldn't we be able to just pass on the flags that were defined directly? I.e., wouldn't you'd want something like\r\n```\r\ntypedef struct {\r\n    NPY_SORTKIND flags;\r\n} PyArrayMethod_SortParameters;\r\n```\r\nThat avoids having to set `.stable` and `.descending` attributes in the code.\r\n\r\np.s. Not 100% the indirection via a `struct` is even needed, but maybe it will become useful to pass on other information later on, without having to extend `context` with yet another slot.\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "numpy/_core/include/numpy/dtype_api.h",
    "pr_number": 29737,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2369124616,
    "comment_created_at": "2025-09-22T15:51:22Z"
  },
  {
    "code": "@@ -2429,8 +2429,8 @@ def _reverse_indexer(self) -> dict[Hashable, npt.NDArray[np.intp]]:\n             ensure_platform_int(self.codes), categories.size\n         )\n         counts = ensure_int64(counts).cumsum()\n-        _result = (r[start:end] for start, end in zip(counts, counts[1:]))\n-        return dict(zip(categories, _result))\n+        _result = (r[start:end] for start, end in zip(counts, counts[1:], strict=False))",
    "comment": "could you use itertools.pairwise here?",
    "line_number": 2432,
    "enriched": "File: pandas/core/arrays/categorical.py\nCode: @@ -2429,8 +2429,8 @@ def _reverse_indexer(self) -> dict[Hashable, npt.NDArray[np.intp]]:\n             ensure_platform_int(self.codes), categories.size\n         )\n         counts = ensure_int64(counts).cumsum()\n-        _result = (r[start:end] for start, end in zip(counts, counts[1:]))\n-        return dict(zip(categories, _result))\n+        _result = (r[start:end] for start, end in zip(counts, counts[1:], strict=False))\nComment: Could you use `itertools.pairwise` here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/core/arrays/categorical.py",
    "pr_number": 62596,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2407287317,
    "comment_created_at": "2025-10-06T16:21:20Z"
  },
  {
    "code": "@@ -3097,3 +3097,28 @@ def test_merge_categorical_key_recursion():\n         right.astype(\"float64\"), on=\"key\", how=\"outer\"\n     )\n     tm.assert_frame_equal(result, expected)\n+\n+\n+def test_merge_pyarrow_datetime_duplicates():\n+    # GH#61926\n+    # Regression test for merge failing on pyarrow datetime columns with duplicates",
    "comment": "can you remove all these comments except for the # gh#61926 above?",
    "line_number": 3104,
    "enriched": "File: pandas/tests/reshape/merge/test_merge.py\nCode: @@ -3097,3 +3097,28 @@ def test_merge_categorical_key_recursion():\n         right.astype(\"float64\"), on=\"key\", how=\"outer\"\n     )\n     tm.assert_frame_equal(result, expected)\n+\n+\n+def test_merge_pyarrow_datetime_duplicates():\n+    # GH#61926\n+    # Regression test for merge failing on pyarrow datetime columns with duplicates\nComment: Can you remove all these comments except for the `# GH#61926` above?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "pandas/tests/reshape/merge/test_merge.py",
    "pr_number": 62592,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2404627476,
    "comment_created_at": "2025-10-05T19:22:02Z"
  },
  {
    "code": "@@ -1143,89 +1397,102 @@ def line(\n \n         This function is useful to plot lines using DataFrame's values\n         as coordinates.\n-        \"\"\"\n-        if color is not None:\n-            kwargs[\"color\"] = color\n-        return self(kind=\"line\", x=x, y=y, **kwargs)\n \n-    @Appender(\n-        \"\"\"\n-        See Also\n-        --------\n-        DataFrame.plot.barh : Horizontal bar plot.\n-        DataFrame.plot : Make plots of a DataFrame.\n-        matplotlib.pyplot.bar : Make a bar plot with matplotlib.\n+        Parameters\n+        ----------\n+        x : label or position, optional\n+            Allows plotting of one column versus another. If not specified,\n+            the index of the DataFrame is used.\n+        y : label or position, optional\n+            Allows plotting of one column versus another. If not specified,\n+            all numerical columns are used.\n+        color : str, array-like, or dict, optional\n+            The color for each of the DataFrame's columns. Possible values are:\n \n-        Examples\n-        --------\n-        Basic plot.\n+            - A single color string referred to by name, RGB or RGBA code,\n+              for instance 'red' or '#a98d19'.\n \n-        .. plot::\n-            :context: close-figs\n+            - A sequence of color strings referred to by name, RGB or RGBA\n+              code, which will be used for each column recursively. For\n+              instance ['green','yellow'] each column's line will be filled in\n+              green or yellow, alternatively. If there is only a single column to\n+              be plotted, then only the first color from the color list will be\n+              used.\n \n-            >>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n-            >>> ax = df.plot.bar(x='lab', y='val', rot=0)\n+            - A dict of the form {column name : color}, so that each column will be\n+              colored accordingly. For example, if your columns are called `a` and\n+              `b`, then passing {'a': 'green', 'b': 'red'} will color lines for\n+              column `a` in green and lines for column `b` in red.\n+\n+        **kwargs\n+            Additional keyword arguments are documented in\n+            :meth:`DataFrame.plot`.\n \n-        Plot a whole dataframe to a bar plot. Each column is assigned a\n-        distinct color, and each row is nested in a group along the\n-        horizontal axis.\n+        Returns\n+        -------\n+        matplotlib.axes.Axes or np.ndarray of them\n+            An ndarray is returned with one :class:`matplotlib.axes.Axes`\n+            per column when ``subplots=True``.\n \n-        .. plot::\n-            :context: close-figs\n+                See Also",
    "comment": "i don't think the indentations of the see alsos in your pr are correct",
    "line_number": 1437,
    "enriched": "File: pandas/plotting/_core.py\nCode: @@ -1143,89 +1397,102 @@ def line(\n \n         This function is useful to plot lines using DataFrame's values\n         as coordinates.\n-        \"\"\"\n-        if color is not None:\n-            kwargs[\"color\"] = color\n-        return self(kind=\"line\", x=x, y=y, **kwargs)\n \n-    @Appender(\n-        \"\"\"\n-        See Also\n-        --------\n-        DataFrame.plot.barh : Horizontal bar plot.\n-        DataFrame.plot : Make plots of a DataFrame.\n-        matplotlib.pyplot.bar : Make a bar plot with matplotlib.\n+        Parameters\n+        ----------\n+        x : label or position, optional\n+            Allows plotting of one column versus another. If not specified,\n+            the index of the DataFrame is used.\n+        y : label or position, optional\n+            Allows plotting of one column versus another. If not specified,\n+            all numerical columns are used.\n+        color : str, array-like, or dict, optional\n+            The color for each of the DataFrame's columns. Possible values are:\n \n-        Examples\n-        --------\n-        Basic plot.\n+            - A single color string referred to by name, RGB or RGBA code,\n+              for instance 'red' or '#a98d19'.\n \n-        .. plot::\n-            :context: close-figs\n+            - A sequence of color strings referred to by name, RGB or RGBA\n+              code, which will be used for each column recursively. For\n+              instance ['green','yellow'] each column's line will be filled in\n+              green or yellow, alternatively. If there is only a single column to\n+              be plotted, then only the first color from the color list will be\n+              used.\n \n-            >>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n-            >>> ax = df.plot.bar(x='lab', y='val', rot=0)\n+            - A dict of the form {column name : color}, so that each column will be\n+              colored accordingly. For example, if your columns are called `a` and\n+              `b`, then passing {'a': 'green', 'b': 'red'} will color lines for\n+              column `a` in green and lines for column `b` in red.\n+\n+        **kwargs\n+            Additional keyword arguments are documented in\n+            :meth:`DataFrame.plot`.\n \n-        Plot a whole dataframe to a bar plot. Each column is assigned a\n-        distinct color, and each row is nested in a group along the\n-        horizontal axis.\n+        Returns\n+        -------\n+        matplotlib.axes.Axes or np.ndarray of them\n+            An ndarray is returned with one :class:`matplotlib.axes.Axes`\n+            per column when ``subplots=True``.\n \n-        .. plot::\n-            :context: close-figs\n+                See Also\nComment: I don't think the indentations of the `See Also`s in your PR are correct ",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "pandas/plotting/_core.py",
    "pr_number": 62584,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2404630522,
    "comment_created_at": "2025-10-05T19:29:11Z"
  },
  {
    "code": "@@ -1669,7 +1669,7 @@ def _has_valid_setitem_indexer(self, indexer) -> bool:\n         if not isinstance(indexer, tuple):\n             indexer = _tuplify(self.ndim, indexer)\n ",
    "comment": "[nitpick] consider adding a comment explaining why strict=false is intentionally used here, as it's the only exception to the strict zip pattern in this pr.",
    "line_number": 1671,
    "enriched": "File: pandas/core/indexing.py\nCode: @@ -1669,7 +1669,7 @@ def _has_valid_setitem_indexer(self, indexer) -> bool:\n         if not isinstance(indexer, tuple):\n             indexer = _tuplify(self.ndim, indexer)\n \nComment: [nitpick] Consider adding a comment explaining why `strict=False` is intentionally used here, as it's the only exception to the strict zip pattern in this PR.\n```suggestion\n\n        # Intentionally use strict=False here: in some cases, indexer may be shorter or longer\n        # than self.obj.axes, and we want to ignore any extra elements rather than raise an error.\n        # This is the only exception to the strict zip pattern in this codebase.\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "pandas/core/indexing.py",
    "pr_number": 62577,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2403941605,
    "comment_created_at": "2025-10-04T12:12:13Z"
  },
  {
    "code": "@@ -1226,6 +1226,7 @@ Other\n - Fixed bug in the :meth:`Series.rank` with object dtype and extremely small float values (:issue:`62036`)\n - Fixed bug where the :class:`DataFrame` constructor misclassified array-like objects with a ``.name`` attribute as :class:`Series` or :class:`Index` (:issue:`61443`)\n - Fixed regression in :meth:`DataFrame.from_records` not initializing subclasses properly (:issue:`57008`)\n+- Fixed regression in :meth:`Series.pow` on Series with all-NA ``float64[pyarrow]`` values; this now returns Series with :class:`NA` values (:issue:`62520`)",
    "comment": "i dont think the problem is in a released version, so no note needed",
    "line_number": 1229,
    "enriched": "File: doc/source/whatsnew/v3.0.0.rst\nCode: @@ -1226,6 +1226,7 @@ Other\n - Fixed bug in the :meth:`Series.rank` with object dtype and extremely small float values (:issue:`62036`)\n - Fixed bug where the :class:`DataFrame` constructor misclassified array-like objects with a ``.name`` attribute as :class:`Series` or :class:`Index` (:issue:`61443`)\n - Fixed regression in :meth:`DataFrame.from_records` not initializing subclasses properly (:issue:`57008`)\n+- Fixed regression in :meth:`Series.pow` on Series with all-NA ``float64[pyarrow]`` values; this now returns Series with :class:`NA` values (:issue:`62520`)\nComment: i dont think the problem is in a released version, so no note needed",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "doc/source/whatsnew/v3.0.0.rst",
    "pr_number": 62572,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2404012775,
    "comment_created_at": "2025-10-04T14:43:19Z"
  },
  {
    "code": "@@ -97,11 +97,11 @@ def test_nat_parse(all_parsers):\n     )\n     df.iloc[3:6, :] = np.nan\n \n-    with tm.ensure_clean(\"__nat_parse_.csv\") as path:\n-        df.to_csv(path)\n+    path = temp_file.parent / \"__nat_parse_.csv\"",
    "comment": "just use temp_file directly (no need to create the __nat_parse_.csv",
    "line_number": 100,
    "enriched": "File: pandas/tests/io/parser/test_parse_dates.py\nCode: @@ -97,11 +97,11 @@ def test_nat_parse(all_parsers):\n     )\n     df.iloc[3:6, :] = np.nan\n \n-    with tm.ensure_clean(\"__nat_parse_.csv\") as path:\n-        df.to_csv(path)\n+    path = temp_file.parent / \"__nat_parse_.csv\"\nComment: Just use `temp_file` directly (no need to create the `__nat_parse_.csv`",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/tests/io/parser/test_parse_dates.py",
    "pr_number": 62556,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2402704189,
    "comment_created_at": "2025-10-03T17:14:42Z"
  },
  {
    "code": "@@ -5359,6 +5371,11 @@ cpdef to_offset(freq, bint is_period=False):\n \n             tups = zip(split[0::4], split[1::4], split[2::4])\n             for n, (sep, stride, name) in enumerate(tups):\n+                if name in deprec_to_valid_alias:\n+                    raise ValueError(INVALID_FREQ_ERR_MSG.format(",
    "comment": "instead, could you make invalid_freq_err_msg a function like:\r\n\r\npython\r\ndef raise_invalid_freq(freq: str, extra_message: str | none = none): -> none:\r\n    msg = f\"invalid frequency: {freq}\"\r\n    if extra_message is not none:\r\n        msg += extra_message\r\n    if name in deprc_to_valid_alias:\r\n        msg += \"did you mean {deprec_to_valid_alias[name]}?\"\r\n    raise valueerror(msg)\r\n\r\n\r\ncan call this function wherever invalid_freq_err_msg. it appears this variable is used in multiple places so it would be nice to consistently provide this hint",
    "line_number": 5375,
    "enriched": "File: pandas/_libs/tslibs/offsets.pyx\nCode: @@ -5359,6 +5371,11 @@ cpdef to_offset(freq, bint is_period=False):\n \n             tups = zip(split[0::4], split[1::4], split[2::4])\n             for n, (sep, stride, name) in enumerate(tups):\n+                if name in deprec_to_valid_alias:\n+                    raise ValueError(INVALID_FREQ_ERR_MSG.format(\nComment: Instead, could you make `INVALID_FREQ_ERR_MSG` a function like:\r\n\r\n```python\r\ndef raise_invalid_freq(freq: str, extra_message: str | None = None): -> None:\r\n    msg = f\"Invalid frequency: {freq}\"\r\n    if extra_message is not None:\r\n        msg += extra_message\r\n    if name in deprc_to_valid_alias:\r\n        msg += \"Did you mean {deprec_to_valid_alias[name]}?\"\r\n    raise ValueError(msg)\r\n```\r\n\r\nCan call this function wherever `INVALID_FREQ_ERR_MSG`. It appears this variable is used in multiple places so it would be nice to consistently provide this hint",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/_libs/tslibs/offsets.pyx",
    "pr_number": 62539,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2399372480,
    "comment_created_at": "2025-10-02T16:24:31Z"
  },
  {
    "code": "@@ -271,3 +271,12 @@ def test_multiindex_assign_aligns_as_implicit_tuple(self):\n         df1[\"C\"] = s1\n         tm.assert_frame_equal(df1, df2)\n         tm.assert_frame_equal(df1, df3)\n+\n+        # GH 62518\n+        df4 = DataFrame(\n+            columns=MultiIndex.from_arrays(\n+                [[\"a\", \"a\", \"z\", \"z\"], pd.Categorical([1, 2, 1, 2])],\n+            ),\n+            dtype=object,\n+        )\n+        df4[\"z\"] = df4[\"z\"].astype(\"int64\")",
    "comment": "could you make this a separate test that uses the exact code snippet in the original issue?",
    "line_number": 282,
    "enriched": "File: pandas/tests/indexing/multiindex/test_multiindex.py\nCode: @@ -271,3 +271,12 @@ def test_multiindex_assign_aligns_as_implicit_tuple(self):\n         df1[\"C\"] = s1\n         tm.assert_frame_equal(df1, df2)\n         tm.assert_frame_equal(df1, df3)\n+\n+        # GH 62518\n+        df4 = DataFrame(\n+            columns=MultiIndex.from_arrays(\n+                [[\"a\", \"a\", \"z\", \"z\"], pd.Categorical([1, 2, 1, 2])],\n+            ),\n+            dtype=object,\n+        )\n+        df4[\"z\"] = df4[\"z\"].astype(\"int64\")\nComment: Could you make this a separate test that uses the exact code snippet in the original issue?",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/tests/indexing/multiindex/test_multiindex.py",
    "pr_number": 62527,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2395152829,
    "comment_created_at": "2025-10-01T16:16:00Z"
  },
  {
    "code": "@@ -1082,28 +1080,133 @@ def test_rolling_sem(frame_or_series):\n     tm.assert_series_equal(result, expected)\n \n \n-@pytest.mark.xfail(\n-    is_platform_arm() or is_platform_power() or is_platform_riscv64(),\n-    reason=\"GH 38921\",\n-)\n @pytest.mark.parametrize(\n-    (\"func\", \"third_value\", \"values\"),\n+    (\"func\", \"values\", \"window\", \"ddof\", \"exp_value\"),\n     [\n-        (\"var\", 1, [5e33, 0, 0.5, 0.5, 2, 0]),\n-        (\"std\", 1, [7.071068e16, 0, 0.7071068, 0.7071068, 1.414214, 0]),\n-        (\"var\", 2, [5e33, 0.5, 0, 0.5, 2, 0]),\n-        (\"std\", 2, [7.071068e16, 0.7071068, 0, 0.7071068, 1.414214, 0]),\n+        (\n+            \"var\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            3,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\n+            \"std\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            3,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\n+            \"var\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            2,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\n+            \"std\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            2,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\"var\", [99999999999999999, 1, 1, 2, 3, 1, 1], 2, 1, 0),\n+        (\"std\", [99999999999999999, 1, 1, 2, 3, 1, 1], 2, 1, 0),\n+        (\"var\", [99999999999999999, 1, 2, 2, 3, 1, 1], 2, 1, 0),\n+        (\"std\", [99999999999999999, 1, 2, 2, 3, 1, 1], 2, 1, 0),\n+        (\"var\", [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], 5, 0, \"numpy_compute\"),\n     ],\n )\n-def test_rolling_var_numerical_issues(func, third_value, values):\n-    # GH: 37051\n-    ds = Series([99999999999999999, 1, third_value, 2, 3, 1, 1])\n-    result = getattr(ds.rolling(2), func)()\n-    expected = Series([np.nan] + values)\n-    tm.assert_series_equal(result, expected)\n+def test_rolling_var_correctness(func, values, window, ddof, exp_value):\n+    # This tests subsume the previous tests under test_rolling_var_numerical_issues",
    "comment": "super nit but you can remove this first comment - if the user cared to see the history they can consult the git blame",
    "line_number": 1166,
    "enriched": "File: pandas/tests/window/test_rolling.py\nCode: @@ -1082,28 +1080,133 @@ def test_rolling_sem(frame_or_series):\n     tm.assert_series_equal(result, expected)\n \n \n-@pytest.mark.xfail(\n-    is_platform_arm() or is_platform_power() or is_platform_riscv64(),\n-    reason=\"GH 38921\",\n-)\n @pytest.mark.parametrize(\n-    (\"func\", \"third_value\", \"values\"),\n+    (\"func\", \"values\", \"window\", \"ddof\", \"exp_value\"),\n     [\n-        (\"var\", 1, [5e33, 0, 0.5, 0.5, 2, 0]),\n-        (\"std\", 1, [7.071068e16, 0, 0.7071068, 0.7071068, 1.414214, 0]),\n-        (\"var\", 2, [5e33, 0.5, 0, 0.5, 2, 0]),\n-        (\"std\", 2, [7.071068e16, 0.7071068, 0, 0.7071068, 1.414214, 0]),\n+        (\n+            \"var\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            3,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\n+            \"std\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            3,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\n+            \"var\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            2,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\n+            \"std\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            2,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\"var\", [99999999999999999, 1, 1, 2, 3, 1, 1], 2, 1, 0),\n+        (\"std\", [99999999999999999, 1, 1, 2, 3, 1, 1], 2, 1, 0),\n+        (\"var\", [99999999999999999, 1, 2, 2, 3, 1, 1], 2, 1, 0),\n+        (\"std\", [99999999999999999, 1, 2, 2, 3, 1, 1], 2, 1, 0),\n+        (\"var\", [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], 5, 0, \"numpy_compute\"),\n     ],\n )\n-def test_rolling_var_numerical_issues(func, third_value, values):\n-    # GH: 37051\n-    ds = Series([99999999999999999, 1, third_value, 2, 3, 1, 1])\n-    result = getattr(ds.rolling(2), func)()\n-    expected = Series([np.nan] + values)\n-    tm.assert_series_equal(result, expected)\n+def test_rolling_var_correctness(func, values, window, ddof, exp_value):\n+    # This tests subsume the previous tests under test_rolling_var_numerical_issues\nComment: Super nit but you can remove this first comment - if the user cared to see the history they can consult the git blame",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "pandas/tests/window/test_rolling.py",
    "pr_number": 62514,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2392136885,
    "comment_created_at": "2025-09-30T16:06:01Z"
  },
  {
    "code": "@@ -421,3 +423,39 @@ def test_hypothesis_delimited_date(\n \n     assert except_out_dateutil == except_in_dateutil\n     assert result == expected\n+\n+\n+@pytest.mark.parametrize(\"input\", [\"21-01-01\", \"01-01-21\"])\n+@pytest.mark.parametrize(\"dayfirst\", [True, False])\n+def test_parse_datetime_string_with_reso_dayfirst(dayfirst, input):\n+    set_option(\"display.date_dayfirst\", dayfirst)",
    "comment": "can you use the option_context context manager instead (and below)?",
    "line_number": 431,
    "enriched": "File: pandas/tests/tslibs/test_parsing.py\nCode: @@ -421,3 +423,39 @@ def test_hypothesis_delimited_date(\n \n     assert except_out_dateutil == except_in_dateutil\n     assert result == expected\n+\n+\n+@pytest.mark.parametrize(\"input\", [\"21-01-01\", \"01-01-21\"])\n+@pytest.mark.parametrize(\"dayfirst\", [True, False])\n+def test_parse_datetime_string_with_reso_dayfirst(dayfirst, input):\n+    set_option(\"display.date_dayfirst\", dayfirst)\nComment: Can you use the `option_context` context manager instead (and below)?",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/tests/tslibs/test_parsing.py",
    "pr_number": 62511,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2392513598,
    "comment_created_at": "2025-09-30T18:44:08Z"
  },
  {
    "code": "@@ -22,8 +22,6 @@ become the default string dtype in pandas 3.0. See\n \n Bug fixes\n ^^^^^^^^^\n-- Fix :meth:`~Series.str.isdigit` to correctly recognize unicode superscript\n-  characters as digits for :class:`StringDtype` backed by PyArrow (:issue:`61466`)",
    "comment": "this was added to the wrong file, the fix is actually included in 2.3.3",
    "line_number": 26,
    "enriched": "File: doc/source/whatsnew/v2.3.2.rst\nCode: @@ -22,8 +22,6 @@ become the default string dtype in pandas 3.0. See\n \n Bug fixes\n ^^^^^^^^^\n-- Fix :meth:`~Series.str.isdigit` to correctly recognize unicode superscript\n-  characters as digits for :class:`StringDtype` backed by PyArrow (:issue:`61466`)\nComment: This was added to the wrong file, the fix is actually included in 2.3.3",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "doc/source/whatsnew/v2.3.2.rst",
    "pr_number": 62499,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2387190681,
    "comment_created_at": "2025-09-29T08:54:21Z"
  },
  {
    "code": "@@ -234,6 +236,16 @@ def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n             # e.g. test_np_max_nested_tuples\n             return result\n         else:\n+            if self.dtype.type is str:  # type: ignore[comparison-overlap]",
    "comment": "nit: would be a nice if this was elif so it could be un-indented one level, but not a big deal.",
    "line_number": 239,
    "enriched": "File: pandas/core/arrays/numpy_.py\nCode: @@ -234,6 +236,16 @@ def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n             # e.g. test_np_max_nested_tuples\n             return result\n         else:\n+            if self.dtype.type is str:  # type: ignore[comparison-overlap]\nComment: nit: Would be a nice if this was `elif` so it could be un-indented one level, but not a big deal.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "pandas/core/arrays/numpy_.py",
    "pr_number": 62498,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2388551302,
    "comment_created_at": "2025-09-29T16:22:00Z"
  },
  {
    "code": "@@ -1740,3 +1740,15 @@ def test_date_range_negative_freq_year_end_inbounds(self, unit):\n             freq=\"-1YE\",\n         )\n         tm.assert_index_equal(rng, exp)\n+\n+    def test_date_range_tzaware_endpoints_accept_ambiguous(self):\n+        # With tz-aware endpoints and a calendar offset (MS),\n+        # date_range should accept `ambiguous=True` and produce\n+        # the same result as passing tz explicitly with naive endpoints.\n+        start = Timestamp(\"1916-08-01\", tz=\"Europe/Oslo\")\n+        end = Timestamp(\"1916-12-01\", tz=\"Europe/Oslo\")\n+        res = date_range(start, end, freq=\"MS\", ambiguous=True)\n+        exp = date_range(\n+            \"1916-08-01\", \"1916-12-01\", freq=\"MS\", tz=\"Europe/Oslo\", ambiguous=True",
    "comment": "could you add another test that uses nonexistent?",
    "line_number": 1752,
    "enriched": "File: pandas/tests/indexes/datetimes/test_date_range.py\nCode: @@ -1740,3 +1740,15 @@ def test_date_range_negative_freq_year_end_inbounds(self, unit):\n             freq=\"-1YE\",\n         )\n         tm.assert_index_equal(rng, exp)\n+\n+    def test_date_range_tzaware_endpoints_accept_ambiguous(self):\n+        # With tz-aware endpoints and a calendar offset (MS),\n+        # date_range should accept `ambiguous=True` and produce\n+        # the same result as passing tz explicitly with naive endpoints.\n+        start = Timestamp(\"1916-08-01\", tz=\"Europe/Oslo\")\n+        end = Timestamp(\"1916-12-01\", tz=\"Europe/Oslo\")\n+        res = date_range(start, end, freq=\"MS\", ambiguous=True)\n+        exp = date_range(\n+            \"1916-08-01\", \"1916-12-01\", freq=\"MS\", tz=\"Europe/Oslo\", ambiguous=True\nComment: Could you add another test that uses `nonexistent`?",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "pandas/tests/indexes/datetimes/test_date_range.py",
    "pr_number": 62493,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2388586009,
    "comment_created_at": "2025-09-29T16:37:15Z"
  },
  {
    "code": "@@ -170,30 +170,30 @@ def parser(request):\n # FILE OUTPUT\n \n \n-def test_file_output_str_read(xml_books, parser, from_file_expected):\n+def test_file_output_str_read(xml_books, parser, from_file_expected, temp_file):\n     df_file = read_xml(xml_books, parser=parser)\n \n-    with tm.ensure_clean(\"test.xml\") as path:\n-        df_file.to_xml(path, parser=parser)\n-        with open(path, \"rb\") as f:\n-            output = f.read().decode(\"utf-8\").strip()\n+    path = temp_file.parent / \"test.xml\"",
    "comment": "for these cases where the test expects a specific file name, could you use the pytest tmp_path fixture instead to build this file instead i.e. path = tmp_path / \"test.xml\"",
    "line_number": 176,
    "enriched": "File: pandas/tests/io/xml/test_to_xml.py\nCode: @@ -170,30 +170,30 @@ def parser(request):\n # FILE OUTPUT\n \n \n-def test_file_output_str_read(xml_books, parser, from_file_expected):\n+def test_file_output_str_read(xml_books, parser, from_file_expected, temp_file):\n     df_file = read_xml(xml_books, parser=parser)\n \n-    with tm.ensure_clean(\"test.xml\") as path:\n-        df_file.to_xml(path, parser=parser)\n-        with open(path, \"rb\") as f:\n-            output = f.read().decode(\"utf-8\").strip()\n+    path = temp_file.parent / \"test.xml\"\nComment: For these cases where the test expects a specific file name, could you use the pytest `tmp_path` fixture instead to build this file instead i.e. `path = tmp_path / \"test.xml\"`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "pandas/tests/io/xml/test_to_xml.py",
    "pr_number": 62475,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2388694997,
    "comment_created_at": "2025-09-29T17:23:33Z"
  },
  {
    "code": "@@ -295,29 +295,29 @@ def test_empty_with_nrows_chunksize(all_parsers, iterator):\n     tm.assert_frame_equal(result, expected)\n \n \n-def test_read_csv_memory_growth_chunksize(all_parsers):\n+def test_read_csv_memory_growth_chunksize(temp_file, all_parsers):\n     # see gh-24805\n     #\n     # Let's just make sure that we don't crash\n     # as we iteratively process all chunks.\n     parser = all_parsers\n \n-    with tm.ensure_clean() as path:\n-        with open(path, \"w\", encoding=\"utf-8\") as f:\n-            for i in range(1000):\n-                f.write(str(i) + \"\\n\")\n-\n-        if parser.engine == \"pyarrow\":\n-            msg = \"The 'chunksize' option is not supported with the 'pyarrow' engine\"\n-            with pytest.raises(ValueError, match=msg):\n-                with parser.read_csv(path, chunksize=20) as result:\n-                    for _ in result:\n-                        pass\n-            return\n-\n-        with parser.read_csv(path, chunksize=20) as result:\n-            for _ in result:\n-                pass\n+    path = str(temp_file)",
    "comment": "can you just use temp_file directly, instead of converting to a str? (unless the test appears to explicitly testing a string path)",
    "line_number": 305,
    "enriched": "File: pandas/tests/io/parser/common/test_chunksize.py\nCode: @@ -295,29 +295,29 @@ def test_empty_with_nrows_chunksize(all_parsers, iterator):\n     tm.assert_frame_equal(result, expected)\n \n \n-def test_read_csv_memory_growth_chunksize(all_parsers):\n+def test_read_csv_memory_growth_chunksize(temp_file, all_parsers):\n     # see gh-24805\n     #\n     # Let's just make sure that we don't crash\n     # as we iteratively process all chunks.\n     parser = all_parsers\n \n-    with tm.ensure_clean() as path:\n-        with open(path, \"w\", encoding=\"utf-8\") as f:\n-            for i in range(1000):\n-                f.write(str(i) + \"\\n\")\n-\n-        if parser.engine == \"pyarrow\":\n-            msg = \"The 'chunksize' option is not supported with the 'pyarrow' engine\"\n-            with pytest.raises(ValueError, match=msg):\n-                with parser.read_csv(path, chunksize=20) as result:\n-                    for _ in result:\n-                        pass\n-            return\n-\n-        with parser.read_csv(path, chunksize=20) as result:\n-            for _ in result:\n-                pass\n+    path = str(temp_file)\nComment: Can you just use `temp_file` directly, instead of converting to a `str`? (Unless the test appears to explicitly testing a string path) ",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/tests/io/parser/common/test_chunksize.py",
    "pr_number": 62474,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2411252883,
    "comment_created_at": "2025-10-07T16:46:11Z"
  },
  {
    "code": "@@ -260,15 +261,15 @@ def _return_result_expected(\n                 kwargs[\"index_col\"] = list(range(rnlvl))\n             kwargs[\"header\"] = list(range(cnlvl))\n \n-            with tm.ensure_clean(\"__tmp_to_csv_moar__\") as path:\n-                df.to_csv(path, encoding=\"utf8\", chunksize=chunksize)\n-                recons = self.read_csv(path, **kwargs)\n+            path = str(temp_file)",
    "comment": "generally, could you try using temp_file as an argument instead of converting to a string when doing these modifications, unless the test appears to explicitly be testing a string argument?",
    "line_number": 264,
    "enriched": "File: pandas/tests/frame/methods/test_to_csv.py\nCode: @@ -260,15 +261,15 @@ def _return_result_expected(\n                 kwargs[\"index_col\"] = list(range(rnlvl))\n             kwargs[\"header\"] = list(range(cnlvl))\n \n-            with tm.ensure_clean(\"__tmp_to_csv_moar__\") as path:\n-                df.to_csv(path, encoding=\"utf8\", chunksize=chunksize)\n-                recons = self.read_csv(path, **kwargs)\n+            path = str(temp_file)\nComment: Generally, could you try using `temp_file` as an argument instead of converting to a string when doing these modifications, unless the test appears to explicitly be testing a string argument?",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/tests/frame/methods/test_to_csv.py",
    "pr_number": 62461,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2388722795,
    "comment_created_at": "2025-09-29T17:36:34Z"
  },
  {
    "code": "@@ -803,7 +803,7 @@ def replace_list(\n         # Exclude anything that we know we won't contain\n         pairs = [\n             (x, y)\n-            for x, y in zip(src_list, dest_list)\n+            for x, y in zip(src_list, dest_list, strict=False)",
    "comment": "is strict=false necessary here?",
    "line_number": 806,
    "enriched": "File: pandas/core/internals/blocks.py\nCode: @@ -803,7 +803,7 @@ def replace_list(\n         # Exclude anything that we know we won't contain\n         pairs = [\n             (x, y)\n-            for x, y in zip(src_list, dest_list)\n+            for x, y in zip(src_list, dest_list, strict=False)\nComment: Is `strict=False` necessary here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/core/internals/blocks.py",
    "pr_number": 62453,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2380190384,
    "comment_created_at": "2025-09-25T19:56:29Z"
  },
  {
    "code": "@@ -1009,7 +1002,6 @@ def _fit(self, X, skip_num_points=0):\n         elif self.init == \"pca\":\n             pca = PCA(\n                 n_components=self.n_components,\n-                svd_solver=\"randomized\",",
    "comment": "note: this change might have a side effect for dense data, but i think our \"auto\" heuristic should behave reasonably well in most cases.",
    "line_number": 1012,
    "enriched": "File: sklearn/manifold/_t_sne.py\nCode: @@ -1009,7 +1002,6 @@ def _fit(self, X, skip_num_points=0):\n         elif self.init == \"pca\":\n             pca = PCA(\n                 n_components=self.n_components,\n-                svd_solver=\"randomized\",\nComment: Note: this change might have a side effect for dense data, but I think our \"auto\" heuristic should behave reasonably well in most cases.",
    "subcategory": "false positive",
    "category": "false positive",
    "file_path": "sklearn/manifold/_t_sne.py",
    "pr_number": 32433,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2413877575,
    "comment_created_at": "2025-10-08T13:31:54Z"
  },
  {
    "code": "@@ -23,7 +23,7 @@ jobs:\n         run: mkdir -p \"$ARTIFACTS_DIR\"\n \n       - name: Download artifact\n-        uses: actions/download-artifact@v4",
    "comment": "i haven't fully understood the issue with the cuda ci, but i am wondering whether the same issue with single artifact as in the cuda ci will happen here.\r\n\r\nthis action is not tested in prs, so ideally you need to test it in your fork. the alternative would be to merge it into main and cross your fingers.",
    "line_number": 26,
    "enriched": "File: .github/workflows/bot-lint-comment.yml\nCode: @@ -23,7 +23,7 @@ jobs:\n         run: mkdir -p \"$ARTIFACTS_DIR\"\n \n       - name: Download artifact\n-        uses: actions/download-artifact@v4\nComment: I haven't fully understood the issue with the CUDA CI, but I am wondering whether the same issue with single artifact as in the CUDA CI will happen here.\r\n\r\nThis action is not tested in PRs, so ideally you need to test it in your fork. The alternative would be to merge it into `main` and cross your fingers. ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".github/workflows/bot-lint-comment.yml",
    "pr_number": 32413,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2409997674,
    "comment_created_at": "2025-10-07T09:28:25Z"
  },
  {
    "code": "@@ -430,6 +437,8 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"ndcg_score\",\n     \"dcg_score\",\n     \"label_ranking_average_precision_score\",\n+    \"d2_log_loss_score\",\n+    \"d2_brier_score\",",
    "comment": "@ogrisel i added these in here because both log_loss and brier_score_loss are present but let me know if this needs to be changed. this helps skip one test that checks for a valueerror.",
    "line_number": 441,
    "enriched": "File: sklearn/metrics/tests/test_common.py\nCode: @@ -430,6 +437,8 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"ndcg_score\",\n     \"dcg_score\",\n     \"label_ranking_average_precision_score\",\n+    \"d2_log_loss_score\",\n+    \"d2_brier_score\",\nComment: @ogrisel I added these in here because both `log_loss` and `brier_score_loss` are present but let me know if this needs to be changed. This helps skip one test that checks for a ValueError.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "sklearn/metrics/tests/test_common.py",
    "pr_number": 32356,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2405577172,
    "comment_created_at": "2025-10-06T10:05:13Z"
  },
  {
    "code": "@@ -137,21 +137,21 @@ def remove_from(alist, to_remove):\n         },\n     },\n     {\n-        \"name\": \"pylatest_conda_forge_mkl_osx-64\",\n+        \"name\": \"pylatest_conda_forge_osx-arm64\",\n         \"type\": \"conda\",\n         \"tag\": \"main-ci\",\n         \"folder\": \"build_tools/azure\",\n-        \"platform\": \"osx-64\",\n+        \"platform\": \"osx-arm64\",\n         \"channels\": [\"conda-forge\"],\n         \"conda_dependencies\": common_dependencies\n         + [\n             \"ccache\",\n             \"compilers\",\n             \"llvm-openmp\",\n+            \"pytorch\",\n+            \"pytorch-cpu\",\n+            \"array-api-strict\",\n         ],\n-        \"package_constraints\": {\n-            \"blas\": \"[build=mkl]\",",
    "comment": "mkl does not exist on conda-forge for osx-arm64 it seems so i removed it",
    "line_number": 153,
    "enriched": "File: build_tools/update_environments_and_lock_files.py\nCode: @@ -137,21 +137,21 @@ def remove_from(alist, to_remove):\n         },\n     },\n     {\n-        \"name\": \"pylatest_conda_forge_mkl_osx-64\",\n+        \"name\": \"pylatest_conda_forge_osx-arm64\",\n         \"type\": \"conda\",\n         \"tag\": \"main-ci\",\n         \"folder\": \"build_tools/azure\",\n-        \"platform\": \"osx-64\",\n+        \"platform\": \"osx-arm64\",\n         \"channels\": [\"conda-forge\"],\n         \"conda_dependencies\": common_dependencies\n         + [\n             \"ccache\",\n             \"compilers\",\n             \"llvm-openmp\",\n+            \"pytorch\",\n+            \"pytorch-cpu\",\n+            \"array-api-strict\",\n         ],\n-        \"package_constraints\": {\n-            \"blas\": \"[build=mkl]\",\nComment: mkl does not exist on conda-forge for osx-arm64 it seems so I removed it",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "build_tools/update_environments_and_lock_files.py",
    "pr_number": 32349,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2409895923,
    "comment_created_at": "2025-10-07T08:51:55Z"
  },
  {
    "code": "@@ -1048,6 +1049,7 @@ def test_param_is_non_default(default_value, test_value):\n         ((1, 2, 3), [1, 2, 3]),\n         ((1, 2, 3), np.array([1, 2, 3])),\n         (np.nan, np.nan),\n+        (np.nan, pd.NA),",
    "comment": "i don't think that we want to treat pd.na as being the same as np.nan.",
    "line_number": 1052,
    "enriched": "File: sklearn/tests/test_base.py\nCode: @@ -1048,6 +1049,7 @@ def test_param_is_non_default(default_value, test_value):\n         ((1, 2, 3), [1, 2, 3]),\n         ((1, 2, 3), np.array([1, 2, 3])),\n         (np.nan, np.nan),\n+        (np.nan, pd.NA),\nComment: I don't think that we want to treat `pd.NA` as being the same as `np.nan`.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "sklearn/tests/test_base.py",
    "pr_number": 32341,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2399018332,
    "comment_created_at": "2025-10-02T14:22:19Z"
  },
  {
    "code": "@@ -1,6 +1,6 @@\n from cython cimport final\n \n-from ...utils._typedefs cimport intp_t, float64_t\n+from sklearn.utils._typedefs cimport intp_t, float64_t",
    "comment": "this change is unrelated and already implemented in your previous pr. please revert it.",
    "line_number": 3,
    "enriched": "File: sklearn/metrics/_pairwise_distances_reduction/_base.pxd.tp\nCode: @@ -1,6 +1,6 @@\n from cython cimport final\n \n-from ...utils._typedefs cimport intp_t, float64_t\n+from sklearn.utils._typedefs cimport intp_t, float64_t\nComment: This change is unrelated and already implemented in your previous PR. Please revert it.",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "sklearn/metrics/_pairwise_distances_reduction/_base.pxd.tp",
    "pr_number": 32324,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2397984153,
    "comment_created_at": "2025-10-02T09:28:35Z"
  },
  {
    "code": "@@ -763,7 +763,7 @@\n         (\"pegasus\", \"Pegasus\"),\n         (\"pegasus_x\", \"PEGASUS-X\"),\n         (\"perceiver\", \"Perceiver\"),\n-        (\"perception_encoder\", \"PerceptionEncoder\"),\n+        (\"perception_encoder\", \"TimmWrapperConfig\"),",
    "comment": "we can just delete it. when models re-use existing vision backbone, we dont usually add them as separate models in auto-map",
    "line_number": 766,
    "enriched": "File: src/transformers/models/auto/configuration_auto.py\nCode: @@ -763,7 +763,7 @@\n         (\"pegasus\", \"Pegasus\"),\n         (\"pegasus_x\", \"PEGASUS-X\"),\n         (\"perceiver\", \"Perceiver\"),\n-        (\"perception_encoder\", \"PerceptionEncoder\"),\n+        (\"perception_encoder\", \"TimmWrapperConfig\"),\nComment: we can just delete it. When models re-use existing vision backbone, we dont usually add them as separate models in auto-map",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/models/auto/configuration_auto.py",
    "pr_number": 41464,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2416268670,
    "comment_created_at": "2025-10-09T10:13:06Z"
  },
  {
    "code": "@@ -170,8 +170,13 @@ def _lazy_load_causal_conv1d():\n     if is_kernels_available():\n         from kernels import get_kernel\n \n-        _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n+        try:\n+            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n+        except FileNotFoundError:\n+            # no kernel binary match, fallback to slow path\n+            _causal_conv1d_cache = (None, None)\n+        else:\n+            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)",
    "comment": "if we don't find an appropriate kernel binary on target platform, we should fall back to slow path, rather than crash.",
    "line_number": 179,
    "enriched": "File: src/transformers/models/falcon_mamba/modeling_falcon_mamba.py\nCode: @@ -170,8 +170,13 @@ def _lazy_load_causal_conv1d():\n     if is_kernels_available():\n         from kernels import get_kernel\n \n-        _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n+        try:\n+            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n+        except FileNotFoundError:\n+            # no kernel binary match, fallback to slow path\n+            _causal_conv1d_cache = (None, None)\n+        else:\n+            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\nComment: If we don't find an appropriate kernel binary on target platform, we should fall back to slow path, rather than crash.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
    "pr_number": 41428,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2411845952,
    "comment_created_at": "2025-10-07T20:36:37Z"
  },
  {
    "code": "@@ -427,7 +427,8 @@ def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n                 \"when creating this class.\"\n             )\n \n-        self.num_key_value_groups = config.num_experts_per_tok\n+        self.num_key_value_groups = 1  # We ignore this by setting it to 1 as we have different repeat patterns\n+        self.top_k = config.num_experts_per_tok",
    "comment": "is this what we have done before #40132, or it's kind new stuff?",
    "line_number": 431,
    "enriched": "File: src/transformers/models/jetmoe/modeling_jetmoe.py\nCode: @@ -427,7 +427,8 @@ def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n                 \"when creating this class.\"\n             )\n \n-        self.num_key_value_groups = config.num_experts_per_tok\n+        self.num_key_value_groups = 1  # We ignore this by setting it to 1 as we have different repeat patterns\n+        self.top_k = config.num_experts_per_tok\nComment: Is this what we have done before #40132, or it's kind new stuff?\r\n\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "src/transformers/models/jetmoe/modeling_jetmoe.py",
    "pr_number": 41423,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2411173953,
    "comment_created_at": "2025-10-07T16:16:02Z"
  },
  {
    "code": "@@ -123,11 +123,6 @@ class PreTrainedConfig(PushToHubMixin):\n         tie_encoder_decoder (`bool`, *optional*, defaults to `False`):\n             Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder\n             and decoder model to have the exact same parameter names.\n-        prune_heads (`dict[int, list[int]]`, *optional*, defaults to `{}`):\n-            Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of\n-            heads to prune in said layer.\n-\n-            For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.",
    "comment": "this won't break the loading if a saved config has this attribute?",
    "line_number": 130,
    "enriched": "File: src/transformers/configuration_utils.py\nCode: @@ -123,11 +123,6 @@ class PreTrainedConfig(PushToHubMixin):\n         tie_encoder_decoder (`bool`, *optional*, defaults to `False`):\n             Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder\n             and decoder model to have the exact same parameter names.\n-        prune_heads (`dict[int, list[int]]`, *optional*, defaults to `{}`):\n-            Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of\n-            heads to prune in said layer.\n-\n-            For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.\nComment: this won't break the loading if a saved config has this attribute?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "src/transformers/configuration_utils.py",
    "pr_number": 41417,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2411105360,
    "comment_created_at": "2025-10-07T15:50:45Z"
  },
  {
    "code": "@@ -607,7 +610,12 @@ def __init__(\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n             streaming: Whether to stream tokens as they are generated\n         \"\"\"\n-        model.set_attn_implementation(model.config._attn_implementation, use_paged=True)\n+        attn_implementation = f\"paged|{model.config._attn_implementation}\"\n+        attn_wrapper = ALL_ATTENTION_FUNCTIONS.get(attn_implementation, None)\n+        if attn_wrapper is None:",
    "comment": "maybe add a comment explaining that this code path is only taken if no match is found ie. the model.config._attn_implementation is a kernel (was not clear to me at first)",
    "line_number": 615,
    "enriched": "File: src/transformers/generation/continuous_batching/continuous_api.py\nCode: @@ -607,7 +610,12 @@ def __init__(\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n             streaming: Whether to stream tokens as they are generated\n         \"\"\"\n-        model.set_attn_implementation(model.config._attn_implementation, use_paged=True)\n+        attn_implementation = f\"paged|{model.config._attn_implementation}\"\n+        attn_wrapper = ALL_ATTENTION_FUNCTIONS.get(attn_implementation, None)\n+        if attn_wrapper is None:\nComment: Maybe add a comment explaining that this code path is only taken if no match is found ie. the `model.config._attn_implementation` is a kernel (was not clear to me at first)",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "src/transformers/generation/continuous_batching/continuous_api.py",
    "pr_number": 41413,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2413261691,
    "comment_created_at": "2025-10-08T09:48:12Z"
  },
  {
    "code": "@@ -2006,17 +1996,13 @@ def _prepare_cache_for_generation(\n             elif \"dynamic\" in generation_config.cache_implementation:\n                 model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n \n-        # Use DynamicCache instance by default. This will avoid back and forth from legacy format that\n-        # keeps copying the cache thus using much more memory\n-        # TODO (joao): remove this `else` when we remove the last traces of the legacy cache format (v4.58.0, search\n-        # for `instance(past_key_values, Cache)` as well). In general, if `cache_implementation` is unset, cache\n-        # initialization should happen inside the model at prefill time.\n-        else:\n-            model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n-\n         # TODO (joao): this logic is incomplete, e.g. `offloaded` should apply to both caches. Refactor this function\n         # to correctly pass parameterization to both caches.\n-        if requires_cross_attention_cache and not isinstance(model_kwargs[cache_name], EncoderDecoderCache):\n+        if (\n+            requires_cross_attention_cache\n+            and cache_name in model_kwargs",
    "comment": "ig this is for special models where name isn't \"past_key_values\". is it not going to fail later in model, if we leave the non-encoderdecodercache cache?",
    "line_number": 2003,
    "enriched": "File: src/transformers/generation/utils.py\nCode: @@ -2006,17 +1996,13 @@ def _prepare_cache_for_generation(\n             elif \"dynamic\" in generation_config.cache_implementation:\n                 model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n \n-        # Use DynamicCache instance by default. This will avoid back and forth from legacy format that\n-        # keeps copying the cache thus using much more memory\n-        # TODO (joao): remove this `else` when we remove the last traces of the legacy cache format (v4.58.0, search\n-        # for `instance(past_key_values, Cache)` as well). In general, if `cache_implementation` is unset, cache\n-        # initialization should happen inside the model at prefill time.\n-        else:\n-            model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n-\n         # TODO (joao): this logic is incomplete, e.g. `offloaded` should apply to both caches. Refactor this function\n         # to correctly pass parameterization to both caches.\n-        if requires_cross_attention_cache and not isinstance(model_kwargs[cache_name], EncoderDecoderCache):\n+        if (\n+            requires_cross_attention_cache\n+            and cache_name in model_kwargs\nComment: ig this is for special models where name isn't \"past_key_values\". Is it not going to fail later in model, if we leave the non-EncoderDecoderCache cache?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "src/transformers/generation/utils.py",
    "pr_number": 41405,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2410373191,
    "comment_created_at": "2025-10-07T11:54:44Z"
  },
  {
    "code": "@@ -573,8 +599,16 @@ def test_generation_siglip_backbone(self):\n         # Make sure that `generate` works\n         output = model.generate(**inputs, max_new_tokens=30)\n \n-        EXPECTED_DECODED_TEXT = \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\"\n-        self.assertTrue(processor.batch_decode(output, skip_special_tokens=True)[0] == EXPECTED_DECODED_TEXT)\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+                (\"xpu\", 3): \"user\\n\\nWhat are these?\\nassistant These are two cats, one with a green collar and the other with a black collar. They are lying on a pink blanket and appear to be sleeping\",\n+                (\"cuda\", None): \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\",\n+            }\n+        )  # fmt: skip\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+\n+        decoded_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n+        self.assertTrue(decoded_text == EXPECTED_DECODED_TEXT)",
    "comment": "let's use self.assertequal, so the error message would be more informative when it fails.",
    "line_number": 611,
    "enriched": "File: tests/models/llava/test_modeling_llava.py\nCode: @@ -573,8 +599,16 @@ def test_generation_siglip_backbone(self):\n         # Make sure that `generate` works\n         output = model.generate(**inputs, max_new_tokens=30)\n \n-        EXPECTED_DECODED_TEXT = \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\"\n-        self.assertTrue(processor.batch_decode(output, skip_special_tokens=True)[0] == EXPECTED_DECODED_TEXT)\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+                (\"xpu\", 3): \"user\\n\\nWhat are these?\\nassistant These are two cats, one with a green collar and the other with a black collar. They are lying on a pink blanket and appear to be sleeping\",\n+                (\"cuda\", None): \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\",\n+            }\n+        )  # fmt: skip\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+\n+        decoded_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n+        self.assertTrue(decoded_text == EXPECTED_DECODED_TEXT)\nComment: Let's use `self.assertEqual`, so the error message would be more informative when it fails.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "tests/models/llava/test_modeling_llava.py",
    "pr_number": 41386,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2410562961,
    "comment_created_at": "2025-10-07T13:02:56Z"
  },
  {
    "code": "@@ -1305,6 +1305,13 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n         # pass defaults to output dictionary\n         output_kwargs.update(default_kwargs)\n \n+        # For `common_kwargs` just update all modality-specific kwargs with same key/values\n+        common_kwargs = kwargs.get(\"common_kwargs\", {})\n+        common_kwargs.update(ModelProcessorKwargs._defaults.get(\"common_kwargs\", {}))",
    "comment": "in that case i think we want to change the order of these lines as well, instead of overriding user values by defaults",
    "line_number": 1310,
    "enriched": "File: src/transformers/processing_utils.py\nCode: @@ -1305,6 +1305,13 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n         # pass defaults to output dictionary\n         output_kwargs.update(default_kwargs)\n \n+        # For `common_kwargs` just update all modality-specific kwargs with same key/values\n+        common_kwargs = kwargs.get(\"common_kwargs\", {})\n+        common_kwargs.update(ModelProcessorKwargs._defaults.get(\"common_kwargs\", {}))\nComment: in that case i think we want to change the order of these lines as well, instead of overriding user values by defaults",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "src/transformers/processing_utils.py",
    "pr_number": 41381,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2409903065,
    "comment_created_at": "2025-10-07T08:54:26Z"
  },
  {
    "code": "@@ -796,12 +789,5 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, embeddings):\n         self.cpmant.input_embedding = embeddings\n \n-    def _reorder_cache(self, past_key_values, beam_idx):",
    "comment": "(good catch!)",
    "line_number": 799,
    "enriched": "File: src/transformers/models/cpmant/modeling_cpmant.py\nCode: @@ -796,12 +789,5 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, embeddings):\n         self.cpmant.input_embedding = embeddings\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\nComment: (good catch!)",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "src/transformers/models/cpmant/modeling_cpmant.py",
    "pr_number": 41378,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2413131680,
    "comment_created_at": "2025-10-08T09:00:56Z"
  },
  {
    "code": "@@ -127,10 +126,16 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n @require_torch\n class JetMoeIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     @slow\n     def test_model_8b_logits(self):\n         input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n-        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\")\n+        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\", device_map=\"auto\", torch_dtype=torch.bfloat16)",
    "comment": "my only nit, don't we want to use fp16 for more consistency or was the model saved in bf16?",
    "line_number": 138,
    "enriched": "File: tests/models/jetmoe/test_modeling_jetmoe.py\nCode: @@ -127,10 +126,16 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n @require_torch\n class JetMoeIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     @slow\n     def test_model_8b_logits(self):\n         input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n-        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\")\n+        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nComment: My only nit, don't we want to use fp16 for more consistency or was the model saved in bf16?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/models/jetmoe/test_modeling_jetmoe.py",
    "pr_number": 41377,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2411156916,
    "comment_created_at": "2025-10-07T16:10:22Z"
  },
  {
    "code": "@@ -5904,10 +5902,10 @@ class AttentionInterface(GeneralInterface):\n         \"flash_attention_3\": flash_attention_forward,\n         \"flash_attention_2\": flash_attention_forward,\n         \"flex_attention\": flex_attention_forward,\n-        \"paged_attention\": paged_attention_forward,\n         \"sdpa\": sdpa_attention_forward,\n-        \"sdpa_paged\": sdpa_attention_paged_forward,\n-        \"eager_paged\": eager_paged_attention_forward,\n+        \"paged|flash_attention2\": paged_attention_forward,\n+        \"paged|sdpa\": sdpa_attention_paged_forward,\n+        \"paged|eager\": eager_paged_attention_forward,",
    "comment": "should paged|flex_attention be an option as well? i see it listed below in the tests",
    "line_number": 5908,
    "enriched": "File: src/transformers/modeling_utils.py\nCode: @@ -5904,10 +5902,10 @@ class AttentionInterface(GeneralInterface):\n         \"flash_attention_3\": flash_attention_forward,\n         \"flash_attention_2\": flash_attention_forward,\n         \"flex_attention\": flex_attention_forward,\n-        \"paged_attention\": paged_attention_forward,\n         \"sdpa\": sdpa_attention_forward,\n-        \"sdpa_paged\": sdpa_attention_paged_forward,\n-        \"eager_paged\": eager_paged_attention_forward,\n+        \"paged|flash_attention2\": paged_attention_forward,\n+        \"paged|sdpa\": sdpa_attention_paged_forward,\n+        \"paged|eager\": eager_paged_attention_forward,\nComment: should `paged|flex_attention` be an option as well? I see it listed below in the tests",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "src/transformers/modeling_utils.py",
    "pr_number": 41370,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2406397043,
    "comment_created_at": "2025-10-06T13:41:47Z"
  },
  {
    "code": "@@ -238,34 +251,40 @@ def __init__(self, hub_dataset: str):\n \n         self.models_root = MODELS_ROOT\n         self.hub_dataset = hub_dataset\n-        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n-        self.dtype = \"auto\"\n         self.tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n-        self.model = (\n-            AutoModel.from_pretrained(\n-                EMBEDDING_MODEL,\n-                torch_dtype=self.dtype if self.device.type == \"cuda\" else torch.float32,\n-            )\n-            .eval()\n-            .to(self.device)\n-        )\n+        self.model = AutoModel.from_pretrained(EMBEDDING_MODEL, torch_dtype=\"auto\", device_map=\"auto\").eval()\n+\n+        self.device = self.model.device\n+        self.index_dir: Path | None = None\n \n     # ---------- HUB IO ----------\n \n+    def _resolve_index_path(self, filename: str) -> Path:\n+        if self.index_dir is None:\n+            return Path(filename)\n+        return self.index_dir / filename\n+\n     def ensure_local_index(self) -> None:\n-        \"\"\"Download index files from Hub if they don't exist locally.\"\"\"\n-        have_all = Path(EMBEDDINGS_PATH).exists() and Path(INDEX_MAP_PATH).exists() and Path(TOKENS_PATH).exists()\n-        if have_all:\n+        \"\"\"Ensure index files are available locally, preferring Hub cache snapshots.\"\"\"\n+        if self.index_dir is not None and all(\n+            (self.index_dir / fname).exists() for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH)\n+        ):\n+            return\n+\n+        workspace_dir = Path.cwd()\n+        if all((workspace_dir / fname).exists() for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH)):\n+            self.index_dir = workspace_dir\n             return\n-        logging.info(f\"downloading index from hub: {self.hub_dataset}\")\n-        for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH):\n-            hf_hub_download(\n-                repo_id=self.hub_dataset,\n-                filename=fname,\n-                repo_type=\"dataset\",\n-                local_dir=\".\",\n-                local_dir_use_symlinks=False,\n-            )\n+\n+        logging.info(f\"downloading index from hub cache: {self.hub_dataset}\")\n+        snapshot_path = snapshot_download(repo_id=self.hub_dataset, repo_type=\"dataset\")\n+        snapshot_dir = Path(snapshot_path)\n+        missing = [\n+            fname for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH) if not (snapshot_dir / fname).exists()\n+        ]\n+        if missing:\n+            raise FileNotFoundError(\"Missing expected files in Hub snapshot: \" + \", \".join(missing))",
    "comment": "cleaner handling, thanks :d",
    "line_number": 286,
    "enriched": "File: utils/modular_model_detector.py\nCode: @@ -238,34 +251,40 @@ def __init__(self, hub_dataset: str):\n \n         self.models_root = MODELS_ROOT\n         self.hub_dataset = hub_dataset\n-        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n-        self.dtype = \"auto\"\n         self.tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n-        self.model = (\n-            AutoModel.from_pretrained(\n-                EMBEDDING_MODEL,\n-                torch_dtype=self.dtype if self.device.type == \"cuda\" else torch.float32,\n-            )\n-            .eval()\n-            .to(self.device)\n-        )\n+        self.model = AutoModel.from_pretrained(EMBEDDING_MODEL, torch_dtype=\"auto\", device_map=\"auto\").eval()\n+\n+        self.device = self.model.device\n+        self.index_dir: Path | None = None\n \n     # ---------- HUB IO ----------\n \n+    def _resolve_index_path(self, filename: str) -> Path:\n+        if self.index_dir is None:\n+            return Path(filename)\n+        return self.index_dir / filename\n+\n     def ensure_local_index(self) -> None:\n-        \"\"\"Download index files from Hub if they don't exist locally.\"\"\"\n-        have_all = Path(EMBEDDINGS_PATH).exists() and Path(INDEX_MAP_PATH).exists() and Path(TOKENS_PATH).exists()\n-        if have_all:\n+        \"\"\"Ensure index files are available locally, preferring Hub cache snapshots.\"\"\"\n+        if self.index_dir is not None and all(\n+            (self.index_dir / fname).exists() for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH)\n+        ):\n+            return\n+\n+        workspace_dir = Path.cwd()\n+        if all((workspace_dir / fname).exists() for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH)):\n+            self.index_dir = workspace_dir\n             return\n-        logging.info(f\"downloading index from hub: {self.hub_dataset}\")\n-        for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH):\n-            hf_hub_download(\n-                repo_id=self.hub_dataset,\n-                filename=fname,\n-                repo_type=\"dataset\",\n-                local_dir=\".\",\n-                local_dir_use_symlinks=False,\n-            )\n+\n+        logging.info(f\"downloading index from hub cache: {self.hub_dataset}\")\n+        snapshot_path = snapshot_download(repo_id=self.hub_dataset, repo_type=\"dataset\")\n+        snapshot_dir = Path(snapshot_path)\n+        missing = [\n+            fname for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH) if not (snapshot_dir / fname).exists()\n+        ]\n+        if missing:\n+            raise FileNotFoundError(\"Missing expected files in Hub snapshot: \" + \", \".join(missing))\nComment: Cleaner handling, thanks :D ",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "utils/modular_model_detector.py",
    "pr_number": 41361,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2405575211,
    "comment_created_at": "2025-10-06T10:04:19Z"
  },
  {
    "code": "@@ -14,6 +14,14 @@ See the License for the specific language governing permissions and\n limitations under the License.\n -->\n \n+# Setup\n+\n+```bash\n+pip install -e \".[dev]\"  # Installs dev dependencies\n+pip install git+https://github.com/huggingface/doc-builder  # Installs doc-builder",
    "comment": "hmm, i think this is a bit redundant as the package installation is detailed in lines 23 and 45 in main branch",
    "line_number": 21,
    "enriched": "File: docs/README.md\nCode: @@ -14,6 +14,14 @@ See the License for the specific language governing permissions and\n limitations under the License.\n -->\n \n+# Setup\n+\n+```bash\n+pip install -e \".[dev]\"  # Installs dev dependencies\n+pip install git+https://github.com/huggingface/doc-builder  # Installs doc-builder\nComment: hmm, i think this is a bit redundant as the package installation is detailed in lines 23 and 45 in main branch",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/README.md",
    "pr_number": 41338,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2405460321,
    "comment_created_at": "2025-10-06T09:20:53Z"
  },
  {
    "code": "@@ -1915,7 +1915,7 @@ def __init__(self, suppress_tokens, device: str = \"cpu\"):\n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)\n-        suppress_token_mask = isin_mps_friendly(vocab_tensor, self.suppress_tokens)\n+        suppress_token_mask = isin_mps_friendly(vocab_tensor, self.suppress_tokens.to(scores.device))\n         scores = torch.where(suppress_token_mask, -float(\"inf\"), scores)",
    "comment": "in multi-device cases(like put 2 devices to run):\r\nin current implementation, in assistant decoding case, assistant model will reuse main model's suppresstokenslogitsprocessor, which place the suppress_tokens in the same device as input_tensor (which is device 0).  assistant model will ingest encoder_outputs of the main model and do the decoder(in whisper case), while encoder_outputs may in device 1 but main model's suppress_tokens which is main model's is in device 0, so lead to runtimeerror: \r\n\r\n> runtimeerror: expected all tensors to be on the same device, but got test_elements is on xpu:0, different from other tensors on xpu:1 (when checking argument in method wrapper_xpu_isin_tensor_tensor)\r\n\r\nso based on current implementation(that assistant model shares main model's suppresstokenslogitsprocessor), i move suppress_tokens to scores.device while doing isin.",
    "line_number": 1919,
    "enriched": "File: src/transformers/generation/logits_process.py\nCode: @@ -1915,7 +1915,7 @@ def __init__(self, suppress_tokens, device: str = \"cpu\"):\n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)\n-        suppress_token_mask = isin_mps_friendly(vocab_tensor, self.suppress_tokens)\n+        suppress_token_mask = isin_mps_friendly(vocab_tensor, self.suppress_tokens.to(scores.device))\n         scores = torch.where(suppress_token_mask, -float(\"inf\"), scores)\nComment: In multi-device cases(like put 2 devices to run):\r\nin current implementation, in assistant decoding case, assistant model will reuse main model's `SuppressTokensLogitsProcessor`, which place the `suppress_tokens` in the same device as `input_tensor` (which is `device 0`).  assistant model will ingest `encoder_outputs` of the main model and do the decoder(in whisper case), while `encoder_outputs` may in `device 1` but main model's `suppress_tokens` which is main model's is in `device 0`, so lead to `RuntimeError`: \r\n\r\n> RuntimeError: Expected all tensors to be on the same device, but got test_elements is on xpu:0, different from other tensors on xpu:1 (when checking argument in method wrapper_XPU_isin_Tensor_Tensor)\r\n\r\nSo based on current implementation(that assistant model shares main model's `SuppressTokensLogitsProcessor`), I move `suppress_tokens` to `scores.device` while doing `isin`.",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "src/transformers/generation/logits_process.py",
    "pr_number": 41332,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2403219068,
    "comment_created_at": "2025-10-03T20:00:39Z"
  },
  {
    "code": "@@ -251,53 +251,40 @@ def __init__(\n         **kwargs,\n     ):\n         if semantic_config is None:\n-            semantic_config = {}\n-            logger.info(\"semantic_config is None. initializing the semantic model with default values.\")\n+            semantic_config = BarkSemanticConfig()\n+            logger.info(\"`semantic_config` is `None`. Initializing the `BarkSemanticConfig` with default values.\")",
    "comment": "let's remove those logs instead wdyt? one of the v5 goals is to have fewer and more informatives warnings and logs!",
    "line_number": 255,
    "enriched": "File: src/transformers/models/bark/configuration_bark.py\nCode: @@ -251,53 +251,40 @@ def __init__(\n         **kwargs,\n     ):\n         if semantic_config is None:\n-            semantic_config = {}\n-            logger.info(\"semantic_config is None. initializing the semantic model with default values.\")\n+            semantic_config = BarkSemanticConfig()\n+            logger.info(\"`semantic_config` is `None`. Initializing the `BarkSemanticConfig` with default values.\")\nComment: Let's remove those logs instead wdyt? One of the v5 goals is to have fewer and more informatives warnings and logs!",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/models/bark/configuration_bark.py",
    "pr_number": 41314,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2406203499,
    "comment_created_at": "2025-10-06T13:10:11Z"
  },
  {
    "code": "@@ -148,7 +169,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # take any layer that contains cache and not empty tensor\n         layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n-        if len(self.key_cache) <= layer_idx:\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].shape[-1] == 0:",
    "comment": "without this check, get_seq_length would return the batch size when the cache is empty, because layers are initialized as torch.tensor([[]] * batch_size, device=device), which is batch size dim for shape[-2].... \ud83e\udee0\ud83e\udee0\ud83e\udee0\ud83e\udee0",
    "line_number": 172,
    "enriched": "File: src/transformers/models/jamba/modeling_jamba.py\nCode: @@ -148,7 +169,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # take any layer that contains cache and not empty tensor\n         layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n-        if len(self.key_cache) <= layer_idx:\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].shape[-1] == 0:\nComment: Without this check, `get_seq_length` would return the batch size when the cache is empty, because layers are initialized as `torch.tensor([[]] * batch_size, device=device)`, which is batch size dim for `shape[-2]`.... \ud83e\udee0\ud83e\udee0\ud83e\udee0\ud83e\udee0",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "src/transformers/models/jamba/modeling_jamba.py",
    "pr_number": 41309,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2402160119,
    "comment_created_at": "2025-10-03T14:28:27Z"
  },
  {
    "code": "@@ -135,6 +135,7 @@ class ConditionalDetrConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"conditional_detr\"\n+    sub_configs = {\"backbone_config\": AutoConfig}",
    "comment": "i am somehow worried that this will cause the problem of permanent changes if a user change something in sub_configs in order to initialize a specific instance.\r\n\r\nthat happens before and quite difficult to debug.\r\n\r\nwould it be possible to do like\r\n\r\n\r\nclass conditionaldetrconfig:\r\n    _ sub_configs  = ...\r\n\r\n    # could make this class method if necessary\r\n    def sub_configs():\r\n        return copy.deepcopy(self._sub_configs)",
    "line_number": 138,
    "enriched": "File: src/transformers/models/conditional_detr/configuration_conditional_detr.py\nCode: @@ -135,6 +135,7 @@ class ConditionalDetrConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"conditional_detr\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\nComment: I am somehow worried that this will cause the problem of permanent changes if a user change something in `sub_configs` in order to initialize a specific instance.\r\n\r\nThat happens before and quite difficult to debug.\r\n\r\nWould it be possible to do like\r\n\r\n```\r\nclass ConditionalDetrConfig:\r\n    _ sub_configs  = ...\r\n\r\n    # could make this class method if necessary\r\n    def sub_configs():\r\n        return copy.deepcopy(self._sub_configs)\r\n```\r\n\r\n\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/models/conditional_detr/configuration_conditional_detr.py",
    "pr_number": 41308,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2401353760,
    "comment_created_at": "2025-10-03T09:29:04Z"
  },
  {
    "code": "@@ -1335,21 +1335,60 @@ def recursive_diff_dict(dict_a, dict_b, config_obj=None):\n     default = config_obj.__class__().to_dict() if config_obj is not None else {}\n     for key, value in dict_a.items():\n         obj_value = getattr(config_obj, str(key), None)\n-        if isinstance(obj_value, PretrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n+        if isinstance(obj_value, PreTrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n             diff_value = recursive_diff_dict(value, dict_b[key], config_obj=obj_value)\n             diff[key] = diff_value\n         elif key not in dict_b or (value != default[key]):\n             diff[key] = value\n     return diff\n \n \n-PretrainedConfig.push_to_hub = copy_func(PretrainedConfig.push_to_hub)\n-if PretrainedConfig.push_to_hub.__doc__ is not None:\n-    PretrainedConfig.push_to_hub.__doc__ = PretrainedConfig.push_to_hub.__doc__.format(\n+PreTrainedConfig.push_to_hub = copy_func(PreTrainedConfig.push_to_hub)\n+if PreTrainedConfig.push_to_hub.__doc__ is not None:\n+    PreTrainedConfig.push_to_hub.__doc__ = PreTrainedConfig.push_to_hub.__doc__.format(\n         object=\"config\", object_class=\"AutoConfig\", object_files=\"configuration file\"\n     )\n \n \n+class DummyMeta(type):\n+    \"\"\"Dummy metaclass so that `isinstance`/`issubclass` checks against `PretrainedConfig` return True as well\n+    for instance/class of `PreTrainedConfig`.\n+\n+    E.g., thanks to this we have the following (note that LlamaConfig only inherits from PreTrainedConfig, not PretrainedConfig):\n+\n+    ```python\n+    from transformers import LlamaConfig, PretrainedConfig  # the old config name\n+\n+    isinstance(LlamaConfig(), PretrainedConfig)\n+    >>> True\n+    isinstance(LlamaConfig, PretrainedConfig)\n+    >>> True\n+    ```\n+\n+    This is because `isinstance(x, C)` actually maps to `C.__instancecheck__(x)`.\n+    \"\"\"\n+\n+    def __instancecheck__(cls, inst):\n+        \"\"\"Implement isinstance(inst, cls).\"\"\"\n+        return any(cls.__subclasscheck__(c) for c in {type(inst), inst.__class__})\n+\n+    def __subclasscheck__(cls, sub):\n+        \"\"\"Implement issubclass(sub, cls).\"\"\"\n+        logger.warning_once(\n+            \"`PretrainedConfig` is deprecated and will be removed in v5. Please use `PreTrainedConfig` instead!\"\n+        )\n+        candidates = {cls, PreTrainedConfig}\n+        return any(c in candidates for c in sub.mro())\n+\n+\n+class PretrainedConfig(PreTrainedConfig, metaclass=DummyMeta):\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`PretrainedConfig` is deprecated and will be removed in v5. Please use `PreTrainedConfig` instead!\"\n+        )\n+        super().__init__(*args, **kwargs)",
    "comment": "this is a bit fancy but it's the most simple way to keep full bc on the old pretrainedconfig object, while still showing deprecation warning when it's used. the other solution would have been a simple class alias such as\r\n\r\npython\r\npretrainedconfig = pretrainedconfig\r\n\r\n\r\nbut then it's impossible to surface warnings, and users are not advised of the deprecation going on on the object!\r\n\r\na simple class inheritance was not enough imo, as i feel that patterns such as isinstance(my_config, pretrainedconfig) are quite common, and would otherwise break old code because all configs are now pretrainedconfig descendants, not pretrainedconfig descendants\r\n\r\ncurrent code ensures that using the old class in isinstance/issubclass checks still works as expected, i.e.\r\n\r\npython\r\nfrom transformers import pretrainedconfig, llamaconfig\r\n\r\n# this raises a warning and returns true\r\nisinstance(llamaconfig(), pretrainedconfig)\r\n>>> pretrainedconfig is deprecated and will be removed in the future. please use pretrainedconfig instead!\r\n>>> true  # true even though llamaconfig inherits from pretrainedconfig, not pretrainedconfig",
    "line_number": 1389,
    "enriched": "File: src/transformers/configuration_utils.py\nCode: @@ -1335,21 +1335,60 @@ def recursive_diff_dict(dict_a, dict_b, config_obj=None):\n     default = config_obj.__class__().to_dict() if config_obj is not None else {}\n     for key, value in dict_a.items():\n         obj_value = getattr(config_obj, str(key), None)\n-        if isinstance(obj_value, PretrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n+        if isinstance(obj_value, PreTrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n             diff_value = recursive_diff_dict(value, dict_b[key], config_obj=obj_value)\n             diff[key] = diff_value\n         elif key not in dict_b or (value != default[key]):\n             diff[key] = value\n     return diff\n \n \n-PretrainedConfig.push_to_hub = copy_func(PretrainedConfig.push_to_hub)\n-if PretrainedConfig.push_to_hub.__doc__ is not None:\n-    PretrainedConfig.push_to_hub.__doc__ = PretrainedConfig.push_to_hub.__doc__.format(\n+PreTrainedConfig.push_to_hub = copy_func(PreTrainedConfig.push_to_hub)\n+if PreTrainedConfig.push_to_hub.__doc__ is not None:\n+    PreTrainedConfig.push_to_hub.__doc__ = PreTrainedConfig.push_to_hub.__doc__.format(\n         object=\"config\", object_class=\"AutoConfig\", object_files=\"configuration file\"\n     )\n \n \n+class DummyMeta(type):\n+    \"\"\"Dummy metaclass so that `isinstance`/`issubclass` checks against `PretrainedConfig` return True as well\n+    for instance/class of `PreTrainedConfig`.\n+\n+    E.g., thanks to this we have the following (note that LlamaConfig only inherits from PreTrainedConfig, not PretrainedConfig):\n+\n+    ```python\n+    from transformers import LlamaConfig, PretrainedConfig  # the old config name\n+\n+    isinstance(LlamaConfig(), PretrainedConfig)\n+    >>> True\n+    isinstance(LlamaConfig, PretrainedConfig)\n+    >>> True\n+    ```\n+\n+    This is because `isinstance(x, C)` actually maps to `C.__instancecheck__(x)`.\n+    \"\"\"\n+\n+    def __instancecheck__(cls, inst):\n+        \"\"\"Implement isinstance(inst, cls).\"\"\"\n+        return any(cls.__subclasscheck__(c) for c in {type(inst), inst.__class__})\n+\n+    def __subclasscheck__(cls, sub):\n+        \"\"\"Implement issubclass(sub, cls).\"\"\"\n+        logger.warning_once(\n+            \"`PretrainedConfig` is deprecated and will be removed in v5. Please use `PreTrainedConfig` instead!\"\n+        )\n+        candidates = {cls, PreTrainedConfig}\n+        return any(c in candidates for c in sub.mro())\n+\n+\n+class PretrainedConfig(PreTrainedConfig, metaclass=DummyMeta):\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`PretrainedConfig` is deprecated and will be removed in v5. Please use `PreTrainedConfig` instead!\"\n+        )\n+        super().__init__(*args, **kwargs)\nComment: This is a bit fancy but it's the most simple way to keep full BC on the old `PretrainedConfig` object, while still showing deprecation warning when it's used. The other solution would have been a simple class alias such as\r\n\r\n```python\r\nPretrainedConfig = PreTrainedConfig\r\n```\r\n\r\nbut then it's impossible to surface warnings, and users are not advised of the deprecation going on on the object!\r\n\r\nA simple class inheritance was not enough IMO, as I feel that patterns such as `isinstance(my_config, PretrainedConfig)` are quite common, and would otherwise break old code because all configs are now `PreTrainedConfig` descendants, not `PretrainedConfig` descendants\r\n\r\nCurrent code ensures that using the old class in `isinstance/issubclass` checks still works as expected, i.e.\r\n\r\n```python\r\nfrom transformers import PretrainedConfig, LlamaConfig\r\n\r\n# This raises a warning and returns True\r\nisinstance(LlamaConfig(), PretrainedConfig)\r\n>>> `PretrainedConfig` is deprecated and will be removed in the future. Please use `PreTrainedConfig` instead!\r\n>>> True  # True even though LlamaConfig inherits from PreTrainedConfig, not PretrainedConfig\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/configuration_utils.py",
    "pr_number": 41300,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2400078325,
    "comment_created_at": "2025-10-02T21:13:41Z"
  },
  {
    "code": "@@ -84,7 +84,6 @@ class TextToAudioPipeline(Pipeline):\n     _load_processor = True\n \n     _pipeline_calls_generate = True\n-    _load_processor = False",
    "comment": "the wrong value has been kept here - only false should remain (the last value that was set)",
    "line_number": 87,
    "enriched": "File: src/transformers/pipelines/text_to_audio.py\nCode: @@ -84,7 +84,6 @@ class TextToAudioPipeline(Pipeline):\n     _load_processor = True\n \n     _pipeline_calls_generate = True\n-    _load_processor = False\nComment: The wrong value has been kept here - only False should remain (the last value that was set)",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "src/transformers/pipelines/text_to_audio.py",
    "pr_number": 41293,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2405889975,
    "comment_created_at": "2025-10-06T11:44:45Z"
  },
  {
    "code": "@@ -64,8 +64,6 @@\n     title: Entrenador\n   - local: sagemaker\n     title: Ejecutar el entrenamiento en Amazon SageMaker\n-  - local: serialization",
    "comment": "in es, it, and pt: serialization.md was very outdated and doesn't match our recent practices -> delete",
    "line_number": 67,
    "enriched": "File: docs/source/es/_toctree.yml\nCode: @@ -64,8 +64,6 @@\n     title: Entrenador\n   - local: sagemaker\n     title: Ejecutar el entrenamiento en Amazon SageMaker\n-  - local: serialization\nComment: in `es`, `it`, and `pt`: `serialization.md` was very outdated and doesn't match our recent practices -> delete",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/source/es/_toctree.yml",
    "pr_number": 41286,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2398735437,
    "comment_created_at": "2025-10-02T12:51:43Z"
  },
  {
    "code": "@@ -123,55 +124,6 @@ def get_max_height_width(\n     return (max_height, max_width)\n \n \n-def get_size_with_aspect_ratio(",
    "comment": "not the same as in image_transforms! (mode_size)",
    "line_number": 126,
    "enriched": "File: src/transformers/models/yolos/image_processing_yolos.py\nCode: @@ -123,55 +124,6 @@ def get_max_height_width(\n     return (max_height, max_width)\n \n \n-def get_size_with_aspect_ratio(\nComment: Not the same as in image_transforms! (mode_size)",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "src/transformers/models/yolos/image_processing_yolos.py",
    "pr_number": 41284,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2403224624,
    "comment_created_at": "2025-10-03T20:03:54Z"
  },
  {
    "code": "@@ -80,30 +81,15 @@ def __init__(self, quantization_config, **kwargs):\n     def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n             raise ImportError(\n-                f\"Using `bitsandbytes` 4-bit quantization requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n+                f\"Using `bitsandbytes` 4-bit quantization requires accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n             )\n-        if not is_bitsandbytes_available(check_library_only=True):\n+        if not is_bitsandbytes_available():\n             raise ImportError(\n-                \"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\n+                f\"Using `bitsandbytes` 4-bit quantization requires bitsandbytes: `pip install -U bitsandbytes>={BITSANDBYTES_MIN_VERSION}`\"\n             )\n-        if not is_torch_available():\n-            raise ImportError(\n-                \"The bitsandbytes library requires PyTorch but it was not found in your environment. \"\n-                \"You can install it with `pip install torch`.\"\n-            )",
    "comment": "why removing the torch check ?",
    "line_number": 93,
    "enriched": "File: src/transformers/quantizers/quantizer_bnb_4bit.py\nCode: @@ -80,30 +81,15 @@ def __init__(self, quantization_config, **kwargs):\n     def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n             raise ImportError(\n-                f\"Using `bitsandbytes` 4-bit quantization requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n+                f\"Using `bitsandbytes` 4-bit quantization requires accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n             )\n-        if not is_bitsandbytes_available(check_library_only=True):\n+        if not is_bitsandbytes_available():\n             raise ImportError(\n-                \"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\n+                f\"Using `bitsandbytes` 4-bit quantization requires bitsandbytes: `pip install -U bitsandbytes>={BITSANDBYTES_MIN_VERSION}`\"\n             )\n-        if not is_torch_available():\n-            raise ImportError(\n-                \"The bitsandbytes library requires PyTorch but it was not found in your environment. \"\n-                \"You can install it with `pip install torch`.\"\n-            )\nComment: why removing the torch check ?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "src/transformers/quantizers/quantizer_bnb_4bit.py",
    "pr_number": 41283,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2398696061,
    "comment_created_at": "2025-10-02T12:42:18Z"
  },
  {
    "code": "@@ -1007,6 +1007,7 @@ def get_video_features(\n \n     @auto_docstring\n     @can_return_tuple\n+    @check_model_inputs",
    "comment": "we don't need both afaik, check_model_inputs will handle tuple/non-tuple as well",
    "line_number": 1010,
    "enriched": "File: src/transformers/models/qwen3_vl/modular_qwen3_vl.py\nCode: @@ -1007,6 +1007,7 @@ def get_video_features(\n \n     @auto_docstring\n     @can_return_tuple\n+    @check_model_inputs\nComment: we don't need both afaik, `check_model_inputs` will handle tuple/non-tuple as well",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
    "pr_number": 41277,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2398014444,
    "comment_created_at": "2025-10-02T09:35:41Z"
  },
  {
    "code": "@@ -615,7 +615,9 @@ def test_torch_whisper_batched(self):\n             {\"text\": \" Nor is Mr. Quilters' manner less interesting than his matter.\"},\n         ]\n \n-        output = speech_recognizer(ds[\"audio\"], batch_size=2)\n+\n+        audio_arrays = [x.get_all_samples().data for x in ds[\"audio\"]]\n+        output = speech_recognizer(audio_arrays, batch_size=2)",
    "comment": "preprocess, as of now, doesn't support multi audiodecoder. for this case, should decode outside then put them in.",
    "line_number": 620,
    "enriched": "File: tests/pipelines/test_pipelines_automatic_speech_recognition.py\nCode: @@ -615,7 +615,9 @@ def test_torch_whisper_batched(self):\n             {\"text\": \" Nor is Mr. Quilters' manner less interesting than his matter.\"},\n         ]\n \n-        output = speech_recognizer(ds[\"audio\"], batch_size=2)\n+\n+        audio_arrays = [x.get_all_samples().data for x in ds[\"audio\"]]\n+        output = speech_recognizer(audio_arrays, batch_size=2)\nComment: `preprocess`, as of now, doesn't support multi `AudioDecoder`. For this case, should decode outside then put them in.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
    "pr_number": 41275,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2396911965,
    "comment_created_at": "2025-10-02T05:09:08Z"
  },
  {
    "code": "@@ -130,10 +130,9 @@ def step(self, action):\n         obs, rews, terminateds, truncateds, infos = self.env.step(action)\n         if not self.is_vector_env:\n             rews = np.array([rews])\n-        self.returns = self.returns * self.gamma + rews\n-        rews = self.normalize(rews)\n-        dones = np.logical_or(terminateds, truncateds)\n-        self.returns[dones] = 0.0\n+        dones = np.logical_or(terminateds, truncateds)            \n+        self.returns = self.returns * self.gamma * (1-dones) + rews",
    "comment": "should this not be (1-terminated) as in cases of truncation, the state values should still be boostrapped.",
    "line_number": 134,
    "enriched": "File: gym/wrappers/normalize.py\nCode: @@ -130,10 +130,9 @@ def step(self, action):\n         obs, rews, terminateds, truncateds, infos = self.env.step(action)\n         if not self.is_vector_env:\n             rews = np.array([rews])\n-        self.returns = self.returns * self.gamma + rews\n-        rews = self.normalize(rews)\n-        dones = np.logical_or(terminateds, truncateds)\n-        self.returns[dones] = 0.0\n+        dones = np.logical_or(terminateds, truncateds)            \n+        self.returns = self.returns * self.gamma * (1-dones) + rews\nComment: Should this not be `(1-terminated)` as in cases of truncation, the state values should still be boostrapped.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "gym/wrappers/normalize.py",
    "pr_number": 3152,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 1029924050,
    "comment_created_at": "2022-11-23T00:02:49Z"
  },
  {
    "code": "@@ -608,8 +608,8 @@ def step(self, action: np.ndarray):\n     def render(self):\n         if self.render_mode is None:\n             gym.logger.warn(",
    "comment": "should this be a warning or an error? i think it should be an error as otherwise users will ignore it and they shouldn't be ignoring it. \r\nalso i think, we should have the render_mode defaulting to human as that was the previous default used",
    "line_number": 610,
    "enriched": "File: gym/envs/box2d/bipedal_walker.py\nCode: @@ -608,8 +608,8 @@ def step(self, action: np.ndarray):\n     def render(self):\n         if self.render_mode is None:\n             gym.logger.warn(\nComment: Should this be a warning or an error? I think it should be an error as otherwise users will ignore it and they shouldn't be ignoring it. \r\nAlso I think, we should have the render_mode defaulting to human as that was the previous default used",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/envs/box2d/bipedal_walker.py",
    "pr_number": 3112,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 986611894,
    "comment_created_at": "2022-10-04T08:57:46Z"
  },
  {
    "code": "@@ -95,10 +95,9 @@ def __init__(\n \n         default_render_kwargs = {}\n         if not env.render_mode:\n-            default_render_kwargs = {\"mode\": \"rgb_array_list\"}\n-            logger.warn(\n+            raise AttributeError(\n                 \"env.render_mode must be specified to use PixelObservationWrapper:\"\n-                \"`gym.make(env_name, render_mode='rgb_array')`.\"\n+                \"`gym.make(env_name, render_mode='single_rgb_array')`.\"",
    "comment": "we can reverted the meaning of the render_modes in v25 back to the previous meaning where rgb_array means single render obs and rgb_array_list will mean list of render obs",
    "line_number": 100,
    "enriched": "File: gym/wrappers/pixel_observation.py\nCode: @@ -95,10 +95,9 @@ def __init__(\n \n         default_render_kwargs = {}\n         if not env.render_mode:\n-            default_render_kwargs = {\"mode\": \"rgb_array_list\"}\n-            logger.warn(\n+            raise AttributeError(\n                 \"env.render_mode must be specified to use PixelObservationWrapper:\"\n-                \"`gym.make(env_name, render_mode='rgb_array')`.\"\n+                \"`gym.make(env_name, render_mode='single_rgb_array')`.\"\nComment: We can reverted the meaning of the render_modes in v25 back to the previous meaning where `rgb_array` means single render obs and `rgb_array_list` will mean list of render obs",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "gym/wrappers/pixel_observation.py",
    "pr_number": 3076,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 965972694,
    "comment_created_at": "2022-09-08T13:29:32Z"
  },
  {
    "code": "@@ -0,0 +1,120 @@\n+\"\"\"A compatibility wrapper converting an old-style environment into a valid environment.\"\"\"\n+import sys\n+from typing import Any, Dict, Optional, Tuple\n+\n+import gym\n+from gym.core import ObsType\n+from gym.utils.step_api_compatibility import convert_to_terminated_truncated_step_api\n+\n+if sys.version_info >= (3, 8):\n+    from typing import Protocol, runtime_checkable\n+elif sys.version_info >= (3, 7):\n+    from typing_extensions import Protocol, runtime_checkable\n+else:\n+    Protocol = object\n+    runtime_checkable = lambda x: x  # noqa: E731\n+\n+\n+@runtime_checkable\n+class LegacyEnv(Protocol):\n+    \"\"\"A protocol for environments using the old step API.\"\"\"\n+\n+    observation_space: gym.Space\n+    action_space: gym.Space\n+\n+    def reset(self) -> Any:\n+        \"\"\"Reset the environment and return the initial observation.\"\"\"\n+        ...\n+\n+    def step(self, action: Any) -> Tuple[Any, float, bool, Dict]:\n+        \"\"\"Run one timestep of the environment's dynamics.\"\"\"\n+        ...\n+\n+    def render(self, mode: Optional[str] = \"human\") -> Any:\n+        \"\"\"Render the environment.\"\"\"\n+        ...\n+\n+    def close(self):\n+        \"\"\"Close the environment.\"\"\"\n+        ...\n+\n+    def seed(self, seed: Optional[int] = None):\n+        \"\"\"Set the seed for this env's random number generator(s).\"\"\"\n+        ...\n+\n+\n+class EnvCompatibility(gym.Env):\n+    r\"\"\"A wrapper which can transform an environment from new step API to old and vice-versa.\n+\n+    Old step API refers to step() method returning (observation, reward, done, info)\n+    New step API refers to step() method returning (observation, reward, terminated, truncated, info)\n+    (Refer to docs for details on the API change)\n+\n+    Known limitations:\n+    - Environments that use `self.np_random` might not work as expected.\n+    - `env.render_mode` cannot be set in `make` if the environment doesn't accept it as a parameter (e.g. via kwargs).\n+\n+    Args:\n+        old_env (gym.Env): the env to wrap. Can be in old or new API\n+\n+    \"\"\"\n+\n+    def __init__(self, old_env: LegacyEnv, render_mode: Optional[str] = None):\n+        \"\"\"A wrapper which converts old-style envs to valid modern envs.\n+\n+        Some information may be lost in the conversion, so we recommend updating your environment.\n+\n+        Args:\n+            old_env (LegacyEnv): the env to wrap, implemented with the old API\n+            render_mode (str): the render mode to use when rendering the environment, passed automatically to env.render\n+        \"\"\"\n+        self.metadata = getattr(old_env, \"metadata\", {\"render_modes\": []})\n+        self.render_mode = render_mode\n+        self.reward_range = getattr(old_env, \"reward_range\", None)\n+        self.spec = getattr(old_env, \"spec\", None)\n+        self.env = old_env\n+\n+        self.observation_space = old_env.observation_space\n+        self.action_space = old_env.action_space\n+\n+    def reset(\n+        self, seed: Optional[int] = None, options: Optional[dict] = None\n+    ) -> Tuple[ObsType, dict]:\n+        \"\"\"Resets the environment.\n+\n+        Args:\n+            seed: the seed to reset the environment with\n+            options: the options to reset the environment with\n+\n+        Returns:\n+            (observation, info)\n+        \"\"\"\n+        if seed is not None:\n+            self.env.seed(seed)\n+        # Options are ignored\n+        return self.env.reset(), {}\n+\n+    def step(self, action: Any) -> Tuple[Any, float, bool, bool, Dict]:\n+        \"\"\"Steps through the environment, returning 5 or 4 items depending on `apply_step_compatibility`.",
    "comment": "does apply_step_compatibility exist?",
    "line_number": 98,
    "enriched": "File: gym/wrappers/compatibility.py\nCode: @@ -0,0 +1,120 @@\n+\"\"\"A compatibility wrapper converting an old-style environment into a valid environment.\"\"\"\n+import sys\n+from typing import Any, Dict, Optional, Tuple\n+\n+import gym\n+from gym.core import ObsType\n+from gym.utils.step_api_compatibility import convert_to_terminated_truncated_step_api\n+\n+if sys.version_info >= (3, 8):\n+    from typing import Protocol, runtime_checkable\n+elif sys.version_info >= (3, 7):\n+    from typing_extensions import Protocol, runtime_checkable\n+else:\n+    Protocol = object\n+    runtime_checkable = lambda x: x  # noqa: E731\n+\n+\n+@runtime_checkable\n+class LegacyEnv(Protocol):\n+    \"\"\"A protocol for environments using the old step API.\"\"\"\n+\n+    observation_space: gym.Space\n+    action_space: gym.Space\n+\n+    def reset(self) -> Any:\n+        \"\"\"Reset the environment and return the initial observation.\"\"\"\n+        ...\n+\n+    def step(self, action: Any) -> Tuple[Any, float, bool, Dict]:\n+        \"\"\"Run one timestep of the environment's dynamics.\"\"\"\n+        ...\n+\n+    def render(self, mode: Optional[str] = \"human\") -> Any:\n+        \"\"\"Render the environment.\"\"\"\n+        ...\n+\n+    def close(self):\n+        \"\"\"Close the environment.\"\"\"\n+        ...\n+\n+    def seed(self, seed: Optional[int] = None):\n+        \"\"\"Set the seed for this env's random number generator(s).\"\"\"\n+        ...\n+\n+\n+class EnvCompatibility(gym.Env):\n+    r\"\"\"A wrapper which can transform an environment from new step API to old and vice-versa.\n+\n+    Old step API refers to step() method returning (observation, reward, done, info)\n+    New step API refers to step() method returning (observation, reward, terminated, truncated, info)\n+    (Refer to docs for details on the API change)\n+\n+    Known limitations:\n+    - Environments that use `self.np_random` might not work as expected.\n+    - `env.render_mode` cannot be set in `make` if the environment doesn't accept it as a parameter (e.g. via kwargs).\n+\n+    Args:\n+        old_env (gym.Env): the env to wrap. Can be in old or new API\n+\n+    \"\"\"\n+\n+    def __init__(self, old_env: LegacyEnv, render_mode: Optional[str] = None):\n+        \"\"\"A wrapper which converts old-style envs to valid modern envs.\n+\n+        Some information may be lost in the conversion, so we recommend updating your environment.\n+\n+        Args:\n+            old_env (LegacyEnv): the env to wrap, implemented with the old API\n+            render_mode (str): the render mode to use when rendering the environment, passed automatically to env.render\n+        \"\"\"\n+        self.metadata = getattr(old_env, \"metadata\", {\"render_modes\": []})\n+        self.render_mode = render_mode\n+        self.reward_range = getattr(old_env, \"reward_range\", None)\n+        self.spec = getattr(old_env, \"spec\", None)\n+        self.env = old_env\n+\n+        self.observation_space = old_env.observation_space\n+        self.action_space = old_env.action_space\n+\n+    def reset(\n+        self, seed: Optional[int] = None, options: Optional[dict] = None\n+    ) -> Tuple[ObsType, dict]:\n+        \"\"\"Resets the environment.\n+\n+        Args:\n+            seed: the seed to reset the environment with\n+            options: the options to reset the environment with\n+\n+        Returns:\n+            (observation, info)\n+        \"\"\"\n+        if seed is not None:\n+            self.env.seed(seed)\n+        # Options are ignored\n+        return self.env.reset(), {}\n+\n+    def step(self, action: Any) -> Tuple[Any, float, bool, bool, Dict]:\n+        \"\"\"Steps through the environment, returning 5 or 4 items depending on `apply_step_compatibility`.\nComment: Does `apply_step_compatibility` exist? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/wrappers/compatibility.py",
    "pr_number": 3066,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 963060206,
    "comment_created_at": "2022-09-05T17:10:00Z"
  },
  {
    "code": "@@ -61,11 +61,11 @@ def generate_random_map(size: int = 8, p: float = 0.8) -> List[str]:\n         A random valid map\n     \"\"\"\n     valid = False\n-    board = []  # initialize to make pyright happy\n+    board: List[List[str]] = []  # initialize to make pyright happy",
    "comment": "np.ndarray? though from experience typing with numpy is a pain",
    "line_number": 64,
    "enriched": "File: gym/envs/toy_text/frozen_lake.py\nCode: @@ -61,11 +61,11 @@ def generate_random_map(size: int = 8, p: float = 0.8) -> List[str]:\n         A random valid map\n     \"\"\"\n     valid = False\n-    board = []  # initialize to make pyright happy\n+    board: List[List[str]] = []  # initialize to make pyright happy\nComment: `np.ndarray`? Though from experience typing with numpy is a pain",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/envs/toy_text/frozen_lake.py",
    "pr_number": 3061,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 962341394,
    "comment_created_at": "2022-09-04T16:45:11Z"
  },
  {
    "code": "@@ -52,13 +53,14 @@ def test_make():\n \n \n def test_make_deprecated():\n-    with pytest.raises(\n-        gym.error.Error,\n-        match=re.escape(\n-            \"Environment version v0 for `Humanoid` is deprecated. Please use `Humanoid-v4` instead.\"\n-        ),\n-    ):\n-        gym.make(\"Humanoid-v0\", disable_env_checker=True)\n+    with warnings.catch_warnings(record=True):",
    "comment": "don't we want to check whether the caught warnings can be safely ignored? as we do in /tests/envs/test_envs.py?",
    "line_number": 56,
    "enriched": "File: tests/envs/test_make.py\nCode: @@ -52,13 +53,14 @@ def test_make():\n \n \n def test_make_deprecated():\n-    with pytest.raises(\n-        gym.error.Error,\n-        match=re.escape(\n-            \"Environment version v0 for `Humanoid` is deprecated. Please use `Humanoid-v4` instead.\"\n-        ),\n-    ):\n-        gym.make(\"Humanoid-v0\", disable_env_checker=True)\n+    with warnings.catch_warnings(record=True):\nComment: Don't we want to check whether the caught warnings can be safely ignored? As we do in /tests/envs/test_envs.py?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/envs/test_make.py",
    "pr_number": 3050,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 958669364,
    "comment_created_at": "2022-08-30T16:01:28Z"
  },
  {
    "code": "@@ -0,0 +1,140 @@\n+from collections import OrderedDict\n+\n+import numpy as np\n+import pytest\n+\n+from gym.spaces import Box, Dict, Discrete\n+\n+\n+def test_dict_init():\n+    with pytest.raises(\n+        AssertionError,\n+        match=r\"^Unexpected Dict space input, expecting dict, OrderedDict or Sequence, actual type: \",\n+    ):\n+        Dict(Discrete(2))\n+\n+    with pytest.raises(\n+        ValueError,\n+        match=\"Dict space keyword 'a' already exists in the spaces dictionary\",\n+    ):\n+        Dict({\"a\": Discrete(3)}, a=Box(0, 1))\n+\n+    with pytest.raises(\n+        AssertionError,\n+        match=\"Dict space element is not an instance of Space: key='b', space=Box\",\n+    ):\n+        Dict(a=Discrete(2), b=\"Box\")\n+\n+    with pytest.warns(None) as warnings:\n+        a = Dict({\"a\": Discrete(2), \"b\": Box(low=0.0, high=1.0)})\n+        b = Dict(OrderedDict(a=Discrete(2), b=Box(low=0.0, high=1.0)))\n+        c = Dict(((\"a\", Discrete(2)), (\"b\", Box(low=0.0, high=1.0))))\n+        d = Dict(a=Discrete(2), b=Box(low=0.0, high=1.0))\n+\n+        assert a == b == c == d\n+    assert len(warnings) == 0\n+\n+    with pytest.warns(None) as warnings:\n+        Dict({1: Discrete(2), \"a\": Discrete(3)})\n+    assert len(warnings) == 0\n+\n+\n+DICT_SPACE = Dict(\n+    {\n+        \"a\": Box(low=0, high=1, shape=(3, 3)),\n+        \"b\": Dict(\n+            {\n+                \"b_1\": Box(low=-100, high=100, shape=(2,)),\n+                \"b_2\": Box(low=-1, high=1, shape=(2,)),\n+            }\n+        ),\n+        \"c\": Discrete(5),\n+    }\n+)\n+\n+\n+def test_dict_seeding():\n+    seeds = DICT_SPACE.seed(\n+        {\n+            \"a\": 0,\n+            \"b\": {\n+                \"b_1\": 1,\n+                \"b_2\": 2,\n+            },\n+            \"c\": 3,\n+        }\n+    )\n+    assert all(isinstance(seed, int) for seed in seeds)\n+\n+    # \"Unpack\" the dict sub-spaces into individual spaces\n+    a = Box(low=0, high=1, shape=(3, 3), seed=0)\n+    b_1 = Box(low=-100, high=100, shape=(2,), seed=1)\n+    b_2 = Box(low=-1, high=1, shape=(2,), seed=2)\n+    c = Discrete(5, seed=3)\n+\n+    for i in range(10):\n+        dict_sample = DICT_SPACE.sample()\n+        assert np.all(dict_sample[\"a\"] == a.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_1\"] == b_1.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_2\"] == b_2.sample())\n+        assert dict_sample[\"c\"] == c.sample()\n+\n+\n+def test_int_seeding():\n+    seeds = DICT_SPACE.seed(1)\n+    assert all(isinstance(seed, int) for seed in seeds)\n+\n+    # rng, seeds = seeding.np_random(1)\n+    # subseeds = rng.choice(np.iinfo(int).max, size=3, replace=False)\n+    # b_rng, b_seeds = seeding.np_random(int(subseeds[1]))\n+    # b_subseeds = b_rng.choice(np.iinfo(int).max, size=2, replace=False)\n+\n+    # \"Unpack\" the dict sub-spaces into individual spaces\n+    a = Box(low=0, high=1, shape=(3, 3), seed=seeds[1])\n+    b_1 = Box(low=-100, high=100, shape=(2,), seed=seeds[3])\n+    b_2 = Box(low=-1, high=1, shape=(2,), seed=seeds[4])\n+    c = Discrete(5, seed=seeds[5])\n+\n+    for i in range(10):\n+        dict_sample = DICT_SPACE.sample()\n+        assert np.all(dict_sample[\"a\"] == a.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_1\"] == b_1.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_2\"] == b_2.sample())\n+        assert dict_sample[\"c\"] == c.sample()\n+\n+\n+def test_none_seeding():\n+    seeds = DICT_SPACE.seed(None)\n+    assert len(seeds) == 4 and all(isinstance(seed, int) for seed in seeds)",
    "comment": "maybe good to add a line to see if results don't repeat for none? \r\neg. \r\npython\r\nseeds1 = dict_space.seed(none)\r\nseeds2 = dict_space.seed(none)\r\n\r\nassert any([seed1 != seed2 for seed1, seed2 in zip(seeds1, seeds2)]) \r\n\r\ncan use all instead of any if we're confident that two same seeds won't be sampled",
    "line_number": 108,
    "enriched": "File: tests/spaces/test_dict.py\nCode: @@ -0,0 +1,140 @@\n+from collections import OrderedDict\n+\n+import numpy as np\n+import pytest\n+\n+from gym.spaces import Box, Dict, Discrete\n+\n+\n+def test_dict_init():\n+    with pytest.raises(\n+        AssertionError,\n+        match=r\"^Unexpected Dict space input, expecting dict, OrderedDict or Sequence, actual type: \",\n+    ):\n+        Dict(Discrete(2))\n+\n+    with pytest.raises(\n+        ValueError,\n+        match=\"Dict space keyword 'a' already exists in the spaces dictionary\",\n+    ):\n+        Dict({\"a\": Discrete(3)}, a=Box(0, 1))\n+\n+    with pytest.raises(\n+        AssertionError,\n+        match=\"Dict space element is not an instance of Space: key='b', space=Box\",\n+    ):\n+        Dict(a=Discrete(2), b=\"Box\")\n+\n+    with pytest.warns(None) as warnings:\n+        a = Dict({\"a\": Discrete(2), \"b\": Box(low=0.0, high=1.0)})\n+        b = Dict(OrderedDict(a=Discrete(2), b=Box(low=0.0, high=1.0)))\n+        c = Dict(((\"a\", Discrete(2)), (\"b\", Box(low=0.0, high=1.0))))\n+        d = Dict(a=Discrete(2), b=Box(low=0.0, high=1.0))\n+\n+        assert a == b == c == d\n+    assert len(warnings) == 0\n+\n+    with pytest.warns(None) as warnings:\n+        Dict({1: Discrete(2), \"a\": Discrete(3)})\n+    assert len(warnings) == 0\n+\n+\n+DICT_SPACE = Dict(\n+    {\n+        \"a\": Box(low=0, high=1, shape=(3, 3)),\n+        \"b\": Dict(\n+            {\n+                \"b_1\": Box(low=-100, high=100, shape=(2,)),\n+                \"b_2\": Box(low=-1, high=1, shape=(2,)),\n+            }\n+        ),\n+        \"c\": Discrete(5),\n+    }\n+)\n+\n+\n+def test_dict_seeding():\n+    seeds = DICT_SPACE.seed(\n+        {\n+            \"a\": 0,\n+            \"b\": {\n+                \"b_1\": 1,\n+                \"b_2\": 2,\n+            },\n+            \"c\": 3,\n+        }\n+    )\n+    assert all(isinstance(seed, int) for seed in seeds)\n+\n+    # \"Unpack\" the dict sub-spaces into individual spaces\n+    a = Box(low=0, high=1, shape=(3, 3), seed=0)\n+    b_1 = Box(low=-100, high=100, shape=(2,), seed=1)\n+    b_2 = Box(low=-1, high=1, shape=(2,), seed=2)\n+    c = Discrete(5, seed=3)\n+\n+    for i in range(10):\n+        dict_sample = DICT_SPACE.sample()\n+        assert np.all(dict_sample[\"a\"] == a.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_1\"] == b_1.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_2\"] == b_2.sample())\n+        assert dict_sample[\"c\"] == c.sample()\n+\n+\n+def test_int_seeding():\n+    seeds = DICT_SPACE.seed(1)\n+    assert all(isinstance(seed, int) for seed in seeds)\n+\n+    # rng, seeds = seeding.np_random(1)\n+    # subseeds = rng.choice(np.iinfo(int).max, size=3, replace=False)\n+    # b_rng, b_seeds = seeding.np_random(int(subseeds[1]))\n+    # b_subseeds = b_rng.choice(np.iinfo(int).max, size=2, replace=False)\n+\n+    # \"Unpack\" the dict sub-spaces into individual spaces\n+    a = Box(low=0, high=1, shape=(3, 3), seed=seeds[1])\n+    b_1 = Box(low=-100, high=100, shape=(2,), seed=seeds[3])\n+    b_2 = Box(low=-1, high=1, shape=(2,), seed=seeds[4])\n+    c = Discrete(5, seed=seeds[5])\n+\n+    for i in range(10):\n+        dict_sample = DICT_SPACE.sample()\n+        assert np.all(dict_sample[\"a\"] == a.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_1\"] == b_1.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_2\"] == b_2.sample())\n+        assert dict_sample[\"c\"] == c.sample()\n+\n+\n+def test_none_seeding():\n+    seeds = DICT_SPACE.seed(None)\n+    assert len(seeds) == 4 and all(isinstance(seed, int) for seed in seeds)\nComment: Maybe good to add a line to see if results don't repeat for `None`? \r\nEg. \r\n```python\r\nseeds1 = DICT_SPACE.seed(None)\r\nseeds2 = DICT_SPACE.seed(None)\r\n\r\nassert any([seed1 != seed2 for seed1, seed2 in zip(seeds1, seeds2)]) \r\n```\r\nCan use `all` instead of `any` if we're confident that two same seeds won't be sampled ",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "tests/spaces/test_dict.py",
    "pr_number": 3047,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 962111237,
    "comment_created_at": "2022-09-03T06:17:27Z"
  },
  {
    "code": "@@ -143,7 +143,7 @@ def test_env_version_suggestions(\n def test_register_versioned_unversioned():\n     # Register versioned then unversioned\n     versioned_env = \"Test/MyEnv-v0\"\n-    gym.register(versioned_env)\n+    gym.register(versioned_env, None)",
    "comment": "is this being type checked? because you're simultaneously disallowing entry_point=none in the type system, and adding an explicit entry_point=none in the usage.",
    "line_number": 146,
    "enriched": "File: tests/envs/test_register.py\nCode: @@ -143,7 +143,7 @@ def test_env_version_suggestions(\n def test_register_versioned_unversioned():\n     # Register versioned then unversioned\n     versioned_env = \"Test/MyEnv-v0\"\n-    gym.register(versioned_env)\n+    gym.register(versioned_env, None)\nComment: Is this being type checked? Because you're simultaneously disallowing `entry_point=None` in the type system, and adding an explicit `entry_point=None` in the usage.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/envs/test_register.py",
    "pr_number": 3041,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 953208688,
    "comment_created_at": "2022-08-24T00:24:11Z"
  },
  {
    "code": "@@ -10,31 +10,18 @@\n \n import gym\n from gym.utils.play import MissingKeysToAction, PlayableGame, play\n+from tests.testing_env import GenericTestEnv\n \n RELEVANT_KEY_1 = ord(\"a\")  # 97\n RELEVANT_KEY_2 = ord(\"d\")  # 100\n IRRELEVANT_KEY = 1\n \n \n-@dataclass\n-class DummyEnvSpec:\n-    id: str\n-\n-\n-class DummyPlayEnv(gym.Env):\n-    def __init__(self, render_mode: Optional[str] = None):\n-        self.render_mode = render_mode\n-\n-    def step(self, action):\n-        obs = np.zeros((1, 1))\n-        rew, terminated, truncated, info = 1, False, False, {}\n-        return obs, rew, terminated, truncated, info\n-\n-    def reset(self, seed=None):\n-        ...\n-\n-    def render(self):\n-        return np.zeros((1, 1))\n+PlayableEnv = partial(\n+    GenericTestEnv,\n+    render_modes=[\"rgb_array\"],",
    "comment": "just a comment unrelated to this pr: i would vote to have metadata arg instead of render_modes and render_fps; render_modes can be easily confused with render_mode. also metadata is more customizable (e.g., i want to test an env with invalid metadata or i want other fields in it).",
    "line_number": 22,
    "enriched": "File: tests/utils/test_play.py\nCode: @@ -10,31 +10,18 @@\n \n import gym\n from gym.utils.play import MissingKeysToAction, PlayableGame, play\n+from tests.testing_env import GenericTestEnv\n \n RELEVANT_KEY_1 = ord(\"a\")  # 97\n RELEVANT_KEY_2 = ord(\"d\")  # 100\n IRRELEVANT_KEY = 1\n \n \n-@dataclass\n-class DummyEnvSpec:\n-    id: str\n-\n-\n-class DummyPlayEnv(gym.Env):\n-    def __init__(self, render_mode: Optional[str] = None):\n-        self.render_mode = render_mode\n-\n-    def step(self, action):\n-        obs = np.zeros((1, 1))\n-        rew, terminated, truncated, info = 1, False, False, {}\n-        return obs, rew, terminated, truncated, info\n-\n-    def reset(self, seed=None):\n-        ...\n-\n-    def render(self):\n-        return np.zeros((1, 1))\n+PlayableEnv = partial(\n+    GenericTestEnv,\n+    render_modes=[\"rgb_array\"],\nComment: Just a comment unrelated to this PR: I would vote to have `metadata` arg instead of `render_modes` and `render_fps`; `render_modes` can be easily confused with `render_mode`. Also `metadata` is more customizable (e.g., I want to test an env with invalid metadata or I want other fields in it).",
    "subcategory": "false positive",
    "category": "false positive",
    "file_path": "tests/utils/test_play.py",
    "pr_number": 3040,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 959869348,
    "comment_created_at": "2022-08-31T17:57:11Z"
  },
  {
    "code": "@@ -15,33 +18,33 @@ class StepAPICompatibility(gym.Wrapper):\n \n     Args:\n         env (gym.Env): the env to wrap. Can be in old or new API\n-        new_step_api (bool): True to use env with new step API, False to use env with old step API. (False by default)\n+        to_termination_truncation_api (bool): True to use env with new step API, False to use env with old step API. (False by default)",
    "comment": "i think it's better to use to_terminated_truncated_api to be consistent with the actual variable names",
    "line_number": 21,
    "enriched": "File: gym/wrappers/step_api_compatibility.py\nCode: @@ -15,33 +18,33 @@ class StepAPICompatibility(gym.Wrapper):\n \n     Args:\n         env (gym.Env): the env to wrap. Can be in old or new API\n-        new_step_api (bool): True to use env with new step API, False to use env with old step API. (False by default)\n+        to_termination_truncation_api (bool): True to use env with new step API, False to use env with old step API. (False by default)\nComment: I think it's better to use `to_terminated_truncated_api` to be consistent with the actual variable names",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "gym/wrappers/step_api_compatibility.py",
    "pr_number": 3028,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 946587481,
    "comment_created_at": "2022-08-16T10:13:09Z"
  },
  {
    "code": "@@ -22,6 +22,9 @@ class DummyEnvSpec:\n \n \n class DummyPlayEnv(gym.Env):\n+    def __init__(self, render_mode=None):",
    "comment": "type annotation for the argument? i know it's only a test, but let's keep it clean. i think it'd be optional[str], right?",
    "line_number": 25,
    "enriched": "File: tests/utils/test_play.py\nCode: @@ -22,6 +22,9 @@ class DummyEnvSpec:\n \n \n class DummyPlayEnv(gym.Env):\n+    def __init__(self, render_mode=None):\nComment: Type annotation for the argument? I know it's only a test, but let's keep it clean. I think it'd be `Optional[str]`, right?",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "tests/utils/test_play.py",
    "pr_number": 3027,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 949999938,
    "comment_created_at": "2022-08-19T09:28:23Z"
  },
  {
    "code": "@@ -193,6 +202,14 @@ def __repr__(self) -> str:\n         \"\"\"Gives a string representation of this space.\"\"\"\n         return \"Dict(\" + \", \".join([f\"{k}: {s}\" for k, s in self.spaces.items()]) + \")\"\n \n+    def __eq__(self, other) -> bool:\n+        \"\"\"Check whether `other` is equivalent to this instance.\"\"\"\n+        return (\n+            isinstance(other, Dict)\n+            # Comparison of `OrderedDict`s is order-sensitive",
    "comment": "why are we not checking the keys are the same? \r\nstrange that we only checking the spaces",
    "line_number": 209,
    "enriched": "File: gym/spaces/dict.py\nCode: @@ -193,6 +202,14 @@ def __repr__(self) -> str:\n         \"\"\"Gives a string representation of this space.\"\"\"\n         return \"Dict(\" + \", \".join([f\"{k}: {s}\" for k, s in self.spaces.items()]) + \")\"\n \n+    def __eq__(self, other) -> bool:\n+        \"\"\"Check whether `other` is equivalent to this instance.\"\"\"\n+        return (\n+            isinstance(other, Dict)\n+            # Comparison of `OrderedDict`s is order-sensitive\nComment: Why are we not checking the keys are the same? \r\nStrange that we only checking the spaces",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/spaces/dict.py",
    "pr_number": 3024,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 945169037,
    "comment_created_at": "2022-08-13T17:00:07Z"
  },
  {
    "code": "@@ -31,201 +27,4 @@ def np_random(seed: Optional[int] = None) -> Tuple[\"RandomNumberGenerator\", Any]\n     return rng, np_seed\n \n \n-# TODO: Remove this class and make it alias to `Generator` in a future Gym release\n-# RandomNumberGenerator = np.random.Generator\n-class RandomNumberGenerator(np.random.Generator):\n-    \"\"\"Random number generator class that inherits from numpy's random Generator class.\"\"\"\n-\n-    def rand(self, *size):\n-        \"\"\"Deprecated rand function using random.\"\"\"\n-        deprecation(\n-            \"Function `rng.rand(*size)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `Generator.random(size)` instead.\"\n-        )\n-\n-        return self.random(size)\n-\n-    random_sample = rand\n-\n-    def randn(self, *size):\n-        \"\"\"Deprecated random standard normal function use standard_normal.\"\"\"\n-        deprecation(\n-            \"Function `rng.randn(*size)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.standard_normal(size)` instead.\"\n-        )\n-\n-        return self.standard_normal(size)\n-\n-    def randint(self, low, high=None, size=None, dtype=int):\n-        \"\"\"Deprecated random integer function use integers.\"\"\"\n-        deprecation(\n-            \"Function `rng.randint(low, [high, size, dtype])` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.integers(low, [high, size, dtype])` instead.\"\n-        )\n-\n-        return self.integers(low=low, high=high, size=size, dtype=dtype)\n-\n-    random_integers = randint\n-\n-    def get_state(self):\n-        \"\"\"Deprecated get rng state use bit_generator.state.\"\"\"\n-        deprecation(\n-            \"Function `rng.get_state()` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.bit_generator.state` instead.\"\n-        )\n-\n-        return self.bit_generator.state\n-\n-    def set_state(self, state):\n-        \"\"\"Deprecated set rng state function use bit_generator.state = state.\"\"\"\n-        deprecation(\n-            \"Function `rng.set_state(state)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.bit_generator.state = state` instead.\"\n-        )\n-\n-        self.bit_generator.state = state\n-\n-    def seed(self, seed=None):\n-        \"\"\"Deprecated seed function use gym.utils.seeding.np_random(seed).\"\"\"\n-        deprecation(\n-            \"Function `rng.seed(seed)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng, seed = gym.utils.seeding.np_random(seed)` to create a separate generator instead.\"\n-        )\n-\n-        self.bit_generator.state = type(self.bit_generator)(seed).state\n-\n-    rand.__doc__ = np.random.rand.__doc__\n-    randn.__doc__ = np.random.randn.__doc__\n-    randint.__doc__ = np.random.randint.__doc__\n-    get_state.__doc__ = np.random.get_state.__doc__\n-    set_state.__doc__ = np.random.set_state.__doc__\n-    seed.__doc__ = np.random.seed.__doc__\n-\n-    def __reduce__(self):\n-        \"\"\"Reduces the Random Number Generator to a RandomNumberGenerator, init_args and additional args.\"\"\"\n-        # np.random.Generator defines __reduce__, but it's hard-coded to\n-        # return a Generator instead of its subclass RandomNumberGenerator.\n-        # We need to override it here, otherwise sampling from a Space will\n-        # be broken after pickling and unpickling, due to using the deprecated\n-        # methods defined above.\n-        # See: https://github.com/numpy/numpy/blob/41d37b714caa1eef72f984d529f1d40ed48ce535/numpy/random/_generator.pyx#L221-L223\n-        # And: https://github.com/numpy/numpy/blob/41d37b714caa1eef72f984d529f1d40ed48ce535/numpy/random/_pickle.py#L17-L37\n-        _, init_args, *args = np.random.Generator.__reduce__(self)\n-        return (RandomNumberGenerator._generator_ctor, init_args, *args)\n-\n-    @staticmethod\n-    def _generator_ctor(bit_generator_name=\"MT19937\"):\n-        # Workaround method for RandomNumberGenerator pickling, see __reduce__ above.\n-        # Ported from numpy.random._pickle.__generator_ctor function.\n-        from numpy.random._pickle import BitGenerators\n-\n-        if bit_generator_name in BitGenerators:\n-            bit_generator = BitGenerators[bit_generator_name]\n-        else:\n-            raise ValueError(\n-                f\"{bit_generator_name} is not a known BitGenerator module.\"\n-            )\n-        return RandomNumberGenerator(bit_generator())\n-\n-\n-RNG = RandomNumberGenerator\n-\n-# Legacy functions\n-\n-\n-def hash_seed(seed: Optional[int] = None, max_bytes: int = 8) -> int:\n-    \"\"\"Any given evaluation is likely to have many PRNG's active at once.\n-\n-    (Most commonly, because the environment is running in multiple processes.)\n-    There's literature indicating that having linear correlations between seeds of multiple PRNG's can correlate the outputs:\n-        http://blogs.unity3d.com/2015/01/07/a-primer-on-repeatable-random-numbers/\n-        http://stackoverflow.com/questions/1554958/how-different-do-random-seeds-need-to-be\n-        http://dl.acm.org/citation.cfm?id=1276928\n-    Thus, for sanity we hash the seeds before using them. (This scheme is likely not crypto-strength, but it should be good enough to get rid of simple correlations.)\n-\n-    Args:\n-        seed: None seeds from an operating system specific randomness source.\n-        max_bytes: Maximum number of bytes to use in the hashed seed.\n-\n-    Returns:\n-        The hashed seed\n-    \"\"\"\n-    deprecation(\n-        \"Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \"\n-    )\n-    if seed is None:\n-        seed = create_seed(max_bytes=max_bytes)\n-    hash = hashlib.sha512(str(seed).encode(\"utf8\")).digest()\n-    return _bigint_from_bytes(hash[:max_bytes])\n-\n-\n-def create_seed(a: Optional[Union[int, str]] = None, max_bytes: int = 8) -> int:\n-    \"\"\"Create a strong random seed.\n-\n-    Otherwise, Python 2 would seed using the system time, which might be non-robust especially in the presence of concurrency.\n-\n-    Args:\n-        a: None seeds from an operating system specific randomness source.\n-        max_bytes: Maximum number of bytes to use in the seed.\n-\n-    Returns:\n-        A seed\n-\n-    Raises:\n-        Error: Invalid type for seed, expects None or str or int\n-    \"\"\"\n-    deprecation(\n-        \"Function `create_seed(a, max_bytes)` is marked as deprecated and will be removed in the future. \"\n-    )\n-    # Adapted from https://svn.python.org/projects/python/tags/r32/Lib/random.py\n-    if a is None:\n-        a = _bigint_from_bytes(os.urandom(max_bytes))\n-    elif isinstance(a, str):\n-        bt = a.encode(\"utf8\")\n-        bt += hashlib.sha512(bt).digest()\n-        a = _bigint_from_bytes(bt[:max_bytes])\n-    elif isinstance(a, int):\n-        a = int(a % 2 ** (8 * max_bytes))\n-    else:\n-        raise error.Error(f\"Invalid type for seed: {type(a)} ({a})\")\n-\n-    return a\n-\n-\n-# TODO: don't hardcode sizeof_int here\n-def _bigint_from_bytes(bt: bytes) -> int:\n-    deprecation(\n-        \"Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \"\n-    )\n-    sizeof_int = 4\n-    padding = sizeof_int - len(bt) % sizeof_int\n-    bt += b\"\\0\" * padding\n-    int_count = int(len(bt) / sizeof_int)\n-    unpacked = struct.unpack(f\"{int_count}I\", bt)\n-    accum = 0\n-    for i, val in enumerate(unpacked):\n-        accum += 2 ** (sizeof_int * 8 * i) * val\n-    return accum\n-\n-\n-def _int_list_from_bigint(bigint: int) -> List[int]:\n-    deprecation(\n-        \"Function `_int_list_from_bigint` is marked as deprecated and will be removed in the future. \"\n-    )\n-    # Special case 0\n-    if bigint < 0:\n-        raise error.Error(f\"Seed must be non-negative, not {bigint}\")\n-    elif bigint == 0:\n-        return [0]\n-\n-    ints: List[int] = []\n-    while bigint > 0:\n-        bigint, mod = divmod(bigint, 2**32)\n-        ints.append(mod)\n-    return ints\n+RandomNumberGenerator = np.random.Generator",
    "comment": "i think we shall leave the short name convenience (line 137).",
    "line_number": 30,
    "enriched": "File: gym/utils/seeding.py\nCode: @@ -31,201 +27,4 @@ def np_random(seed: Optional[int] = None) -> Tuple[\"RandomNumberGenerator\", Any]\n     return rng, np_seed\n \n \n-# TODO: Remove this class and make it alias to `Generator` in a future Gym release\n-# RandomNumberGenerator = np.random.Generator\n-class RandomNumberGenerator(np.random.Generator):\n-    \"\"\"Random number generator class that inherits from numpy's random Generator class.\"\"\"\n-\n-    def rand(self, *size):\n-        \"\"\"Deprecated rand function using random.\"\"\"\n-        deprecation(\n-            \"Function `rng.rand(*size)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `Generator.random(size)` instead.\"\n-        )\n-\n-        return self.random(size)\n-\n-    random_sample = rand\n-\n-    def randn(self, *size):\n-        \"\"\"Deprecated random standard normal function use standard_normal.\"\"\"\n-        deprecation(\n-            \"Function `rng.randn(*size)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.standard_normal(size)` instead.\"\n-        )\n-\n-        return self.standard_normal(size)\n-\n-    def randint(self, low, high=None, size=None, dtype=int):\n-        \"\"\"Deprecated random integer function use integers.\"\"\"\n-        deprecation(\n-            \"Function `rng.randint(low, [high, size, dtype])` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.integers(low, [high, size, dtype])` instead.\"\n-        )\n-\n-        return self.integers(low=low, high=high, size=size, dtype=dtype)\n-\n-    random_integers = randint\n-\n-    def get_state(self):\n-        \"\"\"Deprecated get rng state use bit_generator.state.\"\"\"\n-        deprecation(\n-            \"Function `rng.get_state()` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.bit_generator.state` instead.\"\n-        )\n-\n-        return self.bit_generator.state\n-\n-    def set_state(self, state):\n-        \"\"\"Deprecated set rng state function use bit_generator.state = state.\"\"\"\n-        deprecation(\n-            \"Function `rng.set_state(state)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.bit_generator.state = state` instead.\"\n-        )\n-\n-        self.bit_generator.state = state\n-\n-    def seed(self, seed=None):\n-        \"\"\"Deprecated seed function use gym.utils.seeding.np_random(seed).\"\"\"\n-        deprecation(\n-            \"Function `rng.seed(seed)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng, seed = gym.utils.seeding.np_random(seed)` to create a separate generator instead.\"\n-        )\n-\n-        self.bit_generator.state = type(self.bit_generator)(seed).state\n-\n-    rand.__doc__ = np.random.rand.__doc__\n-    randn.__doc__ = np.random.randn.__doc__\n-    randint.__doc__ = np.random.randint.__doc__\n-    get_state.__doc__ = np.random.get_state.__doc__\n-    set_state.__doc__ = np.random.set_state.__doc__\n-    seed.__doc__ = np.random.seed.__doc__\n-\n-    def __reduce__(self):\n-        \"\"\"Reduces the Random Number Generator to a RandomNumberGenerator, init_args and additional args.\"\"\"\n-        # np.random.Generator defines __reduce__, but it's hard-coded to\n-        # return a Generator instead of its subclass RandomNumberGenerator.\n-        # We need to override it here, otherwise sampling from a Space will\n-        # be broken after pickling and unpickling, due to using the deprecated\n-        # methods defined above.\n-        # See: https://github.com/numpy/numpy/blob/41d37b714caa1eef72f984d529f1d40ed48ce535/numpy/random/_generator.pyx#L221-L223\n-        # And: https://github.com/numpy/numpy/blob/41d37b714caa1eef72f984d529f1d40ed48ce535/numpy/random/_pickle.py#L17-L37\n-        _, init_args, *args = np.random.Generator.__reduce__(self)\n-        return (RandomNumberGenerator._generator_ctor, init_args, *args)\n-\n-    @staticmethod\n-    def _generator_ctor(bit_generator_name=\"MT19937\"):\n-        # Workaround method for RandomNumberGenerator pickling, see __reduce__ above.\n-        # Ported from numpy.random._pickle.__generator_ctor function.\n-        from numpy.random._pickle import BitGenerators\n-\n-        if bit_generator_name in BitGenerators:\n-            bit_generator = BitGenerators[bit_generator_name]\n-        else:\n-            raise ValueError(\n-                f\"{bit_generator_name} is not a known BitGenerator module.\"\n-            )\n-        return RandomNumberGenerator(bit_generator())\n-\n-\n-RNG = RandomNumberGenerator\n-\n-# Legacy functions\n-\n-\n-def hash_seed(seed: Optional[int] = None, max_bytes: int = 8) -> int:\n-    \"\"\"Any given evaluation is likely to have many PRNG's active at once.\n-\n-    (Most commonly, because the environment is running in multiple processes.)\n-    There's literature indicating that having linear correlations between seeds of multiple PRNG's can correlate the outputs:\n-        http://blogs.unity3d.com/2015/01/07/a-primer-on-repeatable-random-numbers/\n-        http://stackoverflow.com/questions/1554958/how-different-do-random-seeds-need-to-be\n-        http://dl.acm.org/citation.cfm?id=1276928\n-    Thus, for sanity we hash the seeds before using them. (This scheme is likely not crypto-strength, but it should be good enough to get rid of simple correlations.)\n-\n-    Args:\n-        seed: None seeds from an operating system specific randomness source.\n-        max_bytes: Maximum number of bytes to use in the hashed seed.\n-\n-    Returns:\n-        The hashed seed\n-    \"\"\"\n-    deprecation(\n-        \"Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \"\n-    )\n-    if seed is None:\n-        seed = create_seed(max_bytes=max_bytes)\n-    hash = hashlib.sha512(str(seed).encode(\"utf8\")).digest()\n-    return _bigint_from_bytes(hash[:max_bytes])\n-\n-\n-def create_seed(a: Optional[Union[int, str]] = None, max_bytes: int = 8) -> int:\n-    \"\"\"Create a strong random seed.\n-\n-    Otherwise, Python 2 would seed using the system time, which might be non-robust especially in the presence of concurrency.\n-\n-    Args:\n-        a: None seeds from an operating system specific randomness source.\n-        max_bytes: Maximum number of bytes to use in the seed.\n-\n-    Returns:\n-        A seed\n-\n-    Raises:\n-        Error: Invalid type for seed, expects None or str or int\n-    \"\"\"\n-    deprecation(\n-        \"Function `create_seed(a, max_bytes)` is marked as deprecated and will be removed in the future. \"\n-    )\n-    # Adapted from https://svn.python.org/projects/python/tags/r32/Lib/random.py\n-    if a is None:\n-        a = _bigint_from_bytes(os.urandom(max_bytes))\n-    elif isinstance(a, str):\n-        bt = a.encode(\"utf8\")\n-        bt += hashlib.sha512(bt).digest()\n-        a = _bigint_from_bytes(bt[:max_bytes])\n-    elif isinstance(a, int):\n-        a = int(a % 2 ** (8 * max_bytes))\n-    else:\n-        raise error.Error(f\"Invalid type for seed: {type(a)} ({a})\")\n-\n-    return a\n-\n-\n-# TODO: don't hardcode sizeof_int here\n-def _bigint_from_bytes(bt: bytes) -> int:\n-    deprecation(\n-        \"Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \"\n-    )\n-    sizeof_int = 4\n-    padding = sizeof_int - len(bt) % sizeof_int\n-    bt += b\"\\0\" * padding\n-    int_count = int(len(bt) / sizeof_int)\n-    unpacked = struct.unpack(f\"{int_count}I\", bt)\n-    accum = 0\n-    for i, val in enumerate(unpacked):\n-        accum += 2 ** (sizeof_int * 8 * i) * val\n-    return accum\n-\n-\n-def _int_list_from_bigint(bigint: int) -> List[int]:\n-    deprecation(\n-        \"Function `_int_list_from_bigint` is marked as deprecated and will be removed in the future. \"\n-    )\n-    # Special case 0\n-    if bigint < 0:\n-        raise error.Error(f\"Seed must be non-negative, not {bigint}\")\n-    elif bigint == 0:\n-        return [0]\n-\n-    ints: List[int] = []\n-    while bigint > 0:\n-        bigint, mod = divmod(bigint, 2**32)\n-        ints.append(mod)\n-    return ints\n+RandomNumberGenerator = np.random.Generator\nComment: ```suggestion\r\nRNG = RandomNumberGenerator = np.random.Generator\r\n```\r\n\r\nI think we shall leave the short name convenience (line 137).",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "gym/utils/seeding.py",
    "pr_number": 3022,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 945105377,
    "comment_created_at": "2022-08-13T07:16:15Z"
  },
  {
    "code": "@@ -547,7 +547,7 @@ def make(\n     id: Union[str, EnvSpec],\n     max_episode_steps: Optional[int] = None,\n     autoreset: bool = False,\n-    new_step_api: bool = False,\n+    new_step_api: bool = True,",
    "comment": "rename to 'apply_step_compatibility' as in a few years time new or old won't be descriptive",
    "line_number": 550,
    "enriched": "File: gym/envs/registration.py\nCode: @@ -547,7 +547,7 @@ def make(\n     id: Union[str, EnvSpec],\n     max_episode_steps: Optional[int] = None,\n     autoreset: bool = False,\n-    new_step_api: bool = False,\n+    new_step_api: bool = True,\nComment: Rename to 'apply_step_compatibility' as in a few years time new or old won't be descriptive",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "gym/envs/registration.py",
    "pr_number": 3019,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 939439238,
    "comment_created_at": "2022-08-05T23:09:28Z"
  },
  {
    "code": "@@ -0,0 +1,105 @@\n+\"\"\"Wrapper for adding time aware observations to environment observation.\"\"\"\n+from collections import OrderedDict\n+\n+import jumpy as jp\n+\n+import gym\n+import gym.spaces as spaces\n+from gym.core import ActType, ObsType\n+from gym.spaces import Box, Dict\n+\n+\n+class TimeAwareObservationV0(gym.ObservationWrapper):\n+    \"\"\"Augment the observation with the current time step in the episode.\n+\n+    Example:\n+        >>> import gym\n+        >>> env = gym.make('CartPole-v1')\n+        >>> env = TimeAwareObservationV0(env)\n+        >>> env.observation_space\n+        Dict(obs: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32), time: Box(0.0, inf, (1,), float32))\n+        >>> _ = env.reset()\n+        >>> env.step(env.action_space.sample())[0]\n+        OrderedDict([('time', array([1.])),\n+        ...  ('obs',\n+        ...    array([ 0.02768888,  0.1745313 ,  0.03663293, -0.32239535], dtype=float32))])\n+\n+    Flatten observation space example:\n+        >>> env = gym.make('CartPole-v1')\n+        >>> env = TimeAwareObservationV0(env, flatten=True)\n+        >>> env.observation_space\n+        Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38  0.0000000e+00], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38 inf], (5,), float32)\n+        >>> _ = env.reset()\n+        >>> env.step(env.action_space.sample())[0]\n+        array([-0.01232257,  0.19335455, -0.02244143, -0.32388705,  1. ], dtype=float32)\n+    \"\"\"\n+\n+    def __init__(self, env: gym.Env, flatten=False):\n+        \"\"\"Initialize :class:`TimeAwareObservationV0`.\n+\n+        Args:\n+            env: The environment to apply the wrapper\n+            flatten: Flatten the observation to a `Box` of a single dimension\n+        \"\"\"\n+        super().__init__(env)\n+        self.flatten = flatten\n+        self.num_envs = getattr(env, \"num_envs\", 1)\n+\n+        self.time_aware_observation_space = Dict(\n+            obs=env.observation_space, time=Box(0, jp.inf, (self.num_envs,))\n+        )\n+\n+        if self.flatten:\n+            self.observation_space = spaces.flatten_space(\n+                self.time_aware_observation_space\n+            )\n+        else:\n+            self.observation_space = self.time_aware_observation_space\n+\n+    def observation(self, observation: ObsType):\n+        \"\"\"Adds to the observation with the current time step.\n+\n+        Args:\n+            observation: The observation to add the time step to\n+\n+        Returns:\n+            The observation with the time step appended to\n+        \"\"\"\n+        observation = OrderedDict(obs=observation, time=self.t)",
    "comment": "why not use the remaining time (as suggested by https://arxiv.org/abs/1712.00378 or done in https://sb3-contrib.readthedocs.io/en/master/common/wrappers.html#timefeaturewrapper)\r\nunbounded observation sounds like a bad idea.",
    "line_number": 68,
    "enriched": "File: gym/dev_wrappers/time_aware_observation.py\nCode: @@ -0,0 +1,105 @@\n+\"\"\"Wrapper for adding time aware observations to environment observation.\"\"\"\n+from collections import OrderedDict\n+\n+import jumpy as jp\n+\n+import gym\n+import gym.spaces as spaces\n+from gym.core import ActType, ObsType\n+from gym.spaces import Box, Dict\n+\n+\n+class TimeAwareObservationV0(gym.ObservationWrapper):\n+    \"\"\"Augment the observation with the current time step in the episode.\n+\n+    Example:\n+        >>> import gym\n+        >>> env = gym.make('CartPole-v1')\n+        >>> env = TimeAwareObservationV0(env)\n+        >>> env.observation_space\n+        Dict(obs: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32), time: Box(0.0, inf, (1,), float32))\n+        >>> _ = env.reset()\n+        >>> env.step(env.action_space.sample())[0]\n+        OrderedDict([('time', array([1.])),\n+        ...  ('obs',\n+        ...    array([ 0.02768888,  0.1745313 ,  0.03663293, -0.32239535], dtype=float32))])\n+\n+    Flatten observation space example:\n+        >>> env = gym.make('CartPole-v1')\n+        >>> env = TimeAwareObservationV0(env, flatten=True)\n+        >>> env.observation_space\n+        Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38  0.0000000e+00], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38 inf], (5,), float32)\n+        >>> _ = env.reset()\n+        >>> env.step(env.action_space.sample())[0]\n+        array([-0.01232257,  0.19335455, -0.02244143, -0.32388705,  1. ], dtype=float32)\n+    \"\"\"\n+\n+    def __init__(self, env: gym.Env, flatten=False):\n+        \"\"\"Initialize :class:`TimeAwareObservationV0`.\n+\n+        Args:\n+            env: The environment to apply the wrapper\n+            flatten: Flatten the observation to a `Box` of a single dimension\n+        \"\"\"\n+        super().__init__(env)\n+        self.flatten = flatten\n+        self.num_envs = getattr(env, \"num_envs\", 1)\n+\n+        self.time_aware_observation_space = Dict(\n+            obs=env.observation_space, time=Box(0, jp.inf, (self.num_envs,))\n+        )\n+\n+        if self.flatten:\n+            self.observation_space = spaces.flatten_space(\n+                self.time_aware_observation_space\n+            )\n+        else:\n+            self.observation_space = self.time_aware_observation_space\n+\n+    def observation(self, observation: ObsType):\n+        \"\"\"Adds to the observation with the current time step.\n+\n+        Args:\n+            observation: The observation to add the time step to\n+\n+        Returns:\n+            The observation with the time step appended to\n+        \"\"\"\n+        observation = OrderedDict(obs=observation, time=self.t)\nComment: why not use the remaining time (as suggested by https://arxiv.org/abs/1712.00378 or done in https://sb3-contrib.readthedocs.io/en/master/common/wrappers.html#timefeaturewrapper)\r\nunbounded observation sounds like a bad idea.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "gym/dev_wrappers/time_aware_observation.py",
    "pr_number": 3013,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 938795417,
    "comment_created_at": "2022-08-05T13:08:23Z"
  },
  {
    "code": "@@ -115,19 +115,14 @@ def seed(self, seed: Optional[Union[dict, int]] = None) -> list:\n                 seeds += self.spaces[key].seed(seed[seed_key])\n         elif isinstance(seed, int):\n             seeds = super().seed(seed)\n-            try:\n-                subseeds = self.np_random.choice(\n-                    np.iinfo(int).max,\n-                    size=len(self.spaces),\n-                    replace=False,  # unique subseed for each subspace\n-                )\n-            except ValueError:\n-                subseeds = self.np_random.choice(\n-                    np.iinfo(int).max,\n-                    size=len(self.spaces),\n-                    replace=True,  # we get more than INT_MAX subspaces\n-                )\n-\n+            subseeds = []\n+            assert (\n+                len(self.spaces) <= np.iinfo(np.int32).max\n+            ), f\"Expected spaces.length <= 2147483647, got {len(self.spaces)}\"",
    "comment": "we're introducing a new exception here (so de facto deprecating a new behavior), admittedly for an *extremely* niche use case, and if anyone encounters it i'd be concerned.\r\n\r\nstill, it's better to maintain backwards compatibility than not. can't we go the same way as in the previous version, so if len(self.spaces) > np.iinfo(np.int32).max, we just relax the uniqueness property?",
    "line_number": 121,
    "enriched": "File: gym/spaces/dict.py\nCode: @@ -115,19 +115,14 @@ def seed(self, seed: Optional[Union[dict, int]] = None) -> list:\n                 seeds += self.spaces[key].seed(seed[seed_key])\n         elif isinstance(seed, int):\n             seeds = super().seed(seed)\n-            try:\n-                subseeds = self.np_random.choice(\n-                    np.iinfo(int).max,\n-                    size=len(self.spaces),\n-                    replace=False,  # unique subseed for each subspace\n-                )\n-            except ValueError:\n-                subseeds = self.np_random.choice(\n-                    np.iinfo(int).max,\n-                    size=len(self.spaces),\n-                    replace=True,  # we get more than INT_MAX subspaces\n-                )\n-\n+            subseeds = []\n+            assert (\n+                len(self.spaces) <= np.iinfo(np.int32).max\n+            ), f\"Expected spaces.length <= 2147483647, got {len(self.spaces)}\"\nComment: We're introducing a new exception here (so de facto deprecating a new behavior), admittedly for an *extremely* niche use case, and if anyone encounters it I'd be concerned.\r\n\r\nStill, it's better to maintain backwards compatibility than not. Can't we go the same way as in the previous version, so if `len(self.spaces) > np.iinfo(np.int32).max`, we just relax the uniqueness property?",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "gym/spaces/dict.py",
    "pr_number": 3011,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 934385408,
    "comment_created_at": "2022-08-01T10:37:39Z"
  },
  {
    "code": "@@ -120,3 +128,20 @@ def test_render_modes(spec):\n             new_env.render()\n             new_env.close()\n     env.close()\n+\n+\n+@pytest.mark.parametrize(\n+    \"env\",\n+    all_testing_initialised_envs,\n+    ids=[env.spec.id for env in all_testing_initialised_envs],\n+)\n+def test_pickle_env(env: gym.Env):\n+    env.np_random, _ = seeding.np_random(0)",
    "comment": "you're not resetting the env before pickling which is likely to mess things up (and i don't think it's supported behavior anyways, since you need to reset an env before doing literally anything with it)",
    "line_number": 139,
    "enriched": "File: tests/envs/test_envs.py\nCode: @@ -120,3 +128,20 @@ def test_render_modes(spec):\n             new_env.render()\n             new_env.close()\n     env.close()\n+\n+\n+@pytest.mark.parametrize(\n+    \"env\",\n+    all_testing_initialised_envs,\n+    ids=[env.spec.id for env in all_testing_initialised_envs],\n+)\n+def test_pickle_env(env: gym.Env):\n+    env.np_random, _ = seeding.np_random(0)\nComment: You're not resetting the env before pickling which is likely to mess things up (and I don't think it's supported behavior anyways, since you need to reset an env before doing literally anything with it)",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "tests/envs/test_envs.py",
    "pr_number": 3006,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 934403655,
    "comment_created_at": "2022-08-01T11:01:32Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+\"\"\"A set of utility functions for lambda wrappers.\"\"\"\n+from functools import singledispatch\n+from typing import Any\n+from typing import Dict as TypingDict\n+from typing import Tuple as TypingTuple\n+\n+from gym.dev_wrappers import FuncArgType\n+from gym.spaces import Box, Dict, Discrete, MultiBinary, MultiDiscrete, Space, Tuple\n+\n+\n+@singledispatch\n+def filter_space(space: Space, args: FuncArgType) -> Any:\n+    \"\"\"Filter space with the provided args.\"\"\"\n+",
    "comment": "raise notimplementederror",
    "line_number": 14,
    "enriched": "File: gym/dev_wrappers/utils/filter_space.py\nCode: @@ -0,0 +1,39 @@\n+\"\"\"A set of utility functions for lambda wrappers.\"\"\"\n+from functools import singledispatch\n+from typing import Any\n+from typing import Dict as TypingDict\n+from typing import Tuple as TypingTuple\n+\n+from gym.dev_wrappers import FuncArgType\n+from gym.spaces import Box, Dict, Discrete, MultiBinary, MultiDiscrete, Space, Tuple\n+\n+\n+@singledispatch\n+def filter_space(space: Space, args: FuncArgType) -> Any:\n+    \"\"\"Filter space with the provided args.\"\"\"\n+\nComment: `raise NotImplementedError`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "gym/dev_wrappers/utils/filter_space.py",
    "pr_number": 3002,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 955543453,
    "comment_created_at": "2022-08-26T01:06:10Z"
  },
  {
    "code": "@@ -106,6 +87,12 @@ class Env(Generic[ObsType, ActType], metaclass=decorator):\n     Note: a default reward range set to :math:`(-\\infty,+\\infty)` already exists. Set it if you want a narrower range.\n     \"\"\"\n \n+    def __init_subclass__(cls) -> None:\n+        \"\"\"Hook used for wrapping render method.\"\"\"\n+        super().__init_subclass__()\n+        if \"render\" in vars(cls):\n+            cls.render = _deprecate_mode(vars(cls)[\"render\"])",
    "comment": "any reason why this can't be cls.render = _deprecate_mode(cls.render)? if not, i'd prefer that for simplicity",
    "line_number": 94,
    "enriched": "File: gym/core.py\nCode: @@ -106,6 +87,12 @@ class Env(Generic[ObsType, ActType], metaclass=decorator):\n     Note: a default reward range set to :math:`(-\\infty,+\\infty)` already exists. Set it if you want a narrower range.\n     \"\"\"\n \n+    def __init_subclass__(cls) -> None:\n+        \"\"\"Hook used for wrapping render method.\"\"\"\n+        super().__init_subclass__()\n+        if \"render\" in vars(cls):\n+            cls.render = _deprecate_mode(vars(cls)[\"render\"])\nComment: Any reason why this can't be `cls.render = _deprecate_mode(cls.render)`? If not, I'd prefer that for simplicity",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/core.py",
    "pr_number": 3001,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 935811631,
    "comment_created_at": "2022-08-02T16:43:23Z"
  },
  {
    "code": "@@ -24,15 +24,16 @@ class CliffWalkingEnv(Env):\n     by Sutton and Barto](http://incompleteideas.net/book/bookdraft2018jan1.pdf).\n \n     With inspiration from:\n-    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n+    [https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py]\n+    (https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py)",
    "comment": "is this meant to be repeated",
    "line_number": 28,
    "enriched": "File: gym/envs/toy_text/cliffwalking.py\nCode: @@ -24,15 +24,16 @@ class CliffWalkingEnv(Env):\n     by Sutton and Barto](http://incompleteideas.net/book/bookdraft2018jan1.pdf).\n \n     With inspiration from:\n-    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n+    [https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py]\n+    (https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py)\nComment: Is this meant to be repeated ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/envs/toy_text/cliffwalking.py",
    "pr_number": 2997,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 929805836,
    "comment_created_at": "2022-07-26T10:38:20Z"
  },
  {
    "code": "@@ -621,30 +621,48 @@ def make(\n     mode = _kwargs.get(\"render_mode\")\n     apply_human_rendering = False\n \n-    # If we have access to metadata we check that \"render_mode\" is valid\n-    if hasattr(env_creator, \"metadata\"):\n-        render_modes = env_creator.metadata[\"render_modes\"]\n-\n-        # We might be able to fall back to the HumanRendering wrapper if 'human' rendering is not supported natively\n-        if (\n-            mode == \"human\"\n-            and \"human\" not in render_modes\n-            and (\"single_rgb_array\" in render_modes or \"rgb_array\" in render_modes)\n-        ):\n+    # If we have access to metadata we check that \"render_mode\" is valid and see if the HumanRendering wrapper needs to be applied\n+    if mode is not None:\n+        if hasattr(env_creator, \"metadata\"):\n+            if isinstance(env_creator.metadata, dict):",
    "comment": "what else could metadata be?",
    "line_number": 627,
    "enriched": "File: gym/envs/registration.py\nCode: @@ -621,30 +621,48 @@ def make(\n     mode = _kwargs.get(\"render_mode\")\n     apply_human_rendering = False\n \n-    # If we have access to metadata we check that \"render_mode\" is valid\n-    if hasattr(env_creator, \"metadata\"):\n-        render_modes = env_creator.metadata[\"render_modes\"]\n-\n-        # We might be able to fall back to the HumanRendering wrapper if 'human' rendering is not supported natively\n-        if (\n-            mode == \"human\"\n-            and \"human\" not in render_modes\n-            and (\"single_rgb_array\" in render_modes or \"rgb_array\" in render_modes)\n-        ):\n+    # If we have access to metadata we check that \"render_mode\" is valid and see if the HumanRendering wrapper needs to be applied\n+    if mode is not None:\n+        if hasattr(env_creator, \"metadata\"):\n+            if isinstance(env_creator.metadata, dict):\nComment: What else could `metadata` be?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/envs/registration.py",
    "pr_number": 2990,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 928278809,
    "comment_created_at": "2022-07-24T15:58:29Z"
  },
  {
    "code": "@@ -99,7 +99,7 @@ class LunarLander(gym.Env, EzPickle):\n     orientation engine, fire main engine, fire right orientation engine.\n \n     ### Observation Space\n-    There are 8 states: the coordinates of the lander in `x` & `y`, its linear\n+    States are eight (8) dimensional vector: the coordinates of the lander in `x` & `y`, its linear",
    "comment": "i'd rephrase this to \"the state is an 8-dimensional vector\"",
    "line_number": 102,
    "enriched": "File: gym/envs/box2d/lunar_lander.py\nCode: @@ -99,7 +99,7 @@ class LunarLander(gym.Env, EzPickle):\n     orientation engine, fire main engine, fire right orientation engine.\n \n     ### Observation Space\n-    There are 8 states: the coordinates of the lander in `x` & `y`, its linear\n+    States are eight (8) dimensional vector: the coordinates of the lander in `x` & `y`, its linear\nComment: I'd rephrase this to \"The state is an 8-dimensional vector\"",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "gym/envs/box2d/lunar_lander.py",
    "pr_number": 2987,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 927780322,
    "comment_created_at": "2022-07-22T15:42:32Z"
  },
  {
    "code": "@@ -0,0 +1,95 @@\n+\"\"\"Lambda reward wrappers that uses jumpy for compatibility with jax (i.e. brax) and numpy environments.\"\"\"\n+from typing import Callable, Optional, Union\n+\n+import jax.numpy as jnp\n+import jumpy as jp\n+import numpy as np\n+\n+import gym\n+from gym.dev_wrappers import FuncArgType\n+from gym.error import InvalidBound\n+\n+\n+class lambda_reward_v0(gym.RewardWrapper):",
    "comment": "please no snake_case class names. i'd even take lambdareward_v0, or lambdarewardv0 over this",
    "line_number": 13,
    "enriched": "File: gym/dev_wrappers/lambda_reward.py\nCode: @@ -0,0 +1,95 @@\n+\"\"\"Lambda reward wrappers that uses jumpy for compatibility with jax (i.e. brax) and numpy environments.\"\"\"\n+from typing import Callable, Optional, Union\n+\n+import jax.numpy as jnp\n+import jumpy as jp\n+import numpy as np\n+\n+import gym\n+from gym.dev_wrappers import FuncArgType\n+from gym.error import InvalidBound\n+\n+\n+class lambda_reward_v0(gym.RewardWrapper):\nComment: Please no snake_case class names. I'd even take LambdaReward_v0, or LambdaRewardV0 over this",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "gym/dev_wrappers/lambda_reward.py",
    "pr_number": 2985,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 927556330,
    "comment_created_at": "2022-07-22T11:21:43Z"
  },
  {
    "code": "@@ -56,7 +56,7 @@ def __init__(\n         low: Union[SupportsFloat, np.ndarray],\n         high: Union[SupportsFloat, np.ndarray],\n         shape: Optional[Sequence[int]] = None,\n-        dtype: Type = np.float32,\n+        dtype: Union[Type, object] = np.float32,",
    "comment": "i'm not so sure about this annotation, since pretty much anything is an object anyways (maaaaaaybe except for some extremely exotic metaclass constructs, i'm not sure)\r\n\r\nwe want it to be either a python type, or a numpy datatype, right? i think numpy has a type annotation for this somewhere in its typing module",
    "line_number": 59,
    "enriched": "File: gym/spaces/box.py\nCode: @@ -56,7 +56,7 @@ def __init__(\n         low: Union[SupportsFloat, np.ndarray],\n         high: Union[SupportsFloat, np.ndarray],\n         shape: Optional[Sequence[int]] = None,\n-        dtype: Type = np.float32,\n+        dtype: Union[Type, object] = np.float32,\nComment: I'm not so sure about this annotation, since pretty much anything is an `object` anyways (maaaaaaybe except for some extremely exotic metaclass constructs, I'm not sure)\r\n\r\nWe want it to be either a python type, or a numpy datatype, right? I think numpy has a type annotation for this somewhere in its typing module",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "gym/spaces/box.py",
    "pr_number": 2977,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 936744773,
    "comment_created_at": "2022-08-03T14:35:21Z"
  },
  {
    "code": "@@ -1,90 +1,118 @@\n import numpy as np\n import pytest\n \n-import gym\n+from gym.spaces import Box, Discrete\n from gym.wrappers import AtariPreprocessing\n+from tests.testing_env import GenericTestEnv, old_step_fn\n \n-pytest.importorskip(\"gym.envs.atari\")\n \n+class AleTesting:",
    "comment": "docstring on class; docstring and type hint on methods",
    "line_number": 9,
    "enriched": "File: tests/wrappers/test_atari_preprocessing.py\nCode: @@ -1,90 +1,118 @@\n import numpy as np\n import pytest\n \n-import gym\n+from gym.spaces import Box, Discrete\n from gym.wrappers import AtariPreprocessing\n+from tests.testing_env import GenericTestEnv, old_step_fn\n \n-pytest.importorskip(\"gym.envs.atari\")\n \n+class AleTesting:\nComment: docstring on class; docstring and type hint on methods",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "tests/wrappers/test_atari_preprocessing.py",
    "pr_number": 2976,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 926082671,
    "comment_created_at": "2022-07-20T21:49:23Z"
  },
  {
    "code": "@@ -493,6 +493,34 @@ def test_symtable_entry_repr(self):\n         expected = f\"<symtable entry top({self.top.get_id()}), line {self.top.get_lineno()}>\"\n         self.assertEqual(repr(self.top._table), expected)\n \n+    def test__symtable_refleak(self):\n+        # Regression test for reference leak in PyUnicode_FSDecoder.\n+        # See https://github.com/python/cpython/issues/139748.\n+        mortal_str = 'this is a mortal string'\n+        # check error path when 'compile_type' AC conversion failed\n+        self.assertRaises(TypeError, symtable.symtable, '', mortal_str, 1)\n+\n+\n+class ComprehensionTests(unittest.TestCase):",
    "comment": "i think those tests are extra ones",
    "line_number": 504,
    "enriched": "File: Lib/test/test_symtable.py\nCode: @@ -493,6 +493,34 @@ def test_symtable_entry_repr(self):\n         expected = f\"<symtable entry top({self.top.get_id()}), line {self.top.get_lineno()}>\"\n         self.assertEqual(repr(self.top._table), expected)\n \n+    def test__symtable_refleak(self):\n+        # Regression test for reference leak in PyUnicode_FSDecoder.\n+        # See https://github.com/python/cpython/issues/139748.\n+        mortal_str = 'this is a mortal string'\n+        # check error path when 'compile_type' AC conversion failed\n+        self.assertRaises(TypeError, symtable.symtable, '', mortal_str, 1)\n+\n+\n+class ComprehensionTests(unittest.TestCase):\nComment: I think those tests are extra ones",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "Lib/test/test_symtable.py",
    "pr_number": 139792,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2414292819,
    "comment_created_at": "2025-10-08T15:39:20Z"
  },
  {
    "code": "@@ -128,7 +128,7 @@ jobs:\n           choco install llvm --allow-downgrade --no-progress --version ${{ matrix.llvm }}.1.0\n           ./PCbuild/build.bat --experimental-jit ${{ matrix.debug && '-d' || '' }} -p ${{ matrix.architecture }}\n \n-      - name: Native macOS\n+      - name: macOS",
    "comment": "let's keep the old name, because there's also \"native linux\" and so on in this file:",
    "line_number": 131,
    "enriched": "File: .github/workflows/jit.yml\nCode: @@ -128,7 +128,7 @@ jobs:\n           choco install llvm --allow-downgrade --no-progress --version ${{ matrix.llvm }}.1.0\n           ./PCbuild/build.bat --experimental-jit ${{ matrix.debug && '-d' || '' }} -p ${{ matrix.architecture }}\n \n-      - name: Native macOS\n+      - name: macOS\nComment: Let's keep the old name, because there's also \"Native Linux\" and so on in this file:\r\n```suggestion\r\n      - name: Native macOS\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": ".github/workflows/jit.yml",
    "pr_number": 139790,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2414336936,
    "comment_created_at": "2025-10-08T15:54:03Z"
  },
  {
    "code": "@@ -527,26 +527,12 @@ def test_symtable_entry_repr(self):\n         expected = f\"<symtable entry top({self.top.get_id()}), line {self.top.get_lineno()}>\"\n         self.assertEqual(repr(self.top._table), expected)\n \n-\n-class ComprehensionTests(unittest.TestCase):\n-    def get_identifiers_recursive(self, st, res):",
    "comment": "what happened to those tests?",
    "line_number": 532,
    "enriched": "File: Lib/test/test_symtable.py\nCode: @@ -527,26 +527,12 @@ def test_symtable_entry_repr(self):\n         expected = f\"<symtable entry top({self.top.get_id()}), line {self.top.get_lineno()}>\"\n         self.assertEqual(repr(self.top._table), expected)\n \n-\n-class ComprehensionTests(unittest.TestCase):\n-    def get_identifiers_recursive(self, st, res):\nComment: What happened to those tests?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "Lib/test/test_symtable.py",
    "pr_number": 139789,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2414290668,
    "comment_created_at": "2025-10-08T15:38:30Z"
  },
  {
    "code": "@@ -20,9 +20,3 @@ select = [\n     \"W\",       # pycodestyle\n     \"YTT\",     # flake8-2020\n ]\n-ignore = [\n-    \"E501\",    # Line too long",
    "comment": "keeping this one makes likely sense given we're auto-formatting.",
    "line_number": 24,
    "enriched": "File: Tools/wasm/.ruff.toml\nCode: @@ -20,9 +20,3 @@ select = [\n     \"W\",       # pycodestyle\n     \"YTT\",     # flake8-2020\n ]\n-ignore = [\n-    \"E501\",    # Line too long\nComment: Keeping this one makes likely sense given we're auto-formatting.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "Tools/wasm/.ruff.toml",
    "pr_number": 139752,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2413220312,
    "comment_created_at": "2025-10-08T09:32:20Z"
  },
  {
    "code": "@@ -420,9 +420,10 @@ static void clear_global_interned_strings(void)\n         return unicode_get_empty();  \\\n     } while (0)\n \n-static inline void\n-unicode_fill(int kind, void *data, Py_UCS4 value,\n-             Py_ssize_t start, Py_ssize_t length)\n+\n+void\n+_PyUnicode_Fill(int kind, void *data, Py_UCS4 value,\n+                Py_ssize_t start, Py_ssize_t length)",
    "comment": "are you sure that it will be inlined? maybe keep static inline unicode_fill() and add a wrapper _pyunicode_fill()?",
    "line_number": 426,
    "enriched": "File: Objects/unicodeobject.c\nCode: @@ -420,9 +420,10 @@ static void clear_global_interned_strings(void)\n         return unicode_get_empty();  \\\n     } while (0)\n \n-static inline void\n-unicode_fill(int kind, void *data, Py_UCS4 value,\n-             Py_ssize_t start, Py_ssize_t length)\n+\n+void\n+_PyUnicode_Fill(int kind, void *data, Py_UCS4 value,\n+                Py_ssize_t start, Py_ssize_t length)\nComment: Are you sure that it will be inlined? Maybe keep static inline `unicode_fill()` and add a wrapper `_PyUnicode_Fill()`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "Objects/unicodeobject.c",
    "pr_number": 139723,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2413526532,
    "comment_created_at": "2025-10-08T11:21:40Z"
  },
  {
    "code": "@@ -1859,6 +1861,12 @@ def __format__(self, format):\n         # Test multiple format specs in same raw f-string\n         self.assertEqual(rf\"{UnchangedFormat():\\xFF} {UnchangedFormat():\\n}\", '\\\\xFF \\\\n')\n \n+    def test_gh139516(self):\n+        # gh-139516\n+        # The '\\n' is explicit to ensure no trailing whitespace which would invalidate the test.\n+        # Must use tokenize instead of compile so that source is parsed by line which exposes the bug.\n+        list(tokenize.tokenize(BytesIO('''f\"{f(a=lambda: '\u00e0'\\n)}\"'''.encode()).readline))",
    "comment": "i am confused. isn't it possible to trigger this in an exec or eval call? or perhaps a file with an encoding?",
    "line_number": 1868,
    "enriched": "File: Lib/test/test_fstring.py\nCode: @@ -1859,6 +1861,12 @@ def __format__(self, format):\n         # Test multiple format specs in same raw f-string\n         self.assertEqual(rf\"{UnchangedFormat():\\xFF} {UnchangedFormat():\\n}\", '\\\\xFF \\\\n')\n \n+    def test_gh139516(self):\n+        # gh-139516\n+        # The '\\n' is explicit to ensure no trailing whitespace which would invalidate the test.\n+        # Must use tokenize instead of compile so that source is parsed by line which exposes the bug.\n+        list(tokenize.tokenize(BytesIO('''f\"{f(a=lambda: '\u00e0'\\n)}\"'''.encode()).readline))\nComment: I am confused. Isn't it possible to trigger this in an `exec` or `eval` call? Or perhaps a file with an encoding?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "Lib/test/test_fstring.py",
    "pr_number": 139657,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2407787516,
    "comment_created_at": "2025-10-06T18:12:08Z"
  },
  {
    "code": "@@ -895,3 +895,101 @@ def scale_and_translate(\n         method,\n         antialias,\n     )\n+\n+\n+def extract_volume_patches(\n+    volumes, size, strides, dilation_rate, padding, data_format\n+):\n+    \"\"\"JAX implementation of extract_volume_patches.\"\"\"\n+    import jax.numpy as jnp\n+\n+    # Convert to channels_last if needed\n+    if data_format == \"channels_first\":\n+        # (B, C, D, H, W) -> (B, D, H, W, C)\n+        volumes = jnp.transpose(volumes, [0, 2, 3, 4, 1])\n+\n+    batch_size, depth, height, width, channels = volumes.shape\n+\n+    # Normalize size, strides, and dilation_rate to tuples\n+    if isinstance(size, int):\n+        pd, ph, pw = size, size, size\n+    else:\n+        pd, ph, pw = size\n+\n+    if isinstance(strides, int):\n+        sd, sh, sw = strides, strides, strides\n+    elif strides is None:\n+        sd, sh, sw = pd, ph, pw\n+    else:\n+        sd, sh, sw = strides\n+\n+    if isinstance(dilation_rate, int):\n+        dd, dh, dw = dilation_rate, dilation_rate, dilation_rate\n+    else:\n+        dd, dh, dw = dilation_rate\n+\n+    # Calculate effective patch size with dilation\n+    eff_pd = pd + (pd - 1) * (dd - 1)\n+    eff_ph = ph + (ph - 1) * (dh - 1)\n+    eff_pw = pw + (pw - 1) * (dw - 1)\n+\n+    # Calculate output dimensions\n+    if padding == \"valid\":\n+        out_d = (depth - eff_pd) // sd + 1\n+        out_h = (height - eff_ph) // sh + 1\n+        out_w = (width - eff_pw) // sw + 1\n+    else:  # same\n+        out_d = (depth + sd - 1) // sd\n+        out_h = (height + sh - 1) // sh\n+        out_w = (width + sw - 1) // sw\n+\n+        # Calculate and apply padding\n+        pad_d = max((out_d - 1) * sd + eff_pd - depth, 0)\n+        pad_h = max((out_h - 1) * sh + eff_ph - height, 0)\n+        pad_w = max((out_w - 1) * sw + eff_pw - width, 0)\n+\n+        pad_d_before = pad_d // 2\n+        pad_d_after = pad_d - pad_d_before\n+        pad_h_before = pad_h // 2\n+        pad_h_after = pad_h - pad_h_before\n+        pad_w_before = pad_w // 2\n+        pad_w_after = pad_w - pad_w_before\n+\n+        volumes = jnp.pad(\n+            volumes,\n+            (\n+                (0, 0),\n+                (pad_d_before, pad_d_after),\n+                (pad_h_before, pad_h_after),\n+                (pad_w_before, pad_w_after),\n+                (0, 0),\n+            ),\n+            mode=\"constant\",\n+        )\n+        depth, height, width = volumes.shape[1:4]\n+\n+    # Extract patches using advanced indexing\n+    patches_list = []\n+    for d_idx in range(out_d):\n+        for h_idx in range(out_h):\n+            for w_idx in range(out_w):\n+                d_start = d_idx * sd\n+                h_start = h_idx * sh\n+                w_start = w_idx * sw\n+\n+                # Extract patch with dilation\n+                patch = volumes[\n+                    :,\n+                    d_start : d_start + eff_pd : dd,\n+                    h_start : h_start + eff_ph : dh,\n+                    w_start : w_start + eff_pw : dw,\n+                    :,\n+                ]\n+\n+                patch_flat = patch.reshape(batch_size, -1)\n+                patches_list.append(patch_flat)\n+",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe use of nested python loops to extract patches is highly inefficient for jax. this approach will lead to very poor performance, especially when jit-compiled, as it can cause excessive unrolling of the loops or slow execution in general. \n\na vectorized approach is necessary for a performant jax implementation. i recommend exploring one of the following options:\n\n1.  **adapt the 2d extract_patches implementation**: the existing 2d extract_patches in keras for jax uses lax.conv_general_dilated, which is a highly optimized operation. this could potentially be extended for 3d volumes.\n2.  **use lax.gather**: construct indices for all patches and use lax.gather to extract them in a single, vectorized operation.\n3.  **use lax.scan**: if a fully vectorized solution is too complex, using lax.scan over one of the dimensions would be significantly more performant than python loops.",
    "line_number": 991,
    "enriched": "File: keras/src/backend/jax/image.py\nCode: @@ -895,3 +895,101 @@ def scale_and_translate(\n         method,\n         antialias,\n     )\n+\n+\n+def extract_volume_patches(\n+    volumes, size, strides, dilation_rate, padding, data_format\n+):\n+    \"\"\"JAX implementation of extract_volume_patches.\"\"\"\n+    import jax.numpy as jnp\n+\n+    # Convert to channels_last if needed\n+    if data_format == \"channels_first\":\n+        # (B, C, D, H, W) -> (B, D, H, W, C)\n+        volumes = jnp.transpose(volumes, [0, 2, 3, 4, 1])\n+\n+    batch_size, depth, height, width, channels = volumes.shape\n+\n+    # Normalize size, strides, and dilation_rate to tuples\n+    if isinstance(size, int):\n+        pd, ph, pw = size, size, size\n+    else:\n+        pd, ph, pw = size\n+\n+    if isinstance(strides, int):\n+        sd, sh, sw = strides, strides, strides\n+    elif strides is None:\n+        sd, sh, sw = pd, ph, pw\n+    else:\n+        sd, sh, sw = strides\n+\n+    if isinstance(dilation_rate, int):\n+        dd, dh, dw = dilation_rate, dilation_rate, dilation_rate\n+    else:\n+        dd, dh, dw = dilation_rate\n+\n+    # Calculate effective patch size with dilation\n+    eff_pd = pd + (pd - 1) * (dd - 1)\n+    eff_ph = ph + (ph - 1) * (dh - 1)\n+    eff_pw = pw + (pw - 1) * (dw - 1)\n+\n+    # Calculate output dimensions\n+    if padding == \"valid\":\n+        out_d = (depth - eff_pd) // sd + 1\n+        out_h = (height - eff_ph) // sh + 1\n+        out_w = (width - eff_pw) // sw + 1\n+    else:  # same\n+        out_d = (depth + sd - 1) // sd\n+        out_h = (height + sh - 1) // sh\n+        out_w = (width + sw - 1) // sw\n+\n+        # Calculate and apply padding\n+        pad_d = max((out_d - 1) * sd + eff_pd - depth, 0)\n+        pad_h = max((out_h - 1) * sh + eff_ph - height, 0)\n+        pad_w = max((out_w - 1) * sw + eff_pw - width, 0)\n+\n+        pad_d_before = pad_d // 2\n+        pad_d_after = pad_d - pad_d_before\n+        pad_h_before = pad_h // 2\n+        pad_h_after = pad_h - pad_h_before\n+        pad_w_before = pad_w // 2\n+        pad_w_after = pad_w - pad_w_before\n+\n+        volumes = jnp.pad(\n+            volumes,\n+            (\n+                (0, 0),\n+                (pad_d_before, pad_d_after),\n+                (pad_h_before, pad_h_after),\n+                (pad_w_before, pad_w_after),\n+                (0, 0),\n+            ),\n+            mode=\"constant\",\n+        )\n+        depth, height, width = volumes.shape[1:4]\n+\n+    # Extract patches using advanced indexing\n+    patches_list = []\n+    for d_idx in range(out_d):\n+        for h_idx in range(out_h):\n+            for w_idx in range(out_w):\n+                d_start = d_idx * sd\n+                h_start = h_idx * sh\n+                w_start = w_idx * sw\n+\n+                # Extract patch with dilation\n+                patch = volumes[\n+                    :,\n+                    d_start : d_start + eff_pd : dd,\n+                    h_start : h_start + eff_ph : dh,\n+                    w_start : w_start + eff_pw : dw,\n+                    :,\n+                ]\n+\n+                patch_flat = patch.reshape(batch_size, -1)\n+                patches_list.append(patch_flat)\n+\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe use of nested Python loops to extract patches is highly inefficient for JAX. This approach will lead to very poor performance, especially when JIT-compiled, as it can cause excessive unrolling of the loops or slow execution in general. \n\nA vectorized approach is necessary for a performant JAX implementation. I recommend exploring one of the following options:\n\n1.  **Adapt the 2D `extract_patches` implementation**: The existing 2D `extract_patches` in Keras for JAX uses `lax.conv_general_dilated`, which is a highly optimized operation. This could potentially be extended for 3D volumes.\n2.  **Use `lax.gather`**: Construct indices for all patches and use `lax.gather` to extract them in a single, vectorized operation.\n3.  **Use `lax.scan`**: If a fully vectorized solution is too complex, using `lax.scan` over one of the dimensions would be significantly more performant than Python loops.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/jax/image.py",
    "pr_number": 21727,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2413702921,
    "comment_created_at": "2025-10-08T12:31:54Z"
  },
  {
    "code": "@@ -5,15 +5,20 @@\n \"\"\"\n \n from keras.src.backend.config import backend as backend\n+from keras.src.backend.config import disable_debug_mode as disable_debug_mode\n from keras.src.backend.config import (\n     disable_flash_attention as disable_flash_attention,\n )\n+from keras.src.backend.config import enable_debug_mode as enable_debug_mode\n from keras.src.backend.config import (\n     enable_flash_attention as enable_flash_attention,\n )\n from keras.src.backend.config import epsilon as epsilon\n from keras.src.backend.config import floatx as floatx\n from keras.src.backend.config import image_data_format as image_data_format\n+from keras.src.backend.config import (\n+    is_debug_mode_enabled as is_debug_mode_enabled,\n+)\n from keras.src.backend.config import (\n     is_flash_attention_enabled as is_flash_attention_enabled,\n )",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe import statements from keras.src.backend.config are styled inconsistently. some are on single lines, while others are grouped using parentheses. for better readability and consistency, it's best to group all imports from the same module. since this file is autogenerated, the generation script might need to be updated to enforce a consistent style.",
    "line_number": 24,
    "enriched": "File: keras/api/_tf_keras/keras/config/__init__.py\nCode: @@ -5,15 +5,20 @@\n \"\"\"\n \n from keras.src.backend.config import backend as backend\n+from keras.src.backend.config import disable_debug_mode as disable_debug_mode\n from keras.src.backend.config import (\n     disable_flash_attention as disable_flash_attention,\n )\n+from keras.src.backend.config import enable_debug_mode as enable_debug_mode\n from keras.src.backend.config import (\n     enable_flash_attention as enable_flash_attention,\n )\n from keras.src.backend.config import epsilon as epsilon\n from keras.src.backend.config import floatx as floatx\n from keras.src.backend.config import image_data_format as image_data_format\n+from keras.src.backend.config import (\n+    is_debug_mode_enabled as is_debug_mode_enabled,\n+)\n from keras.src.backend.config import (\n     is_flash_attention_enabled as is_flash_attention_enabled,\n )\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe import statements from `keras.src.backend.config` are styled inconsistently. Some are on single lines, while others are grouped using parentheses. For better readability and consistency, it's best to group all imports from the same module. Since this file is autogenerated, the generation script might need to be updated to enforce a consistent style.\n\n```suggestion\nfrom keras.src.backend.config import (\n    backend as backend,\n    disable_debug_mode as disable_debug_mode,\n    disable_flash_attention as disable_flash_attention,\n    enable_debug_mode as enable_debug_mode,\n    enable_flash_attention as enable_flash_attention,\n    epsilon as epsilon,\n    floatx as floatx,\n    image_data_format as image_data_format,\n    is_debug_mode_enabled as is_debug_mode_enabled,\n    is_flash_attention_enabled as is_flash_attention_enabled,\n)\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/api/_tf_keras/keras/config/__init__.py",
    "pr_number": 21725,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2410447256,
    "comment_created_at": "2025-10-07T12:23:52Z"
  },
  {
    "code": "@@ -97,3 +97,7 @@ def lstsq(a, b, rcond=None):\n     a = convert_to_tensor(a)\n     b = convert_to_tensor(b)\n     return jnp.linalg.lstsq(a, b, rcond=rcond)[0]\n+\n+\n+def jvp(fun, primals, tangents, has_aux=False):\n+    return jax.jvp(fun, primals, tangents, has_aux=has_aux)",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe return format for jvp with has_aux=true is inconsistent with the api contract defined in keras.ops.linalg.jvp and the tensorflow backend implementation. jax.jvp returns ((primals_out, tangents_out), aux), but the keras api expects (primals_out, tangents_out, aux). the implementation should unpack the result from jax.jvp to match the expected signature.",
    "line_number": 103,
    "enriched": "File: keras/src/backend/jax/linalg.py\nCode: @@ -97,3 +97,7 @@ def lstsq(a, b, rcond=None):\n     a = convert_to_tensor(a)\n     b = convert_to_tensor(b)\n     return jnp.linalg.lstsq(a, b, rcond=rcond)[0]\n+\n+\n+def jvp(fun, primals, tangents, has_aux=False):\n+    return jax.jvp(fun, primals, tangents, has_aux=has_aux)\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe return format for `jvp` with `has_aux=True` is inconsistent with the API contract defined in `keras.ops.linalg.jvp` and the TensorFlow backend implementation. `jax.jvp` returns `((primals_out, tangents_out), aux)`, but the Keras API expects `(primals_out, tangents_out, aux)`. The implementation should unpack the result from `jax.jvp` to match the expected signature.\n\n```suggestion\ndef jvp(fun, primals, tangents, has_aux=False):\n    if not has_aux:\n        return jax.jvp(fun, primals, tangents, has_aux=False)\n    (primals_out, tangents_out), aux = jax.jvp(\n        fun, primals, tangents, has_aux=True\n    )\n    return primals_out, tangents_out, aux\n```",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "keras/src/backend/jax/linalg.py",
    "pr_number": 21720,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2404041911,
    "comment_created_at": "2025-10-04T15:20:27Z"
  },
  {
    "code": "@@ -363,6 +363,7 @@ def dot(inputs, axes=-1, **kwargs):\n         inputs: A list of input tensors (at least 2).\n         axes: Integer or tuple of integers,\n             axis or axes along which to take the dot product.\n+            In which `0` should not be included.",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthanks for adding this important clarification. to improve clarity and provide more context to the user, could you please rephrase this line? explaining *why* axis 0 is disallowed (it's the batch axis) would be helpful.",
    "line_number": 366,
    "enriched": "File: keras/src/layers/merging/dot.py\nCode: @@ -363,6 +363,7 @@ def dot(inputs, axes=-1, **kwargs):\n         inputs: A list of input tensors (at least 2).\n         axes: Integer or tuple of integers,\n             axis or axes along which to take the dot product.\n+            In which `0` should not be included.\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThanks for adding this important clarification. To improve clarity and provide more context to the user, could you please rephrase this line? Explaining *why* axis 0 is disallowed (it's the batch axis) would be helpful.\n\n```suggestion\n            Note that axis `0` (the batch axis) cannot be included.\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/layers/merging/dot.py",
    "pr_number": 21718,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2401448661,
    "comment_created_at": "2025-10-03T10:14:04Z"
  },
  {
    "code": "@@ -43,10 +43,12 @@ class Normalization(DataLayer):\n             will be broadcast to the shape of the kept axes above;\n             if the value(s) cannot be broadcast, an error will be raised when\n             this layer's `build()` method is called.\n+            Should be specified when `variance` is specified.\n         variance: The variance value(s) to use during normalization. The passed\n             value(s) will be broadcast to the shape of the kept axes above;\n             if the value(s) cannot be broadcast, an error will be raised when\n             this layer's `build()` method is called.\n+            Should be specified when `mean` is specified.",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthanks for adding this clarification. to be more direct and accurate, since a valueerror is raised if only one of mean or variance is provided, you could phrase this as 'mean and variance must be specified together.' this is also less circular than specifying each in terms of the other.",
    "line_number": 51,
    "enriched": "File: keras/src/layers/preprocessing/normalization.py\nCode: @@ -43,10 +43,12 @@ class Normalization(DataLayer):\n             will be broadcast to the shape of the kept axes above;\n             if the value(s) cannot be broadcast, an error will be raised when\n             this layer's `build()` method is called.\n+            Should be specified when `variance` is specified.\n         variance: The variance value(s) to use during normalization. The passed\n             value(s) will be broadcast to the shape of the kept axes above;\n             if the value(s) cannot be broadcast, an error will be raised when\n             this layer's `build()` method is called.\n+            Should be specified when `mean` is specified.\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThanks for adding this clarification. To be more direct and accurate, since a `ValueError` is raised if only one of `mean` or `variance` is provided, you could phrase this as '`mean` and `variance` must be specified together.' This is also less circular than specifying each in terms of the other.\n\n```suggestion\n            `mean` and `variance` must be specified together.\n        variance: The variance value(s) to use during normalization. The passed\n            value(s) will be broadcast to the shape of the kept axes above;\n            if the value(s) cannot be broadcast, an error will be raised when\n            this layer's `build()` method is called.\n            `mean` and `variance` must be specified together.\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/layers/preprocessing/normalization.py",
    "pr_number": 21716,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2401389610,
    "comment_created_at": "2025-10-03T09:44:45Z"
  },
  {
    "code": "@@ -23,33 +29,228 @@\n IS_THREAD_SAFE = True\n \n \n+class _ProtectedShardedArray:\n+    \"\"\"Wrapper that prevents deletion of sharded JAX arrays.\n+\n+    This wrapper intercepts delete() calls from jax_memory_cleanup\n+    and prevents deletion of sharded arrays that are needed for inference.\n+    \"\"\"\n+\n+    def __init__(self, array):\n+        self._array = array\n+        self._is_sharded = hasattr(array, \"addressable_shards\")\n+\n+    def __getattr__(self, name):\n+        # Delegate all attribute access to the wrapped array\n+        return getattr(self._array, name)\n+\n+    def delete(self):\n+        \"\"\"Intercept delete() calls and prevent deletion of sharded arrays.\"\"\"\n+        if self._is_sharded:\n+            # Don't actually delete sharded arrays\n+            return\n+        else:\n+            # Allow deletion of non-sharded arrays\n+            self._array.delete()\n+\n+    def __repr__(self):\n+        return f\"_ProtectedShardedArray({self._array})\"\n+\n+\n class JaxVariable(KerasVariable):\n     def __init__(self, *args, layout=None, **kwargs):\n         # Intercept layout parameter so that it is available\n         # during initialization.\n         self._layout = layout\n         super().__init__(*args, **kwargs)\n \n+    def _maybe_create_strong_reference(self, value):\n+        \"\"\"Create a strong ref to a JAX array to prevent GC.\"\"\"\n+        if isinstance(value, jax.Array):\n+            if hasattr(value, \"addressable_shards\"):\n+                # For sharded arrays, hold references to the shards' data.\n+                shard_data = [shard.data for shard in value.addressable_shards]\n+                if not hasattr(self, \"_shard_references\"):\n+                    self._shard_references = []\n+                self._shard_references.append(shard_data)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe current implementation of _maybe_create_strong_reference appends to self._shard_references every time variable.value is accessed. this will cause a memory leak, as the _shard_references list will grow indefinitely during inference or training. the intention is to hold a reference to prevent garbage collection, not to accumulate references on every access.\n\ni suggest replacing the append logic with a direct assignment. this ensures that only the reference to the current sharded array data is held, preventing the memory leak.\n\npython\n                self._shard_references = [shard_data]",
    "line_number": 69,
    "enriched": "File: keras/src/backend/jax/core.py\nCode: @@ -23,33 +29,228 @@\n IS_THREAD_SAFE = True\n \n \n+class _ProtectedShardedArray:\n+    \"\"\"Wrapper that prevents deletion of sharded JAX arrays.\n+\n+    This wrapper intercepts delete() calls from jax_memory_cleanup\n+    and prevents deletion of sharded arrays that are needed for inference.\n+    \"\"\"\n+\n+    def __init__(self, array):\n+        self._array = array\n+        self._is_sharded = hasattr(array, \"addressable_shards\")\n+\n+    def __getattr__(self, name):\n+        # Delegate all attribute access to the wrapped array\n+        return getattr(self._array, name)\n+\n+    def delete(self):\n+        \"\"\"Intercept delete() calls and prevent deletion of sharded arrays.\"\"\"\n+        if self._is_sharded:\n+            # Don't actually delete sharded arrays\n+            return\n+        else:\n+            # Allow deletion of non-sharded arrays\n+            self._array.delete()\n+\n+    def __repr__(self):\n+        return f\"_ProtectedShardedArray({self._array})\"\n+\n+\n class JaxVariable(KerasVariable):\n     def __init__(self, *args, layout=None, **kwargs):\n         # Intercept layout parameter so that it is available\n         # during initialization.\n         self._layout = layout\n         super().__init__(*args, **kwargs)\n \n+    def _maybe_create_strong_reference(self, value):\n+        \"\"\"Create a strong ref to a JAX array to prevent GC.\"\"\"\n+        if isinstance(value, jax.Array):\n+            if hasattr(value, \"addressable_shards\"):\n+                # For sharded arrays, hold references to the shards' data.\n+                shard_data = [shard.data for shard in value.addressable_shards]\n+                if not hasattr(self, \"_shard_references\"):\n+                    self._shard_references = []\n+                self._shard_references.append(shard_data)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe current implementation of `_maybe_create_strong_reference` appends to `self._shard_references` every time `variable.value` is accessed. This will cause a memory leak, as the `_shard_references` list will grow indefinitely during inference or training. The intention is to hold a reference to prevent garbage collection, not to accumulate references on every access.\n\nI suggest replacing the append logic with a direct assignment. This ensures that only the reference to the current sharded array data is held, preventing the memory leak.\n\n```python\n                self._shard_references = [shard_data]\n```",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "keras/src/backend/jax/core.py",
    "pr_number": 21712,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2401167091,
    "comment_created_at": "2025-10-03T08:16:52Z"
  },
  {
    "code": "@@ -136,7 +136,9 @@ def increment():\n                 g\n                 if g is None or self._overwrite_variable_with_gradient(v)\n                 else ops.divide(g, scale)\n-                for g, v in zip(grads, trainable_variables)\n+                for g, v in zip(\n+                    grads, self.inner_optimizer._trainable_variables\n+                )",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis change accesses a \"private\" attribute _trainable_variables of the inner optimizer. this relies on an internal implementation detail of the optimizer class, which could change in the future and break this code. it also violates the principle of encapsulation.[^1]\n\na more robust solution would be to expose the trainable variables via a public property on the baseoptimizer class. since that would require changes in another file, i'll leave it to you to decide on the best path forward.\n\n#### style guide references\n[^1]: apis should not deal with internal implementation details. an api should be about the problem it solves, not about how the code works in the background.",
    "line_number": 141,
    "enriched": "File: keras/src/optimizers/loss_scale_optimizer.py\nCode: @@ -136,7 +136,9 @@ def increment():\n                 g\n                 if g is None or self._overwrite_variable_with_gradient(v)\n                 else ops.divide(g, scale)\n-                for g, v in zip(grads, trainable_variables)\n+                for g, v in zip(\n+                    grads, self.inner_optimizer._trainable_variables\n+                )\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis change accesses a \"private\" attribute `_trainable_variables` of the inner optimizer. This relies on an internal implementation detail of the `Optimizer` class, which could change in the future and break this code. It also violates the principle of encapsulation.[^1]\n\nA more robust solution would be to expose the trainable variables via a public property on the `BaseOptimizer` class. Since that would require changes in another file, I'll leave it to you to decide on the best path forward.\n\n#### Style Guide References\n[^1]: APIs should not deal with internal implementation details. An API should be about the problem it solves, not about how the code works in the background.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/optimizers/loss_scale_optimizer.py",
    "pr_number": 21706,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2395417555,
    "comment_created_at": "2025-10-01T18:03:41Z"
  },
  {
    "code": "@@ -0,0 +1,611 @@\n+import re\n+from typing import Any\n+from typing import Dict\n+from typing import List\n+from typing import Optional\n+from typing import Tuple\n+\n+import numpy as np\n+\n+import keras\n+from keras.src import ops\n+from keras.src import optimizers\n+from keras.src.backend.distributed import backend_resolver\n+\n+\n+class CoordinatedOptimizer:\n+    \"\"\"Manages an optimizer's state for distributed training.\n+\n+    This class is an internal coordinator that handles the complexities of\n+    sharding optimizer states across multiple devices (shards) and\n+    synchronizing gradients according to tensor parallelism rules. It is not\n+    intended to be used directly by the end-user but is a core component of\n+    the `TensorParallelOptimizer`.\n+\n+    Args:\n+        base_optimizer: The Keras optimizer instance\n+            (e.g., `keras.optimizers.Adam`) whose state will be managed.\n+        world_size: The total number of devices/processes in the distributed\n+            setup.\n+        distributed_backend: The distributed communication backend to use.\n+            Defaults to \"auto\".\n+        rank: The rank of the current process. Defaults to 0.\n+        shard_optimizer_states: If `True`, the optimizer's state variables\n+            (e.g., momentum, velocity) will be partitioned across `world_size`\n+            devices. Defaults to `True`.\n+        tensor_parallel_config: An optional configuration object that defines\n+            rules for tensor parallelism, such as which gradients to\n+            all-reduce. Defaults to `None`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        base_optimizer: optimizers.Optimizer,\n+        world_size: int,\n+        distributed_backend: str = \"auto\",\n+        rank: int = 0,\n+        shard_optimizer_states: bool = True,\n+        tensor_parallel_config=None,\n+    ):\n+        self.base_optimizer = base_optimizer\n+        self.world_size = world_size\n+        self.rank = rank\n+        self.shard_optimizer_states = shard_optimizer_states\n+        self.tensor_parallel_config = tensor_parallel_config\n+        self.sharded_states = {}\n+\n+        self.distributed_backend = backend_resolver.get_distributed_backend(\n+            distributed_backend\n+        )\n+\n+        if self.shard_optimizer_states:\n+            if getattr(self.base_optimizer, \"built\", False):\n+                self._initialize_sharded_states()\n+            else:\n+                self.shard_optimizer_states = False\n+\n+    def _parse_variable_name(\n+        self, var_name: str\n+    ) -> Tuple[Optional[str], Optional[str]]:\n+        \"\"\"Parses an optimizer variable name to find its type and parameter.\n+\n+        For ex, it maps 'dense/kernel/_momentum' to\n+        ('momentum', 'dense/kernel').\n+\n+        Args:\n+            var_name: The name of the optimizer state variable.\n+\n+        Returns:\n+            A tuple containing the state type (e.g., 'momentum') and the\n+            associated parameter name. Returns (None, None) if no known\n+            state suffix is found.\n+        \"\"\"\n+        var_name_lower = var_name.lower()\n+        state_map = {\n+            \"_momentum\": \"momentum\",\n+            \"_velocity\": \"velocity\",\n+            \"_m\": \"m\",\n+            \"_v\": \"v\",\n+        }\n+        for suffix, state_name in state_map.items():\n+            if var_name_lower.endswith(suffix):\n+                param_name = var_name[: -len(suffix)]\n+                return state_name, param_name\n+        return None, None\n+\n+    def _get_actual_optimizer_state(self) -> Dict[str, Any]:\n+        \"\"\"Extracts and structures the optimizer's state variables.\n+\n+        This method inspects the `variables` of the `base_optimizer` and\n+        organizes them into a nested dictionary structure based on their type\n+        (e.g., learning rate, iteration count, momentum).\n+\n+        Returns:\n+            A dictionary containing the optimizer's state, organized by\n+            state type. For example:\n+            {'t': <tf.Variable>, 'lr': <tf.Variable>,\n+             'momentum': {'param1': <tf.Variable>}}.\n+        \"\"\"\n+        state_dict = {}\n+        for var in self.base_optimizer.variables:\n+            identifier = getattr(var, \"path\", getattr(var, \"name\", str(var)))\n+            parts = identifier.split(\"/\")\n+            tail = parts[-1]\n+            tail_lower = tail.lower()\n+\n+            if \"iteration\" in tail_lower or tail_lower in {\"iter\", \"t\"}:\n+                state_dict[\"t\"] = var\n+                continue\n+            if \"learning_rate\" in tail_lower or tail_lower in {\"lr\"}:\n+                state_dict[\"lr\"] = var\n+                continue\n+\n+            state_name, param_name = self._parse_variable_name(tail)\n+            if not state_name:\n+                state_name = \"state\"\n+                param_name = tail\n+\n+            if state_name not in state_dict:\n+                state_dict[state_name] = {}\n+            state_dict[state_name][param_name] = var\n+        return state_dict\n+\n+    def _initialize_sharded_states(self):\n+        \"\"\"Partitions the optimizer's state variables across shards.\"\"\"\n+        if not self.shard_optimizer_states or not getattr(\n+            self.base_optimizer, \"built\", False\n+        ):\n+            return\n+\n+        base_state = self._get_actual_optimizer_state()\n+        if not base_state:\n+            self.shard_optimizer_states = False\n+            return\n+\n+        self.sharded_states = {}\n+        for state_name, state_value in base_state.items():\n+            if isinstance(state_value, dict):\n+                self.sharded_states[state_name] = {}\n+                for param_name, param_state_var in state_value.items():\n+                    sharding_dim = 0\n+                    if self.tensor_parallel_config:\n+                        norm_param = param_name.replace(\"/\", \".\")\n+                        for (\n+                            p,\n+                            a,\n+                        ) in self.tensor_parallel_config.state_rules.items():\n+                            if re.search(p, norm_param) and hasattr(a, \"dim\"):\n+                                sharding_dim = a.dim\n+                                break\n+                    self.sharded_states[state_name][param_name] = (\n+                        self._partition_state(param_state_var, dim=sharding_dim)\n+                    )\n+            else:\n+                self.sharded_states[state_name] = self._partition_state(\n+                    state_value, dim=0\n+                )\n+\n+    def _partition_state(\n+        self, state_variable: any, dim: int\n+    ) -> List[np.ndarray]:\n+        \"\"\"Splits a single state variable numpy array into chunks.\n+\n+        If the variable cannot be split along the given dimension, it is\n+        replicated across all shards.\n+\n+        Args:\n+            state_variable: The optimizer state variable.\n+            dim: The dimension along which to partition the variable.\n+\n+        Returns:\n+            A list of NumPy arrays, where each array is a partition of the\n+            original state variable for a specific shard.\n+        \"\"\"\n+        state_array = keras.ops.convert_to_numpy(state_variable)\n+        if state_array.ndim > dim and state_array.shape[dim] >= self.world_size:\n+            return np.array_split(state_array, self.world_size, axis=dim)\n+        else:\n+            return [np.copy(state_array) for _ in range(self.world_size)]\n+\n+    def get_config(self) -> Dict[str, Any]:\n+        return {\n+            \"base_optimizer\": self.base_optimizer.get_config(),\n+            \"world_size\": self.world_size,\n+            \"shard_optimizer_states\": self.shard_optimizer_states,\n+        }\n+\n+    def apply_gradients(\n+        self, gradients_and_vars: List[List[tuple]], shard_models: List\n+    ):\n+        \"\"\"Coordinates gradient synchronization and application.\n+\n+        This method first synchronizes gradients across all shards based on\n+        tensor parallelism rules. Then, it applies the gradients using either\n+        sharded optimizer states or replicated states.\n+\n+        Args:\n+            gradients_and_vars: A list of lists, where each inner list contains\n+                (gradient, variable) tuples for a specific model shard.\n+            shard_models: A list of the sharded model instances.\n+\n+        Raises:\n+            ValueError: If the number of gradient sets does not match the\n+                world size.\n+        \"\"\"\n+        if len(gradients_and_vars) != self.world_size:\n+            error_msg = (\n+                f\"Expected {self.world_size} gradient sets, \"\n+                f\"got {len(gradients_and_vars)}\"\n+            )\n+            raise ValueError(error_msg)\n+\n+        synchronized_gradients = self._synchronize_gradients(gradients_and_vars)\n+\n+        if self.shard_optimizer_states and self.sharded_states:\n+            self._apply_gradients_with_sharded_states(\n+                synchronized_gradients, shard_models\n+            )\n+        else:\n+            self._apply_gradients_with_replicated_states(\n+                synchronized_gradients, shard_models\n+            )\n+\n+    def _apply_gradients_with_sharded_states(\n+        self, synchronized_gradients: List[List[tuple]], shard_models: List\n+    ):\n+        \"\"\"Applies gradients to each shard using its local optimizer state.\n+\n+        For each shard, this method loads the corresponding partition of the\n+        optimizer state into the base optimizer and then applies the shard's\n+        gradients.\n+\n+        Args:\n+            synchronized_gradients: The gradients after synchronization.\n+            shard_models: The list of sharded models.\n+        \"\"\"\n+        for shard_idx, shard_grads in enumerate(synchronized_gradients):\n+            local_states = self._get_local_optimizer_states(shard_idx)\n+            self._update_optimizer_internal_state(\n+                self.base_optimizer, local_states\n+            )\n+            self.base_optimizer.apply_gradients(shard_grads)\n+\n+    def _apply_gradients_with_replicated_states(\n+        self, synchronized_gradients: List[List[tuple]], shard_models: List\n+    ):\n+        \"\"\"Averages gradients across all shards and applies them once.\n+\n+        This method is used when optimizer state sharding is disabled. It\n+        calculates the average of the gradients for each variable across all\n+        shards and applies the averaged gradients using the single, replicated\n+        optimizer state.\n+\n+        Args:\n+            synchronized_gradients: The gradients after synchronization.\n+            shard_models: The list of sharded models.\n+        \"\"\"\n+        num_vars = len(synchronized_gradients[0])\n+        averaged_grads_and_vars = []\n+\n+        for i in range(num_vars):\n+            variable = synchronized_gradients[0][i][1]\n+            grads_for_var = [\n+                shard_grads[i][0]\n+                for shard_grads in synchronized_gradients\n+                if shard_grads[i][0] is not None\n+            ]\n+\n+            if not grads_for_var:\n+                continue\n+\n+            summed_grad = grads_for_var[0]\n+            for grad in grads_for_var[1:]:\n+                summed_grad += grad\n+            averaged_grad = summed_grad / len(grads_for_var)\n+            averaged_grads_and_vars.append((averaged_grad, variable))\n+\n+        if averaged_grads_and_vars:\n+            self.base_optimizer.apply_gradients(averaged_grads_and_vars)\n+\n+    def _get_local_optimizer_states(self, shard_idx: int) -> Dict[str, Any]:\n+        \"\"\"Constructs the state dictionary for a single shard.\n+\n+        Args:\n+            shard_idx: The index of the shard for which to retrieve the state.\n+\n+        Returns:\n+            A dictionary containing the optimizer state variables specific to\n+            the given shard index.\n+        \"\"\"\n+        local_states = {}\n+        for state_name, state_value in self.sharded_states.items():\n+            if isinstance(state_value, dict):\n+                local_states[state_name] = {}\n+                for param_name, param_states in state_value.items():\n+                    local_states[state_name][param_name] = param_states[\n+                        shard_idx\n+                    ]\n+            else:\n+                local_states[state_name] = state_value[shard_idx]\n+        return local_states\n+\n+    def _update_optimizer_internal_state(self, optimizer, local_states: dict):\n+        \"\"\"Assigns local sharded state values to the optimizer's variables.\n+\n+        This method updates the `base_optimizer`'s internal state variables\n+        in-place with the values from a specific shard's state partition.\n+\n+        Args:\n+            optimizer: The Keras optimizer instance to update.\n+            local_states: A dictionary of state values for a single shard.\n+        \"\"\"\n+        if not hasattr(optimizer, \"variables\") or not optimizer.variables:\n+            return\n+\n+        for var in optimizer.variables:\n+            identifier = getattr(var, \"path\", getattr(var, \"name\", str(var)))\n+            parts = identifier.split(\"/\")\n+            tail = parts[-1]\n+            tail_lower = tail.lower()\n+\n+            if \"iteration\" in tail_lower or tail_lower in {\"iter\", \"t\"}:\n+                if \"t\" in local_states:\n+                    var.assign(local_states[\"t\"])\n+                continue\n+            if \"learning_rate\" in tail_lower or tail_lower in {\"lr\"}:\n+                if \"lr\" in local_states:\n+                    var.assign(local_states[\"lr\"])\n+                continue\n+\n+            state_name, param_name_in_opt = self._parse_variable_name(tail)\n+            if (\n+                state_name in local_states\n+                and param_name_in_opt in local_states[state_name]\n+            ):\n+                local_param_state = local_states[state_name][param_name_in_opt]\n+                if var.shape == local_param_state.shape:\n+                    var.assign(local_param_state)\n+\n+    def _synchronize_gradients(\n+        self, gradients_and_vars: List[List[tuple]]\n+    ) -> List[List[tuple]]:\n+        \"\"\"Synchronizes gradients across shards based on tensor parallel rules.\n+\n+        Specifically, it performs an all-reduce operation on gradients of\n+        weights that are split along a \"column\" dimension in tensor parallelism.\n+        Other gradients are passed through unchanged.\n+\n+        Args:\n+            gradients_and_vars: The list of (gradient, variable) lists from\n+                all shards.\n+\n+        Returns:\n+            The list of (gradient, variable) lists after synchronization.\n+        \"\"\"\n+        if not self.tensor_parallel_config:\n+            return gradients_and_vars\n+\n+        rules = self.tensor_parallel_config.state_rules.items()\n+        column_parallel_patterns = {\n+            pattern\n+            for pattern, action in rules\n+            if hasattr(action, \"sharding_type\")\n+            and action.sharding_type == \"column\"\n+        }\n+\n+        if not column_parallel_patterns:\n+            return gradients_and_vars\n+\n+        num_weights = len(gradients_and_vars[0])\n+        for i in range(num_weights):\n+            variable = gradients_and_vars[0][i][1]\n+            var_name = getattr(variable, \"path\", getattr(variable, \"name\", \"\"))\n+\n+            if any(\n+                re.search(pattern, var_name)\n+                for pattern in column_parallel_patterns\n+            ):\n+                grads_to_reduce = [\n+                    g_and_v[i][0]\n+                    for g_and_v in gradients_and_vars\n+                    if g_and_v[i][0] is not None\n+                ]\n+                if grads_to_reduce:\n+                    synced_grad = self._allreduce_gradients(grads_to_reduce)[0]\n+                    for shard_idx in range(self.world_size):\n+                        gradients_and_vars[shard_idx][i] = (\n+                            synced_grad,\n+                            variable,\n+                        )\n+        return gradients_and_vars\n+\n+    def _allreduce_gradients(self, gradients: List[Any]) -> List[Any]:\n+        \"\"\"Performs a mean all-reduce operation on a list of gradients.\n+\n+        If a distributed backend is available, it uses it. Otherwise, it\n+        falls back to a local mean calculation.\n+\n+        Args:\n+            gradients: A list of gradients (one from each shard) to be averaged.\n+\n+        Returns:\n+            A list where each element is the mean of the input gradients.\n+        \"\"\"\n+        if not gradients:\n+            return []\n+\n+        if (\n+            self.distributed_backend is not None\n+            and self.distributed_backend.is_initialized\n+        ):\n+            numpy_grad = keras.ops.convert_to_numpy(gradients[0])\n+            synced_numpy = self.distributed_backend.allreduce(\n+                numpy_grad, op=\"mean\"\n+            )\n+            synced_tensor = keras.ops.convert_to_tensor(synced_numpy)\n+            return [synced_tensor for _ in range(self.world_size)]\n+\n+        stacked_grads = keras.ops.stack(\n+            [keras.ops.convert_to_tensor(g) for g in gradients], axis=0\n+        )\n+        mean_grad = keras.ops.mean(stacked_grads, axis=0)\n+        return [mean_grad for _ in range(len(gradients))]\n+\n+    def get_weights(self) -> List[np.ndarray]:\n+        \"\"\"Returns the weights of the base optimizer.\"\"\"\n+        return self.base_optimizer.get_weights()\n+\n+    def set_weights(self, weights: List[np.ndarray]):\n+        \"\"\"Sets the weights of the base optimizer.\"\"\"\n+        self.base_optimizer.set_weights(weights)\n+\n+    def enable_optimizer_state_sharding(self):\n+        \"\"\"Enables and initializes optimizer state sharding.\n+\n+        If sharding is not already active, this method sets the flag and\n+        triggers the partitioning of the optimizer's states.\n+        \"\"\"\n+        if not self.shard_optimizer_states:\n+            self.shard_optimizer_states = True\n+            self._initialize_sharded_states()\n+\n+    def disable_optimizer_state_sharding(self):\n+        \"\"\"Disables sharding and clears any sharded states.\n+\n+        This reverts the optimizer to using a single, replicated state.\n+        \"\"\"\n+        if self.shard_optimizer_states:\n+            self.shard_optimizer_states = False\n+            self.sharded_states = {}\n+\n+\n+class TensorParallelOptimizer(optimizers.Optimizer):\n+    \"\"\"A Keras Optimizer wrapper for tensor-parallel distributed training.\n+\n+    This optimizer wraps a standard Keras optimizer (e.g., Adam, SGD) and\n+    delegates the complex tasks of state management and gradient synchronization\n+    to a `CoordinatedOptimizer` instance. It is designed to work with models\n+    that have been sharded for tensor parallelism.\n+\n+    When `apply_gradients` is called with a list of gradient lists (one for each\n+    model shard), it uses the `CoordinatedOptimizer` to handle synchronization\n+    and state sharding. Otherwise, it behaves like the base optimizer.\n+\n+    Args:\n+        base_optimizer: A Keras optimizer instance or a string identifier\n+            (e.g., 'adam', 'sgd').\n+        world_size: The total number of devices/processes in the distributed\n+            setup.\n+        distributed_backend: The distributed communication backend to use.\n+            Defaults to \"auto\".\n+        tensor_parallel_config: An optional configuration object that defines\n+            rules for tensor parallelism. Defaults to `None`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        base_optimizer: optimizers.Optimizer,\n+        world_size: int,\n+        distributed_backend: str = \"auto\",\n+        tensor_parallel_config=None,\n+    ):\n+        if isinstance(base_optimizer, str):\n+            opt_lower = base_optimizer.lower()\n+            if opt_lower == \"adam\":\n+                resolved_base_optimizer = optimizers.Adam()\n+            elif opt_lower == \"sgd\":\n+                resolved_base_optimizer = optimizers.SGD()\n+            elif opt_lower == \"rmsprop\":\n+                resolved_base_optimizer = optimizers.RMSprop()\n+            else:\n+                raise ValueError(f\"Unknown optimizer string: {base_optimizer}\")\n+        else:\n+            resolved_base_optimizer = base_optimizer\n+\n+        lr_value = float(\n+            ops.convert_to_numpy(resolved_base_optimizer.learning_rate)\n+        )",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe initialization of lr_value by casting resolved_base_optimizer.learning_rate to a float will fail if a keras.optimizers.schedules.learningrateschedule is used, as it cannot be converted to a single float. this is a critical bug that prevents the use of learning rate schedules with tensorparalleloptimizer. the learning_rate property of this class already delegates to the base optimizer, so the learning_rate argument to super().__init__ should be handled differently to support schedules.\n\npython\n        if isinstance(\n            resolved_base_optimizer.learning_rate,\n            keras.optimizers.schedules.learningrateschedule,\n        ):\n            lr_value = float(\n                ops.convert_to_numpy(\n                    resolved_base_optimizer.learning_rate.initial_learning_rate\n                )\n            )\n        else:\n            lr_value = float(\n                ops.convert_to_numpy(resolved_base_optimizer.learning_rate)\n            )",
    "line_number": 507,
    "enriched": "File: keras/src/distribution/tensor_parallel/coordinated_optimizer.py\nCode: @@ -0,0 +1,611 @@\n+import re\n+from typing import Any\n+from typing import Dict\n+from typing import List\n+from typing import Optional\n+from typing import Tuple\n+\n+import numpy as np\n+\n+import keras\n+from keras.src import ops\n+from keras.src import optimizers\n+from keras.src.backend.distributed import backend_resolver\n+\n+\n+class CoordinatedOptimizer:\n+    \"\"\"Manages an optimizer's state for distributed training.\n+\n+    This class is an internal coordinator that handles the complexities of\n+    sharding optimizer states across multiple devices (shards) and\n+    synchronizing gradients according to tensor parallelism rules. It is not\n+    intended to be used directly by the end-user but is a core component of\n+    the `TensorParallelOptimizer`.\n+\n+    Args:\n+        base_optimizer: The Keras optimizer instance\n+            (e.g., `keras.optimizers.Adam`) whose state will be managed.\n+        world_size: The total number of devices/processes in the distributed\n+            setup.\n+        distributed_backend: The distributed communication backend to use.\n+            Defaults to \"auto\".\n+        rank: The rank of the current process. Defaults to 0.\n+        shard_optimizer_states: If `True`, the optimizer's state variables\n+            (e.g., momentum, velocity) will be partitioned across `world_size`\n+            devices. Defaults to `True`.\n+        tensor_parallel_config: An optional configuration object that defines\n+            rules for tensor parallelism, such as which gradients to\n+            all-reduce. Defaults to `None`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        base_optimizer: optimizers.Optimizer,\n+        world_size: int,\n+        distributed_backend: str = \"auto\",\n+        rank: int = 0,\n+        shard_optimizer_states: bool = True,\n+        tensor_parallel_config=None,\n+    ):\n+        self.base_optimizer = base_optimizer\n+        self.world_size = world_size\n+        self.rank = rank\n+        self.shard_optimizer_states = shard_optimizer_states\n+        self.tensor_parallel_config = tensor_parallel_config\n+        self.sharded_states = {}\n+\n+        self.distributed_backend = backend_resolver.get_distributed_backend(\n+            distributed_backend\n+        )\n+\n+        if self.shard_optimizer_states:\n+            if getattr(self.base_optimizer, \"built\", False):\n+                self._initialize_sharded_states()\n+            else:\n+                self.shard_optimizer_states = False\n+\n+    def _parse_variable_name(\n+        self, var_name: str\n+    ) -> Tuple[Optional[str], Optional[str]]:\n+        \"\"\"Parses an optimizer variable name to find its type and parameter.\n+\n+        For ex, it maps 'dense/kernel/_momentum' to\n+        ('momentum', 'dense/kernel').\n+\n+        Args:\n+            var_name: The name of the optimizer state variable.\n+\n+        Returns:\n+            A tuple containing the state type (e.g., 'momentum') and the\n+            associated parameter name. Returns (None, None) if no known\n+            state suffix is found.\n+        \"\"\"\n+        var_name_lower = var_name.lower()\n+        state_map = {\n+            \"_momentum\": \"momentum\",\n+            \"_velocity\": \"velocity\",\n+            \"_m\": \"m\",\n+            \"_v\": \"v\",\n+        }\n+        for suffix, state_name in state_map.items():\n+            if var_name_lower.endswith(suffix):\n+                param_name = var_name[: -len(suffix)]\n+                return state_name, param_name\n+        return None, None\n+\n+    def _get_actual_optimizer_state(self) -> Dict[str, Any]:\n+        \"\"\"Extracts and structures the optimizer's state variables.\n+\n+        This method inspects the `variables` of the `base_optimizer` and\n+        organizes them into a nested dictionary structure based on their type\n+        (e.g., learning rate, iteration count, momentum).\n+\n+        Returns:\n+            A dictionary containing the optimizer's state, organized by\n+            state type. For example:\n+            {'t': <tf.Variable>, 'lr': <tf.Variable>,\n+             'momentum': {'param1': <tf.Variable>}}.\n+        \"\"\"\n+        state_dict = {}\n+        for var in self.base_optimizer.variables:\n+            identifier = getattr(var, \"path\", getattr(var, \"name\", str(var)))\n+            parts = identifier.split(\"/\")\n+            tail = parts[-1]\n+            tail_lower = tail.lower()\n+\n+            if \"iteration\" in tail_lower or tail_lower in {\"iter\", \"t\"}:\n+                state_dict[\"t\"] = var\n+                continue\n+            if \"learning_rate\" in tail_lower or tail_lower in {\"lr\"}:\n+                state_dict[\"lr\"] = var\n+                continue\n+\n+            state_name, param_name = self._parse_variable_name(tail)\n+            if not state_name:\n+                state_name = \"state\"\n+                param_name = tail\n+\n+            if state_name not in state_dict:\n+                state_dict[state_name] = {}\n+            state_dict[state_name][param_name] = var\n+        return state_dict\n+\n+    def _initialize_sharded_states(self):\n+        \"\"\"Partitions the optimizer's state variables across shards.\"\"\"\n+        if not self.shard_optimizer_states or not getattr(\n+            self.base_optimizer, \"built\", False\n+        ):\n+            return\n+\n+        base_state = self._get_actual_optimizer_state()\n+        if not base_state:\n+            self.shard_optimizer_states = False\n+            return\n+\n+        self.sharded_states = {}\n+        for state_name, state_value in base_state.items():\n+            if isinstance(state_value, dict):\n+                self.sharded_states[state_name] = {}\n+                for param_name, param_state_var in state_value.items():\n+                    sharding_dim = 0\n+                    if self.tensor_parallel_config:\n+                        norm_param = param_name.replace(\"/\", \".\")\n+                        for (\n+                            p,\n+                            a,\n+                        ) in self.tensor_parallel_config.state_rules.items():\n+                            if re.search(p, norm_param) and hasattr(a, \"dim\"):\n+                                sharding_dim = a.dim\n+                                break\n+                    self.sharded_states[state_name][param_name] = (\n+                        self._partition_state(param_state_var, dim=sharding_dim)\n+                    )\n+            else:\n+                self.sharded_states[state_name] = self._partition_state(\n+                    state_value, dim=0\n+                )\n+\n+    def _partition_state(\n+        self, state_variable: any, dim: int\n+    ) -> List[np.ndarray]:\n+        \"\"\"Splits a single state variable numpy array into chunks.\n+\n+        If the variable cannot be split along the given dimension, it is\n+        replicated across all shards.\n+\n+        Args:\n+            state_variable: The optimizer state variable.\n+            dim: The dimension along which to partition the variable.\n+\n+        Returns:\n+            A list of NumPy arrays, where each array is a partition of the\n+            original state variable for a specific shard.\n+        \"\"\"\n+        state_array = keras.ops.convert_to_numpy(state_variable)\n+        if state_array.ndim > dim and state_array.shape[dim] >= self.world_size:\n+            return np.array_split(state_array, self.world_size, axis=dim)\n+        else:\n+            return [np.copy(state_array) for _ in range(self.world_size)]\n+\n+    def get_config(self) -> Dict[str, Any]:\n+        return {\n+            \"base_optimizer\": self.base_optimizer.get_config(),\n+            \"world_size\": self.world_size,\n+            \"shard_optimizer_states\": self.shard_optimizer_states,\n+        }\n+\n+    def apply_gradients(\n+        self, gradients_and_vars: List[List[tuple]], shard_models: List\n+    ):\n+        \"\"\"Coordinates gradient synchronization and application.\n+\n+        This method first synchronizes gradients across all shards based on\n+        tensor parallelism rules. Then, it applies the gradients using either\n+        sharded optimizer states or replicated states.\n+\n+        Args:\n+            gradients_and_vars: A list of lists, where each inner list contains\n+                (gradient, variable) tuples for a specific model shard.\n+            shard_models: A list of the sharded model instances.\n+\n+        Raises:\n+            ValueError: If the number of gradient sets does not match the\n+                world size.\n+        \"\"\"\n+        if len(gradients_and_vars) != self.world_size:\n+            error_msg = (\n+                f\"Expected {self.world_size} gradient sets, \"\n+                f\"got {len(gradients_and_vars)}\"\n+            )\n+            raise ValueError(error_msg)\n+\n+        synchronized_gradients = self._synchronize_gradients(gradients_and_vars)\n+\n+        if self.shard_optimizer_states and self.sharded_states:\n+            self._apply_gradients_with_sharded_states(\n+                synchronized_gradients, shard_models\n+            )\n+        else:\n+            self._apply_gradients_with_replicated_states(\n+                synchronized_gradients, shard_models\n+            )\n+\n+    def _apply_gradients_with_sharded_states(\n+        self, synchronized_gradients: List[List[tuple]], shard_models: List\n+    ):\n+        \"\"\"Applies gradients to each shard using its local optimizer state.\n+\n+        For each shard, this method loads the corresponding partition of the\n+        optimizer state into the base optimizer and then applies the shard's\n+        gradients.\n+\n+        Args:\n+            synchronized_gradients: The gradients after synchronization.\n+            shard_models: The list of sharded models.\n+        \"\"\"\n+        for shard_idx, shard_grads in enumerate(synchronized_gradients):\n+            local_states = self._get_local_optimizer_states(shard_idx)\n+            self._update_optimizer_internal_state(\n+                self.base_optimizer, local_states\n+            )\n+            self.base_optimizer.apply_gradients(shard_grads)\n+\n+    def _apply_gradients_with_replicated_states(\n+        self, synchronized_gradients: List[List[tuple]], shard_models: List\n+    ):\n+        \"\"\"Averages gradients across all shards and applies them once.\n+\n+        This method is used when optimizer state sharding is disabled. It\n+        calculates the average of the gradients for each variable across all\n+        shards and applies the averaged gradients using the single, replicated\n+        optimizer state.\n+\n+        Args:\n+            synchronized_gradients: The gradients after synchronization.\n+            shard_models: The list of sharded models.\n+        \"\"\"\n+        num_vars = len(synchronized_gradients[0])\n+        averaged_grads_and_vars = []\n+\n+        for i in range(num_vars):\n+            variable = synchronized_gradients[0][i][1]\n+            grads_for_var = [\n+                shard_grads[i][0]\n+                for shard_grads in synchronized_gradients\n+                if shard_grads[i][0] is not None\n+            ]\n+\n+            if not grads_for_var:\n+                continue\n+\n+            summed_grad = grads_for_var[0]\n+            for grad in grads_for_var[1:]:\n+                summed_grad += grad\n+            averaged_grad = summed_grad / len(grads_for_var)\n+            averaged_grads_and_vars.append((averaged_grad, variable))\n+\n+        if averaged_grads_and_vars:\n+            self.base_optimizer.apply_gradients(averaged_grads_and_vars)\n+\n+    def _get_local_optimizer_states(self, shard_idx: int) -> Dict[str, Any]:\n+        \"\"\"Constructs the state dictionary for a single shard.\n+\n+        Args:\n+            shard_idx: The index of the shard for which to retrieve the state.\n+\n+        Returns:\n+            A dictionary containing the optimizer state variables specific to\n+            the given shard index.\n+        \"\"\"\n+        local_states = {}\n+        for state_name, state_value in self.sharded_states.items():\n+            if isinstance(state_value, dict):\n+                local_states[state_name] = {}\n+                for param_name, param_states in state_value.items():\n+                    local_states[state_name][param_name] = param_states[\n+                        shard_idx\n+                    ]\n+            else:\n+                local_states[state_name] = state_value[shard_idx]\n+        return local_states\n+\n+    def _update_optimizer_internal_state(self, optimizer, local_states: dict):\n+        \"\"\"Assigns local sharded state values to the optimizer's variables.\n+\n+        This method updates the `base_optimizer`'s internal state variables\n+        in-place with the values from a specific shard's state partition.\n+\n+        Args:\n+            optimizer: The Keras optimizer instance to update.\n+            local_states: A dictionary of state values for a single shard.\n+        \"\"\"\n+        if not hasattr(optimizer, \"variables\") or not optimizer.variables:\n+            return\n+\n+        for var in optimizer.variables:\n+            identifier = getattr(var, \"path\", getattr(var, \"name\", str(var)))\n+            parts = identifier.split(\"/\")\n+            tail = parts[-1]\n+            tail_lower = tail.lower()\n+\n+            if \"iteration\" in tail_lower or tail_lower in {\"iter\", \"t\"}:\n+                if \"t\" in local_states:\n+                    var.assign(local_states[\"t\"])\n+                continue\n+            if \"learning_rate\" in tail_lower or tail_lower in {\"lr\"}:\n+                if \"lr\" in local_states:\n+                    var.assign(local_states[\"lr\"])\n+                continue\n+\n+            state_name, param_name_in_opt = self._parse_variable_name(tail)\n+            if (\n+                state_name in local_states\n+                and param_name_in_opt in local_states[state_name]\n+            ):\n+                local_param_state = local_states[state_name][param_name_in_opt]\n+                if var.shape == local_param_state.shape:\n+                    var.assign(local_param_state)\n+\n+    def _synchronize_gradients(\n+        self, gradients_and_vars: List[List[tuple]]\n+    ) -> List[List[tuple]]:\n+        \"\"\"Synchronizes gradients across shards based on tensor parallel rules.\n+\n+        Specifically, it performs an all-reduce operation on gradients of\n+        weights that are split along a \"column\" dimension in tensor parallelism.\n+        Other gradients are passed through unchanged.\n+\n+        Args:\n+            gradients_and_vars: The list of (gradient, variable) lists from\n+                all shards.\n+\n+        Returns:\n+            The list of (gradient, variable) lists after synchronization.\n+        \"\"\"\n+        if not self.tensor_parallel_config:\n+            return gradients_and_vars\n+\n+        rules = self.tensor_parallel_config.state_rules.items()\n+        column_parallel_patterns = {\n+            pattern\n+            for pattern, action in rules\n+            if hasattr(action, \"sharding_type\")\n+            and action.sharding_type == \"column\"\n+        }\n+\n+        if not column_parallel_patterns:\n+            return gradients_and_vars\n+\n+        num_weights = len(gradients_and_vars[0])\n+        for i in range(num_weights):\n+            variable = gradients_and_vars[0][i][1]\n+            var_name = getattr(variable, \"path\", getattr(variable, \"name\", \"\"))\n+\n+            if any(\n+                re.search(pattern, var_name)\n+                for pattern in column_parallel_patterns\n+            ):\n+                grads_to_reduce = [\n+                    g_and_v[i][0]\n+                    for g_and_v in gradients_and_vars\n+                    if g_and_v[i][0] is not None\n+                ]\n+                if grads_to_reduce:\n+                    synced_grad = self._allreduce_gradients(grads_to_reduce)[0]\n+                    for shard_idx in range(self.world_size):\n+                        gradients_and_vars[shard_idx][i] = (\n+                            synced_grad,\n+                            variable,\n+                        )\n+        return gradients_and_vars\n+\n+    def _allreduce_gradients(self, gradients: List[Any]) -> List[Any]:\n+        \"\"\"Performs a mean all-reduce operation on a list of gradients.\n+\n+        If a distributed backend is available, it uses it. Otherwise, it\n+        falls back to a local mean calculation.\n+\n+        Args:\n+            gradients: A list of gradients (one from each shard) to be averaged.\n+\n+        Returns:\n+            A list where each element is the mean of the input gradients.\n+        \"\"\"\n+        if not gradients:\n+            return []\n+\n+        if (\n+            self.distributed_backend is not None\n+            and self.distributed_backend.is_initialized\n+        ):\n+            numpy_grad = keras.ops.convert_to_numpy(gradients[0])\n+            synced_numpy = self.distributed_backend.allreduce(\n+                numpy_grad, op=\"mean\"\n+            )\n+            synced_tensor = keras.ops.convert_to_tensor(synced_numpy)\n+            return [synced_tensor for _ in range(self.world_size)]\n+\n+        stacked_grads = keras.ops.stack(\n+            [keras.ops.convert_to_tensor(g) for g in gradients], axis=0\n+        )\n+        mean_grad = keras.ops.mean(stacked_grads, axis=0)\n+        return [mean_grad for _ in range(len(gradients))]\n+\n+    def get_weights(self) -> List[np.ndarray]:\n+        \"\"\"Returns the weights of the base optimizer.\"\"\"\n+        return self.base_optimizer.get_weights()\n+\n+    def set_weights(self, weights: List[np.ndarray]):\n+        \"\"\"Sets the weights of the base optimizer.\"\"\"\n+        self.base_optimizer.set_weights(weights)\n+\n+    def enable_optimizer_state_sharding(self):\n+        \"\"\"Enables and initializes optimizer state sharding.\n+\n+        If sharding is not already active, this method sets the flag and\n+        triggers the partitioning of the optimizer's states.\n+        \"\"\"\n+        if not self.shard_optimizer_states:\n+            self.shard_optimizer_states = True\n+            self._initialize_sharded_states()\n+\n+    def disable_optimizer_state_sharding(self):\n+        \"\"\"Disables sharding and clears any sharded states.\n+\n+        This reverts the optimizer to using a single, replicated state.\n+        \"\"\"\n+        if self.shard_optimizer_states:\n+            self.shard_optimizer_states = False\n+            self.sharded_states = {}\n+\n+\n+class TensorParallelOptimizer(optimizers.Optimizer):\n+    \"\"\"A Keras Optimizer wrapper for tensor-parallel distributed training.\n+\n+    This optimizer wraps a standard Keras optimizer (e.g., Adam, SGD) and\n+    delegates the complex tasks of state management and gradient synchronization\n+    to a `CoordinatedOptimizer` instance. It is designed to work with models\n+    that have been sharded for tensor parallelism.\n+\n+    When `apply_gradients` is called with a list of gradient lists (one for each\n+    model shard), it uses the `CoordinatedOptimizer` to handle synchronization\n+    and state sharding. Otherwise, it behaves like the base optimizer.\n+\n+    Args:\n+        base_optimizer: A Keras optimizer instance or a string identifier\n+            (e.g., 'adam', 'sgd').\n+        world_size: The total number of devices/processes in the distributed\n+            setup.\n+        distributed_backend: The distributed communication backend to use.\n+            Defaults to \"auto\".\n+        tensor_parallel_config: An optional configuration object that defines\n+            rules for tensor parallelism. Defaults to `None`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        base_optimizer: optimizers.Optimizer,\n+        world_size: int,\n+        distributed_backend: str = \"auto\",\n+        tensor_parallel_config=None,\n+    ):\n+        if isinstance(base_optimizer, str):\n+            opt_lower = base_optimizer.lower()\n+            if opt_lower == \"adam\":\n+                resolved_base_optimizer = optimizers.Adam()\n+            elif opt_lower == \"sgd\":\n+                resolved_base_optimizer = optimizers.SGD()\n+            elif opt_lower == \"rmsprop\":\n+                resolved_base_optimizer = optimizers.RMSprop()\n+            else:\n+                raise ValueError(f\"Unknown optimizer string: {base_optimizer}\")\n+        else:\n+            resolved_base_optimizer = base_optimizer\n+\n+        lr_value = float(\n+            ops.convert_to_numpy(resolved_base_optimizer.learning_rate)\n+        )\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe initialization of `lr_value` by casting `resolved_base_optimizer.learning_rate` to a float will fail if a `keras.optimizers.schedules.LearningRateSchedule` is used, as it cannot be converted to a single float. This is a critical bug that prevents the use of learning rate schedules with `TensorParallelOptimizer`. The `learning_rate` property of this class already delegates to the base optimizer, so the `learning_rate` argument to `super().__init__` should be handled differently to support schedules.\n\n```python\n        if isinstance(\n            resolved_base_optimizer.learning_rate,\n            keras.optimizers.schedules.LearningRateSchedule,\n        ):\n            lr_value = float(\n                ops.convert_to_numpy(\n                    resolved_base_optimizer.learning_rate.initial_learning_rate\n                )\n            )\n        else:\n            lr_value = float(\n                ops.convert_to_numpy(resolved_base_optimizer.learning_rate)\n            )\n```",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "keras/src/distribution/tensor_parallel/coordinated_optimizer.py",
    "pr_number": 21703,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2393700437,
    "comment_created_at": "2025-10-01T07:30:32Z"
  },
  {
    "code": "@@ -113,6 +111,27 @@ def set_model(self, model):\n         for callback in self.callbacks:\n             callback.set_model(model)\n \n+    def _on_begin(self):\n+        \"\"\"Called by `on_train/test/predict_begin`.\"\"\"",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis docstring could be more descriptive about what the method does, in addition to where it's called from. this aligns with the style guide's recommendation for helpful documentation[^1]. a more descriptive docstring would improve code clarity for future maintainers.\n\n\n\n#### style guide references\n[^1]: the style guide states that docstrings should explain what a piece of code is about and why/how it should be used, to aid understanding.",
    "line_number": 115,
    "enriched": "File: keras/src/callbacks/callback_list.py\nCode: @@ -113,6 +111,27 @@ def set_model(self, model):\n         for callback in self.callbacks:\n             callback.set_model(model)\n \n+    def _on_begin(self):\n+        \"\"\"Called by `on_train/test/predict_begin`.\"\"\"\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis docstring could be more descriptive about what the method does, in addition to where it's called from. This aligns with the style guide's recommendation for helpful documentation[^1]. A more descriptive docstring would improve code clarity for future maintainers.\n\n```suggestion\n        \"\"\"Manages the start of an async block, creating the executor if it's the outermost block.\"\"\"\n```\n\n#### Style Guide References\n[^1]: The style guide states that docstrings should explain what a piece of code is about and why/how it should be used, to aid understanding.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/callbacks/callback_list.py",
    "pr_number": 21701,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2388868447,
    "comment_created_at": "2025-09-29T18:29:48Z"
  },
  {
    "code": "@@ -0,0 +1,40 @@\n+import logging\n+\n+from keras.src.backend.distributed.base import BaseDistributedBackend\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def get_distributed_backend(\n+    backend_name: str = \"auto\",\n+) -> BaseDistributedBackend:\n+    \"\"\"\n+    Factory to get the best available or a specific distributed backend.\n+    \"\"\"\n+    if backend_name == \"auto\":\n+        try:\n+            from keras.src.backend.jax.distributed_backend import (\n+                JaxDistributedBackend,\n+            )\n+\n+            logger.info(\"Auto-detected JAX for distributed backend.\")\n+            return JaxDistributedBackend()\n+        except ImportError:\n+            pass\n+\n+    elif backend_name == \"jax\":\n+        from keras.src.backend.jax.distributed_backend import (\n+            JaxDistributedBackend,\n+        )\n+\n+        return JaxDistributedBackend()\n+    elif backend_name == \"tensorflow\":\n+        pass\n+    elif backend_name == \"torch\":\n+        pass\n+\n+        return TorchDistributedBackend()\n+    elif backend_name == \"numpy\":\n+        pass",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthis part of the factory function has several issues:\n1. the return torchdistributedbackend() on line 36 is incorrectly indented and will be executed for tensorflow, torch, and numpy backends.\n2. torchdistributedbackend is not defined, which will cause a nameerror.\n3. the branches for tensorflow, torch, and numpy are not implemented. using pass can lead to unexpected behavior, as the function might fall through and attempt to execute code outside the if/elif chain.\n\nto make the code more robust and prevent runtime errors, i suggest explicitly raising a notimplementederror for backends that are not yet supported.\n\npython\n    elif backend_name == \"tensorflow\":\n        raise notimplementederror(\"tensorflow distributed backend is not yet implemented.\")\n    elif backend_name == \"torch\":\n        raise notimplementederror(\"torch distributed backend is not yet implemented.\")\n    elif backend_name == \"numpy\":\n        raise notimplementederror(\"numpy distributed backend is not yet implemented.\")",
    "line_number": 38,
    "enriched": "File: keras/src/backend/distributed/factory.py\nCode: @@ -0,0 +1,40 @@\n+import logging\n+\n+from keras.src.backend.distributed.base import BaseDistributedBackend\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def get_distributed_backend(\n+    backend_name: str = \"auto\",\n+) -> BaseDistributedBackend:\n+    \"\"\"\n+    Factory to get the best available or a specific distributed backend.\n+    \"\"\"\n+    if backend_name == \"auto\":\n+        try:\n+            from keras.src.backend.jax.distributed_backend import (\n+                JaxDistributedBackend,\n+            )\n+\n+            logger.info(\"Auto-detected JAX for distributed backend.\")\n+            return JaxDistributedBackend()\n+        except ImportError:\n+            pass\n+\n+    elif backend_name == \"jax\":\n+        from keras.src.backend.jax.distributed_backend import (\n+            JaxDistributedBackend,\n+        )\n+\n+        return JaxDistributedBackend()\n+    elif backend_name == \"tensorflow\":\n+        pass\n+    elif backend_name == \"torch\":\n+        pass\n+\n+        return TorchDistributedBackend()\n+    elif backend_name == \"numpy\":\n+        pass\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThis part of the factory function has several issues:\n1. The `return TorchDistributedBackend()` on line 36 is incorrectly indented and will be executed for `tensorflow`, `torch`, and `numpy` backends.\n2. `TorchDistributedBackend` is not defined, which will cause a `NameError`.\n3. The branches for `tensorflow`, `torch`, and `numpy` are not implemented. Using `pass` can lead to unexpected behavior, as the function might fall through and attempt to execute code outside the `if/elif` chain.\n\nTo make the code more robust and prevent runtime errors, I suggest explicitly raising a `NotImplementedError` for backends that are not yet supported.\n\n```python\n    elif backend_name == \"tensorflow\":\n        raise NotImplementedError(\"TensorFlow distributed backend is not yet implemented.\")\n    elif backend_name == \"torch\":\n        raise NotImplementedError(\"Torch distributed backend is not yet implemented.\")\n    elif backend_name == \"numpy\":\n        raise NotImplementedError(\"NumPy distributed backend is not yet implemented.\")\n```",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "keras/src/backend/distributed/factory.py",
    "pr_number": 21699,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2388310330,
    "comment_created_at": "2025-09-29T15:01:40Z"
  },
  {
    "code": "@@ -164,13 +164,14 @@ def celu(x, alpha=1.0):\n \n def glu(x, axis=-1):\n     x = convert_to_tensor(x)\n+    dtype = x.dtype\n     if x.shape[axis] % 2 != 0:\n         raise ValueError(\n             \"axis size must be divisible by 2. \"\n             f\"Received: x.shape={x.shape} with axis={axis}\"\n         )\n     x1, x2 = np.split(x, 2, axis)\n-    return x1 * (1 / (1 + np.exp(-x2)))\n+    return (x1 * (1 / (1 + np.exp(-x2)))).astype(dtype)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nfor better code reuse and maintainability, consider using the sigmoid function that is already defined in this file. the expression 1 / (1 + np.exp(-x2)) is an implementation of the sigmoid function. using the existing sigmoid function would make the code more readable and consistent with other backend implementations of glu.",
    "line_number": 174,
    "enriched": "File: keras/src/backend/numpy/nn.py\nCode: @@ -164,13 +164,14 @@ def celu(x, alpha=1.0):\n \n def glu(x, axis=-1):\n     x = convert_to_tensor(x)\n+    dtype = x.dtype\n     if x.shape[axis] % 2 != 0:\n         raise ValueError(\n             \"axis size must be divisible by 2. \"\n             f\"Received: x.shape={x.shape} with axis={axis}\"\n         )\n     x1, x2 = np.split(x, 2, axis)\n-    return x1 * (1 / (1 + np.exp(-x2)))\n+    return (x1 * (1 / (1 + np.exp(-x2)))).astype(dtype)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nFor better code reuse and maintainability, consider using the `sigmoid` function that is already defined in this file. The expression `1 / (1 + np.exp(-x2))` is an implementation of the sigmoid function. Using the existing `sigmoid` function would make the code more readable and consistent with other backend implementations of `glu`.\n\n```suggestion\n    return (x1 * sigmoid(x2)).astype(dtype)\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/numpy/nn.py",
    "pr_number": 21696,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2379343972,
    "comment_created_at": "2025-09-25T14:23:08Z"
  },
  {
    "code": "@@ -316,9 +324,10 @@ def _build_metrics_set(\n         return flat_metrics\n \n     def _flatten_y(self, y):\n-        if isinstance(y, dict) and self.output_names:\n+        names = getattr(self, \"_resolved_output_names\", self.output_names)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe use of getattr with a default value here can be a bit confusing, as it suggests a fallback path that doesn't appear to be exercised in the current implementation. the _flatten_y method is consistently called after build() has been executed (either from build() itself or from update_state()), ensuring self._resolved_output_names is always set.\n\na direct assignment, names = self._resolved_output_names, would make the dependency on the build step more explicit. this would lead to a clear attributeerror if _flatten_y were ever called prematurely, which is preferable to silently falling back to a potentially incorrect value. this change would improve code clarity and maintainability by simplifying the mental model of the execution flow.[^1]\n\n\n\n#### style guide references\n[^1]: this suggestion aligns with the principle of designing simple and consistent workflows that reflect simple and consistent mental models. by removing the implicit fallback, the code becomes more straightforward and easier to reason about.",
    "line_number": 327,
    "enriched": "File: keras/src/trainers/compile_utils.py\nCode: @@ -316,9 +324,10 @@ def _build_metrics_set(\n         return flat_metrics\n \n     def _flatten_y(self, y):\n-        if isinstance(y, dict) and self.output_names:\n+        names = getattr(self, \"_resolved_output_names\", self.output_names)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe use of `getattr` with a default value here can be a bit confusing, as it suggests a fallback path that doesn't appear to be exercised in the current implementation. The `_flatten_y` method is consistently called after `build()` has been executed (either from `build()` itself or from `update_state()`), ensuring `self._resolved_output_names` is always set.\n\nA direct assignment, `names = self._resolved_output_names`, would make the dependency on the `build` step more explicit. This would lead to a clear `AttributeError` if `_flatten_y` were ever called prematurely, which is preferable to silently falling back to a potentially incorrect value. This change would improve code clarity and maintainability by simplifying the mental model of the execution flow.[^1]\n\n```suggestion\n        names = self._resolved_output_names\n```\n\n#### Style Guide References\n[^1]: This suggestion aligns with the principle of designing simple and consistent workflows that reflect simple and consistent mental models. By removing the implicit fallback, the code becomes more straightforward and easier to reason about.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/trainers/compile_utils.py",
    "pr_number": 21694,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2377966776,
    "comment_created_at": "2025-09-25T07:03:40Z"
  },
  {
    "code": "@@ -86,6 +86,27 @@ def test_result_type_with_int64(self, dtype):\n         out = backend.result_type(x1.dtype, x2.dtype)\n         self.assertEqual(out, \"int64\")\n \n+    @parameterized.named_parameters(\n+        named_product(\n+            dtype=[\n+                \"float16\",\n+                \"bfloat16\",\n+                \"float32\",\n+                \"float64\",\n+            ]",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe test for float64 promotion currently only covers promotion with other float types. to make it more comprehensive and ensure correctness for all type interactions, it would be beneficial to also test promotion with integer types, as float64 should be the result type in those cases as well on the tensorflow backend.\n\npython\n            dtype=[\n                \"float16\",\n                \"bfloat16\",\n                \"float32\",\n                \"float64\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"uint8\",\n                \"uint16\",\n            ]",
    "line_number": 96,
    "enriched": "File: keras/src/backend/common/dtypes_test.py\nCode: @@ -86,6 +86,27 @@ def test_result_type_with_int64(self, dtype):\n         out = backend.result_type(x1.dtype, x2.dtype)\n         self.assertEqual(out, \"int64\")\n \n+    @parameterized.named_parameters(\n+        named_product(\n+            dtype=[\n+                \"float16\",\n+                \"bfloat16\",\n+                \"float32\",\n+                \"float64\",\n+            ]\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe test for `float64` promotion currently only covers promotion with other float types. To make it more comprehensive and ensure correctness for all type interactions, it would be beneficial to also test promotion with integer types, as `float64` should be the result type in those cases as well on the TensorFlow backend.\n\n```python\n            dtype=[\n                \"float16\",\n                \"bfloat16\",\n                \"float32\",\n                \"float64\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"uint8\",\n                \"uint16\",\n            ]\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/backend/common/dtypes_test.py",
    "pr_number": 21693,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2377672821,
    "comment_created_at": "2025-09-25T04:20:03Z"
  },
  {
    "code": "@@ -862,6 +862,17 @@ def test_legacy_load_own_variables(self):\n             # outputs_grad_amax_history.\n             \"7\": np.random.random((1024,)).astype(\"float32\"),\n         }\n+        gptq_store = {\n+            # bias\n+            \"0\": np.random.random((16,)).astype(\"float32\"),\n+            \"1\": np.random.randint(0, 16, size=(16, 8), dtype=\"uint8\"),\n+            # kernel_scale.\n+            \"2\": np.random.random((16, 1)).astype(\"float32\"),\n+            # kernel_zero\n+            \"3\": np.random.random((16, 1)).astype(\"uint8\"),",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe kernel_zero test data is being generated with np.random.random((16, 1)).astype(\"uint8\"). np.random.random() returns float values in the range [0.0, 1.0), which will all be truncated to 0 when cast to uint8. this means the test is only running with zero values for kernel_zero. to properly test with random unsigned 8-bit integers, you should use np.random.randint().",
    "line_number": 871,
    "enriched": "File: keras/src/layers/core/dense_test.py\nCode: @@ -862,6 +862,17 @@ def test_legacy_load_own_variables(self):\n             # outputs_grad_amax_history.\n             \"7\": np.random.random((1024,)).astype(\"float32\"),\n         }\n+        gptq_store = {\n+            # bias\n+            \"0\": np.random.random((16,)).astype(\"float32\"),\n+            \"1\": np.random.randint(0, 16, size=(16, 8), dtype=\"uint8\"),\n+            # kernel_scale.\n+            \"2\": np.random.random((16, 1)).astype(\"float32\"),\n+            # kernel_zero\n+            \"3\": np.random.random((16, 1)).astype(\"uint8\"),\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe `kernel_zero` test data is being generated with `np.random.random((16, 1)).astype(\"uint8\")`. `np.random.random()` returns float values in the range `[0.0, 1.0)`, which will all be truncated to `0` when cast to `uint8`. This means the test is only running with zero values for `kernel_zero`. To properly test with random unsigned 8-bit integers, you should use `np.random.randint()`.\n\n```suggestion\n            \"3\": np.random.randint(0, 256, size=(16, 1), dtype=\"uint8\"),\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/layers/core/dense_test.py",
    "pr_number": 21689,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2375157892,
    "comment_created_at": "2025-09-24T09:24:37Z"
  },
  {
    "code": "@@ -308,12 +308,11 @@ def append(x1, x2, axis=None):\n \n def arange(start, stop=None, step=1, dtype=None):\n     if dtype is None:\n-        dtypes_to_resolve = [\n-            getattr(start, \"dtype\", type(start)),\n-            getattr(step, \"dtype\", type(step)),\n-        ]\n+        dtypes_to_resolve = [getattr(start, \"dtype\", type(start))]\n         if stop is not None:\n             dtypes_to_resolve.append(getattr(stop, \"dtype\", type(stop)))\n+        if step is not None:\n+            dtypes_to_resolve.append(getattr(step, \"dtype\", type(step)))\n         dtype = dtypes.result_type(*dtypes_to_resolve)\n     dtype = standardize_dtype(dtype)\n     return jnp.arange(start, stop, step=step, dtype=dtype)",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe signature for arange should be updated to step=none to align with the public api and other backend implementations. currently, it's step=1, which will cause an error when none is passed from the ops layer. additionally, jnp.arange does not accept step=none, so a check is needed to default step to 1 if it is none.",
    "line_number": 318,
    "enriched": "File: keras/src/backend/jax/numpy.py\nCode: @@ -308,12 +308,11 @@ def append(x1, x2, axis=None):\n \n def arange(start, stop=None, step=1, dtype=None):\n     if dtype is None:\n-        dtypes_to_resolve = [\n-            getattr(start, \"dtype\", type(start)),\n-            getattr(step, \"dtype\", type(step)),\n-        ]\n+        dtypes_to_resolve = [getattr(start, \"dtype\", type(start))]\n         if stop is not None:\n             dtypes_to_resolve.append(getattr(stop, \"dtype\", type(stop)))\n+        if step is not None:\n+            dtypes_to_resolve.append(getattr(step, \"dtype\", type(step)))\n         dtype = dtypes.result_type(*dtypes_to_resolve)\n     dtype = standardize_dtype(dtype)\n     return jnp.arange(start, stop, step=step, dtype=dtype)\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe signature for `arange` should be updated to `step=None` to align with the public API and other backend implementations. Currently, it's `step=1`, which will cause an error when `None` is passed from the ops layer. Additionally, `jnp.arange` does not accept `step=None`, so a check is needed to default `step` to 1 if it is `None`.\n\n```suggestion\ndef arange(start, stop=None, step=None, dtype=None):\n    if dtype is None:\n        dtypes_to_resolve = [getattr(start, \"dtype\", type(start))]\n        if stop is not None:\n            dtypes_to_resolve.append(getattr(stop, \"dtype\", type(stop)))\n        if step is not None:\n            dtypes_to_resolve.append(getattr(step, \"dtype\", type(step)))\n        dtype = dtypes.result_type(*dtypes_to_resolve)\n    dtype = standardize_dtype(dtype)\n    if step is None:\n        step = 1\n    return jnp.arange(start, stop, step=step, dtype=dtype)\n\n```",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "keras/src/backend/jax/numpy.py",
    "pr_number": 21688,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2373772353,
    "comment_created_at": "2025-09-24T00:44:09Z"
  },
  {
    "code": "@@ -1593,7 +1593,47 @@ def size(x):\n \n \n def sort(x, axis=-1):\n-    raise NotImplementedError(\"`sort` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_shape = x.get_partial_shape()\n+    rank = x_shape.rank.get_length()\n+    \n+    if rank == 0:\n+        return OpenVINOKerasTensor(x)\n+    \n+    # Handle axis=None by flattening the input\n+    if axis is None:\n+        x = ov_opset.reshape(x, [-1], False).output(0)\n+        axis = 0\n+    # Handle negative axis\n+    elif axis < 0:\n+        axis = rank + axis\n+    \n+    # Get the size of the dimension to sort\n+    shape_tensor = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n+    k = ov_opset.gather(\n+        shape_tensor,\n+        ov_opset.constant([axis], Type.i32).output(0),\n+        ov_opset.constant(0, Type.i32).output(0)\n+    ).output(0)\n+    \n+    # Convert k to a scalar value\n+    k_scalar = ov_opset.squeeze(k, [0]).output(0)\n+    \n+    # Use topk with k=size_of_axis to get all elements sorted\n+    topk_outputs = ov_opset.topk(\n+        x,\n+        k=k_scalar,\n+        axis=axis,\n+        mode=\"min\",\n+        sort=\"value\",\n+        stable=True\n+    )\n+    \n+    # Get the sorted values\n+    sorted_values = topk_outputs.output(0)\n+    \n+    return OpenVINOKerasTensor(sorted_values)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe implementation is correct. however, for consistency with the rest of the file, it's better to explicitly create constants for operator arguments instead of passing python lists directly. this applies to reshape on line 1605 and squeeze on line 1620. [^1]\n\nfor example:\n*   ov_opset.reshape(x, [-1], false) should be ov_opset.reshape(x, ov_opset.constant([-1], type.i32), false)\n*   ov_opset.squeeze(k, [0]) should be ov_opset.squeeze(k, ov_opset.constant([0], type.i32))\n\ni'll provide a suggestion for the whole function to apply these changes.\n\npython\n    x = get_ov_output(x)\n    x_shape = x.get_partial_shape()\n    rank = x_shape.rank.get_length()\n    \n    if rank == 0:\n        return openvinokerastensor(x)\n    \n    # handle axis=none by flattening the input\n    if axis is none:\n        x = ov_opset.reshape(x, ov_opset.constant([-1], type.i32), false).output(0)\n        axis = 0\n    # handle negative axis\n    elif axis < 0:\n        axis = rank + axis\n    \n    # get the size of the dimension to sort\n    shape_tensor = ov_opset.shape_of(x, output_type=type.i32).output(0)\n    k = ov_opset.gather(\n        shape_tensor,\n        ov_opset.constant([axis], type.i32).output(0),\n        ov_opset.constant(0, type.i32).output(0)\n    ).output(0)\n    \n    # convert k to a scalar value\n    k_scalar = ov_opset.squeeze(k, ov_opset.constant([0], type.i32)).output(0)\n    \n    # use topk with k=size_of_axis to get all elements sorted\n    topk_outputs = ov_opset.topk(\n        x,\n        k=k_scalar,\n        axis=axis,\n        mode=\"min\",\n        sort=\"value\",\n        stable=true\n    )\n    \n    # get the sorted values\n    sorted_values = topk_outputs.output(0)\n    \n    return openvinokerastensor(sorted_values)\n\n\n#### style guide references\n[^1]: the style guide emphasizes internal consistency. the suggested change aligns the code with the prevalent pattern of explicitly creating constant tensors for operator arguments within this file, as seen in other functions like mean, ravel, and even within this function for the gather op. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#l71-l72)",
    "line_number": 1635,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -1593,7 +1593,47 @@ def size(x):\n \n \n def sort(x, axis=-1):\n-    raise NotImplementedError(\"`sort` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_shape = x.get_partial_shape()\n+    rank = x_shape.rank.get_length()\n+    \n+    if rank == 0:\n+        return OpenVINOKerasTensor(x)\n+    \n+    # Handle axis=None by flattening the input\n+    if axis is None:\n+        x = ov_opset.reshape(x, [-1], False).output(0)\n+        axis = 0\n+    # Handle negative axis\n+    elif axis < 0:\n+        axis = rank + axis\n+    \n+    # Get the size of the dimension to sort\n+    shape_tensor = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n+    k = ov_opset.gather(\n+        shape_tensor,\n+        ov_opset.constant([axis], Type.i32).output(0),\n+        ov_opset.constant(0, Type.i32).output(0)\n+    ).output(0)\n+    \n+    # Convert k to a scalar value\n+    k_scalar = ov_opset.squeeze(k, [0]).output(0)\n+    \n+    # Use topk with k=size_of_axis to get all elements sorted\n+    topk_outputs = ov_opset.topk(\n+        x,\n+        k=k_scalar,\n+        axis=axis,\n+        mode=\"min\",\n+        sort=\"value\",\n+        stable=True\n+    )\n+    \n+    # Get the sorted values\n+    sorted_values = topk_outputs.output(0)\n+    \n+    return OpenVINOKerasTensor(sorted_values)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe implementation is correct. However, for consistency with the rest of the file, it's better to explicitly create constants for operator arguments instead of passing Python lists directly. This applies to `reshape` on line 1605 and `squeeze` on line 1620. [^1]\n\nFor example:\n*   `ov_opset.reshape(x, [-1], False)` should be `ov_opset.reshape(x, ov_opset.constant([-1], Type.i32), False)`\n*   `ov_opset.squeeze(k, [0])` should be `ov_opset.squeeze(k, ov_opset.constant([0], Type.i32))`\n\nI'll provide a suggestion for the whole function to apply these changes.\n\n```python\n    x = get_ov_output(x)\n    x_shape = x.get_partial_shape()\n    rank = x_shape.rank.get_length()\n    \n    if rank == 0:\n        return OpenVINOKerasTensor(x)\n    \n    # Handle axis=None by flattening the input\n    if axis is None:\n        x = ov_opset.reshape(x, ov_opset.constant([-1], Type.i32), False).output(0)\n        axis = 0\n    # Handle negative axis\n    elif axis < 0:\n        axis = rank + axis\n    \n    # Get the size of the dimension to sort\n    shape_tensor = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n    k = ov_opset.gather(\n        shape_tensor,\n        ov_opset.constant([axis], Type.i32).output(0),\n        ov_opset.constant(0, Type.i32).output(0)\n    ).output(0)\n    \n    # Convert k to a scalar value\n    k_scalar = ov_opset.squeeze(k, ov_opset.constant([0], Type.i32)).output(0)\n    \n    # Use topk with k=size_of_axis to get all elements sorted\n    topk_outputs = ov_opset.topk(\n        x,\n        k=k_scalar,\n        axis=axis,\n        mode=\"min\",\n        sort=\"value\",\n        stable=True\n    )\n    \n    # Get the sorted values\n    sorted_values = topk_outputs.output(0)\n    \n    return OpenVINOKerasTensor(sorted_values)\n```\n\n#### Style Guide References\n[^1]: The style guide emphasizes internal consistency. The suggested change aligns the code with the prevalent pattern of explicitly creating constant tensors for operator arguments within this file, as seen in other functions like `mean`, `ravel`, and even within this function for the `gather` op. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#L71-L72)",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21687,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2372873570,
    "comment_created_at": "2025-09-23T16:28:43Z"
  },
  {
    "code": "@@ -916,3 +953,44 @@ def _get_gptq_group_size(self, config):\n                 \"either through a `dtype_policy` of type \"\n                 \"`GPTQDTypePolicy` or the `config` argument.\"\n             )\n+\n+    def _get_gptq_weight_bits(self, config):",
    "comment": "should we move this function to quantizers.gptq or another location? it might be needed by any layers that support gptq.",
    "line_number": 956,
    "enriched": "File: keras/src/layers/core/dense.py\nCode: @@ -916,3 +953,44 @@ def _get_gptq_group_size(self, config):\n                 \"either through a `dtype_policy` of type \"\n                 \"`GPTQDTypePolicy` or the `config` argument.\"\n             )\n+\n+    def _get_gptq_weight_bits(self, config):\nComment: Should we move this function to `quantizers.gptq` or another location? It might be needed by any layers that support GPTQ.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "keras/src/layers/core/dense.py",
    "pr_number": 21686,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2373901496,
    "comment_created_at": "2025-09-24T02:43:40Z"
  },
  {
    "code": "@@ -228,3 +228,25 @@ def unstack(x, axis=0):\n         lax.index_in_dim(x, i, axis, keepdims=False)\n         for i in range(x.shape[axis])\n     ]\n+\n+\n+def unfold(input, kernel_size, dilation=1, padding=0, stride=1):\n+    filter_shape = (kernel_size, kernel_size)\n+\n+    window_strides = (stride, stride)\n+    lhs_dilation = (dilation, dilation)\n+\n+    if isinstance(padding, int):\n+        padding = ((padding, padding), (padding, padding))\n+    else:\n+        padding = ((padding[0], padding[0]), (padding[1], padding[1]))\n+\n+    patches = lax.conv_general_dilated_patches(\n+        lhs=input,\n+        filter_shape=filter_shape,\n+        window_strides=window_strides,\n+        padding=padding,\n+        lhs_dilation=lhs_dilation,\n+    )\n+\n+    return patches",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe current implementation assumes that kernel_size, dilation, and stride are integers. however, the api contract allows them to be tuples of two integers as well. this will lead to incorrect behavior when tuples are passed. please add a helper function to handle both integer and tuple inputs, similar to the tensorflow backend implementation.\n\npython\ndef unfold(input, kernel_size, dilation=1, padding=0, stride=1):\n    def _pair(x):\n        return (x, x) if isinstance(x, int) else x\n\n    filter_shape = _pair(kernel_size)\n    window_strides = _pair(stride)\n    lhs_dilation = _pair(dilation)\n\n    if isinstance(padding, int):\n        padding = ((padding, padding), (padding, padding))\n    else:\n        padding = ((padding[0], padding[0]), (padding[1], padding[1]))\n\n    patches = lax.conv_general_dilated_patches(\n        lhs=input,\n        filter_shape=filter_shape,\n        window_strides=window_strides,\n        padding=padding,\n        lhs_dilation=lhs_dilation,\n    )\n\n    return patches",
    "line_number": 252,
    "enriched": "File: keras/src/backend/jax/rnn.py\nCode: @@ -228,3 +228,25 @@ def unstack(x, axis=0):\n         lax.index_in_dim(x, i, axis, keepdims=False)\n         for i in range(x.shape[axis])\n     ]\n+\n+\n+def unfold(input, kernel_size, dilation=1, padding=0, stride=1):\n+    filter_shape = (kernel_size, kernel_size)\n+\n+    window_strides = (stride, stride)\n+    lhs_dilation = (dilation, dilation)\n+\n+    if isinstance(padding, int):\n+        padding = ((padding, padding), (padding, padding))\n+    else:\n+        padding = ((padding[0], padding[0]), (padding[1], padding[1]))\n+\n+    patches = lax.conv_general_dilated_patches(\n+        lhs=input,\n+        filter_shape=filter_shape,\n+        window_strides=window_strides,\n+        padding=padding,\n+        lhs_dilation=lhs_dilation,\n+    )\n+\n+    return patches\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe current implementation assumes that `kernel_size`, `dilation`, and `stride` are integers. However, the API contract allows them to be tuples of two integers as well. This will lead to incorrect behavior when tuples are passed. Please add a helper function to handle both integer and tuple inputs, similar to the TensorFlow backend implementation.\n\n```python\ndef unfold(input, kernel_size, dilation=1, padding=0, stride=1):\n    def _pair(x):\n        return (x, x) if isinstance(x, int) else x\n\n    filter_shape = _pair(kernel_size)\n    window_strides = _pair(stride)\n    lhs_dilation = _pair(dilation)\n\n    if isinstance(padding, int):\n        padding = ((padding, padding), (padding, padding))\n    else:\n        padding = ((padding[0], padding[0]), (padding[1], padding[1]))\n\n    patches = lax.conv_general_dilated_patches(\n        lhs=input,\n        filter_shape=filter_shape,\n        window_strides=window_strides,\n        padding=padding,\n        lhs_dilation=lhs_dilation,\n    )\n\n    return patches\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "keras/src/backend/jax/rnn.py",
    "pr_number": 21685,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2368908088,
    "comment_created_at": "2025-09-22T15:07:24Z"
  },
  {
    "code": "@@ -1036,3 +1037,16 @@ def test_gptq_serialization(self):\n         new_layer = layers.EinsumDense.from_config(config)\n         new_layer.build((None, 3))\n         self.assertEqual(new_layer.quantization_mode, \"gptq\")\n+\n+    def test_int4_kernel_returns_unpacked_form(self):\n+        \"\"\"Test that the `kernel` property returns the unpacked int4 kernel.\"\"\"\n+        layer = layers.EinsumDense(\n+            equation=\"ab,bc->ac\",\n+            output_shape=(2,),\n+        )\n+        layer.build((None, 2))\n+        layer.quantize(\"int4\")\n+        packed_kernel = layer._kernel\n+        self.assertAllClose(\n+            layer.kernel, quantizers.unpack_int4(packed_kernel, orig_len=2)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nfor consistency with the test in dense_test.py, it's better to call unpack_int4 with positional arguments instead of keyword arguments.[^1]\n\nwhile using a keyword argument here is functionally correct, using positional arguments consistently across the test suite improves readability and maintainability.\n\n\n\n#### style guide references\n[^1]:",
    "line_number": 1051,
    "enriched": "File: keras/src/layers/core/einsum_dense_test.py\nCode: @@ -1036,3 +1037,16 @@ def test_gptq_serialization(self):\n         new_layer = layers.EinsumDense.from_config(config)\n         new_layer.build((None, 3))\n         self.assertEqual(new_layer.quantization_mode, \"gptq\")\n+\n+    def test_int4_kernel_returns_unpacked_form(self):\n+        \"\"\"Test that the `kernel` property returns the unpacked int4 kernel.\"\"\"\n+        layer = layers.EinsumDense(\n+            equation=\"ab,bc->ac\",\n+            output_shape=(2,),\n+        )\n+        layer.build((None, 2))\n+        layer.quantize(\"int4\")\n+        packed_kernel = layer._kernel\n+        self.assertAllClose(\n+            layer.kernel, quantizers.unpack_int4(packed_kernel, orig_len=2)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nFor consistency with the test in `dense_test.py`, it's better to call `unpack_int4` with positional arguments instead of keyword arguments.[^1]\n\nWhile using a keyword argument here is functionally correct, using positional arguments consistently across the test suite improves readability and maintainability.\n\n```suggestion\n            layer.kernel, quantizers.unpack_int4(packed_kernel, 2)\n```\n\n#### Style Guide References\n[^1]: ",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/layers/core/einsum_dense_test.py",
    "pr_number": 21684,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2365741747,
    "comment_created_at": "2025-09-20T17:56:16Z"
  },
  {
    "code": "@@ -382,6 +350,27 @@ def _check_load_own_variables(self, store):\n                 f\"Expected: {[v.name for v in all_vars]}\"\n             )\n \n+    # Per-mode variable spec.\n+    MODE_SPEC = {",
    "comment": "i suggest renaming mode_spec to quantization_variable_spec. the current name is now somewhat ambiguous because the spec is stored as a class attribute. additionally, given that this value is a constant, it might be better to define it as a private constant outside the class rather than keeping it at the class level. wdyt?",
    "line_number": 354,
    "enriched": "File: keras/src/layers/core/dense.py\nCode: @@ -382,6 +350,27 @@ def _check_load_own_variables(self, store):\n                 f\"Expected: {[v.name for v in all_vars]}\"\n             )\n \n+    # Per-mode variable spec.\n+    MODE_SPEC = {\nComment: I suggest renaming `MODE_SPEC` to `QUANTIZATION_VARIABLE_SPEC`. The current name is now somewhat ambiguous because the spec is stored as a class attribute. Additionally, given that this value is a constant, it might be better to define it as a private constant outside the class rather than keeping it at the class level. WDYT?",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "keras/src/layers/core/dense.py",
    "pr_number": 21681,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2366752821,
    "comment_created_at": "2025-09-22T05:23:38Z"
  },
  {
    "code": "@@ -328,6 +328,10 @@ def test_variable_path_creation(self):\n         v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n         self.assertEqual(v.path, \"test_var\")\n \n+        with backend.name_scope(\"test_scope\"):\n+            v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n+            self.assertEqual(v.path, \"test_scope/test_var\")",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis is a good test case for a single level of name scope. to make the test more robust, consider adding a case for nested name scopes to ensure that path construction works correctly with multiple levels of hierarchy.\n\npython\n        with backend.name_scope(\"test_scope\"):\n            v = backend.variable(initializer=np.ones((2, 2)), name=\"test_var\")\n            self.assertequal(v.path, \"test_scope/test_var\")\n\n        with backend.name_scope(\"outer\"):\n            with backend.name_scope(\"inner\"):\n                v = backend.variable(initializer=np.ones((2, 2)), name=\"test_var\")\n                self.assertequal(v.path, \"outer/inner/test_var\")",
    "line_number": 333,
    "enriched": "File: keras/src/backend/common/variables_test.py\nCode: @@ -328,6 +328,10 @@ def test_variable_path_creation(self):\n         v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n         self.assertEqual(v.path, \"test_var\")\n \n+        with backend.name_scope(\"test_scope\"):\n+            v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n+            self.assertEqual(v.path, \"test_scope/test_var\")\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis is a good test case for a single level of name scope. To make the test more robust, consider adding a case for nested name scopes to ensure that path construction works correctly with multiple levels of hierarchy.\n\n```python\n        with backend.name_scope(\"test_scope\"):\n            v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n            self.assertEqual(v.path, \"test_scope/test_var\")\n\n        with backend.name_scope(\"outer\"):\n            with backend.name_scope(\"inner\"):\n                v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n                self.assertEqual(v.path, \"outer/inner/test_var\")\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/backend/common/variables_test.py",
    "pr_number": 21680,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2360515054,
    "comment_created_at": "2025-09-18T17:50:53Z"
  },
  {
    "code": "@@ -55,6 +60,25 @@ def test_result_type_with_tensor(self, dtype1, dtype2):\n         expected = jnp.result_type(x1_jax, x2_jax).name\n         self.assertEqual(out, expected)\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\", reason=\"TensorFlow only\"\n+    )\n+    def test_result_type_with_int64(self):\n+        # https://github.com/keras-team/keras/issues/21677\n+        x1 = ops.ones((1,), dtype=\"int64\")\n+        for dtype in (\n+            \"int8\",\n+            \"int16\",\n+            \"int32\",\n+            \"int64\",\n+            \"uint8\",\n+            \"uint16\",\n+            \"uint32\",\n+        ):\n+            x2 = ops.ones((1,), dtype=dtype)\n+            out = backend.result_type(x1.dtype, x2.dtype)\n+            self.assertEqual(out, \"int64\")\n+",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nfor better readability and consistency with other tests in this file, you could use @parameterized.named_parameters to test against different dtypes instead of a for loop. this makes each test case more explicit and easier to debug if one of them fails.[^1]\n\npython\n    @pytest.mark.skipif(\n        backend.backend() != \"tensorflow\", reason=\"tensorflow only\"\n    )\n    @parameterized.named_parameters(\n        named_product(\n            dtype=[\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n            ]\n        )\n    )\n    def test_result_type_with_int64(self, dtype):\n        # https://github.com/keras-team/keras/issues/21677\n        x1 = ops.ones((1,), dtype=\"int64\")\n        x2 = ops.ones((1,), dtype=dtype)\n        out = backend.result_type(x1.dtype, x2.dtype)\n        self.assertequal(out, \"int64\")\n\n\n#### style guide references\n[^1]: the style guide emphasizes consistency. using @parameterized.named_parameters would align this test with the established pattern in this file, improving consistency. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#l71-l72)",
    "line_number": 81,
    "enriched": "File: keras/src/backend/common/dtypes_test.py\nCode: @@ -55,6 +60,25 @@ def test_result_type_with_tensor(self, dtype1, dtype2):\n         expected = jnp.result_type(x1_jax, x2_jax).name\n         self.assertEqual(out, expected)\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\", reason=\"TensorFlow only\"\n+    )\n+    def test_result_type_with_int64(self):\n+        # https://github.com/keras-team/keras/issues/21677\n+        x1 = ops.ones((1,), dtype=\"int64\")\n+        for dtype in (\n+            \"int8\",\n+            \"int16\",\n+            \"int32\",\n+            \"int64\",\n+            \"uint8\",\n+            \"uint16\",\n+            \"uint32\",\n+        ):\n+            x2 = ops.ones((1,), dtype=dtype)\n+            out = backend.result_type(x1.dtype, x2.dtype)\n+            self.assertEqual(out, \"int64\")\n+\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nFor better readability and consistency with other tests in this file, you could use `@parameterized.named_parameters` to test against different dtypes instead of a `for` loop. This makes each test case more explicit and easier to debug if one of them fails.[^1]\n\n```python\n    @pytest.mark.skipif(\n        backend.backend() != \"tensorflow\", reason=\"TensorFlow only\"\n    )\n    @parameterized.named_parameters(\n        named_product(\n            dtype=[\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n            ]\n        )\n    )\n    def test_result_type_with_int64(self, dtype):\n        # https://github.com/keras-team/keras/issues/21677\n        x1 = ops.ones((1,), dtype=\"int64\")\n        x2 = ops.ones((1,), dtype=dtype)\n        out = backend.result_type(x1.dtype, x2.dtype)\n        self.assertEqual(out, \"int64\")\n```\n\n#### Style Guide References\n[^1]: The style guide emphasizes consistency. Using `@parameterized.named_parameters` would align this test with the established pattern in this file, improving consistency. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#L71-L72)",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/backend/common/dtypes_test.py",
    "pr_number": 21679,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2358969962,
    "comment_created_at": "2025-09-18T12:24:40Z"
  },
  {
    "code": "@@ -22,21 +22,14 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n-    ov_type = OPENVINO_DTYPES[dtype]\n-    seed = draw_seed(seed)\n-    if isinstance(seed, OpenVINOKerasTensor):\n-        seed1, seed2 = convert_to_numpy(seed)\n+    seed_val = draw_seed(seed)\n+    if isinstance(seed_val, OpenVINOKerasTensor):\n+        seed_data = convert_to_numpy(seed_val)\n     else:\n-        seed1, seed2 = draw_seed(seed).data\n-    minval_const = ov_opset.constant(minval, dtype=dtype)\n-    maxval_const = ov_opset.constant(maxval, dtype=dtype)\n-    if isinstance(shape, tuple):\n-        shape = list(shape)\n-    output_shape_const = ov_opset.constant(shape, dtype=Type.i32)\n-    random_uniform = ov_opset.random_uniform(\n-        output_shape_const, minval_const, maxval_const, ov_type, seed1, seed2\n-    )\n-    return OpenVINOKerasTensor(random_uniform.output(0))\n+        seed_data = seed_val.data\n+    rng = np.random.default_rng(seed_data)\n+    random_values = rng.uniform(minval, maxval, size=shape).astype(dtype)\n+    return OpenVINOKerasTensor(ov_opset.constant(random_values).output(0))",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthis change fixes the non-deterministic output of ov_opset.random_uniform by using numpy.random.uniform instead. however, this introduces a critical regression: it breaks support for symbolic shapes.\n\nthe previous implementation could handle shapes with none dimensions (e.g., (none, 32)) because it treated shape as a symbolic input to the openvino graph. the new implementation uses rng.uniform(..., size=shape), which will fail if shape contains none values, as numpy requires concrete shapes for array creation. this will break many common use cases, such as using random_uniform as an initializer in a layer that accepts inputs with a dynamic batch size.\n\nto address this while still providing a workaround for the openvino bug, i suggest a hybrid approach that checks if the shape is symbolic and uses the appropriate implementation.\n\nhere is a suggested implementation to replace the current uniform function. you will also need to add import warnings at the top of the file.\n\npython\ndef uniform(shape, minval=0.0, maxval=1.0, dtype=none, seed=none):\n    dtype = dtype or floatx()\n    if any(s is none for s in shape):\n        warnings.warn(\n            \"keras.random.uniform with symbolic shapes may be non-deterministic \"\n            \"in the openvino backend due to an upstream bug. see \"\n            \"https://github.com/openvinotoolkit/openvino/issues/32045 for \"\n            \"details.\"\n        )\n        ov_type = openvino_dtypes[dtype]\n        seed_val = draw_seed(seed)\n        seed1, seed2 = convert_to_numpy(seed_val)\n        minval_const = ov_opset.constant(minval, dtype=dtype)\n        maxval_const = ov_opset.constant(maxval, dtype=dtype)\n        shape_list = [-1 if s is none else s for s in shape]\n        output_shape_const = ov_opset.constant(shape_list, dtype=type.i32)\n        random_uniform = ov_opset.random_uniform(\n            output_shape_const, minval_const, maxval_const, ov_type, seed1, seed2\n        )\n        return openvinokerastensor(random_uniform.output(0))\n\n    seed_val = draw_seed(seed)\n    seed_data = convert_to_numpy(seed_val)\n    rng = np.random.default_rng(seed_data)\n    random_values = rng.uniform(minval, maxval, size=shape).astype(dtype)\n    return openvinokerastensor(ov_opset.constant(random_values).output(0))",
    "line_number": 32,
    "enriched": "File: keras/src/backend/openvino/random.py\nCode: @@ -22,21 +22,14 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n-    ov_type = OPENVINO_DTYPES[dtype]\n-    seed = draw_seed(seed)\n-    if isinstance(seed, OpenVINOKerasTensor):\n-        seed1, seed2 = convert_to_numpy(seed)\n+    seed_val = draw_seed(seed)\n+    if isinstance(seed_val, OpenVINOKerasTensor):\n+        seed_data = convert_to_numpy(seed_val)\n     else:\n-        seed1, seed2 = draw_seed(seed).data\n-    minval_const = ov_opset.constant(minval, dtype=dtype)\n-    maxval_const = ov_opset.constant(maxval, dtype=dtype)\n-    if isinstance(shape, tuple):\n-        shape = list(shape)\n-    output_shape_const = ov_opset.constant(shape, dtype=Type.i32)\n-    random_uniform = ov_opset.random_uniform(\n-        output_shape_const, minval_const, maxval_const, ov_type, seed1, seed2\n-    )\n-    return OpenVINOKerasTensor(random_uniform.output(0))\n+        seed_data = seed_val.data\n+    rng = np.random.default_rng(seed_data)\n+    random_values = rng.uniform(minval, maxval, size=shape).astype(dtype)\n+    return OpenVINOKerasTensor(ov_opset.constant(random_values).output(0))\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThis change fixes the non-deterministic output of `ov_opset.random_uniform` by using `numpy.random.uniform` instead. However, this introduces a critical regression: it breaks support for symbolic shapes.\n\nThe previous implementation could handle shapes with `None` dimensions (e.g., `(None, 32)`) because it treated `shape` as a symbolic input to the OpenVINO graph. The new implementation uses `rng.uniform(..., size=shape)`, which will fail if `shape` contains `None` values, as NumPy requires concrete shapes for array creation. This will break many common use cases, such as using `random_uniform` as an initializer in a layer that accepts inputs with a dynamic batch size.\n\nTo address this while still providing a workaround for the OpenVINO bug, I suggest a hybrid approach that checks if the `shape` is symbolic and uses the appropriate implementation.\n\nHere is a suggested implementation to replace the current `uniform` function. You will also need to add `import warnings` at the top of the file.\n\n```python\ndef uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    dtype = dtype or floatx()\n    if any(s is None for s in shape):\n        warnings.warn(\n            \"keras.random.uniform with symbolic shapes may be non-deterministic \"\n            \"in the OpenVINO backend due to an upstream bug. See \"\n            \"https://github.com/openvinotoolkit/openvino/issues/32045 for \"\n            \"details.\"\n        )\n        ov_type = OPENVINO_DTYPES[dtype]\n        seed_val = draw_seed(seed)\n        seed1, seed2 = convert_to_numpy(seed_val)\n        minval_const = ov_opset.constant(minval, dtype=dtype)\n        maxval_const = ov_opset.constant(maxval, dtype=dtype)\n        shape_list = [-1 if s is None else s for s in shape]\n        output_shape_const = ov_opset.constant(shape_list, dtype=Type.i32)\n        random_uniform = ov_opset.random_uniform(\n            output_shape_const, minval_const, maxval_const, ov_type, seed1, seed2\n        )\n        return OpenVINOKerasTensor(random_uniform.output(0))\n\n    seed_val = draw_seed(seed)\n    seed_data = convert_to_numpy(seed_val)\n    rng = np.random.default_rng(seed_data)\n    random_values = rng.uniform(minval, maxval, size=shape).astype(dtype)\n    return OpenVINOKerasTensor(ov_opset.constant(random_values).output(0))\n```",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/random.py",
    "pr_number": 21670,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2353168494,
    "comment_created_at": "2025-09-16T17:21:24Z"
  },
  {
    "code": "@@ -1149,7 +1149,115 @@ def maximum(x1, x2):\n \n \n def median(x, axis=None, keepdims=False):\n-    raise NotImplementedError(\"`median` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_shape = x.get_partial_shape()\n+    rank = x_shape.rank.get_length()\n+    \n+    if rank == 0:\n+        return OpenVINOKerasTensor(x)\n+    \n+    # Handle axis=None by flattening the input\n+    original_shape = None\n+    flattened_all = False\n+    if axis is None:\n+        original_shape = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n+        x = ov_opset.reshape(x, [-1], False).output(0)\n+        axis = 0\n+        original_rank = rank\n+        rank = 1\n+        flattened_all = True\n+    else:\n+        # Handle tuple axis - for median, we only support single axis\n+        if isinstance(axis, (tuple, list)):\n+            if len(axis) != 1:\n+                raise ValueError(\"median only supports single axis reduction\")\n+            axis = axis[0]\n+        \n+        # Handle negative axis\n+        if axis < 0:\n+            axis = rank + axis\n+        original_rank = rank\n+    \n+    # Get the size of the dimension to sort\n+    shape_tensor = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n+    k = ov_opset.gather(\n+        shape_tensor,\n+        ov_opset.constant([axis], Type.i32).output(0),\n+        ov_opset.constant(0, Type.i32).output(0)\n+    ).output(0)\n+    \n+    # Convert k to a scalar value\n+    k_scalar = ov_opset.squeeze(k, [0]).output(0)\n+    \n+    # Use topk with k=size_of_axis to get all elements sorted\n+    topk_outputs = ov_opset.topk(\n+        x,\n+        k=k_scalar,\n+        axis=axis,\n+        mode=\"min\",\n+        sort=\"value\",\n+        stable=True\n+    )",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthis implementation does not handle cases where the reduction axis has a size of 0 (i.e., k=0). in such cases, numpy.median typically returns nan and raises a runtimewarning. the current logic will likely fail during graph execution because even_idx1 would be -1, leading to an attempt to gather from an empty tensor with a negative index.\n\nplease add logic to handle the k=0 case to align with numpy's behavior and prevent potential runtime errors. this could involve checking if k_scalar is zero and, if so, constructing a tensor of nans with the appropriate output shape.",
    "line_number": 1200,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -1149,7 +1149,115 @@ def maximum(x1, x2):\n \n \n def median(x, axis=None, keepdims=False):\n-    raise NotImplementedError(\"`median` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_shape = x.get_partial_shape()\n+    rank = x_shape.rank.get_length()\n+    \n+    if rank == 0:\n+        return OpenVINOKerasTensor(x)\n+    \n+    # Handle axis=None by flattening the input\n+    original_shape = None\n+    flattened_all = False\n+    if axis is None:\n+        original_shape = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n+        x = ov_opset.reshape(x, [-1], False).output(0)\n+        axis = 0\n+        original_rank = rank\n+        rank = 1\n+        flattened_all = True\n+    else:\n+        # Handle tuple axis - for median, we only support single axis\n+        if isinstance(axis, (tuple, list)):\n+            if len(axis) != 1:\n+                raise ValueError(\"median only supports single axis reduction\")\n+            axis = axis[0]\n+        \n+        # Handle negative axis\n+        if axis < 0:\n+            axis = rank + axis\n+        original_rank = rank\n+    \n+    # Get the size of the dimension to sort\n+    shape_tensor = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n+    k = ov_opset.gather(\n+        shape_tensor,\n+        ov_opset.constant([axis], Type.i32).output(0),\n+        ov_opset.constant(0, Type.i32).output(0)\n+    ).output(0)\n+    \n+    # Convert k to a scalar value\n+    k_scalar = ov_opset.squeeze(k, [0]).output(0)\n+    \n+    # Use topk with k=size_of_axis to get all elements sorted\n+    topk_outputs = ov_opset.topk(\n+        x,\n+        k=k_scalar,\n+        axis=axis,\n+        mode=\"min\",\n+        sort=\"value\",\n+        stable=True\n+    )\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThis implementation does not handle cases where the reduction axis has a size of 0 (i.e., `k=0`). In such cases, `numpy.median` typically returns `nan` and raises a `RuntimeWarning`. The current logic will likely fail during graph execution because `even_idx1` would be -1, leading to an attempt to gather from an empty tensor with a negative index.\n\nPlease add logic to handle the `k=0` case to align with NumPy's behavior and prevent potential runtime errors. This could involve checking if `k_scalar` is zero and, if so, constructing a tensor of `NaN`s with the appropriate output shape.",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21667,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2347932770,
    "comment_created_at": "2025-09-15T05:46:59Z"
  },
  {
    "code": "@@ -967,9 +967,30 @@ def isnan(x):\n \n \n def isneginf(x):\n-    raise NotImplementedError(\n-        \"`isneginf` is not supported with openvino backend\"\n-    )\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    \n+    if x_type.is_integral() or x_type == Type.boolean:\n+        shape = ov_opset.shape_of(x, \"i32\").output(0)\n+        false_const = ov_opset.constant(False, Type.boolean).output(0)\n+        return OpenVINOKerasTensor(ov_opset.broadcast(false_const, shape).output(0))\n+    \n+    if x_type == Type.bf16:\n+        x_f32 = ov_opset.convert(x, Type.f32).output(0)\n+        neg_inf = ov_opset.constant(-np.inf, Type.f32).output(0)\n+        is_neg_inf = ov_opset.equal(x_f32, neg_inf).output(0)\n+    else:\n+        if x_type == Type.f16:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f16).output(0)\n+        elif x_type == Type.f32:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f32).output(0)\n+        elif x_type == Type.f64:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f64).output(0)\n+        else:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f32).output(0)\n+        is_neg_inf = ov_opset.equal(x, neg_inf).output(0)\n+    \n+    return OpenVINOKerasTensor(is_neg_inf)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe implementation of isneginf can be significantly simplified and made more robust.\n\nfor integer and boolean types, you can use the existing zeros_like helper for better readability. for floating-point types, the openvino is_inf operation can directly detect negative infinity using attributes. this avoids manual comparisons and complex type handling, which also fixes a potential type mismatch bug in the current implementation.\n\nthis approach is cleaner and aligns with other functions in this file like nan_to_num. i've provided a suggested implementation.\n\npython\n    x = get_ov_output(x)\n    x_type = x.get_element_type()\n\n    if x_type.is_integral() or x_type == type.boolean:\n        # integers and booleans cannot be infinity.\n        return zeros_like(x, dtype=\"bool\")\n\n    # for floating point types, use is_inf with attributes.\n    is_neg_inf = ov_opset.is_inf(\n        x, attributes={\"detect_positive\": false, \"detect_negative\": true}\n    ).output(0)\n    return openvinokerastensor(is_neg_inf)",
    "line_number": 993,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -967,9 +967,30 @@ def isnan(x):\n \n \n def isneginf(x):\n-    raise NotImplementedError(\n-        \"`isneginf` is not supported with openvino backend\"\n-    )\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    \n+    if x_type.is_integral() or x_type == Type.boolean:\n+        shape = ov_opset.shape_of(x, \"i32\").output(0)\n+        false_const = ov_opset.constant(False, Type.boolean).output(0)\n+        return OpenVINOKerasTensor(ov_opset.broadcast(false_const, shape).output(0))\n+    \n+    if x_type == Type.bf16:\n+        x_f32 = ov_opset.convert(x, Type.f32).output(0)\n+        neg_inf = ov_opset.constant(-np.inf, Type.f32).output(0)\n+        is_neg_inf = ov_opset.equal(x_f32, neg_inf).output(0)\n+    else:\n+        if x_type == Type.f16:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f16).output(0)\n+        elif x_type == Type.f32:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f32).output(0)\n+        elif x_type == Type.f64:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f64).output(0)\n+        else:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f32).output(0)\n+        is_neg_inf = ov_opset.equal(x, neg_inf).output(0)\n+    \n+    return OpenVINOKerasTensor(is_neg_inf)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe implementation of `isneginf` can be significantly simplified and made more robust.\n\nFor integer and boolean types, you can use the existing `zeros_like` helper for better readability. For floating-point types, the OpenVINO `is_inf` operation can directly detect negative infinity using attributes. This avoids manual comparisons and complex type handling, which also fixes a potential type mismatch bug in the current implementation.\n\nThis approach is cleaner and aligns with other functions in this file like `nan_to_num`. I've provided a suggested implementation.\n\n```python\n    x = get_ov_output(x)\n    x_type = x.get_element_type()\n\n    if x_type.is_integral() or x_type == Type.boolean:\n        # Integers and booleans cannot be infinity.\n        return zeros_like(x, dtype=\"bool\")\n\n    # For floating point types, use is_inf with attributes.\n    is_neg_inf = ov_opset.is_inf(\n        x, attributes={\"detect_positive\": False, \"detect_negative\": True}\n    ).output(0)\n    return OpenVINOKerasTensor(is_neg_inf)\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21665,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2347334930,
    "comment_created_at": "2025-09-14T13:35:15Z"
  },
  {
    "code": "@@ -46,6 +46,10 @@ def subtract(x1, x2):\n     x1 = get_ov_output(x1, element_type)\n     x2 = get_ov_output(x2, element_type)\n     x1, x2 = _align_operand_types(x1, x2, \"subtract()\")\n+    if x1.get_element_type() == Type.boolean:\n+        return OpenVINOKerasTensor(\n+            ov_opset.logical_xor(x1, x2).output(0)\n+        )",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthis change correctly handles subtraction for two boolean tensors by using logical_xor. however, it introduces incorrect behavior for mixed-type operations, such as subtracting a numeric scalar from a boolean tensor.\n\nfor example, an operation like subtract(bool_tensor, 5) will be incorrectly computed. the current implementation's preceding boilerplate logic will cast the numeric literal 5 to a boolean (true), and then this new logic will apply logical_xor. the expected behavior, consistent with numpy, is to promote the boolean tensor to a numeric type and perform numeric subtraction, resulting in a numeric tensor.\n\nthis is a critical issue as it leads to incorrect results. the root cause lies in the operand preparation logic (lines 41-47) which is outside the scope of this change. a complete fix would require refactoring that section. given the constraint to only modify the added lines, it's hard to provide a suggestion, but this implementation is incorrect as it stands.",
    "line_number": 52,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -46,6 +46,10 @@ def subtract(x1, x2):\n     x1 = get_ov_output(x1, element_type)\n     x2 = get_ov_output(x2, element_type)\n     x1, x2 = _align_operand_types(x1, x2, \"subtract()\")\n+    if x1.get_element_type() == Type.boolean:\n+        return OpenVINOKerasTensor(\n+            ov_opset.logical_xor(x1, x2).output(0)\n+        )\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThis change correctly handles subtraction for two boolean tensors by using `logical_xor`. However, it introduces incorrect behavior for mixed-type operations, such as subtracting a numeric scalar from a boolean tensor.\n\nFor example, an operation like `subtract(bool_tensor, 5)` will be incorrectly computed. The current implementation's preceding boilerplate logic will cast the numeric literal `5` to a boolean (`True`), and then this new logic will apply `logical_xor`. The expected behavior, consistent with NumPy, is to promote the boolean tensor to a numeric type and perform numeric subtraction, resulting in a numeric tensor.\n\nThis is a critical issue as it leads to incorrect results. The root cause lies in the operand preparation logic (lines 41-47) which is outside the scope of this change. A complete fix would require refactoring that section. Given the constraint to only modify the added lines, it's hard to provide a suggestion, but this implementation is incorrect as it stands.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21664,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2346569383,
    "comment_created_at": "2025-09-13T12:52:19Z"
  },
  {
    "code": "@@ -193,6 +193,27 @@ def tree_unflatten(cls, metadata, children):\n         # For optree / dmtree\n         return cls(children)\n \n+    def torchtree_flatten(self):\n+        # For torchtree\n+        # Returns (values, metadata)\n+        return (self, None)",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe current implementation of torchtree_flatten returns (self, none). when torch.utils._pytree.tree_flatten processes a registered custom node, it expects the flatten function to return its children. by returning self, you are making the trackedlist instance a child of itself, which will lead to infinite recursion when tree_flatten is called. the flatten function should return the list's elements as children.",
    "line_number": 199,
    "enriched": "File: keras/src/utils/tracking.py\nCode: @@ -193,6 +193,27 @@ def tree_unflatten(cls, metadata, children):\n         # For optree / dmtree\n         return cls(children)\n \n+    def torchtree_flatten(self):\n+        # For torchtree\n+        # Returns (values, metadata)\n+        return (self, None)\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe current implementation of `torchtree_flatten` returns `(self, None)`. When `torch.utils._pytree.tree_flatten` processes a registered custom node, it expects the flatten function to return its children. By returning `self`, you are making the `TrackedList` instance a child of itself, which will lead to infinite recursion when `tree_flatten` is called. The flatten function should return the list's elements as children.\n\n```suggestion\n    def torchtree_flatten(self):\n        # For torchtree\n        # Returns (values, metadata)\n        return list(self), None\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "keras/src/utils/tracking.py",
    "pr_number": 21661,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2352654762,
    "comment_created_at": "2025-09-16T14:11:40Z"
  },
  {
    "code": "@@ -92,11 +92,14 @@ def _parameterize_data(self, data):\n         return parametrize_data\n \n     def _get_compiled_model(self, data):\n-        if (\n-            self.ov_compiled_model is not None\n-            and get_device() == self.ov_device\n-        ):\n-            return self.ov_compiled_model\n+        # OpenVINO compiled model cache is disabled for now.\n+        # For more information, please visit:\n+        # https://github.com/openvinotoolkit/openvino/issues/32045\n+        # if (\n+        #     self.ov_compiled_model is not None\n+        #     and get_device() == self.ov_device\n+        # ):\n+        #     return self.ov_compiled_model",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\ninstead of commenting out the caching logic, consider using a boolean flag to disable it. this approach is cleaner, self-documenting, and makes it trivial to re-enable the caching mechanism once the underlying bug is fixed. it also avoids leaving commented-out 'dead code' in the codebase.",
    "line_number": 102,
    "enriched": "File: keras/src/backend/openvino/trainer.py\nCode: @@ -92,11 +92,14 @@ def _parameterize_data(self, data):\n         return parametrize_data\n \n     def _get_compiled_model(self, data):\n-        if (\n-            self.ov_compiled_model is not None\n-            and get_device() == self.ov_device\n-        ):\n-            return self.ov_compiled_model\n+        # OpenVINO compiled model cache is disabled for now.\n+        # For more information, please visit:\n+        # https://github.com/openvinotoolkit/openvino/issues/32045\n+        # if (\n+        #     self.ov_compiled_model is not None\n+        #     and get_device() == self.ov_device\n+        # ):\n+        #     return self.ov_compiled_model\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nInstead of commenting out the caching logic, consider using a boolean flag to disable it. This approach is cleaner, self-documenting, and makes it trivial to re-enable the caching mechanism once the underlying bug is fixed. It also avoids leaving commented-out 'dead code' in the codebase.\n\n```suggestion\n        # OpenVINO compiled model cache is disabled for now.\n        # For more information, please visit:\n        # https://github.com/openvinotoolkit/openvino/issues/32045\n        use_cache = False\n        if (\n            use_cache\n            and self.ov_compiled_model is not None\n            and get_device() == self.ov_device\n        ):\n            return self.ov_compiled_model\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/openvino/trainer.py",
    "pr_number": 21659,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2341306299,
    "comment_created_at": "2025-09-11T14:59:44Z"
  },
  {
    "code": "@@ -2403,7 +2403,9 @@ def take(x, indices, axis=None):\n \n     def fix_negative_indices(i):\n         # Correct the indices using \"fill\" mode which is the same as in jax\n-        return tf.where(i < 0, i + tf.cast(tf.shape(x)[axis], i.dtype), i)\n+        return tf.where(\n+            i < 0, tf.cast(i + tf.cast(tf.shape(x)[axis], i.dtype), i.dtype), i\n+        )",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nwhile this fix is correct, the implementation is a bit dense. refactoring it to use an intermediate variable for the corrected indices would make the code more readable and easier to maintain. this also provides a good place to add a comment explaining why this explicit cast is necessary, which is helpful for future contributors.",
    "line_number": 2408,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -2403,7 +2403,9 @@ def take(x, indices, axis=None):\n \n     def fix_negative_indices(i):\n         # Correct the indices using \"fill\" mode which is the same as in jax\n-        return tf.where(i < 0, i + tf.cast(tf.shape(x)[axis], i.dtype), i)\n+        return tf.where(\n+            i < 0, tf.cast(i + tf.cast(tf.shape(x)[axis], i.dtype), i.dtype), i\n+        )\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nWhile this fix is correct, the implementation is a bit dense. Refactoring it to use an intermediate variable for the corrected indices would make the code more readable and easier to maintain. This also provides a good place to add a comment explaining why this explicit cast is necessary, which is helpful for future contributors.\n\n```suggestion\n        positive_i = tf.cast(i + tf.cast(tf.shape(x)[axis], i.dtype), i.dtype)\n        # Explicitly cast back to original dtype to fix TF casting issue for int64.\n        return tf.where(i < 0, positive_i, i)\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21656,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2337351878,
    "comment_created_at": "2025-09-10T16:57:21Z"
  },
  {
    "code": "@@ -442,6 +442,7 @@ def quantize(self, mode, config=None, **kwargs):\n                     \"`keras.quantizers.GPTQConfig`.\"\n                 )\n             gptq_quantize(self, config)\n+            self._post_quantize(mode, **kwargs)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nas the _post_quantize method does not use the mode and **kwargs parameters, this call should be simplified by removing them. this change depends on the suggested refactoring of the _post_quantize method itself.",
    "line_number": 445,
    "enriched": "File: keras/src/models/model.py\nCode: @@ -442,6 +442,7 @@ def quantize(self, mode, config=None, **kwargs):\n                     \"`keras.quantizers.GPTQConfig`.\"\n                 )\n             gptq_quantize(self, config)\n+            self._post_quantize(mode, **kwargs)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nAs the `_post_quantize` method does not use the `mode` and `**kwargs` parameters, this call should be simplified by removing them. This change depends on the suggested refactoring of the `_post_quantize` method itself.\n\n```suggestion\n            self._post_quantize()\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/models/model.py",
    "pr_number": 21654,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2336821105,
    "comment_created_at": "2025-09-10T13:44:53Z"
  },
  {
    "code": "@@ -2302,8 +2302,11 @@ def searchsorted(sorted_sequence, values, side=\"left\"):\n             \"to extend it to N-D sequences. Received: \"\n             f\"sorted_sequence.shape={sorted_sequence.shape}\"\n         )\n+    sequence_len = sorted_sequence.shape[0]\n     out_type = (\n-        \"int32\" if len(sorted_sequence) <= np.iinfo(np.int32).max else \"int64\"\n+        \"int32\"\n+        if sequence_len is None or sequence_len <= np.iinfo(np.int32).max\n+        else \"int64\"",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\ndefaulting to int32 when sequence_len is none is risky. if the tensor at runtime has a length greater than int32.max, this could lead to an integer overflow, which can be a silent error. it would be safer to default to int64 when the length is unknown to prevent potential data corruption. this would also align better with the behavior of numpy.searchsorted which typically returns int64 on 64-bit systems.",
    "line_number": 2309,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -2302,8 +2302,11 @@ def searchsorted(sorted_sequence, values, side=\"left\"):\n             \"to extend it to N-D sequences. Received: \"\n             f\"sorted_sequence.shape={sorted_sequence.shape}\"\n         )\n+    sequence_len = sorted_sequence.shape[0]\n     out_type = (\n-        \"int32\" if len(sorted_sequence) <= np.iinfo(np.int32).max else \"int64\"\n+        \"int32\"\n+        if sequence_len is None or sequence_len <= np.iinfo(np.int32).max\n+        else \"int64\"\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nDefaulting to `int32` when `sequence_len` is `None` is risky. If the tensor at runtime has a length greater than `int32.max`, this could lead to an integer overflow, which can be a silent error. It would be safer to default to `int64` when the length is unknown to prevent potential data corruption. This would also align better with the behavior of `numpy.searchsorted` which typically returns `int64` on 64-bit systems.\n\n```suggestion\n        \"int32\"\n        if sequence_len is not None and sequence_len <= np.iinfo(np.int32).max\n        else \"int64\"\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21652,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2334943095,
    "comment_created_at": "2025-09-09T22:19:55Z"
  },
  {
    "code": "@@ -34,7 +34,8 @@ class Iterator(PyDataset):\n \n     white_list_formats = (\"png\", \"jpg\", \"jpeg\", \"bmp\", \"ppm\", \"tif\", \"tiff\")\n \n-    def __init__(self, n, batch_size, shuffle, seed):\n+    def __init__(self, n, batch_size, shuffle, seed, **kwargs):",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nwhile adding **kwargs and calling super().__init__ is correct, the class docstring for iterator should also be updated to reflect the newly available arguments from pydataset (workers, use_multiprocessing, max_queue_size). this improves api discoverability for users.[^1]\n\n#### style guide references\n[^1]: the style guide emphasizes that documentation is an integral part of the api and key to a good user experience. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#l137-l137)",
    "line_number": 37,
    "enriched": "File: keras/src/legacy/preprocessing/image.py\nCode: @@ -34,7 +34,8 @@ class Iterator(PyDataset):\n \n     white_list_formats = (\"png\", \"jpg\", \"jpeg\", \"bmp\", \"ppm\", \"tif\", \"tiff\")\n \n-    def __init__(self, n, batch_size, shuffle, seed):\n+    def __init__(self, n, batch_size, shuffle, seed, **kwargs):\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nWhile adding `**kwargs` and calling `super().__init__` is correct, the class docstring for `Iterator` should also be updated to reflect the newly available arguments from `PyDataset` (`workers`, `use_multiprocessing`, `max_queue_size`). This improves API discoverability for users.[^1]\n\n#### Style Guide References\n[^1]: The style guide emphasizes that documentation is an integral part of the API and key to a good user experience. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#L137-L137)",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/legacy/preprocessing/image.py",
    "pr_number": 21651,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2334505828,
    "comment_created_at": "2025-09-09T19:01:25Z"
  },
  {
    "code": "@@ -80,6 +80,12 @@ def export_onnx(\n                 \"The model provided has never called. \"\n                 \"It must be called at least once before export.\"\n             )\n+    input_names = []\n+    for i, spec in enumerate(input_signature):\n+        name = getattr(spec, \"name\", None)\n+        if name is None:\n+            name = f\"input_{i}\"\n+        input_names.append(name)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis logic for creating input_names can be made more concise and pythonic by using a list comprehension. this also improves robustness by handling cases where spec.name might be an empty string, not just none.\n\npython\n    input_names = [\n        getattr(spec, \"name\", none) or f\"input_{i}\"\n        for i, spec in enumerate(input_signature)\n    ]",
    "line_number": 88,
    "enriched": "File: keras/src/export/onnx.py\nCode: @@ -80,6 +80,12 @@ def export_onnx(\n                 \"The model provided has never called. \"\n                 \"It must be called at least once before export.\"\n             )\n+    input_names = []\n+    for i, spec in enumerate(input_signature):\n+        name = getattr(spec, \"name\", None)\n+        if name is None:\n+            name = f\"input_{i}\"\n+        input_names.append(name)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis logic for creating `input_names` can be made more concise and Pythonic by using a list comprehension. This also improves robustness by handling cases where `spec.name` might be an empty string, not just `None`.\n\n```python\n    input_names = [\n        getattr(spec, \"name\", None) or f\"input_{i}\"\n        for i, spec in enumerate(input_signature)\n    ]\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/export/onnx.py",
    "pr_number": 21646,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2332579763,
    "comment_created_at": "2025-09-09T08:52:06Z"
  },
  {
    "code": "@@ -594,7 +594,7 @@ def test_quantize_gptq_combinations(self, dataset, config):\n         kl = _mean_kl(p_ref, p_q)\n \n         self.assertGreaterEqual(\n-            top1_match, 0.6, f\"Top-1 agreement too low: {top1_match:.3f}\"\n+            top1_match, 0.5, f\"Top-1 agreement too low: {top1_match:.3f}\"",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nfor better readability and maintainability, consider defining 0.5 as a named constant, for example top1_match_threshold. this makes the purpose of the value clearer and easier to adjust in the future if needed. the same could be applied to the kl divergence threshold 0.50 in the next assertion for consistency.",
    "line_number": 597,
    "enriched": "File: keras/src/quantizers/gptq_test.py\nCode: @@ -594,7 +594,7 @@ def test_quantize_gptq_combinations(self, dataset, config):\n         kl = _mean_kl(p_ref, p_q)\n \n         self.assertGreaterEqual(\n-            top1_match, 0.6, f\"Top-1 agreement too low: {top1_match:.3f}\"\n+            top1_match, 0.5, f\"Top-1 agreement too low: {top1_match:.3f}\"\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nFor better readability and maintainability, consider defining `0.5` as a named constant, for example `TOP1_MATCH_THRESHOLD`. This makes the purpose of the value clearer and easier to adjust in the future if needed. The same could be applied to the KL divergence threshold `0.50` in the next assertion for consistency.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/quantizers/gptq_test.py",
    "pr_number": 21645,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2331941610,
    "comment_created_at": "2025-09-09T03:55:25Z"
  },
  {
    "code": "@@ -0,0 +1,1918 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Test suite for model verification with Parallax (Keras AutoShardDistribution),\n+using KerasNLP presets and the Tiny Shakespeare dataset.\n+\n+This script compares the training performance (loss and perplexity) of a\n+baseline model against its Parallax-sharded equivalent. The goal is to verify\n+the correctness and performance of Keras's automatic sharding capabilities.\n+\n+It is generalized to run tests for multiple model architectures, including:\n+- Gemma\n+- GPT-2\n+- Bloom\n+- OPT\n+\"\"\"\n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"\n+os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=2\"\n+\n+# Add these imports at the top of your script\n+import psutil\n+import os\n+\n+# Get the current process to monitor its memory. Place this near the top.\n+PROCESS = psutil.Process(os.getpid())\n+\n+# Replace your ENTIRE old log_memory_usage function with this new one.\n+def log_memory_usage(stage_name):\n+    \"\"\"\n+    Logs the Resident Set Size (RSS) of the current process using psutil.\n+    This is compatible with CPU, GPU, and other backends.\n+    \"\"\"\n+    print(f\"\\n--- Process Memory Usage at: {stage_name} ---\")\n+    mem_info = PROCESS.memory_info()\n+    rss_mb = mem_info.rss / 1024 / 1024  # rss is the Resident Set Size\n+    # print(f\"  Process RSS: {rss_mb:.2f} MB\")\n+    # print(\"-\" * 45)\n+\n+\"\"\"Utilities for distribution strategy with JAX backend.\"\"\"\n+class MergeableGraph:\n+    \"\"\"A graph that supports merging nodes.\"\"\"\n+\n+    def __init__(self):\n+        self._parent = {}\n+        self._edges = set()\n+\n+    def get_root(self, node):\n+        if node not in self._parent:\n+            self._parent[node] = node\n+            return node\n+        if self._parent[node] == node:\n+            return node\n+        self._parent[node] = self.get_root(self._parent[node])\n+        return self._parent[node]\n+\n+    def merge_nodes(self, node1, node2):\n+        root1 = self.get_root(node1)\n+        root2 = self.get_root(node2)\n+        if root1 != root2:\n+            self._parent[root1] = root2\n+\n+    def add_edge(self, node1, node2):\n+        root1 = self.get_root(node1)\n+        root2 = self.get_root(node2)\n+        if root1 != root2:\n+            self._edges.add(tuple(sorted((root1, root2))))\n+\n+    def get_edges(self):\n+        return self._edges\n+    \n+import jax\n+import numpy as np\n+import collections\n+import itertools\n+from keras.src.backend.common import global_state\n+from keras.src.random import seed_generator\n+from keras.src.utils import jax_utils\n+from keras.src.utils import rng_utils\n+from jax import tree_util\n+\n+\n+def list_devices(device_type=None):\n+    \"\"\"Return all the available devices based on the device type.\n+\n+    Note that this should return the global devices in a distributed setting.\n+\n+    Args:\n+        device_type: string of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`. Defaults to `\"gpu\"`\n+            or `\"tpu\"` if available when device_type is not provided. Otherwise\n+            will return the `\"cpu\"` devices.\n+\n+    Return:\n+        List of devices that are available for distribute computation.\n+    \"\"\"\n+    device_type = device_type.lower() if device_type else None\n+    jax_devices = jax.devices(backend=device_type)\n+    return [f\"{device.platform}:{device.id}\" for device in jax_devices]\n+\n+\n+def distribute_variable(value, layout):\n+    \"\"\"Create a distributed variable for JAX.\n+\n+    Since JAX doesn't have a variable class, this will just return a `jax.Array`\n+    with the corresponding layout/sharding specified.\n+\n+    Note that this function should be used in eager context, not in jitted\n+    function.\n+\n+    Args:\n+        value: the initial value of the variable.\n+        layout: `TensorLayout` for the created variable, or a\n+            JAX-supported layout instance (e.g. `jax.sharding.Sharding`).\n+\n+    Returns:\n+        jax.Array which is the distributed variable.\n+    \"\"\"\n+    return distribute_tensor(value, layout)\n+\n+\n+def distribute_tensor(tensor, layout):\n+    \"\"\"Distribute the tensor based on the layout.\n+\n+    Note that this function can be used both in eager context, or within a\n+    jitted function.\n+\n+    Args:\n+        tensor: `jax.Array` that need to be distributed.\n+        layout: `TensorLayout` for the created variable, or a\n+            JAX-supported layout instance (e.g. `jax.sharding.Sharding`).\n+\n+    Returns:\n+        Distributed value.\n+    \"\"\"\n+    # Avoid circular imports.\n+    from keras.src.distribution import TensorLayout\n+\n+    if isinstance(layout, TensorLayout):\n+        layout = layout.backend_layout\n+\n+    # TODO(scottzhu): This might not be a cheap check, we should consider\n+    # have some proper JAX API for doing this check.\n+    if jax_utils.is_in_jax_tracing_scope():\n+        return jax.lax.with_sharding_constraint(tensor, layout)\n+\n+    # Skip relayout if unnecessary.\n+    if isinstance(tensor, jax.Array):\n+        if isinstance(\n+            layout, jax.sharding.Sharding\n+        ) and tensor.sharding.is_equivalent_to(layout, ndim=len(tensor.shape)):\n+            return tensor\n+        # JAX explicit \"layout\" support.\n+        elif hasattr(layout, \"layout\"):\n+            current_layout = getattr(tensor, \"layout\", None)\n+            if current_layout == layout:\n+                return tensor\n+        # JAX explicit \"format\" support.\n+        elif hasattr(layout, \"format\"):\n+            current_layout = getattr(tensor, \"format\", None)\n+            if current_layout == layout:\n+                return tensor\n+\n+    return jax.device_put(tensor, layout)\n+\n+\n+def distribute_data_input(per_process_batch, layout, batch_dim_name):\n+    \"\"\"Distribute the input data with the corresponding layout.\n+\n+    Note that the inputs here is a local worker batch. Within the local worker,\n+    the data need to be further partitioned to map to each of the devices.\n+\n+    Args:\n+        inputs: `jax.Array` that is already sharded to a local process size.\n+        layout: `TensorLayout` for the distribution information, or a\n+            `jax.sharding.Sharding` instance.\n+\n+    Returns:\n+        A global batch distributed according to `layout`.\n+    \"\"\"\n+    # Avoid circular imports.\n+    from keras.src.distribution import TensorLayout\n+\n+    if isinstance(layout, TensorLayout):\n+        layout = layout.backend_layout\n+\n+    return jax.make_array_from_process_local_data(layout, per_process_batch)\n+\n+\n+def initialize_rng():\n+    \"\"\"Initializes the global random number generator across processes.\n+\n+    This is required for consistent initialization in multi-host settings.\n+    \"\"\"\n+    global_seed = rng_utils.get_random_seed()\n+    # Only set a random seed if not already set\n+    # via keras.config.set_random_seed()\n+    if global_seed is None:\n+        # Generate a random seed on each CPU host and psum them to get a single\n+        # consistent seed across all processes.\n+        cpu_devices = jax.devices(\"cpu\")\n+        num_local_cpu_devices = jax.local_device_count(\"cpu\")\n+        # Seed must be in range [0, 2^32 - 1], so to ensure proper range and\n+        # avoid signed integer overflow, we use uint32.\n+        local_seed = jax.numpy.asarray(\n+            [seed_generator.make_default_seed()] * num_local_cpu_devices,\n+            dtype=jax.numpy.uint32,\n+        )\n+        # Sum across processes and pull out the first item.\n+        global_seed = jax.pmap(\n+            lambda x: jax.lax.psum(x, \"all\"),\n+            axis_name=\"all\",\n+            devices=cpu_devices,\n+        )(local_seed).item(0)\n+        # Set the global seed.\n+        rng_utils.set_random_seed(global_seed)\n+\n+    # Check if the global seed generator is set and ensure it has an initialized\n+    # seed.  Otherwise, reset the seed to the global seed.\n+    global_seed_generator = global_state.get_global_attribute(\n+        \"global_seed_generator\"\n+    )\n+    if global_seed_generator is not None:\n+        seed = global_seed_generator.get_config()[\"seed\"]\n+        if seed is None:\n+            global_state.set_global_attribute(\n+                \"global_seed_generator\",\n+                seed_generator.SeedGenerator(\n+                    seed=global_seed,\n+                    name=global_seed_generator.name,\n+                    backend=global_seed_generator.backend,\n+                ),\n+            )\n+\n+\n+def initialize(job_addresses, num_processes, process_id):\n+    if job_addresses and \",\" in job_addresses:\n+        # When user provide all the job addresses, we will split and get the\n+        # first one, which is the coordinator.\n+        job_addresses = job_addresses.split(\",\")\n+        # Do a sanity check to make sure the number of addresses also match\n+        # the num_processes.\n+        if num_processes is not None and num_processes != len(job_addresses):\n+            raise ValueError(\n+                f\"The provided job_addresses {job_addresses} has \"\n+                f\"{len(job_addresses)} jobs, but num_processes is \"\n+                f\"{num_processes}\"\n+            )\n+        coordinator_address = job_addresses[0]\n+    else:\n+        coordinator_address = job_addresses\n+\n+    jax.distributed.initialize(\n+        coordinator_address=coordinator_address,\n+        num_processes=num_processes,\n+        process_id=process_id,\n+    )\n+\n+    # Ensure the random number generator is initialized across processes.\n+    initialize_rng()\n+\n+\n+def num_processes():\n+    \"\"\"Return the number of processes for the current distribution setting.\"\"\"\n+    return jax.process_count()\n+\n+\n+def process_id():\n+    \"\"\"Return the current process ID for the distribution setting.\"\"\"\n+    return jax.process_index()\n+\n+\n+def _to_backend_device(device_name):\n+    if isinstance(device_name, jax.Device):\n+        return device_name\n+    device_name = str(device_name)\n+    if \":\" not in device_name:\n+        device_type, device_id = device_name, 0\n+    else:\n+        device_type, device_id = device_name.split(\":\")\n+\n+    devices = jax.devices(backend=device_type)\n+    for device in devices:\n+        if device.platform == device_type and device.id == int(device_id):\n+            return device\n+    raise ValueError(f\"Device not found: {device_name}\")\n+\n+\n+def _to_backend_mesh(device_mesh):\n+    \"\"\"Convert the DeviceMesh to JAX backend specific Mesh.\n+\n+    Args:\n+        device_mesh: DeviceMesh instance to convert.\n+\n+    Returns:\n+        A `jax.sharding.Mesh` instance.\n+    \"\"\"\n+    shape = device_mesh.devices.shape\n+    devices = [_to_backend_device(d) for d in device_mesh.devices.flatten()]\n+    devices = np.array(devices).reshape(shape)\n+    return jax.sharding.Mesh(devices, device_mesh.axis_names)\n+\n+\n+def _to_backend_layout(tensor_layout):\n+    \"\"\"Convert the TensorLayout to JAX backend specific Sharding.\n+\n+    Args:\n+        tensor_layout: TensorLayout instance to convert.\n+\n+    Returns:\n+        A `jax.sharding.NamedSharding` instance.\n+    \"\"\"\n+    if tensor_layout.device_mesh is None:\n+        raise ValueError(\n+            \"Cannot create sharding when device mesh is not set \"\n+            \"for TensorLayout.\"\n+        )\n+    partition_spec = jax.sharding.PartitionSpec(*tensor_layout.axes)\n+    jax_mesh = tensor_layout.device_mesh.backend_mesh\n+    return jax.sharding.NamedSharding(jax_mesh, partition_spec)\n+\n+\n+_JAX_CLASSES_DEFINED = False\n+JaxGraph = None\n+JaxShardingPlanner = None\n+JaxShardApplier = None\n+\n+\n+def _define_and_register_jax_classes():\n+    global _JAX_CLASSES_DEFINED, JaxGraph, JaxShardingPlanner, JaxShardApplier\n+    if _JAX_CLASSES_DEFINED:\n+        return\n+\n+    # from keras.src.distribution.autoshard_utils import MergeableGraph\n+\n+    def parse_jaxpr(jaxpr) -> MergeableGraph:\n+        graph = MergeableGraph()\n+\n+        def same_axis(node1, node2):\n+            var1, axis1 = node1\n+            var2, axis2 = node2\n+            if var1.aval.shape[axis1] != var2.aval.shape[axis2]:\n+                return\n+            graph.merge_nodes(node1, node2)\n+\n+        def parse_dot_general(eqn):\n+            lhs, rhs = eqn.invars\n+            out = eqn.outvars[0]\n+            (lc, rc), (lb, rb) = eqn.params[\"dimension_numbers\"]\n+            for l, r in zip(lc, rc):\n+                same_axis((lhs, l), (rhs, r))\n+            o_offset = 0\n+            for l, r in zip(lb, rb):\n+                same_axis((lhs, l), (rhs, r))\n+                same_axis((lhs, l), (out, o_offset))\n+                o_offset += 1\n+            for i in range(lhs.aval.ndim):\n+                if i not in lb and i not in lc:\n+                    same_axis((lhs, i), (out, o_offset))\n+                    o_offset += 1\n+            for j in range(rhs.aval.ndim):\n+                if j not in rb and j not in rc:\n+                    same_axis((rhs, j), (out, o_offset))\n+                    o_offset += 1\n+\n+        def parse_reshape(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            in_idx, out_idx, in_prod, out_prod = 0, 0, 1, 1\n+            while in_idx < invar.aval.ndim and out_idx < out.aval.ndim:\n+                if (\n+                    in_prod == out_prod\n+                    and invar.aval.shape[in_idx] == out.aval.shape[out_idx]\n+                ):\n+                    if invar.aval.shape[in_idx] > 1:\n+                        same_axis((invar, in_idx), (out, out_idx))\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    out_prod *= out.aval.shape[out_idx]\n+                    in_idx += 1\n+                    out_idx += 1\n+                elif in_prod < out_prod:\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    in_idx += 1\n+                else:\n+                    out_prod *= out.aval.shape[out_idx]\n+                    out_idx += 1\n+\n+        def parse_transpose(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            for i, j in enumerate(eqn.params[\"permutation\"]):\n+                same_axis((invar, j), (out, i))\n+\n+        def parse_elementwise_with_broadcast(eqn):\n+            out = eqn.outvars[0]\n+            for invar in eqn.invars:\n+                if invar.aval.ndim == 0:\n+                    continue\n+                for i in range(1, min(invar.aval.ndim, out.aval.ndim) + 1):\n+                    in_axis, out_axis = -i, -i\n+                    if invar.aval.shape[in_axis] == out.aval.shape[out_axis]:\n+                        same_axis(\n+                            (invar, invar.aval.ndim + in_axis),\n+                            (out, out.aval.ndim + out_axis),\n+                        )\n+\n+        for var in jaxpr.jaxpr.invars:\n+            for i, j in itertools.combinations(range(var.aval.ndim), 2):\n+                graph.add_edge((var, i), (var, j))\n+\n+        for eqn in jaxpr.eqns:\n+            for outvar in eqn.outvars:\n+                for i, j in itertools.combinations(range(outvar.aval.ndim), 2):\n+                    graph.add_edge((outvar, i), (outvar, j))\n+\n+            primitive_parsers = {\n+                \"dot_general\": parse_dot_general,\n+                \"reshape\": parse_reshape,\n+                \"transpose\": parse_transpose,\n+            }\n+            parser = primitive_parsers.get(\n+                eqn.primitive.name, parse_elementwise_with_broadcast\n+            )\n+            parser(eqn)\n+        return graph\n+\n+    def shard_model(\n+        jaxpr,\n+        out_avals,\n+        trainable_params,\n+        non_trainable_params,\n+        args,\n+        kwargs,\n+        min_shard_size=1,\n+        data_axis_name=\"data\",\n+        model_axis_name=\"model\",\n+    ):\n+        graph = parse_jaxpr(jaxpr)\n+\n+        t_params_flat, t_params_treedef = tree_util.tree_flatten(\n+            trainable_params\n+        )\n+        nt_params_flat, nt_params_treedef = tree_util.tree_flatten(\n+            non_trainable_params\n+        )\n+        args_flat, args_treedef = tree_util.tree_flatten(args)\n+        kwargs_flat, kwargs_treedef = tree_util.tree_flatten(kwargs)\n+        _, outputs_treedef = tree_util.tree_flatten(out_avals)\n+\n+        pos = 0\n+        t_param_invars = jaxpr.jaxpr.invars[pos : pos + len(t_params_flat)]\n+        pos += len(t_params_flat)\n+        nt_param_invars = jaxpr.jaxpr.invars[pos : pos + len(nt_params_flat)]\n+        pos += len(nt_params_flat)\n+        arg_invars = jaxpr.jaxpr.invars[pos : pos + len(args_flat)]\n+        pos += len(args_flat)\n+        kwarg_invars = jaxpr.jaxpr.invars[pos:]\n+\n+        all_param_invars = t_param_invars + nt_param_invars\n+        data_invars = arg_invars + kwarg_invars\n+\n+        seen = collections.Counter()\n+        for var in all_param_invars:\n+            for i in range(var.aval.ndim):\n+                if var.aval.shape[i] >= min_shard_size:\n+                    seen.update([graph.get_root((var, i))])\n+\n+        model_axis_root = max(seen, key=seen.get) if seen else None\n+\n+        data_axes_roots = []\n+        for var in data_invars:\n+            for i in range(var.aval.ndim):\n+                root = graph.get_root((var, i))\n+                if root not in seen and root not in data_axes_roots:\n+                    data_axes_roots.append(root)\n+\n+        def assign_layouts(vars_flat, is_params=False):\n+            assignments = []\n+            for var in vars_flat:\n+                layout = [None] * var.aval.ndim\n+                for i in range(var.aval.ndim):\n+                    if var.aval.shape[i] < min_shard_size:\n+                        continue\n+                    root = graph.get_root((var, i))\n+                    if (\n+                        is_params\n+                        and model_axis_root\n+                        and root == model_axis_root\n+                    ):\n+                        layout[i] = model_axis_name\n+                    elif not is_params and root in data_axes_roots:\n+                        name = data_axis_name\n+                        if len(data_axes_roots) > 1:\n+                            name += str(data_axes_roots.index(root))\n+                        layout[i] = name\n+                assignments.append(layout)\n+            return assignments\n+\n+        params_assignments = tree_util.tree_unflatten(\n+            t_params_treedef, assign_layouts(t_param_invars, is_params=True)\n+        )\n+        return params_assignments\n+\n+    class _JaxGraph:\n+        def __init__(\n+            self,\n+            jaxpr,\n+            trainable_variables,\n+            non_trainable_variables,\n+            in_treedefs,\n+            out_avals,\n+        ):\n+            self.jaxpr = jaxpr\n+            self.trainable_variables = trainable_variables\n+            self.non_trainable_variables = non_trainable_variables\n+            self.in_treedefs = in_treedefs\n+            self.out_avals = out_avals\n+\n+        @classmethod\n+        def from_model(cls, model, *args, **kwargs):\n+            def stateless_fn(\n+                trainable_vars, non_trainable_vars, f_args, f_kwargs\n+            ):\n+                return model.stateless_call(\n+                    trainable_vars, non_trainable_vars, *f_args, **f_kwargs\n+                )\n+\n+            trainable_vars = model.trainable_variables\n+            non_trainable_vars = model.non_trainable_variables\n+            in_treedefs = tree_util.tree_structure(\n+                (trainable_vars, non_trainable_vars, args, kwargs)\n+            )\n+\n+            closed_jaxpr, out_avals = jax.make_jaxpr(\n+                stateless_fn, return_shape=True\n+            )(trainable_vars, non_trainable_vars, args, kwargs)\n+\n+            return cls(\n+                closed_jaxpr,\n+                trainable_vars,\n+                non_trainable_vars,\n+                in_treedefs,\n+                out_avals,\n+            )\n+\n+    class _JaxShardingPlanner:\n+        def plan(self, graph, device_mesh):\n+            all_in_avals = [var.aval for var in graph.jaxpr.jaxpr.invars]\n+            all_in_leaves = tree_util.tree_unflatten(\n+                graph.in_treedefs, all_in_avals\n+            )\n+            _, _, args_aval_tree, kwargs_aval_tree = all_in_leaves\n+\n+            dummy_args = tree_util.tree_map(\n+                lambda x: np.zeros(x.shape, x.dtype), args_aval_tree\n+            )\n+            dummy_kwargs = tree_util.tree_map(\n+                lambda x: np.zeros(x.shape, x.dtype), kwargs_aval_tree\n+            )\n+\n+            param_assignments = shard_model(\n+                jaxpr=graph.jaxpr,\n+                out_avals=graph.out_avals,\n+                trainable_params=graph.trainable_variables,\n+                non_trainable_params=graph.non_trainable_variables,\n+                args=dummy_args,\n+                kwargs=dummy_kwargs,\n+            )\n+\n+            param_vars_flat, _ = tree_util.tree_flatten(\n+                graph.trainable_variables\n+            )\n+            param_layouts_flat, _ = tree_util.tree_flatten(param_assignments)\n+\n+            parameter_layout_dict = {\n+                var.path: tuple(layout) if layout else None\n+                for var, layout in zip(param_vars_flat, param_layouts_flat)\n+            }\n+            return parameter_layout_dict\n+\n+    class _JaxShardApplier:\n+        def apply(self, model, plan):\n+            for var in model.variables:\n+                layout = plan.get(var.path)\n+                if layout:\n+                    var.layout = layout\n+\n+    JaxGraph = _JaxGraph\n+    JaxShardingPlanner = _JaxShardingPlanner\n+    JaxShardApplier = _JaxShardApplier\n+    _JAX_CLASSES_DEFINED = True\n+\n+\n+def get_sharding_planner():\n+    \"\"\"Returns an instance of the JAX sharding planner.\"\"\"\n+    _define_and_register_jax_classes()\n+    return JaxShardingPlanner()\n+\n+\n+def get_shard_applier():\n+    \"\"\"Returns an instance of the JAX shard applier.\"\"\"\n+    _define_and_register_jax_classes()\n+    return JaxShardApplier()\n+\n+\n+def create_graph_from_model(model, *args, **kwargs):\n+    \"\"\"Returns a JAX graph representation of the Keras model.\"\"\"\n+    _define_and_register_jax_classes()\n+    return JaxGraph.from_model(model, *args, **kwargs)\n+\n+\n+import os\n+import time\n+import logging\n+import numpy as np\n+import keras\n+import keras_nlp\n+import matplotlib.pyplot as plt\n+import tensorflow as tf\n+import tensorflow_datasets as tfds\n+import jax\n+from keras.src.distribution import distribution_lib\n+# --- Basic Configuration ---\n+logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n+logger = logging.getLogger(__name__)\n+import collections\n+import contextlib\n+import os\n+import re\n+import warnings\n+\n+import numpy as np\n+\n+from keras.src.api_export import keras_export\n+from keras.src.backend import KerasTensor\n+from keras.src.backend import distribution_lib\n+from keras.src.backend.common import global_state\n+\"\"\"Unified high-level distribution APIs across backends.\n+\n+Currently only the JAX backend is supported. The TensorFlow backend\n+will be supported in the future (via tf.dtensor API).\n+\"\"\"\n+\n+import collections\n+import contextlib\n+import os\n+import re\n+import warnings\n+\n+import numpy as np\n+\n+from keras.src.api_export import keras_export\n+from keras.src.backend import KerasTensor\n+from keras.src.backend import distribution_lib\n+from keras.src.backend.common import global_state\n+\n+DEFAULT_BATCH_DIM_NAME = \"batch\"\n+GLOBAL_ATTRIBUTE_NAME = \"distribution\"\n+\n+\n+@keras_export(\"keras.distribution.list_devices\")\n+def list_devices(device_type=None):\n+    \"\"\"Return all the available devices based on the device type.\n+\n+    Note: in a distributed setting, global devices are returned.\n+\n+    Args:\n+        device_type: string, one of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`.\n+            Defaults to `\"gpu\"` or `\"tpu\"` if available when\n+            `device_type` is not provided. Otherwise\n+            will return the `\"cpu\"` devices.\n+\n+    Return:\n+        List of devices that are available for distribute computation.\n+    \"\"\"\n+    return list_devices(device_type)\n+\n+\n+@keras_export(\"keras.distribution.initialize\")\n+def initialize(job_addresses=None, num_processes=None, process_id=None):\n+    \"\"\"Initialize the distribution system for multi-host/process setting.\n+\n+    Calling `initialize` will prepare the backend for execution on multi-host\n+    GPU or TPUs. It should be called before any computations.\n+\n+    Note that the parameters can also be injected via environment variables,\n+    which can be better controlled by the launch script at startup time.\n+    For certain backend that also rely on the environment variables to\n+    configure, Keras will properly forward them.\n+\n+    Args:\n+        job_addresses: string. Comma separated IP addresses for all the jobs\n+            that will form the whole computation cluster. Note that for JAX\n+            backend, only the address for job 0 (coodinator) is needed. For\n+            certain runtime like cloud TPU, this value can be `None`, and the\n+            backend will figure it out with the TPU environment variables. You\n+            can also config this value via environment variable\n+            `KERAS_DISTRIBUTION_JOB_ADDRESSES`.\n+        num_processes: int. The number of worker/processes that will form the\n+            whole computation cluster. For certain runtime like cloud TPU, this\n+            value can be `None`, and the backend will figure it out with the TPU\n+            environment variables. You can also configure this value via\n+            environment variable `KERAS_DISTRIBUTION_NUM_PROCESSES`.\n+        process_id: int. The ID number of the current worker/process. The value\n+            should be ranged from `0` to `num_processes - 1`. `0` will indicate\n+            the current worker/process is the master/coordinate job. You can\n+            also configure this value via environment variable\n+            `KERAS_DISTRIBUTION_PROCESS_ID`.\n+\n+        Example:\n+            Suppose there are two GPU processes, and process 0 is running at\n+            address `10.0.0.1:1234`, and process 1 is running at address\n+            `10.0.0.2:2345`. To configure such cluster, you can run\n+\n+        On process 0:\n+        ```python\n+        keras.distribute.initialize(\n+            job_addresses=\"10.0.0.1:1234,10.0.0.2:2345\",\n+            num_processes=2,\n+            process_id=0)\n+        ```\n+\n+        On process 1:\n+        ```python\n+        keras.distribute.initialize(\n+            job_addresses=\"10.0.0.1:1234,10.0.0.2:2345\",\n+            num_processes=2,\n+            process_id=1)\n+        ```\n+\n+        or via the environment variables:\n+        On process 0:\n+        ```python\n+        os.environ[\n+            \"KERAS_DISTRIBUTION_JOB_ADDRESSES\"] = \"10.0.0.1:1234,10.0.0.2:2345\"\n+        os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"] = \"2\"\n+        os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"] = \"0\"\n+        keras.distribute.initialize()\n+        ```\n+\n+        On process 1:\n+        ```python\n+        os.environ[\n+            \"KERAS_DISTRIBUTION_JOB_ADDRESSES\"] = \"10.0.0.1:1234,10.0.0.2:2345\"\n+        os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"] = \"2\"\n+        os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"] = \"1\"\n+        keras.distribute.initialize()\n+        ```\n+\n+        Also note that for JAX backend, the `job_addresses` can be further\n+        reduced to just the master/coordinator address, which is\n+        `10.0.0.1:1234`.\n+    \"\"\"\n+    if (\n+        job_addresses is None\n+        and \"KERAS_DISTRIBUTION_JOB_ADDRESSES\" in os.environ\n+    ):\n+        job_addresses = os.environ[\"KERAS_DISTRIBUTION_JOB_ADDRESSES\"]\n+    if (\n+        num_processes is None\n+        and \"KERAS_DISTRIBUTION_NUM_PROCESSES\" in os.environ\n+    ):\n+        num_processes = int(os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"])\n+    if process_id is None and \"KERAS_DISTRIBUTION_PROCESS_ID\" in os.environ:\n+        process_id = int(os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"])\n+    initialize(job_addresses, num_processes, process_id)\n+\n+\n+@keras_export(\"keras.distribution.DeviceMesh\")\n+class DeviceMesh:\n+    \"\"\"A cluster of computation devices for distributed computation.\n+\n+    This API is aligned with `jax.sharding.Mesh` and `tf.dtensor.Mesh`, which\n+    represents the computation devices in the global context.\n+\n+    See more details in [jax.sharding.Mesh](\n+        https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh)\n+    and [tf.dtensor.Mesh](\n+        https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Mesh).\n+\n+    Args:\n+        shape: tuple of list of integers. The shape of the overall\n+            `DeviceMesh`, e.g. `(8,)` for a data parallel only distribution,\n+            or `(4, 2)` for a model+data parallel distribution.\n+        axis_names: List of string. The logical name of the each axis for\n+            the `DeviceMesh`. The length of the `axis_names` should match to\n+            the rank of the `shape`. The `axis_names` will be used to\n+            match/create the `TensorLayout` when distribute the data and\n+            variables.\n+        devices: Optional list of devices. Defaults to all the available\n+            devices locally from `keras.distribution.list_devices()`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        shape,\n+        axis_names,\n+        devices=None,\n+    ):\n+        if not shape or not axis_names:\n+            raise ValueError(\n+                \"Shape and axis_names cannot be empty. Received: \"\n+                f\"shape={shape}, axis_names={axis_names}\"\n+            )\n+\n+        if len(shape) != len(axis_names):\n+            raise ValueError(\n+                \"Shape and axis_names should have same size. \"\n+                f\"Received: shape={shape}, axis_names={axis_names}\"\n+            )\n+        if devices is None:\n+            devices = list_devices()\n+        devices = np.array(devices)\n+        if np.prod(shape) != np.prod(devices.shape):\n+            raise ValueError(\n+                \"Shape does not match the number of devices. \"\n+                f\"Received: shape={shape}; devices.shape=\"\n+                f\"{devices.shape}\"\n+            )\n+\n+        self._shape = shape\n+        self._axis_names = axis_names\n+        self._devices = np.reshape(devices, shape)\n+\n+    @property\n+    def shape(self):\n+        return self._shape\n+\n+    @property\n+    def axis_names(self):\n+        return self._axis_names\n+\n+    @property\n+    def devices(self):\n+        return self._devices\n+\n+    @property\n+    def backend_mesh(self):\n+        if not hasattr(self, \"_backend_mesh\"):\n+            self._backend_mesh = _to_backend_mesh(self)\n+        return self._backend_mesh\n+\n+    def __repr__(self):\n+        return (\n+            f\"<{self.__class__.__name__} \"\n+            f\"shape={self.shape}, axis_names={self.axis_names}>\"\n+        )\n+\n+    def __str__(self):\n+        return self.__repr__()\n+\n+\n+@keras_export(\"keras.distribution.TensorLayout\")\n+class TensorLayout:\n+    \"\"\"A layout to apply to a tensor.\n+\n+    This API is aligned with `jax.sharding.NamedSharding`\n+    and `tf.dtensor.Layout`.\n+\n+    See more details in [jax.sharding.NamedSharding](\n+        https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding)\n+    and [tf.dtensor.Layout](\n+        https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Layout).\n+\n+    Args:\n+        axes: tuple of strings that should map to the `axis_names` in\n+            a `DeviceMesh`. For any dimensions that doesn't need any sharding,\n+            A `None` can be used a placeholder.\n+        device_mesh: Optional `DeviceMesh` that will be used to create\n+            the layout. The actual mapping of tensor to physical device\n+            is not known until the mesh is specified.\n+    \"\"\"\n+\n+    def __init__(self, axes, device_mesh=None):\n+        self._axes = tuple(axes)\n+        self._device_mesh = device_mesh\n+        self._validate_axes()\n+\n+    @property\n+    def axes(self):\n+        return self._axes\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    @device_mesh.setter\n+    def device_mesh(self, device_mesh):\n+        if self._device_mesh is not None:\n+            raise ValueError(\n+                \"Cannot override device mesh value. Existing \"\n+                f\"value is {self._device_mesh}\"\n+            )\n+        self._device_mesh = device_mesh\n+        self._validate_axes()\n+\n+    @property\n+    def backend_layout(self):\n+        if not hasattr(self, \"_backend_layout\"):\n+            self._backend_layout = _to_backend_layout(self)\n+        return self._backend_layout\n+\n+    def _validate_axes(self):\n+        if self._device_mesh:\n+            valid_axis_names = set(self._device_mesh.axis_names)\n+            axis_names = set(self._axes) - set([None])\n+            if axis_names - valid_axis_names:\n+                raise ValueError(\n+                    \"Invalid axis names for Layout. Valid axis \"\n+                    f\"names: {valid_axis_names}, Got {axis_names}\"\n+                )\n+\n+    def __repr__(self):\n+        return (\n+            f\"<{self.__class__.__name__} \"\n+            f\"axes={self.axes}, device_mesh={self.device_mesh}>\"\n+        )\n+\n+    def __str__(self):\n+        return self.__repr__()\n+\n+\n+class Distribution:\n+    \"\"\"Base class for variable distribution strategies.\n+\n+    A `Distribution` has following key functionalities:\n+\n+    1. Distribute the model variables to a `DeviceMesh`.\n+    2. Distribute the input data to a `DeviceMesh`.\n+    3. Distribute an intermediate state tensor in the model.\n+\n+    It can create a context scope so that the framework to properly detect the\n+    `Distribution` and distribute the variable/data accordingly.\n+\n+    Args:\n+        device_mesh: A `DeviceMesh` instance.\n+        batch_dim_name: Optional string name for the batch dimension.\n+            Defaults to None.\n+        auto_shard_dataset: Automatically shard the dataset amongst\n+            processes in a multi-process setting. Set to `False` if the dataset\n+            is already sharded across hosts.  Defaults to `True`.\n+    \"\"\"\n+\n+    def __init__(\n+        self, device_mesh, batch_dim_name=None, auto_shard_dataset=True\n+    ):\n+        self._device_mesh = device_mesh\n+        self._batch_dim_name = batch_dim_name\n+        self._auto_shard_dataset = auto_shard_dataset\n+\n+    def get_data_layout(self, data_shape):\n+        \"\"\"Retrieve the `TensorLayout` for the input data.\n+\n+        Args:\n+            data_shape: shape for the input data in list or tuple format.\n+\n+        Returns:\n+            The `TensorLayout` for the data, which can be used by\n+            `backend.distribute_value()` to redistribute a input data.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def get_variable_layout(self, variable):\n+        \"\"\"Retrieve the `TensorLayout` for the variable.\n+\n+        Args:\n+            variable: A `Variable` instance.\n+\n+        return:\n+            The `TensorLayout` for the variable, which can be used by\n+            `backend.distribute_value()` to redistribute a variable.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def get_tensor_layout(self, path):\n+        \"\"\"Retrieve the `TensorLayout` for the intermediate tensor.\n+\n+        Args:\n+            path: a string path for the corresponding tensor.\n+\n+        return:\n+            The `TensorLayout` for the intermediate tensor, which can be used\n+            by `backend.relayout()` to reshard the tensor. Could also return\n+            None.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    @contextlib.contextmanager\n+    def scope(self):\n+        \"\"\"Context manager to make the `Distribution` current.\"\"\"\n+        original_scope = distribution()\n+        set_distribution(self)\n+        try:\n+            yield\n+        finally:\n+            set_distribution(original_scope)\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    @property\n+    def batch_dim_name(self):\n+        return self._batch_dim_name\n+\n+    @property\n+    def auto_shard_dataset(self):\n+        return self._auto_shard_dataset\n+\n+    @auto_shard_dataset.setter\n+    def auto_shard_dataset(self, auto_shard_dataset):\n+        self._auto_shard_dataset = auto_shard_dataset\n+\n+    def distribute_dataset(self, dataset):\n+        \"\"\"Create a distributed dataset from the original global dataset.\n+\n+        Args:\n+            dataset: the original global dataset instance.\n+\n+        Returns:\n+            If `auto_shard_dataset` is `True`, returns a sharded dataset that\n+            only produces data for the current local worker/process.  Otherwise,\n+            returns the original dataset.\n+\n+        Raises:\n+            ValueError: if auto-sharding is requested in a multi-process\n+            setting, but the dataset type is not supported.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def __repr__(self):\n+        return f\"<{self.__class__.__name__} device_mesh={self.device_mesh}>\"\n+\n+    def __str__(self):\n+        return self.__repr__()\n+\n+@keras_export(\"keras.distribution.AutoShardDistribution\")\n+class AutoShardDistribution(Distribution):\n+    def __init__(\n+        self,\n+        device_mesh=None,\n+    ):\n+        if device_mesh is None:\n+            devices = np.array(list_devices())\n+            axis_names = [DEFAULT_BATCH_DIM_NAME] + [\n+                f\"model_{i}\" for i in range(devices.ndim - 1)\n+            ]\n+            device_mesh = DeviceMesh(\n+                shape=devices.shape,\n+                axis_names=axis_names,\n+                devices=devices,\n+            )\n+        super().__init__(device_mesh, device_mesh.axis_names[0])\n+        self._sharding_plan = None\n+        self._sharding_planner = None\n+        self._shard_applier = None\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+\n+    def _get_backend_components(self):\n+        if self._sharding_planner and self._shard_applier:\n+            return\n+        self._sharding_planner = get_sharding_planner()\n+        self._shard_applier = get_shard_applier()\n+\n+    def shard(self, model, *args, **kwargs):\n+        self._get_backend_components()\n+        graph = create_graph_from_model(model, *args, **kwargs)\n+\n+        plan = self._sharding_planner.plan(graph, self.device_mesh)\n+        self._sharding_plan = plan\n+        self._shard_applier.apply(model, self._sharding_plan)\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self.batch_dim_name\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+    def get_tensor_layout(self, path):\n+        return None\n+\n+    def distribute_dataset(self, dataset):\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        if not tf.available or not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for auto-sharding, \"\n+                f\"got {type(dataset)}\"\n+            )\n+\n+        from tensorflow.python.data.experimental.ops import (\n+            distribute as tf_data_distribute,\n+        )\n+\n+        batch_size = tf_data_distribute.compute_batch_size(dataset)\n+        if batch_size.numpy() < 0:\n+            raise ValueError(\n+                \"The batch size of the input dataset is \"\n+                \"unknown. Please config the batch size for \"\n+                \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n+            )\n+        per_worker_batch_size = tf_data_distribute.batch_sizes_for_worker(\n+            global_batch_size=batch_size,\n+            num_workers=self._num_process,\n+            num_replicas_per_worker=1,  # We hard code this for now.\n+            worker_index=self._process_id,\n+        )\n+        distributed_dataset = dataset.rebatch(per_worker_batch_size)\n+        distributed_dataset = tf_data_distribute._AutoShardDataset(\n+            distributed_dataset,\n+            num_workers=self._num_process,\n+            index=self._process_id,\n+            num_replicas=self._num_process,\n+        )\n+        return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+\n+@keras_export(\"keras.distribution.DataParallel\")\n+class DataParallel(Distribution):\n+    \"\"\"Distribution for data parallelism.\n+\n+    You can choose to create this instance by either specifying\n+    the `device_mesh` or `devices` arguments (but not both).\n+\n+    The `device_mesh` argument is expected to be a `DeviceMesh` instance,\n+    and is expected to be 1D only. In case that the mesh has multiple axes,\n+    then the first axis will be treated as the data parallel dimension\n+    (and a warning will be raised).\n+\n+    When a list of `devices` are provided, they will be used to construct a\n+    1D mesh.\n+\n+    When both `mesh` and `devices` are absent, then `list_devices()`\n+    will be used to detect any available devices and create a 1D mesh from\n+    them.\n+\n+    Args:\n+        device_mesh: Optional `DeviceMesh` instance.\n+        devices: Optional list of devices.\n+        auto_shard_dataset: Automatically shard the dataset amongst\n+            processes in a multi-process setting. Set to `False` if the dataset\n+            is already sharded across hosts.  Defaults to `True`.\n+    \"\"\"\n+\n+    def __init__(self, device_mesh=None, devices=None, auto_shard_dataset=True):\n+        if device_mesh:\n+            self._initialize_with_device_mesh(device_mesh, auto_shard_dataset)\n+        elif devices:\n+            self._initialize_mesh_from_devices(devices, auto_shard_dataset)\n+        else:\n+            self._initialize_mesh_from_list_devices(auto_shard_dataset)\n+\n+        # Those following attributes might get convert to public methods.\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+        self._is_multi_process = self._num_process > 1\n+\n+    def _initialize_with_device_mesh(self, device_mesh, auto_shard_dataset):\n+        if not isinstance(device_mesh, DeviceMesh):\n+            raise ValueError(\n+                \"Expect `mesh` to be an instance of `DeviceMesh`. \"\n+                f\"Received: mesh={device_mesh} (of type {type(device_mesh)})\"\n+            )\n+        super().__init__(\n+            device_mesh, device_mesh.axis_names[0], auto_shard_dataset\n+        )\n+        if self.device_mesh.devices.ndim != 1:\n+            warnings.warn(\n+                \"Expect the input mesh to be 1D, but received \"\n+                \"mesh.devices.ndim=%d. \"\n+                \"The first axis will be used for data-parallel sharding.\",\n+                device_mesh.devices.ndim,\n+            )\n+\n+    def _initialize_mesh_from_devices(self, devices, auto_shard_dataset):\n+        devices = np.array(devices)\n+        device_mesh = DeviceMesh(\n+            shape=devices.shape,\n+            axis_names=[DEFAULT_BATCH_DIM_NAME],\n+            devices=devices,\n+        )\n+        super().__init__(\n+            device_mesh, DEFAULT_BATCH_DIM_NAME, auto_shard_dataset\n+        )\n+\n+    def _initialize_mesh_from_list_devices(self, auto_shard_dataset):\n+        devices = np.array(list_devices())\n+        device_mesh = DeviceMesh(\n+            shape=devices.shape,\n+            axis_names=[DEFAULT_BATCH_DIM_NAME],\n+            devices=devices,\n+        )\n+        super().__init__(\n+            device_mesh, DEFAULT_BATCH_DIM_NAME, auto_shard_dataset\n+        )\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self.batch_dim_name  # Shard on the first dim\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        # First check if the variable already has a layout assigned.\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        # Otherwise, replicate variable.\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+    def get_tensor_layout(self, path):\n+        # For data parallel training, the intermediate state is not changed.\n+        return None\n+\n+    def distribute_dataset(self, dataset):\n+        if not self._is_multi_process or not self.auto_shard_dataset:\n+            return dataset\n+\n+        # Try to distribute a global tf.data.Dataset.\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        if not tf.available or not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for auto-sharding, \"\n+                f\"got {type(dataset)}\"\n+            )\n+\n+        from tensorflow.python.data.experimental.ops import (\n+            distribute as tf_data_distribute,\n+        )\n+\n+        batch_size = tf_data_distribute.compute_batch_size(dataset)\n+        if batch_size.numpy() < 0:\n+            raise ValueError(\n+                \"The batch size of the input dataset is \"\n+                \"unknown. Please config the batch size for \"\n+                \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n+            )\n+        per_worker_batch_size = tf_data_distribute.batch_sizes_for_worker(\n+            global_batch_size=batch_size,\n+            num_workers=self._num_process,\n+            num_replicas_per_worker=1,  # We hard code this for now.\n+            worker_index=self._process_id,\n+        )\n+        distributed_dataset = dataset.rebatch(per_worker_batch_size)\n+        distributed_dataset = tf_data_distribute._AutoShardDataset(\n+            distributed_dataset,\n+            num_workers=self._num_process,\n+            index=self._process_id,\n+            num_replicas=self._num_process,\n+        )\n+        return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+\n+\n+@keras_export(\"keras.distribution.ModelParallel\")\n+class ModelParallel(Distribution):\n+    \"\"\"Distribution that shards model variables.\n+\n+    Compare to `DataParallel` which replicates the variables across all devices,\n+    `ModelParallel` allows you to shard variables in addition to the input data.\n+\n+    To construct a `ModelParallel` distribution, you need to provide a\n+    `DeviceMesh` and a `LayoutMap`.\n+\n+    1. `DeviceMesh` contains physical device information. The axis names in\n+        the mesh will be used to map the variable and data layout.\n+    2. `LayoutMap` contains the mapping between variable paths to their\n+        corresponding `TensorLayout`.\n+\n+    Example:\n+\n+    ```python\n+    devices = list_devices()    # Assume there are 8 devices.\n+\n+    # Create a mesh with 2 devices for data parallelism and 4 devices for\n+    # model parallelism.\n+    device_mesh = DeviceMesh(shape=(2, 4), axis_names=('batch', 'model'),\n+                             devices=devices)\n+    # Create a layout map that shard the `Dense` layer and `Conv2D`\n+    # layer variables on the last dimension.\n+    # Based on the `device_mesh`, this means the variables\n+    # will be split across 4 devices. Any other variable that doesn't\n+    # match any key in the layout map will be fully replicated.\n+    layout_map = LayoutMap(device_mesh)\n+    layout_map['dense.*kernel'] = (None, 'model')\n+    layout_map['dense.*bias'] = ('model',)\n+    layout_map['conv2d.*kernel'] = (None, None, None, 'model')\n+    layout_map['conv2d.*bias'] = ('model',)\n+\n+    distribution = ModelParallel(\n+        layout_map=layout_map,\n+        batch_dim_name='batch',\n+    )\n+\n+    # Set the global distribution, or via `with distribution.scope():`\n+    set_distribution(distribution)\n+\n+    model = model_creation()\n+    model.compile()\n+    model.fit(data)\n+    ```\n+\n+    You can quickly update the device mesh shape to change the sharding factor\n+    of the variables. E.g.\n+\n+    ```python\n+    # With only the shape change for the device mesh, the variables will be\n+    # sharded across 8 devices instead of 4, which further reduces the memory\n+    # footprint of variables on each of the device.\n+    device_mesh = DeviceMesh(\n+        shape=(1, 8),\n+        axis_names=('batch', 'model'),\n+        devices=devices,\n+    )\n+    ```\n+\n+    To figure out a proper layout mapping rule for all the model variables, you\n+    can first list out all the model variable paths, which will be used as the\n+    key to map the variables to `TensorLayout`.\n+\n+    e.g.\n+\n+    ```python\n+    model = create_model()\n+    for v in model.variables:\n+        print(v.path)\n+    ```\n+\n+    Args:\n+        layout_map: `LayoutMap` instance which map the variable path to the\n+            corresponding tensor layout.\n+        batch_dim_name: Optional string, the axis name in the device mesh\n+            (of the `layout_map` object)\n+            that will be used to distribute data. If unspecified, the\n+            first axis from the device mesh will be used.\n+        auto_shard_dataset: Automatically shard the dataset amongst\n+            processes in a multi-process setting. Set to `False` if the dataset\n+            is already sharded across hosts.  Defaults to `True`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        layout_map=None,\n+        batch_dim_name=None,\n+        auto_shard_dataset=True,\n+        **kwargs,\n+    ):\n+        kwargs.pop(\"device_mesh\", None)\n+        if layout_map is None:\n+            raise ValueError(\"You must specify a layout_map argument.\")\n+        if not isinstance(layout_map, LayoutMap):\n+            raise ValueError(\n+                \"Argument `layout_map` must be a `LayoutMap` instance. \"\n+                f\"Received: layout_map={layout_map}\"\n+            )\n+        device_mesh = layout_map.device_mesh\n+        batch_dim_name = batch_dim_name or device_mesh.axis_names[0]\n+        super().__init__(device_mesh, batch_dim_name, auto_shard_dataset)\n+        self._layout_map = layout_map\n+\n+        # Those following attributes might get convert to public methods.\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+        self._is_multi_process = self._num_process > 1\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self.batch_dim_name  # Shard on the first dim\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        # First check if the variable already has a layout assigned.\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        # Check the layout map.\n+        variable_layout = self._layout_map[variable.path]\n+        if variable_layout is not None:\n+            return variable_layout\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+    def get_tensor_layout(self, path):\n+        return self._layout_map[path]\n+\n+    def distribute_dataset(self, dataset):\n+        if not self._is_multi_process or not self.auto_shard_dataset:\n+            return dataset\n+\n+        # Try to distribute a global tf.data.Dataset.\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        if not tf.available or not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for auto-sharding, \"\n+                f\"got {type(dataset)}\"\n+            )\n+\n+        from tensorflow.python.data.experimental.ops import (\n+            distribute as tf_data_distribute,\n+        )\n+\n+        global_batch_size = tf_data_distribute.compute_batch_size(dataset)\n+        if global_batch_size.numpy() < 0:\n+            raise ValueError(\n+                \"The batch size of the input dataset is \"\n+                \"unknown. Please config the batch size for \"\n+                \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n+            )\n+\n+        # We need to compute the per-process/worker/host batch size.\n+        # This will depend on how many model replicas we have on each process.\n+        # Note that this might be smaller than one if model replicas are sharded\n+        # across multiple processes.\n+        mesh_batch_dim_index = self.device_mesh.axis_names.index(\n+            self.batch_dim_name\n+        )\n+        num_model_replicas = self.device_mesh.shape[mesh_batch_dim_index]\n+        if num_model_replicas == 1:\n+            # No sharding is needed in this case. Each process will have the\n+            # global batch size, and data from the iterator will need to be\n+            # replicated across all processes.\n+            return dataset.prefetch(tf.data.AUTOTUNE)\n+        num_model_replicas_per_process = num_model_replicas / self._num_process\n+        if num_model_replicas_per_process >= 1:\n+            # Each process will have one or more full model replicas. Data will\n+            # be sharded across all processes without replication.\n+            if global_batch_size % self._num_process != 0:\n+                raise ValueError(\n+                    \"Global batch size must be divisible by the number of \"\n+                    f\"processes. `global_batch_size`={global_batch_size} and \"\n+                    f\"`num_process`={self._num_process}\"\n+                )\n+            per_process_batch_size = global_batch_size // self._num_process\n+            distributed_dataset = dataset.rebatch(per_process_batch_size)\n+            distributed_dataset = distributed_dataset.shard(\n+                num_shards=self._num_process,\n+                index=self._process_id,\n+            )\n+            return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+        else:\n+            # Model replicas are sharded across multiple processes. Data will be\n+            # sharded across model replicas, and replicated across processes\n+            # within the same model replica.\n+            if global_batch_size % num_model_replicas != 0:\n+                raise ValueError(\n+                    \"Global batch size must be divisible by the number of \"\n+                    f\"replicas. `global_batch_size`={global_batch_size} and \"\n+                    f\"`num_model_replicas`={num_model_replicas}\"\n+                )\n+            per_process_batch_size = global_batch_size // num_model_replicas\n+            distributed_dataset = dataset.rebatch(per_process_batch_size)\n+            processes_per_replica = self._num_process // num_model_replicas\n+            # TODO: Figure out what the convention is for data sharding id.\n+            data_shard_id = self._process_id % processes_per_replica\n+            distributed_dataset = distributed_dataset.shard(\n+                num_shards=num_model_replicas,\n+                index=data_shard_id,\n+            )\n+            return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+\n+\n+@keras_export(\"keras.distribution.LayoutMap\")\n+class LayoutMap(collections.abc.MutableMapping):\n+    \"\"\"A dict-like object that maps string to `TensorLayout` instances.\n+\n+    `LayoutMap` uses a string as key and a `TensorLayout` as value. There is a\n+    behavior difference between a normal Python dict and this class. The string\n+    key will be treated as a regex when retrieving the value. See the docstring\n+    of `get` for more details.\n+\n+    See below for a usage example. You can define the naming schema\n+    of the `TensorLayout`, and then retrieve the corresponding\n+    `TensorLayout` instance.\n+\n+    In the normal case, the key to query is usually the `variable.path`, which\n+    is the identifier of the variable.\n+\n+    As shortcut, tuple or list of axis names are also allowed when inserting\n+    as value, and will be converted to `TensorLayout`.\n+\n+    ```python\n+    layout_map = LayoutMap(device_mesh)\n+    layout_map['dense.*kernel'] = (None, 'model')\n+    layout_map['dense.*bias'] = ('model',)\n+    layout_map['conv2d.*kernel'] = (None, None, None, 'model')\n+    layout_map['conv2d.*bias'] = ('model',)\n+\n+    layout_1 = layout_map['dense_1.kernel']             # layout_1 == layout_2d\n+    layout_2 = layout_map['dense_1.bias']               # layout_2 == layout_1d\n+    layout_3 = layout_map['dense_2.kernel']             # layout_3 == layout_2d\n+    layout_4 = layout_map['dense_2.bias']               # layout_4 == layout_1d\n+    layout_5 = layout_map['my_model/conv2d_123/kernel'] # layout_5 == layout_4d\n+    layout_6 = layout_map['my_model/conv2d_123/bias']   # layout_6 == layout_1d\n+    layout_7 = layout_map['my_model/conv3d_1/kernel']   # layout_7 == None\n+    layout_8 = layout_map['my_model/conv3d_1/bias']     # layout_8 == None\n+    ```\n+\n+    Args:\n+        device_mesh: `keras.distribution.DeviceMesh` instance.\n+    \"\"\"\n+\n+    def __init__(self, device_mesh):\n+        self._layout_map = collections.OrderedDict()\n+        self._device_mesh = device_mesh\n+\n+    def __getitem__(self, key):\n+        \"\"\"Retrieves the corresponding layout by the string key.\n+\n+        When there isn't an exact match, all the existing keys in the layout map\n+        will be treated as a regex and map against the input key again. When\n+        there are multiple matches for the regex, an `ValueError` will be\n+        raised. Returns `None` if there isn't any match found.\n+\n+        Args:\n+            key: String key to query a layout.\n+\n+        Returns:\n+            Corresponding layout based on the query.\n+        \"\"\"\n+        if key in self._layout_map:\n+            return self._layout_map[key]\n+\n+        matching_keys = []\n+        for k in self._layout_map:\n+            if re.search(k, key):\n+                matching_keys.append(k)\n+        if len(matching_keys) > 1:\n+            raise ValueError(\n+                f\"Path '{key}' matches multiple layout \"\n+                f\"specification keys: {matching_keys}. Please make \"\n+                \"sure each tensor/variable path only matches at most \"\n+                \"one layout specification key in the LayoutMap.\"\n+            )\n+        elif len(matching_keys) == 1:\n+            return self._layout_map[matching_keys[0]]\n+        return None\n+\n+    def __setitem__(self, key, layout):\n+        \"\"\"Insert TensorLayout to the LayoutMap.\n+\n+        Args:\n+            key: String key for the `TensorLayout`.\n+            layout: The `TensorLayout`. As a shortcut, tuple of string and None\n+                are also acceptable, and will be converted to `TensorLayout`.\n+        \"\"\"\n+        if key in self._layout_map:\n+            raise ValueError(\n+                f\"{key} already exist in the LayoutMap with \"\n+                f\"value {self._layout_map[key]}. Please make sure to \"\n+                \"not use duplicated keys.\"\n+            )\n+        if isinstance(layout, tuple):\n+            layout = TensorLayout(axes=layout, device_mesh=None)\n+\n+        if not isinstance(layout, TensorLayout):\n+            raise ValueError(\n+                f\"{layout} should be a TensorLayout type, got {type(layout)}\"\n+            )\n+        self._maybe_populate_device_mesh(layout)\n+        self._layout_map[key] = layout\n+\n+    def __delitem__(self, key):\n+        # let the dict to handle the key missing error\n+        return self._layout_map.pop(key)\n+\n+    def __len__(self):\n+        return len(self._layout_map)\n+\n+    def __iter__(self):\n+        return iter(self._layout_map)\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    def _maybe_populate_device_mesh(self, layout):\n+        if layout.device_mesh is None and self.device_mesh is not None:\n+            layout.device_mesh = self.device_mesh\n+\n+\n+LayoutMap.get.__doc__ = LayoutMap.__getitem__.__doc__\n+\n+\n+@keras_export(\"keras.distribution.distribute_tensor\")\n+def distribute_tensor(tensor, layout):\n+    \"\"\"Change the layout of a Tensor value in the jit function execution.\n+\n+    Args:\n+        tensor: a Tensor to change the layout.\n+        layout: `TensorLayout` to be applied on the value.\n+\n+    Returns:\n+        a new value with the specified tensor layout.\n+    \"\"\"\n+    if isinstance(tensor, KerasTensor):\n+        return tensor\n+    return distribute_tensor(tensor, layout)\n+\n+\n+@keras_export(\"keras.distribution.distribution\")\n+def distribution():\n+    \"\"\"Retrieve the current distribution from global context.\"\"\"\n+    return global_state.get_global_attribute(GLOBAL_ATTRIBUTE_NAME)\n+\n+\n+@keras_export(\"keras.distribution.set_distribution\")\n+def set_distribution(value):\n+    \"\"\"Set the distribution as the global distribution setting.\n+\n+    Args:\n+        value: a `Distribution` instance.\n+    \"\"\"\n+    global_state.set_global_attribute(GLOBAL_ATTRIBUTE_NAME, value)\n+",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthis test file contains a very large amount of duplicated code from keras/src/backend/jax/distribution_lib.py and keras/src/distribution/distribution_lib.py. this is a significant maintenance concern, as any changes in the source files will not be reflected here, leading to outdated and incorrect tests. please refactor this file to import the necessary functions and classes directly instead of copy-pasting them.",
    "line_number": 1617,
    "enriched": "File: keras/src/distribution/test_memory.py\nCode: @@ -0,0 +1,1918 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Test suite for model verification with Parallax (Keras AutoShardDistribution),\n+using KerasNLP presets and the Tiny Shakespeare dataset.\n+\n+This script compares the training performance (loss and perplexity) of a\n+baseline model against its Parallax-sharded equivalent. The goal is to verify\n+the correctness and performance of Keras's automatic sharding capabilities.\n+\n+It is generalized to run tests for multiple model architectures, including:\n+- Gemma\n+- GPT-2\n+- Bloom\n+- OPT\n+\"\"\"\n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"\n+os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=2\"\n+\n+# Add these imports at the top of your script\n+import psutil\n+import os\n+\n+# Get the current process to monitor its memory. Place this near the top.\n+PROCESS = psutil.Process(os.getpid())\n+\n+# Replace your ENTIRE old log_memory_usage function with this new one.\n+def log_memory_usage(stage_name):\n+    \"\"\"\n+    Logs the Resident Set Size (RSS) of the current process using psutil.\n+    This is compatible with CPU, GPU, and other backends.\n+    \"\"\"\n+    print(f\"\\n--- Process Memory Usage at: {stage_name} ---\")\n+    mem_info = PROCESS.memory_info()\n+    rss_mb = mem_info.rss / 1024 / 1024  # rss is the Resident Set Size\n+    # print(f\"  Process RSS: {rss_mb:.2f} MB\")\n+    # print(\"-\" * 45)\n+\n+\"\"\"Utilities for distribution strategy with JAX backend.\"\"\"\n+class MergeableGraph:\n+    \"\"\"A graph that supports merging nodes.\"\"\"\n+\n+    def __init__(self):\n+        self._parent = {}\n+        self._edges = set()\n+\n+    def get_root(self, node):\n+        if node not in self._parent:\n+            self._parent[node] = node\n+            return node\n+        if self._parent[node] == node:\n+            return node\n+        self._parent[node] = self.get_root(self._parent[node])\n+        return self._parent[node]\n+\n+    def merge_nodes(self, node1, node2):\n+        root1 = self.get_root(node1)\n+        root2 = self.get_root(node2)\n+        if root1 != root2:\n+            self._parent[root1] = root2\n+\n+    def add_edge(self, node1, node2):\n+        root1 = self.get_root(node1)\n+        root2 = self.get_root(node2)\n+        if root1 != root2:\n+            self._edges.add(tuple(sorted((root1, root2))))\n+\n+    def get_edges(self):\n+        return self._edges\n+    \n+import jax\n+import numpy as np\n+import collections\n+import itertools\n+from keras.src.backend.common import global_state\n+from keras.src.random import seed_generator\n+from keras.src.utils import jax_utils\n+from keras.src.utils import rng_utils\n+from jax import tree_util\n+\n+\n+def list_devices(device_type=None):\n+    \"\"\"Return all the available devices based on the device type.\n+\n+    Note that this should return the global devices in a distributed setting.\n+\n+    Args:\n+        device_type: string of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`. Defaults to `\"gpu\"`\n+            or `\"tpu\"` if available when device_type is not provided. Otherwise\n+            will return the `\"cpu\"` devices.\n+\n+    Return:\n+        List of devices that are available for distribute computation.\n+    \"\"\"\n+    device_type = device_type.lower() if device_type else None\n+    jax_devices = jax.devices(backend=device_type)\n+    return [f\"{device.platform}:{device.id}\" for device in jax_devices]\n+\n+\n+def distribute_variable(value, layout):\n+    \"\"\"Create a distributed variable for JAX.\n+\n+    Since JAX doesn't have a variable class, this will just return a `jax.Array`\n+    with the corresponding layout/sharding specified.\n+\n+    Note that this function should be used in eager context, not in jitted\n+    function.\n+\n+    Args:\n+        value: the initial value of the variable.\n+        layout: `TensorLayout` for the created variable, or a\n+            JAX-supported layout instance (e.g. `jax.sharding.Sharding`).\n+\n+    Returns:\n+        jax.Array which is the distributed variable.\n+    \"\"\"\n+    return distribute_tensor(value, layout)\n+\n+\n+def distribute_tensor(tensor, layout):\n+    \"\"\"Distribute the tensor based on the layout.\n+\n+    Note that this function can be used both in eager context, or within a\n+    jitted function.\n+\n+    Args:\n+        tensor: `jax.Array` that need to be distributed.\n+        layout: `TensorLayout` for the created variable, or a\n+            JAX-supported layout instance (e.g. `jax.sharding.Sharding`).\n+\n+    Returns:\n+        Distributed value.\n+    \"\"\"\n+    # Avoid circular imports.\n+    from keras.src.distribution import TensorLayout\n+\n+    if isinstance(layout, TensorLayout):\n+        layout = layout.backend_layout\n+\n+    # TODO(scottzhu): This might not be a cheap check, we should consider\n+    # have some proper JAX API for doing this check.\n+    if jax_utils.is_in_jax_tracing_scope():\n+        return jax.lax.with_sharding_constraint(tensor, layout)\n+\n+    # Skip relayout if unnecessary.\n+    if isinstance(tensor, jax.Array):\n+        if isinstance(\n+            layout, jax.sharding.Sharding\n+        ) and tensor.sharding.is_equivalent_to(layout, ndim=len(tensor.shape)):\n+            return tensor\n+        # JAX explicit \"layout\" support.\n+        elif hasattr(layout, \"layout\"):\n+            current_layout = getattr(tensor, \"layout\", None)\n+            if current_layout == layout:\n+                return tensor\n+        # JAX explicit \"format\" support.\n+        elif hasattr(layout, \"format\"):\n+            current_layout = getattr(tensor, \"format\", None)\n+            if current_layout == layout:\n+                return tensor\n+\n+    return jax.device_put(tensor, layout)\n+\n+\n+def distribute_data_input(per_process_batch, layout, batch_dim_name):\n+    \"\"\"Distribute the input data with the corresponding layout.\n+\n+    Note that the inputs here is a local worker batch. Within the local worker,\n+    the data need to be further partitioned to map to each of the devices.\n+\n+    Args:\n+        inputs: `jax.Array` that is already sharded to a local process size.\n+        layout: `TensorLayout` for the distribution information, or a\n+            `jax.sharding.Sharding` instance.\n+\n+    Returns:\n+        A global batch distributed according to `layout`.\n+    \"\"\"\n+    # Avoid circular imports.\n+    from keras.src.distribution import TensorLayout\n+\n+    if isinstance(layout, TensorLayout):\n+        layout = layout.backend_layout\n+\n+    return jax.make_array_from_process_local_data(layout, per_process_batch)\n+\n+\n+def initialize_rng():\n+    \"\"\"Initializes the global random number generator across processes.\n+\n+    This is required for consistent initialization in multi-host settings.\n+    \"\"\"\n+    global_seed = rng_utils.get_random_seed()\n+    # Only set a random seed if not already set\n+    # via keras.config.set_random_seed()\n+    if global_seed is None:\n+        # Generate a random seed on each CPU host and psum them to get a single\n+        # consistent seed across all processes.\n+        cpu_devices = jax.devices(\"cpu\")\n+        num_local_cpu_devices = jax.local_device_count(\"cpu\")\n+        # Seed must be in range [0, 2^32 - 1], so to ensure proper range and\n+        # avoid signed integer overflow, we use uint32.\n+        local_seed = jax.numpy.asarray(\n+            [seed_generator.make_default_seed()] * num_local_cpu_devices,\n+            dtype=jax.numpy.uint32,\n+        )\n+        # Sum across processes and pull out the first item.\n+        global_seed = jax.pmap(\n+            lambda x: jax.lax.psum(x, \"all\"),\n+            axis_name=\"all\",\n+            devices=cpu_devices,\n+        )(local_seed).item(0)\n+        # Set the global seed.\n+        rng_utils.set_random_seed(global_seed)\n+\n+    # Check if the global seed generator is set and ensure it has an initialized\n+    # seed.  Otherwise, reset the seed to the global seed.\n+    global_seed_generator = global_state.get_global_attribute(\n+        \"global_seed_generator\"\n+    )\n+    if global_seed_generator is not None:\n+        seed = global_seed_generator.get_config()[\"seed\"]\n+        if seed is None:\n+            global_state.set_global_attribute(\n+                \"global_seed_generator\",\n+                seed_generator.SeedGenerator(\n+                    seed=global_seed,\n+                    name=global_seed_generator.name,\n+                    backend=global_seed_generator.backend,\n+                ),\n+            )\n+\n+\n+def initialize(job_addresses, num_processes, process_id):\n+    if job_addresses and \",\" in job_addresses:\n+        # When user provide all the job addresses, we will split and get the\n+        # first one, which is the coordinator.\n+        job_addresses = job_addresses.split(\",\")\n+        # Do a sanity check to make sure the number of addresses also match\n+        # the num_processes.\n+        if num_processes is not None and num_processes != len(job_addresses):\n+            raise ValueError(\n+                f\"The provided job_addresses {job_addresses} has \"\n+                f\"{len(job_addresses)} jobs, but num_processes is \"\n+                f\"{num_processes}\"\n+            )\n+        coordinator_address = job_addresses[0]\n+    else:\n+        coordinator_address = job_addresses\n+\n+    jax.distributed.initialize(\n+        coordinator_address=coordinator_address,\n+        num_processes=num_processes,\n+        process_id=process_id,\n+    )\n+\n+    # Ensure the random number generator is initialized across processes.\n+    initialize_rng()\n+\n+\n+def num_processes():\n+    \"\"\"Return the number of processes for the current distribution setting.\"\"\"\n+    return jax.process_count()\n+\n+\n+def process_id():\n+    \"\"\"Return the current process ID for the distribution setting.\"\"\"\n+    return jax.process_index()\n+\n+\n+def _to_backend_device(device_name):\n+    if isinstance(device_name, jax.Device):\n+        return device_name\n+    device_name = str(device_name)\n+    if \":\" not in device_name:\n+        device_type, device_id = device_name, 0\n+    else:\n+        device_type, device_id = device_name.split(\":\")\n+\n+    devices = jax.devices(backend=device_type)\n+    for device in devices:\n+        if device.platform == device_type and device.id == int(device_id):\n+            return device\n+    raise ValueError(f\"Device not found: {device_name}\")\n+\n+\n+def _to_backend_mesh(device_mesh):\n+    \"\"\"Convert the DeviceMesh to JAX backend specific Mesh.\n+\n+    Args:\n+        device_mesh: DeviceMesh instance to convert.\n+\n+    Returns:\n+        A `jax.sharding.Mesh` instance.\n+    \"\"\"\n+    shape = device_mesh.devices.shape\n+    devices = [_to_backend_device(d) for d in device_mesh.devices.flatten()]\n+    devices = np.array(devices).reshape(shape)\n+    return jax.sharding.Mesh(devices, device_mesh.axis_names)\n+\n+\n+def _to_backend_layout(tensor_layout):\n+    \"\"\"Convert the TensorLayout to JAX backend specific Sharding.\n+\n+    Args:\n+        tensor_layout: TensorLayout instance to convert.\n+\n+    Returns:\n+        A `jax.sharding.NamedSharding` instance.\n+    \"\"\"\n+    if tensor_layout.device_mesh is None:\n+        raise ValueError(\n+            \"Cannot create sharding when device mesh is not set \"\n+            \"for TensorLayout.\"\n+        )\n+    partition_spec = jax.sharding.PartitionSpec(*tensor_layout.axes)\n+    jax_mesh = tensor_layout.device_mesh.backend_mesh\n+    return jax.sharding.NamedSharding(jax_mesh, partition_spec)\n+\n+\n+_JAX_CLASSES_DEFINED = False\n+JaxGraph = None\n+JaxShardingPlanner = None\n+JaxShardApplier = None\n+\n+\n+def _define_and_register_jax_classes():\n+    global _JAX_CLASSES_DEFINED, JaxGraph, JaxShardingPlanner, JaxShardApplier\n+    if _JAX_CLASSES_DEFINED:\n+        return\n+\n+    # from keras.src.distribution.autoshard_utils import MergeableGraph\n+\n+    def parse_jaxpr(jaxpr) -> MergeableGraph:\n+        graph = MergeableGraph()\n+\n+        def same_axis(node1, node2):\n+            var1, axis1 = node1\n+            var2, axis2 = node2\n+            if var1.aval.shape[axis1] != var2.aval.shape[axis2]:\n+                return\n+            graph.merge_nodes(node1, node2)\n+\n+        def parse_dot_general(eqn):\n+            lhs, rhs = eqn.invars\n+            out = eqn.outvars[0]\n+            (lc, rc), (lb, rb) = eqn.params[\"dimension_numbers\"]\n+            for l, r in zip(lc, rc):\n+                same_axis((lhs, l), (rhs, r))\n+            o_offset = 0\n+            for l, r in zip(lb, rb):\n+                same_axis((lhs, l), (rhs, r))\n+                same_axis((lhs, l), (out, o_offset))\n+                o_offset += 1\n+            for i in range(lhs.aval.ndim):\n+                if i not in lb and i not in lc:\n+                    same_axis((lhs, i), (out, o_offset))\n+                    o_offset += 1\n+            for j in range(rhs.aval.ndim):\n+                if j not in rb and j not in rc:\n+                    same_axis((rhs, j), (out, o_offset))\n+                    o_offset += 1\n+\n+        def parse_reshape(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            in_idx, out_idx, in_prod, out_prod = 0, 0, 1, 1\n+            while in_idx < invar.aval.ndim and out_idx < out.aval.ndim:\n+                if (\n+                    in_prod == out_prod\n+                    and invar.aval.shape[in_idx] == out.aval.shape[out_idx]\n+                ):\n+                    if invar.aval.shape[in_idx] > 1:\n+                        same_axis((invar, in_idx), (out, out_idx))\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    out_prod *= out.aval.shape[out_idx]\n+                    in_idx += 1\n+                    out_idx += 1\n+                elif in_prod < out_prod:\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    in_idx += 1\n+                else:\n+                    out_prod *= out.aval.shape[out_idx]\n+                    out_idx += 1\n+\n+        def parse_transpose(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            for i, j in enumerate(eqn.params[\"permutation\"]):\n+                same_axis((invar, j), (out, i))\n+\n+        def parse_elementwise_with_broadcast(eqn):\n+            out = eqn.outvars[0]\n+            for invar in eqn.invars:\n+                if invar.aval.ndim == 0:\n+                    continue\n+                for i in range(1, min(invar.aval.ndim, out.aval.ndim) + 1):\n+                    in_axis, out_axis = -i, -i\n+                    if invar.aval.shape[in_axis] == out.aval.shape[out_axis]:\n+                        same_axis(\n+                            (invar, invar.aval.ndim + in_axis),\n+                            (out, out.aval.ndim + out_axis),\n+                        )\n+\n+        for var in jaxpr.jaxpr.invars:\n+            for i, j in itertools.combinations(range(var.aval.ndim), 2):\n+                graph.add_edge((var, i), (var, j))\n+\n+        for eqn in jaxpr.eqns:\n+            for outvar in eqn.outvars:\n+                for i, j in itertools.combinations(range(outvar.aval.ndim), 2):\n+                    graph.add_edge((outvar, i), (outvar, j))\n+\n+            primitive_parsers = {\n+                \"dot_general\": parse_dot_general,\n+                \"reshape\": parse_reshape,\n+                \"transpose\": parse_transpose,\n+            }\n+            parser = primitive_parsers.get(\n+                eqn.primitive.name, parse_elementwise_with_broadcast\n+            )\n+            parser(eqn)\n+        return graph\n+\n+    def shard_model(\n+        jaxpr,\n+        out_avals,\n+        trainable_params,\n+        non_trainable_params,\n+        args,\n+        kwargs,\n+        min_shard_size=1,\n+        data_axis_name=\"data\",\n+        model_axis_name=\"model\",\n+    ):\n+        graph = parse_jaxpr(jaxpr)\n+\n+        t_params_flat, t_params_treedef = tree_util.tree_flatten(\n+            trainable_params\n+        )\n+        nt_params_flat, nt_params_treedef = tree_util.tree_flatten(\n+            non_trainable_params\n+        )\n+        args_flat, args_treedef = tree_util.tree_flatten(args)\n+        kwargs_flat, kwargs_treedef = tree_util.tree_flatten(kwargs)\n+        _, outputs_treedef = tree_util.tree_flatten(out_avals)\n+\n+        pos = 0\n+        t_param_invars = jaxpr.jaxpr.invars[pos : pos + len(t_params_flat)]\n+        pos += len(t_params_flat)\n+        nt_param_invars = jaxpr.jaxpr.invars[pos : pos + len(nt_params_flat)]\n+        pos += len(nt_params_flat)\n+        arg_invars = jaxpr.jaxpr.invars[pos : pos + len(args_flat)]\n+        pos += len(args_flat)\n+        kwarg_invars = jaxpr.jaxpr.invars[pos:]\n+\n+        all_param_invars = t_param_invars + nt_param_invars\n+        data_invars = arg_invars + kwarg_invars\n+\n+        seen = collections.Counter()\n+        for var in all_param_invars:\n+            for i in range(var.aval.ndim):\n+                if var.aval.shape[i] >= min_shard_size:\n+                    seen.update([graph.get_root((var, i))])\n+\n+        model_axis_root = max(seen, key=seen.get) if seen else None\n+\n+        data_axes_roots = []\n+        for var in data_invars:\n+            for i in range(var.aval.ndim):\n+                root = graph.get_root((var, i))\n+                if root not in seen and root not in data_axes_roots:\n+                    data_axes_roots.append(root)\n+\n+        def assign_layouts(vars_flat, is_params=False):\n+            assignments = []\n+            for var in vars_flat:\n+                layout = [None] * var.aval.ndim\n+                for i in range(var.aval.ndim):\n+                    if var.aval.shape[i] < min_shard_size:\n+                        continue\n+                    root = graph.get_root((var, i))\n+                    if (\n+                        is_params\n+                        and model_axis_root\n+                        and root == model_axis_root\n+                    ):\n+                        layout[i] = model_axis_name\n+                    elif not is_params and root in data_axes_roots:\n+                        name = data_axis_name\n+                        if len(data_axes_roots) > 1:\n+                            name += str(data_axes_roots.index(root))\n+                        layout[i] = name\n+                assignments.append(layout)\n+            return assignments\n+\n+        params_assignments = tree_util.tree_unflatten(\n+            t_params_treedef, assign_layouts(t_param_invars, is_params=True)\n+        )\n+        return params_assignments\n+\n+    class _JaxGraph:\n+        def __init__(\n+            self,\n+            jaxpr,\n+            trainable_variables,\n+            non_trainable_variables,\n+            in_treedefs,\n+            out_avals,\n+        ):\n+            self.jaxpr = jaxpr\n+            self.trainable_variables = trainable_variables\n+            self.non_trainable_variables = non_trainable_variables\n+            self.in_treedefs = in_treedefs\n+            self.out_avals = out_avals\n+\n+        @classmethod\n+        def from_model(cls, model, *args, **kwargs):\n+            def stateless_fn(\n+                trainable_vars, non_trainable_vars, f_args, f_kwargs\n+            ):\n+                return model.stateless_call(\n+                    trainable_vars, non_trainable_vars, *f_args, **f_kwargs\n+                )\n+\n+            trainable_vars = model.trainable_variables\n+            non_trainable_vars = model.non_trainable_variables\n+            in_treedefs = tree_util.tree_structure(\n+                (trainable_vars, non_trainable_vars, args, kwargs)\n+            )\n+\n+            closed_jaxpr, out_avals = jax.make_jaxpr(\n+                stateless_fn, return_shape=True\n+            )(trainable_vars, non_trainable_vars, args, kwargs)\n+\n+            return cls(\n+                closed_jaxpr,\n+                trainable_vars,\n+                non_trainable_vars,\n+                in_treedefs,\n+                out_avals,\n+            )\n+\n+    class _JaxShardingPlanner:\n+        def plan(self, graph, device_mesh):\n+            all_in_avals = [var.aval for var in graph.jaxpr.jaxpr.invars]\n+            all_in_leaves = tree_util.tree_unflatten(\n+                graph.in_treedefs, all_in_avals\n+            )\n+            _, _, args_aval_tree, kwargs_aval_tree = all_in_leaves\n+\n+            dummy_args = tree_util.tree_map(\n+                lambda x: np.zeros(x.shape, x.dtype), args_aval_tree\n+            )\n+            dummy_kwargs = tree_util.tree_map(\n+                lambda x: np.zeros(x.shape, x.dtype), kwargs_aval_tree\n+            )\n+\n+            param_assignments = shard_model(\n+                jaxpr=graph.jaxpr,\n+                out_avals=graph.out_avals,\n+                trainable_params=graph.trainable_variables,\n+                non_trainable_params=graph.non_trainable_variables,\n+                args=dummy_args,\n+                kwargs=dummy_kwargs,\n+            )\n+\n+            param_vars_flat, _ = tree_util.tree_flatten(\n+                graph.trainable_variables\n+            )\n+            param_layouts_flat, _ = tree_util.tree_flatten(param_assignments)\n+\n+            parameter_layout_dict = {\n+                var.path: tuple(layout) if layout else None\n+                for var, layout in zip(param_vars_flat, param_layouts_flat)\n+            }\n+            return parameter_layout_dict\n+\n+    class _JaxShardApplier:\n+        def apply(self, model, plan):\n+            for var in model.variables:\n+                layout = plan.get(var.path)\n+                if layout:\n+                    var.layout = layout\n+\n+    JaxGraph = _JaxGraph\n+    JaxShardingPlanner = _JaxShardingPlanner\n+    JaxShardApplier = _JaxShardApplier\n+    _JAX_CLASSES_DEFINED = True\n+\n+\n+def get_sharding_planner():\n+    \"\"\"Returns an instance of the JAX sharding planner.\"\"\"\n+    _define_and_register_jax_classes()\n+    return JaxShardingPlanner()\n+\n+\n+def get_shard_applier():\n+    \"\"\"Returns an instance of the JAX shard applier.\"\"\"\n+    _define_and_register_jax_classes()\n+    return JaxShardApplier()\n+\n+\n+def create_graph_from_model(model, *args, **kwargs):\n+    \"\"\"Returns a JAX graph representation of the Keras model.\"\"\"\n+    _define_and_register_jax_classes()\n+    return JaxGraph.from_model(model, *args, **kwargs)\n+\n+\n+import os\n+import time\n+import logging\n+import numpy as np\n+import keras\n+import keras_nlp\n+import matplotlib.pyplot as plt\n+import tensorflow as tf\n+import tensorflow_datasets as tfds\n+import jax\n+from keras.src.distribution import distribution_lib\n+# --- Basic Configuration ---\n+logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n+logger = logging.getLogger(__name__)\n+import collections\n+import contextlib\n+import os\n+import re\n+import warnings\n+\n+import numpy as np\n+\n+from keras.src.api_export import keras_export\n+from keras.src.backend import KerasTensor\n+from keras.src.backend import distribution_lib\n+from keras.src.backend.common import global_state\n+\"\"\"Unified high-level distribution APIs across backends.\n+\n+Currently only the JAX backend is supported. The TensorFlow backend\n+will be supported in the future (via tf.dtensor API).\n+\"\"\"\n+\n+import collections\n+import contextlib\n+import os\n+import re\n+import warnings\n+\n+import numpy as np\n+\n+from keras.src.api_export import keras_export\n+from keras.src.backend import KerasTensor\n+from keras.src.backend import distribution_lib\n+from keras.src.backend.common import global_state\n+\n+DEFAULT_BATCH_DIM_NAME = \"batch\"\n+GLOBAL_ATTRIBUTE_NAME = \"distribution\"\n+\n+\n+@keras_export(\"keras.distribution.list_devices\")\n+def list_devices(device_type=None):\n+    \"\"\"Return all the available devices based on the device type.\n+\n+    Note: in a distributed setting, global devices are returned.\n+\n+    Args:\n+        device_type: string, one of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`.\n+            Defaults to `\"gpu\"` or `\"tpu\"` if available when\n+            `device_type` is not provided. Otherwise\n+            will return the `\"cpu\"` devices.\n+\n+    Return:\n+        List of devices that are available for distribute computation.\n+    \"\"\"\n+    return list_devices(device_type)\n+\n+\n+@keras_export(\"keras.distribution.initialize\")\n+def initialize(job_addresses=None, num_processes=None, process_id=None):\n+    \"\"\"Initialize the distribution system for multi-host/process setting.\n+\n+    Calling `initialize` will prepare the backend for execution on multi-host\n+    GPU or TPUs. It should be called before any computations.\n+\n+    Note that the parameters can also be injected via environment variables,\n+    which can be better controlled by the launch script at startup time.\n+    For certain backend that also rely on the environment variables to\n+    configure, Keras will properly forward them.\n+\n+    Args:\n+        job_addresses: string. Comma separated IP addresses for all the jobs\n+            that will form the whole computation cluster. Note that for JAX\n+            backend, only the address for job 0 (coodinator) is needed. For\n+            certain runtime like cloud TPU, this value can be `None`, and the\n+            backend will figure it out with the TPU environment variables. You\n+            can also config this value via environment variable\n+            `KERAS_DISTRIBUTION_JOB_ADDRESSES`.\n+        num_processes: int. The number of worker/processes that will form the\n+            whole computation cluster. For certain runtime like cloud TPU, this\n+            value can be `None`, and the backend will figure it out with the TPU\n+            environment variables. You can also configure this value via\n+            environment variable `KERAS_DISTRIBUTION_NUM_PROCESSES`.\n+        process_id: int. The ID number of the current worker/process. The value\n+            should be ranged from `0` to `num_processes - 1`. `0` will indicate\n+            the current worker/process is the master/coordinate job. You can\n+            also configure this value via environment variable\n+            `KERAS_DISTRIBUTION_PROCESS_ID`.\n+\n+        Example:\n+            Suppose there are two GPU processes, and process 0 is running at\n+            address `10.0.0.1:1234`, and process 1 is running at address\n+            `10.0.0.2:2345`. To configure such cluster, you can run\n+\n+        On process 0:\n+        ```python\n+        keras.distribute.initialize(\n+            job_addresses=\"10.0.0.1:1234,10.0.0.2:2345\",\n+            num_processes=2,\n+            process_id=0)\n+        ```\n+\n+        On process 1:\n+        ```python\n+        keras.distribute.initialize(\n+            job_addresses=\"10.0.0.1:1234,10.0.0.2:2345\",\n+            num_processes=2,\n+            process_id=1)\n+        ```\n+\n+        or via the environment variables:\n+        On process 0:\n+        ```python\n+        os.environ[\n+            \"KERAS_DISTRIBUTION_JOB_ADDRESSES\"] = \"10.0.0.1:1234,10.0.0.2:2345\"\n+        os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"] = \"2\"\n+        os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"] = \"0\"\n+        keras.distribute.initialize()\n+        ```\n+\n+        On process 1:\n+        ```python\n+        os.environ[\n+            \"KERAS_DISTRIBUTION_JOB_ADDRESSES\"] = \"10.0.0.1:1234,10.0.0.2:2345\"\n+        os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"] = \"2\"\n+        os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"] = \"1\"\n+        keras.distribute.initialize()\n+        ```\n+\n+        Also note that for JAX backend, the `job_addresses` can be further\n+        reduced to just the master/coordinator address, which is\n+        `10.0.0.1:1234`.\n+    \"\"\"\n+    if (\n+        job_addresses is None\n+        and \"KERAS_DISTRIBUTION_JOB_ADDRESSES\" in os.environ\n+    ):\n+        job_addresses = os.environ[\"KERAS_DISTRIBUTION_JOB_ADDRESSES\"]\n+    if (\n+        num_processes is None\n+        and \"KERAS_DISTRIBUTION_NUM_PROCESSES\" in os.environ\n+    ):\n+        num_processes = int(os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"])\n+    if process_id is None and \"KERAS_DISTRIBUTION_PROCESS_ID\" in os.environ:\n+        process_id = int(os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"])\n+    initialize(job_addresses, num_processes, process_id)\n+\n+\n+@keras_export(\"keras.distribution.DeviceMesh\")\n+class DeviceMesh:\n+    \"\"\"A cluster of computation devices for distributed computation.\n+\n+    This API is aligned with `jax.sharding.Mesh` and `tf.dtensor.Mesh`, which\n+    represents the computation devices in the global context.\n+\n+    See more details in [jax.sharding.Mesh](\n+        https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh)\n+    and [tf.dtensor.Mesh](\n+        https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Mesh).\n+\n+    Args:\n+        shape: tuple of list of integers. The shape of the overall\n+            `DeviceMesh`, e.g. `(8,)` for a data parallel only distribution,\n+            or `(4, 2)` for a model+data parallel distribution.\n+        axis_names: List of string. The logical name of the each axis for\n+            the `DeviceMesh`. The length of the `axis_names` should match to\n+            the rank of the `shape`. The `axis_names` will be used to\n+            match/create the `TensorLayout` when distribute the data and\n+            variables.\n+        devices: Optional list of devices. Defaults to all the available\n+            devices locally from `keras.distribution.list_devices()`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        shape,\n+        axis_names,\n+        devices=None,\n+    ):\n+        if not shape or not axis_names:\n+            raise ValueError(\n+                \"Shape and axis_names cannot be empty. Received: \"\n+                f\"shape={shape}, axis_names={axis_names}\"\n+            )\n+\n+        if len(shape) != len(axis_names):\n+            raise ValueError(\n+                \"Shape and axis_names should have same size. \"\n+                f\"Received: shape={shape}, axis_names={axis_names}\"\n+            )\n+        if devices is None:\n+            devices = list_devices()\n+        devices = np.array(devices)\n+        if np.prod(shape) != np.prod(devices.shape):\n+            raise ValueError(\n+                \"Shape does not match the number of devices. \"\n+                f\"Received: shape={shape}; devices.shape=\"\n+                f\"{devices.shape}\"\n+            )\n+\n+        self._shape = shape\n+        self._axis_names = axis_names\n+        self._devices = np.reshape(devices, shape)\n+\n+    @property\n+    def shape(self):\n+        return self._shape\n+\n+    @property\n+    def axis_names(self):\n+        return self._axis_names\n+\n+    @property\n+    def devices(self):\n+        return self._devices\n+\n+    @property\n+    def backend_mesh(self):\n+        if not hasattr(self, \"_backend_mesh\"):\n+            self._backend_mesh = _to_backend_mesh(self)\n+        return self._backend_mesh\n+\n+    def __repr__(self):\n+        return (\n+            f\"<{self.__class__.__name__} \"\n+            f\"shape={self.shape}, axis_names={self.axis_names}>\"\n+        )\n+\n+    def __str__(self):\n+        return self.__repr__()\n+\n+\n+@keras_export(\"keras.distribution.TensorLayout\")\n+class TensorLayout:\n+    \"\"\"A layout to apply to a tensor.\n+\n+    This API is aligned with `jax.sharding.NamedSharding`\n+    and `tf.dtensor.Layout`.\n+\n+    See more details in [jax.sharding.NamedSharding](\n+        https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding)\n+    and [tf.dtensor.Layout](\n+        https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Layout).\n+\n+    Args:\n+        axes: tuple of strings that should map to the `axis_names` in\n+            a `DeviceMesh`. For any dimensions that doesn't need any sharding,\n+            A `None` can be used a placeholder.\n+        device_mesh: Optional `DeviceMesh` that will be used to create\n+            the layout. The actual mapping of tensor to physical device\n+            is not known until the mesh is specified.\n+    \"\"\"\n+\n+    def __init__(self, axes, device_mesh=None):\n+        self._axes = tuple(axes)\n+        self._device_mesh = device_mesh\n+        self._validate_axes()\n+\n+    @property\n+    def axes(self):\n+        return self._axes\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    @device_mesh.setter\n+    def device_mesh(self, device_mesh):\n+        if self._device_mesh is not None:\n+            raise ValueError(\n+                \"Cannot override device mesh value. Existing \"\n+                f\"value is {self._device_mesh}\"\n+            )\n+        self._device_mesh = device_mesh\n+        self._validate_axes()\n+\n+    @property\n+    def backend_layout(self):\n+        if not hasattr(self, \"_backend_layout\"):\n+            self._backend_layout = _to_backend_layout(self)\n+        return self._backend_layout\n+\n+    def _validate_axes(self):\n+        if self._device_mesh:\n+            valid_axis_names = set(self._device_mesh.axis_names)\n+            axis_names = set(self._axes) - set([None])\n+            if axis_names - valid_axis_names:\n+                raise ValueError(\n+                    \"Invalid axis names for Layout. Valid axis \"\n+                    f\"names: {valid_axis_names}, Got {axis_names}\"\n+                )\n+\n+    def __repr__(self):\n+        return (\n+            f\"<{self.__class__.__name__} \"\n+            f\"axes={self.axes}, device_mesh={self.device_mesh}>\"\n+        )\n+\n+    def __str__(self):\n+        return self.__repr__()\n+\n+\n+class Distribution:\n+    \"\"\"Base class for variable distribution strategies.\n+\n+    A `Distribution` has following key functionalities:\n+\n+    1. Distribute the model variables to a `DeviceMesh`.\n+    2. Distribute the input data to a `DeviceMesh`.\n+    3. Distribute an intermediate state tensor in the model.\n+\n+    It can create a context scope so that the framework to properly detect the\n+    `Distribution` and distribute the variable/data accordingly.\n+\n+    Args:\n+        device_mesh: A `DeviceMesh` instance.\n+        batch_dim_name: Optional string name for the batch dimension.\n+            Defaults to None.\n+        auto_shard_dataset: Automatically shard the dataset amongst\n+            processes in a multi-process setting. Set to `False` if the dataset\n+            is already sharded across hosts.  Defaults to `True`.\n+    \"\"\"\n+\n+    def __init__(\n+        self, device_mesh, batch_dim_name=None, auto_shard_dataset=True\n+    ):\n+        self._device_mesh = device_mesh\n+        self._batch_dim_name = batch_dim_name\n+        self._auto_shard_dataset = auto_shard_dataset\n+\n+    def get_data_layout(self, data_shape):\n+        \"\"\"Retrieve the `TensorLayout` for the input data.\n+\n+        Args:\n+            data_shape: shape for the input data in list or tuple format.\n+\n+        Returns:\n+            The `TensorLayout` for the data, which can be used by\n+            `backend.distribute_value()` to redistribute a input data.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def get_variable_layout(self, variable):\n+        \"\"\"Retrieve the `TensorLayout` for the variable.\n+\n+        Args:\n+            variable: A `Variable` instance.\n+\n+        return:\n+            The `TensorLayout` for the variable, which can be used by\n+            `backend.distribute_value()` to redistribute a variable.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def get_tensor_layout(self, path):\n+        \"\"\"Retrieve the `TensorLayout` for the intermediate tensor.\n+\n+        Args:\n+            path: a string path for the corresponding tensor.\n+\n+        return:\n+            The `TensorLayout` for the intermediate tensor, which can be used\n+            by `backend.relayout()` to reshard the tensor. Could also return\n+            None.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    @contextlib.contextmanager\n+    def scope(self):\n+        \"\"\"Context manager to make the `Distribution` current.\"\"\"\n+        original_scope = distribution()\n+        set_distribution(self)\n+        try:\n+            yield\n+        finally:\n+            set_distribution(original_scope)\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    @property\n+    def batch_dim_name(self):\n+        return self._batch_dim_name\n+\n+    @property\n+    def auto_shard_dataset(self):\n+        return self._auto_shard_dataset\n+\n+    @auto_shard_dataset.setter\n+    def auto_shard_dataset(self, auto_shard_dataset):\n+        self._auto_shard_dataset = auto_shard_dataset\n+\n+    def distribute_dataset(self, dataset):\n+        \"\"\"Create a distributed dataset from the original global dataset.\n+\n+        Args:\n+            dataset: the original global dataset instance.\n+\n+        Returns:\n+            If `auto_shard_dataset` is `True`, returns a sharded dataset that\n+            only produces data for the current local worker/process.  Otherwise,\n+            returns the original dataset.\n+\n+        Raises:\n+            ValueError: if auto-sharding is requested in a multi-process\n+            setting, but the dataset type is not supported.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def __repr__(self):\n+        return f\"<{self.__class__.__name__} device_mesh={self.device_mesh}>\"\n+\n+    def __str__(self):\n+        return self.__repr__()\n+\n+@keras_export(\"keras.distribution.AutoShardDistribution\")\n+class AutoShardDistribution(Distribution):\n+    def __init__(\n+        self,\n+        device_mesh=None,\n+    ):\n+        if device_mesh is None:\n+            devices = np.array(list_devices())\n+            axis_names = [DEFAULT_BATCH_DIM_NAME] + [\n+                f\"model_{i}\" for i in range(devices.ndim - 1)\n+            ]\n+            device_mesh = DeviceMesh(\n+                shape=devices.shape,\n+                axis_names=axis_names,\n+                devices=devices,\n+            )\n+        super().__init__(device_mesh, device_mesh.axis_names[0])\n+        self._sharding_plan = None\n+        self._sharding_planner = None\n+        self._shard_applier = None\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+\n+    def _get_backend_components(self):\n+        if self._sharding_planner and self._shard_applier:\n+            return\n+        self._sharding_planner = get_sharding_planner()\n+        self._shard_applier = get_shard_applier()\n+\n+    def shard(self, model, *args, **kwargs):\n+        self._get_backend_components()\n+        graph = create_graph_from_model(model, *args, **kwargs)\n+\n+        plan = self._sharding_planner.plan(graph, self.device_mesh)\n+        self._sharding_plan = plan\n+        self._shard_applier.apply(model, self._sharding_plan)\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self.batch_dim_name\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+    def get_tensor_layout(self, path):\n+        return None\n+\n+    def distribute_dataset(self, dataset):\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        if not tf.available or not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for auto-sharding, \"\n+                f\"got {type(dataset)}\"\n+            )\n+\n+        from tensorflow.python.data.experimental.ops import (\n+            distribute as tf_data_distribute,\n+        )\n+\n+        batch_size = tf_data_distribute.compute_batch_size(dataset)\n+        if batch_size.numpy() < 0:\n+            raise ValueError(\n+                \"The batch size of the input dataset is \"\n+                \"unknown. Please config the batch size for \"\n+                \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n+            )\n+        per_worker_batch_size = tf_data_distribute.batch_sizes_for_worker(\n+            global_batch_size=batch_size,\n+            num_workers=self._num_process,\n+            num_replicas_per_worker=1,  # We hard code this for now.\n+            worker_index=self._process_id,\n+        )\n+        distributed_dataset = dataset.rebatch(per_worker_batch_size)\n+        distributed_dataset = tf_data_distribute._AutoShardDataset(\n+            distributed_dataset,\n+            num_workers=self._num_process,\n+            index=self._process_id,\n+            num_replicas=self._num_process,\n+        )\n+        return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+\n+@keras_export(\"keras.distribution.DataParallel\")\n+class DataParallel(Distribution):\n+    \"\"\"Distribution for data parallelism.\n+\n+    You can choose to create this instance by either specifying\n+    the `device_mesh` or `devices` arguments (but not both).\n+\n+    The `device_mesh` argument is expected to be a `DeviceMesh` instance,\n+    and is expected to be 1D only. In case that the mesh has multiple axes,\n+    then the first axis will be treated as the data parallel dimension\n+    (and a warning will be raised).\n+\n+    When a list of `devices` are provided, they will be used to construct a\n+    1D mesh.\n+\n+    When both `mesh` and `devices` are absent, then `list_devices()`\n+    will be used to detect any available devices and create a 1D mesh from\n+    them.\n+\n+    Args:\n+        device_mesh: Optional `DeviceMesh` instance.\n+        devices: Optional list of devices.\n+        auto_shard_dataset: Automatically shard the dataset amongst\n+            processes in a multi-process setting. Set to `False` if the dataset\n+            is already sharded across hosts.  Defaults to `True`.\n+    \"\"\"\n+\n+    def __init__(self, device_mesh=None, devices=None, auto_shard_dataset=True):\n+        if device_mesh:\n+            self._initialize_with_device_mesh(device_mesh, auto_shard_dataset)\n+        elif devices:\n+            self._initialize_mesh_from_devices(devices, auto_shard_dataset)\n+        else:\n+            self._initialize_mesh_from_list_devices(auto_shard_dataset)\n+\n+        # Those following attributes might get convert to public methods.\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+        self._is_multi_process = self._num_process > 1\n+\n+    def _initialize_with_device_mesh(self, device_mesh, auto_shard_dataset):\n+        if not isinstance(device_mesh, DeviceMesh):\n+            raise ValueError(\n+                \"Expect `mesh` to be an instance of `DeviceMesh`. \"\n+                f\"Received: mesh={device_mesh} (of type {type(device_mesh)})\"\n+            )\n+        super().__init__(\n+            device_mesh, device_mesh.axis_names[0], auto_shard_dataset\n+        )\n+        if self.device_mesh.devices.ndim != 1:\n+            warnings.warn(\n+                \"Expect the input mesh to be 1D, but received \"\n+                \"mesh.devices.ndim=%d. \"\n+                \"The first axis will be used for data-parallel sharding.\",\n+                device_mesh.devices.ndim,\n+            )\n+\n+    def _initialize_mesh_from_devices(self, devices, auto_shard_dataset):\n+        devices = np.array(devices)\n+        device_mesh = DeviceMesh(\n+            shape=devices.shape,\n+            axis_names=[DEFAULT_BATCH_DIM_NAME],\n+            devices=devices,\n+        )\n+        super().__init__(\n+            device_mesh, DEFAULT_BATCH_DIM_NAME, auto_shard_dataset\n+        )\n+\n+    def _initialize_mesh_from_list_devices(self, auto_shard_dataset):\n+        devices = np.array(list_devices())\n+        device_mesh = DeviceMesh(\n+            shape=devices.shape,\n+            axis_names=[DEFAULT_BATCH_DIM_NAME],\n+            devices=devices,\n+        )\n+        super().__init__(\n+            device_mesh, DEFAULT_BATCH_DIM_NAME, auto_shard_dataset\n+        )\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self.batch_dim_name  # Shard on the first dim\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        # First check if the variable already has a layout assigned.\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        # Otherwise, replicate variable.\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+    def get_tensor_layout(self, path):\n+        # For data parallel training, the intermediate state is not changed.\n+        return None\n+\n+    def distribute_dataset(self, dataset):\n+        if not self._is_multi_process or not self.auto_shard_dataset:\n+            return dataset\n+\n+        # Try to distribute a global tf.data.Dataset.\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        if not tf.available or not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for auto-sharding, \"\n+                f\"got {type(dataset)}\"\n+            )\n+\n+        from tensorflow.python.data.experimental.ops import (\n+            distribute as tf_data_distribute,\n+        )\n+\n+        batch_size = tf_data_distribute.compute_batch_size(dataset)\n+        if batch_size.numpy() < 0:\n+            raise ValueError(\n+                \"The batch size of the input dataset is \"\n+                \"unknown. Please config the batch size for \"\n+                \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n+            )\n+        per_worker_batch_size = tf_data_distribute.batch_sizes_for_worker(\n+            global_batch_size=batch_size,\n+            num_workers=self._num_process,\n+            num_replicas_per_worker=1,  # We hard code this for now.\n+            worker_index=self._process_id,\n+        )\n+        distributed_dataset = dataset.rebatch(per_worker_batch_size)\n+        distributed_dataset = tf_data_distribute._AutoShardDataset(\n+            distributed_dataset,\n+            num_workers=self._num_process,\n+            index=self._process_id,\n+            num_replicas=self._num_process,\n+        )\n+        return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+\n+\n+@keras_export(\"keras.distribution.ModelParallel\")\n+class ModelParallel(Distribution):\n+    \"\"\"Distribution that shards model variables.\n+\n+    Compare to `DataParallel` which replicates the variables across all devices,\n+    `ModelParallel` allows you to shard variables in addition to the input data.\n+\n+    To construct a `ModelParallel` distribution, you need to provide a\n+    `DeviceMesh` and a `LayoutMap`.\n+\n+    1. `DeviceMesh` contains physical device information. The axis names in\n+        the mesh will be used to map the variable and data layout.\n+    2. `LayoutMap` contains the mapping between variable paths to their\n+        corresponding `TensorLayout`.\n+\n+    Example:\n+\n+    ```python\n+    devices = list_devices()    # Assume there are 8 devices.\n+\n+    # Create a mesh with 2 devices for data parallelism and 4 devices for\n+    # model parallelism.\n+    device_mesh = DeviceMesh(shape=(2, 4), axis_names=('batch', 'model'),\n+                             devices=devices)\n+    # Create a layout map that shard the `Dense` layer and `Conv2D`\n+    # layer variables on the last dimension.\n+    # Based on the `device_mesh`, this means the variables\n+    # will be split across 4 devices. Any other variable that doesn't\n+    # match any key in the layout map will be fully replicated.\n+    layout_map = LayoutMap(device_mesh)\n+    layout_map['dense.*kernel'] = (None, 'model')\n+    layout_map['dense.*bias'] = ('model',)\n+    layout_map['conv2d.*kernel'] = (None, None, None, 'model')\n+    layout_map['conv2d.*bias'] = ('model',)\n+\n+    distribution = ModelParallel(\n+        layout_map=layout_map,\n+        batch_dim_name='batch',\n+    )\n+\n+    # Set the global distribution, or via `with distribution.scope():`\n+    set_distribution(distribution)\n+\n+    model = model_creation()\n+    model.compile()\n+    model.fit(data)\n+    ```\n+\n+    You can quickly update the device mesh shape to change the sharding factor\n+    of the variables. E.g.\n+\n+    ```python\n+    # With only the shape change for the device mesh, the variables will be\n+    # sharded across 8 devices instead of 4, which further reduces the memory\n+    # footprint of variables on each of the device.\n+    device_mesh = DeviceMesh(\n+        shape=(1, 8),\n+        axis_names=('batch', 'model'),\n+        devices=devices,\n+    )\n+    ```\n+\n+    To figure out a proper layout mapping rule for all the model variables, you\n+    can first list out all the model variable paths, which will be used as the\n+    key to map the variables to `TensorLayout`.\n+\n+    e.g.\n+\n+    ```python\n+    model = create_model()\n+    for v in model.variables:\n+        print(v.path)\n+    ```\n+\n+    Args:\n+        layout_map: `LayoutMap` instance which map the variable path to the\n+            corresponding tensor layout.\n+        batch_dim_name: Optional string, the axis name in the device mesh\n+            (of the `layout_map` object)\n+            that will be used to distribute data. If unspecified, the\n+            first axis from the device mesh will be used.\n+        auto_shard_dataset: Automatically shard the dataset amongst\n+            processes in a multi-process setting. Set to `False` if the dataset\n+            is already sharded across hosts.  Defaults to `True`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        layout_map=None,\n+        batch_dim_name=None,\n+        auto_shard_dataset=True,\n+        **kwargs,\n+    ):\n+        kwargs.pop(\"device_mesh\", None)\n+        if layout_map is None:\n+            raise ValueError(\"You must specify a layout_map argument.\")\n+        if not isinstance(layout_map, LayoutMap):\n+            raise ValueError(\n+                \"Argument `layout_map` must be a `LayoutMap` instance. \"\n+                f\"Received: layout_map={layout_map}\"\n+            )\n+        device_mesh = layout_map.device_mesh\n+        batch_dim_name = batch_dim_name or device_mesh.axis_names[0]\n+        super().__init__(device_mesh, batch_dim_name, auto_shard_dataset)\n+        self._layout_map = layout_map\n+\n+        # Those following attributes might get convert to public methods.\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+        self._is_multi_process = self._num_process > 1\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self.batch_dim_name  # Shard on the first dim\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        # First check if the variable already has a layout assigned.\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        # Check the layout map.\n+        variable_layout = self._layout_map[variable.path]\n+        if variable_layout is not None:\n+            return variable_layout\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+    def get_tensor_layout(self, path):\n+        return self._layout_map[path]\n+\n+    def distribute_dataset(self, dataset):\n+        if not self._is_multi_process or not self.auto_shard_dataset:\n+            return dataset\n+\n+        # Try to distribute a global tf.data.Dataset.\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        if not tf.available or not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for auto-sharding, \"\n+                f\"got {type(dataset)}\"\n+            )\n+\n+        from tensorflow.python.data.experimental.ops import (\n+            distribute as tf_data_distribute,\n+        )\n+\n+        global_batch_size = tf_data_distribute.compute_batch_size(dataset)\n+        if global_batch_size.numpy() < 0:\n+            raise ValueError(\n+                \"The batch size of the input dataset is \"\n+                \"unknown. Please config the batch size for \"\n+                \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n+            )\n+\n+        # We need to compute the per-process/worker/host batch size.\n+        # This will depend on how many model replicas we have on each process.\n+        # Note that this might be smaller than one if model replicas are sharded\n+        # across multiple processes.\n+        mesh_batch_dim_index = self.device_mesh.axis_names.index(\n+            self.batch_dim_name\n+        )\n+        num_model_replicas = self.device_mesh.shape[mesh_batch_dim_index]\n+        if num_model_replicas == 1:\n+            # No sharding is needed in this case. Each process will have the\n+            # global batch size, and data from the iterator will need to be\n+            # replicated across all processes.\n+            return dataset.prefetch(tf.data.AUTOTUNE)\n+        num_model_replicas_per_process = num_model_replicas / self._num_process\n+        if num_model_replicas_per_process >= 1:\n+            # Each process will have one or more full model replicas. Data will\n+            # be sharded across all processes without replication.\n+            if global_batch_size % self._num_process != 0:\n+                raise ValueError(\n+                    \"Global batch size must be divisible by the number of \"\n+                    f\"processes. `global_batch_size`={global_batch_size} and \"\n+                    f\"`num_process`={self._num_process}\"\n+                )\n+            per_process_batch_size = global_batch_size // self._num_process\n+            distributed_dataset = dataset.rebatch(per_process_batch_size)\n+            distributed_dataset = distributed_dataset.shard(\n+                num_shards=self._num_process,\n+                index=self._process_id,\n+            )\n+            return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+        else:\n+            # Model replicas are sharded across multiple processes. Data will be\n+            # sharded across model replicas, and replicated across processes\n+            # within the same model replica.\n+            if global_batch_size % num_model_replicas != 0:\n+                raise ValueError(\n+                    \"Global batch size must be divisible by the number of \"\n+                    f\"replicas. `global_batch_size`={global_batch_size} and \"\n+                    f\"`num_model_replicas`={num_model_replicas}\"\n+                )\n+            per_process_batch_size = global_batch_size // num_model_replicas\n+            distributed_dataset = dataset.rebatch(per_process_batch_size)\n+            processes_per_replica = self._num_process // num_model_replicas\n+            # TODO: Figure out what the convention is for data sharding id.\n+            data_shard_id = self._process_id % processes_per_replica\n+            distributed_dataset = distributed_dataset.shard(\n+                num_shards=num_model_replicas,\n+                index=data_shard_id,\n+            )\n+            return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+\n+\n+@keras_export(\"keras.distribution.LayoutMap\")\n+class LayoutMap(collections.abc.MutableMapping):\n+    \"\"\"A dict-like object that maps string to `TensorLayout` instances.\n+\n+    `LayoutMap` uses a string as key and a `TensorLayout` as value. There is a\n+    behavior difference between a normal Python dict and this class. The string\n+    key will be treated as a regex when retrieving the value. See the docstring\n+    of `get` for more details.\n+\n+    See below for a usage example. You can define the naming schema\n+    of the `TensorLayout`, and then retrieve the corresponding\n+    `TensorLayout` instance.\n+\n+    In the normal case, the key to query is usually the `variable.path`, which\n+    is the identifier of the variable.\n+\n+    As shortcut, tuple or list of axis names are also allowed when inserting\n+    as value, and will be converted to `TensorLayout`.\n+\n+    ```python\n+    layout_map = LayoutMap(device_mesh)\n+    layout_map['dense.*kernel'] = (None, 'model')\n+    layout_map['dense.*bias'] = ('model',)\n+    layout_map['conv2d.*kernel'] = (None, None, None, 'model')\n+    layout_map['conv2d.*bias'] = ('model',)\n+\n+    layout_1 = layout_map['dense_1.kernel']             # layout_1 == layout_2d\n+    layout_2 = layout_map['dense_1.bias']               # layout_2 == layout_1d\n+    layout_3 = layout_map['dense_2.kernel']             # layout_3 == layout_2d\n+    layout_4 = layout_map['dense_2.bias']               # layout_4 == layout_1d\n+    layout_5 = layout_map['my_model/conv2d_123/kernel'] # layout_5 == layout_4d\n+    layout_6 = layout_map['my_model/conv2d_123/bias']   # layout_6 == layout_1d\n+    layout_7 = layout_map['my_model/conv3d_1/kernel']   # layout_7 == None\n+    layout_8 = layout_map['my_model/conv3d_1/bias']     # layout_8 == None\n+    ```\n+\n+    Args:\n+        device_mesh: `keras.distribution.DeviceMesh` instance.\n+    \"\"\"\n+\n+    def __init__(self, device_mesh):\n+        self._layout_map = collections.OrderedDict()\n+        self._device_mesh = device_mesh\n+\n+    def __getitem__(self, key):\n+        \"\"\"Retrieves the corresponding layout by the string key.\n+\n+        When there isn't an exact match, all the existing keys in the layout map\n+        will be treated as a regex and map against the input key again. When\n+        there are multiple matches for the regex, an `ValueError` will be\n+        raised. Returns `None` if there isn't any match found.\n+\n+        Args:\n+            key: String key to query a layout.\n+\n+        Returns:\n+            Corresponding layout based on the query.\n+        \"\"\"\n+        if key in self._layout_map:\n+            return self._layout_map[key]\n+\n+        matching_keys = []\n+        for k in self._layout_map:\n+            if re.search(k, key):\n+                matching_keys.append(k)\n+        if len(matching_keys) > 1:\n+            raise ValueError(\n+                f\"Path '{key}' matches multiple layout \"\n+                f\"specification keys: {matching_keys}. Please make \"\n+                \"sure each tensor/variable path only matches at most \"\n+                \"one layout specification key in the LayoutMap.\"\n+            )\n+        elif len(matching_keys) == 1:\n+            return self._layout_map[matching_keys[0]]\n+        return None\n+\n+    def __setitem__(self, key, layout):\n+        \"\"\"Insert TensorLayout to the LayoutMap.\n+\n+        Args:\n+            key: String key for the `TensorLayout`.\n+            layout: The `TensorLayout`. As a shortcut, tuple of string and None\n+                are also acceptable, and will be converted to `TensorLayout`.\n+        \"\"\"\n+        if key in self._layout_map:\n+            raise ValueError(\n+                f\"{key} already exist in the LayoutMap with \"\n+                f\"value {self._layout_map[key]}. Please make sure to \"\n+                \"not use duplicated keys.\"\n+            )\n+        if isinstance(layout, tuple):\n+            layout = TensorLayout(axes=layout, device_mesh=None)\n+\n+        if not isinstance(layout, TensorLayout):\n+            raise ValueError(\n+                f\"{layout} should be a TensorLayout type, got {type(layout)}\"\n+            )\n+        self._maybe_populate_device_mesh(layout)\n+        self._layout_map[key] = layout\n+\n+    def __delitem__(self, key):\n+        # let the dict to handle the key missing error\n+        return self._layout_map.pop(key)\n+\n+    def __len__(self):\n+        return len(self._layout_map)\n+\n+    def __iter__(self):\n+        return iter(self._layout_map)\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    def _maybe_populate_device_mesh(self, layout):\n+        if layout.device_mesh is None and self.device_mesh is not None:\n+            layout.device_mesh = self.device_mesh\n+\n+\n+LayoutMap.get.__doc__ = LayoutMap.__getitem__.__doc__\n+\n+\n+@keras_export(\"keras.distribution.distribute_tensor\")\n+def distribute_tensor(tensor, layout):\n+    \"\"\"Change the layout of a Tensor value in the jit function execution.\n+\n+    Args:\n+        tensor: a Tensor to change the layout.\n+        layout: `TensorLayout` to be applied on the value.\n+\n+    Returns:\n+        a new value with the specified tensor layout.\n+    \"\"\"\n+    if isinstance(tensor, KerasTensor):\n+        return tensor\n+    return distribute_tensor(tensor, layout)\n+\n+\n+@keras_export(\"keras.distribution.distribution\")\n+def distribution():\n+    \"\"\"Retrieve the current distribution from global context.\"\"\"\n+    return global_state.get_global_attribute(GLOBAL_ATTRIBUTE_NAME)\n+\n+\n+@keras_export(\"keras.distribution.set_distribution\")\n+def set_distribution(value):\n+    \"\"\"Set the distribution as the global distribution setting.\n+\n+    Args:\n+        value: a `Distribution` instance.\n+    \"\"\"\n+    global_state.set_global_attribute(GLOBAL_ATTRIBUTE_NAME, value)\n+\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThis test file contains a very large amount of duplicated code from `keras/src/backend/jax/distribution_lib.py` and `keras/src/distribution/distribution_lib.py`. This is a significant maintenance concern, as any changes in the source files will not be reflected here, leading to outdated and incorrect tests. Please refactor this file to import the necessary functions and classes directly instead of copy-pasting them.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/distribution/test_memory.py",
    "pr_number": 21644,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2331844224,
    "comment_created_at": "2025-09-09T02:35:39Z"
  },
  {
    "code": "@@ -65,28 +65,28 @@ class StringLookup(IndexLookup):\n             If this value is 0, OOV inputs will cause an error when calling\n             the layer. Defaults to `1`.\n         mask_token: A token that represents masked inputs. When `output_mode` is\n-            `\"int\"`, the token is included in vocabulary and mapped to index 0.\n-            In other output modes, the token will not appear\n-            in the vocabulary and instances of the mask token\n-            in the input will be dropped. If set to `None`,\n-            no mask term will be added. Defaults to `None`.\n+            `\"int\"`, the token is included in the vocabulary and mapped to index\n+            0.\n+            In other output modes, the token will not appear in the vocabulary\n+            and instances of the mask token in the input will be dropped.\n+            If set to `None`, no mask term will be added. Defaults to `None`.\n         oov_token: Only used when `invert` is True. The token to return for OOV\n             indices. Defaults to `\"[UNK]\"`.\n         vocabulary: Optional. Either an array of integers or a string path to a\n-            text file. If passing an array, can pass a tuple, list,\n-            1D NumPy array, or 1D tensor containing the integer vocbulary terms.\n-            If passing a file path, the file should contain one line per term\n-            in the vocabulary. If this argument is set,\n-            there is no need to `adapt()` the layer.\n+            text file. If passing an array, you can pass a tuple, list, 1D NumPy\n+            array, or 1D tensor containing the integer vocabulary terms.\n+            If passing a file path, the file should contain one line per term in\n+            the vocabulary. If this argument is set, there is no need to\n+            `adapt()` the layer.",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe description for the vocabulary argument is incorrect. it states that the array should contain \"integer vocabulary terms\" and can be an \"array of integers\". for stringlookup, the vocabulary consists of strings. this appears to be a copy-paste error, likely from integerlookup.\n\nto improve clarity and correctness, i suggest updating the description to specify string types.",
    "line_number": 80,
    "enriched": "File: keras/src/layers/preprocessing/string_lookup.py\nCode: @@ -65,28 +65,28 @@ class StringLookup(IndexLookup):\n             If this value is 0, OOV inputs will cause an error when calling\n             the layer. Defaults to `1`.\n         mask_token: A token that represents masked inputs. When `output_mode` is\n-            `\"int\"`, the token is included in vocabulary and mapped to index 0.\n-            In other output modes, the token will not appear\n-            in the vocabulary and instances of the mask token\n-            in the input will be dropped. If set to `None`,\n-            no mask term will be added. Defaults to `None`.\n+            `\"int\"`, the token is included in the vocabulary and mapped to index\n+            0.\n+            In other output modes, the token will not appear in the vocabulary\n+            and instances of the mask token in the input will be dropped.\n+            If set to `None`, no mask term will be added. Defaults to `None`.\n         oov_token: Only used when `invert` is True. The token to return for OOV\n             indices. Defaults to `\"[UNK]\"`.\n         vocabulary: Optional. Either an array of integers or a string path to a\n-            text file. If passing an array, can pass a tuple, list,\n-            1D NumPy array, or 1D tensor containing the integer vocbulary terms.\n-            If passing a file path, the file should contain one line per term\n-            in the vocabulary. If this argument is set,\n-            there is no need to `adapt()` the layer.\n+            text file. If passing an array, you can pass a tuple, list, 1D NumPy\n+            array, or 1D tensor containing the integer vocabulary terms.\n+            If passing a file path, the file should contain one line per term in\n+            the vocabulary. If this argument is set, there is no need to\n+            `adapt()` the layer.\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe description for the `vocabulary` argument is incorrect. It states that the array should contain \"integer vocabulary terms\" and can be an \"array of integers\". For `StringLookup`, the vocabulary consists of strings. This appears to be a copy-paste error, likely from `IntegerLookup`.\n\nTo improve clarity and correctness, I suggest updating the description to specify string types.\n\n```suggestion\n        vocabulary: Optional. Either an array of strings or a string path to a\n            text file. If passing an array, you can pass a tuple, list, 1D NumPy\n            array, or 1D tensor containing the string vocabulary terms.\n            If passing a file path, the file should contain one line per term in\n            the vocabulary. If this argument is set, there is no need to\n            `adapt()` the layer.\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/layers/preprocessing/string_lookup.py",
    "pr_number": 21643,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2331684906,
    "comment_created_at": "2025-09-09T00:39:43Z"
  },
  {
    "code": "@@ -288,6 +288,54 @@ def get_config(self):\n         return config\n \n \n+@keras_export(\"keras.dtype_policies.GPTQDTypePolicy\")\n+class GPTQDTypePolicy(QuantizedDTypePolicy):\n+    \"\"\"Quantized dtype policy for GPTQ quantization.\n+\n+    This policy helps propagate quantization settings for GPTQ\n+    when loading a GPTQ quantized model in Keras format.\n+\n+    Args:\n+        mode: The quantization mode, \"gptq\".\n+        source_name: The source dtype policy name, e.g. \"float32\".\n+        weight_bits: Number of bits to quantize weights to. Supported values\n+            are 2, 3, 4, and 8.\n+        group_size: The group size for quantization. Supported values are\n+            -1 (for whole-tensor quantization) or any positive integer.\n+            Typically a smaller group size leads to better accuracy but\n+            slower speed.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        mode,\n+        source_name=None,\n+        weight_bits=None,\n+        group_size=None,",
    "comment": "i don't think it will work if weight_bits is none or group_size is none.",
    "line_number": 314,
    "enriched": "File: keras/src/dtype_policies/dtype_policy.py\nCode: @@ -288,6 +288,54 @@ def get_config(self):\n         return config\n \n \n+@keras_export(\"keras.dtype_policies.GPTQDTypePolicy\")\n+class GPTQDTypePolicy(QuantizedDTypePolicy):\n+    \"\"\"Quantized dtype policy for GPTQ quantization.\n+\n+    This policy helps propagate quantization settings for GPTQ\n+    when loading a GPTQ quantized model in Keras format.\n+\n+    Args:\n+        mode: The quantization mode, \"gptq\".\n+        source_name: The source dtype policy name, e.g. \"float32\".\n+        weight_bits: Number of bits to quantize weights to. Supported values\n+            are 2, 3, 4, and 8.\n+        group_size: The group size for quantization. Supported values are\n+            -1 (for whole-tensor quantization) or any positive integer.\n+            Typically a smaller group size leads to better accuracy but\n+            slower speed.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        mode,\n+        source_name=None,\n+        weight_bits=None,\n+        group_size=None,\nComment: I don't think it will work if `weight_bits` is `None` or `group_size` is `None`.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/dtype_policies/dtype_policy.py",
    "pr_number": 21641,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2338171832,
    "comment_created_at": "2025-09-10T23:43:27Z"
  },
  {
    "code": "@@ -7,59 +10,61 @@\n class TorchLayer(torch.nn.Module):\n     @property\n     def torch_params(self):\n-        if not hasattr(self, \"_torch_params\"):\n-            self._track_variables()\n-        return self._torch_params\n+        warnings.warn(\n+            \"`layer.torch_param` is deprecated and will be removed in a future \"\n+            \"release. Please use `layer.variables` (Keras style) or \"\n+            \"`layer.named_parameters()` (Torch style) instead.\"\n+        )\n+        return list(self.named_parameters())\n \n     def _post_build(self):\n         # Do not track variables when in a stateless scope.\n         # The variables are not initialized.\n         if in_stateless_scope():\n             return\n-        self._track_variables()\n-\n-    def _track_variables(self):\n-        # set torch_params attribute will have module automatically track\n-        # parameters.\n-        self._torch_params = torch.nn.ParameterDict(\n-            {variable.path: variable.value for variable in self.variables}\n-        )\n-\n-    def named_parameters(\n-        self,\n-        prefix=\"\",\n-        recurse=True,\n-        remove_duplicate=True,\n-    ):\n-        if not hasattr(self, \"_torch_params\"):\n-            self._track_variables()\n-        return torch.nn.Module.named_parameters(\n-            self, prefix, recurse, remove_duplicate\n-        )\n-\n-    def forward(self, *args, **kwargs):\n-        return Operation.__call__(self, *args, **kwargs)\n \n     def _setattr_hook(self, name, value):\n         from keras.src.layers import Layer\n \n-        if (\n-            isinstance(value, torch.nn.Module)\n-            and not isinstance(value, Layer)\n-            and not name == \"_torch_params\"\n-        ):\n+        if isinstance(value, torch.nn.Module) and not isinstance(value, Layer):\n             from keras.src.utils.torch_utils import TorchModuleWrapper\n \n             if not isinstance(self, TorchModuleWrapper):\n                 value = TorchModuleWrapper(value)\n         return name, value\n \n-    def _post_track_variable(self, variable):\n-        if hasattr(self, \"_torch_params\"):\n-            if variable.path not in self.torch_params:\n-                self.torch_params[variable.path] = variable.value\n+    # Overrided torch.nn.Module methods.\n+\n+    def forward(self, *args, **kwargs):\n+        return Operation.__call__(self, *args, **kwargs)\n+\n+    def named_parameters(self, prefix=\"\", recurse=True, remove_duplicate=True):\n+        seen = set()\n+        for layer in self._flatten_layers(\n+            include_self=False, recursive=recurse\n+        ):\n+            for variable in layer.variables:\n+                if remove_duplicate and id(variable) in seen:\n+                    continue\n+                seen.add(id(variable))\n+                name = prefix + variable.path\n+                yield name, variable.value\n+        for variable in self.variables:\n+            if remove_duplicate and id(variable) in seen:\n+                continue\n+            seen.add(id(variable))\n+            name = prefix + variable.path\n+            yield name, variable.value",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe current implementation of named_parameters is inefficient and has a few issues:\n\n1.  **inefficiency and redundancy**: it iterates through _flatten_layers and then separately through self.variables. since self.variables is a recursive property that already includes all variables from sublayers, the first loop is redundant. the seen set prevents duplication, but the approach is unnecessarily complex.\n2.  **incorrect recurse=false behavior**: when recurse=false, _flatten_layers correctly returns only direct child layers. however, the code then calls layer.variables on them, which is a recursive property, thus fetching variables from grandchildren and beyond. this contradicts the expectation for recurse=false.\n3.  **incorrectly includes non-parameters**: the implementation iterates over layer.variables, which includes seedgenerator states. in pytorch, \"parameters\" typically refer only to model weights, so iterating over layer.weights would be more appropriate.\n\ni suggest a simpler and more correct implementation that addresses these points.",
    "line_number": 57,
    "enriched": "File: keras/src/backend/torch/layer.py\nCode: @@ -7,59 +10,61 @@\n class TorchLayer(torch.nn.Module):\n     @property\n     def torch_params(self):\n-        if not hasattr(self, \"_torch_params\"):\n-            self._track_variables()\n-        return self._torch_params\n+        warnings.warn(\n+            \"`layer.torch_param` is deprecated and will be removed in a future \"\n+            \"release. Please use `layer.variables` (Keras style) or \"\n+            \"`layer.named_parameters()` (Torch style) instead.\"\n+        )\n+        return list(self.named_parameters())\n \n     def _post_build(self):\n         # Do not track variables when in a stateless scope.\n         # The variables are not initialized.\n         if in_stateless_scope():\n             return\n-        self._track_variables()\n-\n-    def _track_variables(self):\n-        # set torch_params attribute will have module automatically track\n-        # parameters.\n-        self._torch_params = torch.nn.ParameterDict(\n-            {variable.path: variable.value for variable in self.variables}\n-        )\n-\n-    def named_parameters(\n-        self,\n-        prefix=\"\",\n-        recurse=True,\n-        remove_duplicate=True,\n-    ):\n-        if not hasattr(self, \"_torch_params\"):\n-            self._track_variables()\n-        return torch.nn.Module.named_parameters(\n-            self, prefix, recurse, remove_duplicate\n-        )\n-\n-    def forward(self, *args, **kwargs):\n-        return Operation.__call__(self, *args, **kwargs)\n \n     def _setattr_hook(self, name, value):\n         from keras.src.layers import Layer\n \n-        if (\n-            isinstance(value, torch.nn.Module)\n-            and not isinstance(value, Layer)\n-            and not name == \"_torch_params\"\n-        ):\n+        if isinstance(value, torch.nn.Module) and not isinstance(value, Layer):\n             from keras.src.utils.torch_utils import TorchModuleWrapper\n \n             if not isinstance(self, TorchModuleWrapper):\n                 value = TorchModuleWrapper(value)\n         return name, value\n \n-    def _post_track_variable(self, variable):\n-        if hasattr(self, \"_torch_params\"):\n-            if variable.path not in self.torch_params:\n-                self.torch_params[variable.path] = variable.value\n+    # Overrided torch.nn.Module methods.\n+\n+    def forward(self, *args, **kwargs):\n+        return Operation.__call__(self, *args, **kwargs)\n+\n+    def named_parameters(self, prefix=\"\", recurse=True, remove_duplicate=True):\n+        seen = set()\n+        for layer in self._flatten_layers(\n+            include_self=False, recursive=recurse\n+        ):\n+            for variable in layer.variables:\n+                if remove_duplicate and id(variable) in seen:\n+                    continue\n+                seen.add(id(variable))\n+                name = prefix + variable.path\n+                yield name, variable.value\n+        for variable in self.variables:\n+            if remove_duplicate and id(variable) in seen:\n+                continue\n+            seen.add(id(variable))\n+            name = prefix + variable.path\n+            yield name, variable.value\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe current implementation of `named_parameters` is inefficient and has a few issues:\n\n1.  **Inefficiency and Redundancy**: It iterates through `_flatten_layers` and then separately through `self.variables`. Since `self.variables` is a recursive property that already includes all variables from sublayers, the first loop is redundant. The `seen` set prevents duplication, but the approach is unnecessarily complex.\n2.  **Incorrect `recurse=False` Behavior**: When `recurse=False`, `_flatten_layers` correctly returns only direct child layers. However, the code then calls `layer.variables` on them, which is a recursive property, thus fetching variables from grandchildren and beyond. This contradicts the expectation for `recurse=False`.\n3.  **Incorrectly Includes Non-Parameters**: The implementation iterates over `layer.variables`, which includes `SeedGenerator` states. In PyTorch, \"parameters\" typically refer only to model weights, so iterating over `layer.weights` would be more appropriate.\n\nI suggest a simpler and more correct implementation that addresses these points.\n\n```suggestion\n    def named_parameters(self, prefix=\"\", recurse=True, remove_duplicate=True):\n        if recurse:\n            # self.weights is recursive and does not include seeds.\n            variables_to_iterate = self.weights\n        else:\n            # Direct weights of this layer.\n            variables_to_iterate = (\n                self._trainable_variables + self._non_trainable_variables\n            )\n\n        seen = set()\n        for variable in variables_to_iterate:\n            if remove_duplicate and id(variable) in seen:\n                continue\n            seen.add(id(variable))\n            name = prefix + variable.path\n            yield name, variable.value\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/torch/layer.py",
    "pr_number": 21640,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2330447631,
    "comment_created_at": "2025-09-08T14:33:33Z"
  },
  {
    "code": "@@ -3888,6 +3888,37 @@ def kron(x1, x2):\n     return backend.numpy.kron(x1, x2)\n \n \n+class Lcm(Operation):\n+    def call(self, x1, x2):\n+        return backend.numpy.lcm(x1, x2)\n+\n+    def compute_output_spec(self, x1, x2):\n+        x1_shape = getattr(x1, \"shape\", [])\n+        x2_shape = getattr(x2, \"shape\", [])\n+        output_shape = broadcast_shapes(x1_shape, x2_shape)\n+\n+        x1_type = backend.standardize_dtype(getattr(x1, \"dtype\", type(x1)))\n+        x2_type = backend.standardize_dtype(getattr(x2, \"dtype\", type(x2)))\n+        dtype = dtypes.result_type(x1_type, x2_type)\n+        return KerasTensor(output_shape, dtype=dtype)\n+\n+\n+@keras_export([\"keras.ops.lcm\", \"keras.ops.numpy.lcm\"])\n+def lcm(x1, x2):\n+    \"\"\"Least common multiple of `x1` and `x2`, element-wise.\n+\n+    Args:\n+        x1: First input tensor (integer type).\n+        x2: Second input tensor (integer type).\n+\n+    Returns:\n+        Output tensor, element-wise least common multiple of `x1` and `x2`.\n+    \"\"\"",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe docstring for the lcm function is missing a usage example. according to the keras api design guidelines, all docstrings should include code examples to demonstrate how to use the api.[^1]\n\nplease add an example to illustrate its usage.\n\npython\n    \"\"\"least common multiple of x1 and x2, element-wise.\n\n    args:\n        x1: first input tensor (integer type).\n        x2: second input tensor (integer type).\n\n    returns:\n        output tensor, element-wise least common multiple of x1 and x2.\n\n    examples:\n\n    >>> x1 = keras.ops.array([2, 3, 4])\n    >>> x2 = keras.ops.array([5, 6, 7])\n    >>> keras.ops.lcm(x1, x2)\n    array([10,  6, 28], dtype=int32)\n    \"\"\"\n\n\n#### style guide references\n[^1]: the style guide mandates that all docstrings should include code examples to show how to use the api.",
    "line_number": 3916,
    "enriched": "File: keras/src/ops/numpy.py\nCode: @@ -3888,6 +3888,37 @@ def kron(x1, x2):\n     return backend.numpy.kron(x1, x2)\n \n \n+class Lcm(Operation):\n+    def call(self, x1, x2):\n+        return backend.numpy.lcm(x1, x2)\n+\n+    def compute_output_spec(self, x1, x2):\n+        x1_shape = getattr(x1, \"shape\", [])\n+        x2_shape = getattr(x2, \"shape\", [])\n+        output_shape = broadcast_shapes(x1_shape, x2_shape)\n+\n+        x1_type = backend.standardize_dtype(getattr(x1, \"dtype\", type(x1)))\n+        x2_type = backend.standardize_dtype(getattr(x2, \"dtype\", type(x2)))\n+        dtype = dtypes.result_type(x1_type, x2_type)\n+        return KerasTensor(output_shape, dtype=dtype)\n+\n+\n+@keras_export([\"keras.ops.lcm\", \"keras.ops.numpy.lcm\"])\n+def lcm(x1, x2):\n+    \"\"\"Least common multiple of `x1` and `x2`, element-wise.\n+\n+    Args:\n+        x1: First input tensor (integer type).\n+        x2: Second input tensor (integer type).\n+\n+    Returns:\n+        Output tensor, element-wise least common multiple of `x1` and `x2`.\n+    \"\"\"\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe docstring for the `lcm` function is missing a usage example. According to the Keras API design guidelines, all docstrings should include code examples to demonstrate how to use the API.[^1]\n\nPlease add an example to illustrate its usage.\n\n```python\n    \"\"\"Least common multiple of `x1` and `x2`, element-wise.\n\n    Args:\n        x1: First input tensor (integer type).\n        x2: Second input tensor (integer type).\n\n    Returns:\n        Output tensor, element-wise least common multiple of `x1` and `x2`.\n\n    Examples:\n\n    >>> x1 = keras.ops.array([2, 3, 4])\n    >>> x2 = keras.ops.array([5, 6, 7])\n    >>> keras.ops.lcm(x1, x2)\n    array([10,  6, 28], dtype=int32)\n    \"\"\"\n```\n\n#### Style Guide References\n[^1]: The style guide mandates that all docstrings should include code examples to show how to use the API.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/ops/numpy.py",
    "pr_number": 21636,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2328625282,
    "comment_created_at": "2025-09-07T10:44:57Z"
  },
  {
    "code": "@@ -1711,6 +1711,53 @@ def isposinf(x):\n     return tf.math.equal(x, tf.constant(float(\"inf\"), dtype=x.dtype))\n \n \n+def kron(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    ndim_x1 = tf.rank(x1)\n+    ndim_x2 = tf.rank(x2)\n+\n+    def expand_front(x, num):\n+        for _ in range(num):\n+            x = tf.expand_dims(x, axis=0)\n+        return x\n+\n+    x1 = tf.cond(\n+        ndim_x1 < ndim_x2,\n+        lambda: expand_front(x1, ndim_x2 - ndim_x1),\n+        lambda: x1,\n+    )\n+    x2 = tf.cond(\n+        ndim_x2 < ndim_x1,\n+        lambda: expand_front(x2, ndim_x1 - ndim_x2),\n+        lambda: x2,\n+    )\n+\n+    x1_reshaped = tf.reshape(\n+        x1,\n+        tf.reshape(\n+            tf.stack([tf.shape(x1), tf.ones_like(tf.shape(x1))], axis=1), [-1]\n+        ),\n+    )\n+    x2_reshaped = tf.reshape(\n+        x2,\n+        tf.reshape(\n+            tf.stack([tf.ones_like(tf.shape(x2)), tf.shape(x2)], axis=1), [-1]\n+        ),\n+    )\n+\n+    out = tf.multiply(x1_reshaped, x2_reshaped)\n+\n+    out_shape = tf.shape(x1) * tf.shape(x2)\n+    out = tf.reshape(out, out_shape)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nin the tensorflow backend, the shape of the output tensor is calculated using standard python multiplication (*). however, tensorflow's tf.shape returns a tensorflow tensor, not a python tuple. this could lead to errors during graph construction, as tensorflow needs to evaluate the tensor to determine its value. it's safer to use tf.shape to ensure that the output shape is also a tensorflow tensor.\n\nconsider using tf.concat to combine the shapes instead of the * operator to ensure compatibility with tensorflow's graph execution.\n\npython\nout_shape = tf.concat([tf.shape(x1), tf.shape(x2)], axis=0)\n\n\npython\nout_shape = tf.concat([tf.shape(x1), tf.shape(x2)], axis=0)",
    "line_number": 1757,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1711,6 +1711,53 @@ def isposinf(x):\n     return tf.math.equal(x, tf.constant(float(\"inf\"), dtype=x.dtype))\n \n \n+def kron(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    ndim_x1 = tf.rank(x1)\n+    ndim_x2 = tf.rank(x2)\n+\n+    def expand_front(x, num):\n+        for _ in range(num):\n+            x = tf.expand_dims(x, axis=0)\n+        return x\n+\n+    x1 = tf.cond(\n+        ndim_x1 < ndim_x2,\n+        lambda: expand_front(x1, ndim_x2 - ndim_x1),\n+        lambda: x1,\n+    )\n+    x2 = tf.cond(\n+        ndim_x2 < ndim_x1,\n+        lambda: expand_front(x2, ndim_x1 - ndim_x2),\n+        lambda: x2,\n+    )\n+\n+    x1_reshaped = tf.reshape(\n+        x1,\n+        tf.reshape(\n+            tf.stack([tf.shape(x1), tf.ones_like(tf.shape(x1))], axis=1), [-1]\n+        ),\n+    )\n+    x2_reshaped = tf.reshape(\n+        x2,\n+        tf.reshape(\n+            tf.stack([tf.ones_like(tf.shape(x2)), tf.shape(x2)], axis=1), [-1]\n+        ),\n+    )\n+\n+    out = tf.multiply(x1_reshaped, x2_reshaped)\n+\n+    out_shape = tf.shape(x1) * tf.shape(x2)\n+    out = tf.reshape(out, out_shape)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nIn the TensorFlow backend, the shape of the output tensor is calculated using standard Python multiplication (`*`). However, TensorFlow's `tf.shape` returns a TensorFlow tensor, not a Python tuple. This could lead to errors during graph construction, as TensorFlow needs to evaluate the tensor to determine its value. It's safer to use `tf.shape` to ensure that the output shape is also a TensorFlow tensor.\n\nConsider using `tf.concat` to combine the shapes instead of the `*` operator to ensure compatibility with TensorFlow's graph execution.\n\n```python\nout_shape = tf.concat([tf.shape(x1), tf.shape(x2)], axis=0)\n```\n\n```python\nout_shape = tf.concat([tf.shape(x1), tf.shape(x2)], axis=0)\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21633,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2318970325,
    "comment_created_at": "2025-09-03T13:23:09Z"
  },
  {
    "code": "@@ -3,7 +3,9 @@\n if is_nnx_enabled():\n     from flax import nnx\n \n-    BaseLayer = nnx.Module\n+    class BaseLayer(nnx.Module):\n+        def __init_subclass__(cls, **kwargs):\n+            super().__init_subclass__(pytree=False, **kwargs)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis new baselayer class is a good change to control how keras layers interact with jax's pytree system when nnx is enabled. to improve maintainability and help future developers understand the reasoning behind this implementation, consider adding a docstring to the baselayer class explaining why pytree=false is necessary. this aligns with the keras api design guidelines regarding documentation.[^1]\n\n\n\n#### style guide references\n[^1]: documentation is an integral part of the api and user experience. while this is an internal class, a docstring explaining the purpose of this class and the pytree=false setting would be very helpful for maintainability. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#l137-l137)",
    "line_number": 8,
    "enriched": "File: keras/src/backend/jax/layer.py\nCode: @@ -3,7 +3,9 @@\n if is_nnx_enabled():\n     from flax import nnx\n \n-    BaseLayer = nnx.Module\n+    class BaseLayer(nnx.Module):\n+        def __init_subclass__(cls, **kwargs):\n+            super().__init_subclass__(pytree=False, **kwargs)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis new `BaseLayer` class is a good change to control how Keras layers interact with JAX's PyTree system when NNX is enabled. To improve maintainability and help future developers understand the reasoning behind this implementation, consider adding a docstring to the `BaseLayer` class explaining why `pytree=False` is necessary. This aligns with the Keras API design guidelines regarding documentation.[^1]\n\n```suggestion\n    class BaseLayer(nnx.Module):\n        \"\"\"Base layer for JAX backend with NNX.\n\n        This class ensures that Keras layers are not registered as PyTrees,\n        allowing Keras to manage its own state tracking without conflicting\n        with JAX's tree traversal mechanisms.\n        \"\"\"\n\n        def __init_subclass__(cls, **kwargs):\n            super().__init_subclass__(pytree=False, **kwargs)\n```\n\n#### Style Guide References\n[^1]: Documentation is an integral part of the API and user experience. While this is an internal class, a docstring explaining the purpose of this class and the `pytree=False` setting would be very helpful for maintainability. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#L137-L137)",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/backend/jax/layer.py",
    "pr_number": 21632,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2317242270,
    "comment_created_at": "2025-09-02T21:47:04Z"
  },
  {
    "code": "@@ -68,7 +68,9 @@ def train_step(self, data):\n             )\n             self._loss_tracker.update_state(\n                 loss_module.unscale_loss_for_distribution(loss),\n-                sample_weight=tf.shape(tree.flatten(x)[0])[0],\n+                sample_weight=tf.shape(\n+                    next(i for i in tree.flatten(x) if i is not None)\n+                )[0],",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis change correctly handles cases where the first input is none. however, it introduces a risk of a stopiteration error if all inputs in x are none. this can be difficult to debug, especially inside a tf.function.\n\na more robust approach would be to handle this edge case explicitly, for example by raising a valueerror with a clear message.\n\nalso, this logic is duplicated in test_step. consider extracting it into a private helper method to improve maintainability and ensure consistency.",
    "line_number": 73,
    "enriched": "File: keras/src/backend/tensorflow/trainer.py\nCode: @@ -68,7 +68,9 @@ def train_step(self, data):\n             )\n             self._loss_tracker.update_state(\n                 loss_module.unscale_loss_for_distribution(loss),\n-                sample_weight=tf.shape(tree.flatten(x)[0])[0],\n+                sample_weight=tf.shape(\n+                    next(i for i in tree.flatten(x) if i is not None)\n+                )[0],\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis change correctly handles cases where the first input is `None`. However, it introduces a risk of a `StopIteration` error if all inputs in `x` are `None`. This can be difficult to debug, especially inside a `tf.function`.\n\nA more robust approach would be to handle this edge case explicitly, for example by raising a `ValueError` with a clear message.\n\nAlso, this logic is duplicated in `test_step`. Consider extracting it into a private helper method to improve maintainability and ensure consistency.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/trainer.py",
    "pr_number": 21630,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2312665471,
    "comment_created_at": "2025-08-31T23:05:13Z"
  },
  {
    "code": "@@ -9,15 +10,109 @@\n from keras.src.layers import EinsumDense\n from keras.src.layers import Embedding\n from keras.src.quantizers.gptq import GPTQ\n-from keras.src.quantizers.gptq_quant import GPTQQuantization\n \n \n-def get_dataloader(tokenizer, sequence_length, dataset, num_samples=128):\n+@contextmanager\n+def stream_hessians(layers_map, gptq_objects):\n     \"\"\"\n-    Prepares and chunks the calibration dataloader, repeating short datasets.\n+    Temporarily monkey-patch each target layer's `call` method so\n+    that input activations are streamed into the GPTQ instance\n+    running Hessian estimate at capture time.\n+\n+    On `__enter__`: For every (name, layer) in `layers_map`, replaces\n+     `layer.call` with a wrapper that:\n+     1) extracts the layer input from `*args`/`**kwargs`,\n+     2) reshapes it to 2D `[-1, rows]` where\n+      `rows = gptq_objects[name].rows`,\n+     3) calls `gptq_objects[name].update_hessian_with_batch(x2d)`\n+     4) delegates to the original `layer.call` and returns its\n+      output.\n+\n+    On `__exit__`: All original `layer.call` methods are restored even if an\n+     exception occurs.\n+\n+    * Space complexity: O(d**2) per layer (for the Hessian).\n+    * No weights are modified; only GPTQ statistics are updated.\n+\n+    Args:\n+        layers_map: Dict[str, Layer]. Mapping from logical layer names to\n+         the Keras layers that should be patched during calibration. Keys must\n+         match `gptq_objects`.\n+        gptq_objects: Dict[str, GPTQ]. Mapping from names to GPTQ instances.\n+\n+    Yields:\n+        None: The patched state is active only within the `with` block. After\n+         exit, all layers are unpatched and safe to use normally.\n+\n+    Example:\n+    ```python\n+    >>> with stream_hessians(layers_map, gptq_objects):\n+    ...     for sample in calibration_inputs:\n+    ...         if len(sample.shape) == 2:\n+    ...             sample = ops.expand_dims(sample, 0)\n+    ...         _ = block(sample)   # hooks update Hessians on-the-fly\n+    # <- original layer.call methods restored here",
    "comment": "don't you need >>> here ?",
    "line_number": 54,
    "enriched": "File: keras/src/quantizers/gptq_core.py\nCode: @@ -9,15 +10,109 @@\n from keras.src.layers import EinsumDense\n from keras.src.layers import Embedding\n from keras.src.quantizers.gptq import GPTQ\n-from keras.src.quantizers.gptq_quant import GPTQQuantization\n \n \n-def get_dataloader(tokenizer, sequence_length, dataset, num_samples=128):\n+@contextmanager\n+def stream_hessians(layers_map, gptq_objects):\n     \"\"\"\n-    Prepares and chunks the calibration dataloader, repeating short datasets.\n+    Temporarily monkey-patch each target layer's `call` method so\n+    that input activations are streamed into the GPTQ instance\n+    running Hessian estimate at capture time.\n+\n+    On `__enter__`: For every (name, layer) in `layers_map`, replaces\n+     `layer.call` with a wrapper that:\n+     1) extracts the layer input from `*args`/`**kwargs`,\n+     2) reshapes it to 2D `[-1, rows]` where\n+      `rows = gptq_objects[name].rows`,\n+     3) calls `gptq_objects[name].update_hessian_with_batch(x2d)`\n+     4) delegates to the original `layer.call` and returns its\n+      output.\n+\n+    On `__exit__`: All original `layer.call` methods are restored even if an\n+     exception occurs.\n+\n+    * Space complexity: O(d**2) per layer (for the Hessian).\n+    * No weights are modified; only GPTQ statistics are updated.\n+\n+    Args:\n+        layers_map: Dict[str, Layer]. Mapping from logical layer names to\n+         the Keras layers that should be patched during calibration. Keys must\n+         match `gptq_objects`.\n+        gptq_objects: Dict[str, GPTQ]. Mapping from names to GPTQ instances.\n+\n+    Yields:\n+        None: The patched state is active only within the `with` block. After\n+         exit, all layers are unpatched and safe to use normally.\n+\n+    Example:\n+    ```python\n+    >>> with stream_hessians(layers_map, gptq_objects):\n+    ...     for sample in calibration_inputs:\n+    ...         if len(sample.shape) == 2:\n+    ...             sample = ops.expand_dims(sample, 0)\n+    ...         _ = block(sample)   # hooks update Hessians on-the-fly\n+    # <- original layer.call methods restored here\nComment: Don't you need `>>>` here ?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "keras/src/quantizers/gptq_core.py",
    "pr_number": 21628,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2317538904,
    "comment_created_at": "2025-09-03T01:25:01Z"
  },
  {
    "code": "@@ -104,3 +104,52 @@ def test_tf_data_compatibility(self):\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(4).map(layer)\n         output = next(iter(ds)).numpy()\n         self.assertAllClose(output, np.array([2, 3, 4, 0]))\n+\n+    def test_one_hot_output_with_higher_rank_input(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        layer = layers.IntegerLookup(\n+            vocabulary=vocabulary, output_mode=\"one_hot\"\n+        )\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 2, 4))\n+        expected_output = np.array(\n+            [\n+                [[0, 1, 0, 0], [0, 0, 1, 0]],\n+                [[0, 0, 0, 1], [1, 0, 0, 0]],\n+            ]\n+        )\n+        self.assertAllClose(output_data, expected_output)\n+        output_data_3d = layer(np.expand_dims(input_data, axis=0))\n+        self.assertEqual(output_data_3d.shape, (1, 2, 2, 4))\n+        self.assertAllClose(\n+            output_data_3d, np.expand_dims(expected_output, axis=0)\n+        )\n+\n+    def test_multi_hot_output_shape(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        layer = layers.IntegerLookup(\n+            vocabulary=vocabulary, output_mode=\"multi_hot\"\n+        )\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 4))\n+\n+    def test_count_output_shape(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        layer = layers.IntegerLookup(vocabulary=vocabulary, output_mode=\"count\")\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 4))\n+\n+    def test_tf_idf_output_shape(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        idf_weights = [1.0, 1.0, 1.0]\n+        layer = layers.IntegerLookup(\n+            vocabulary=vocabulary,\n+            idf_weights=idf_weights,\n+            output_mode=\"tf_idf\",\n+        )\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 4))",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nto improve maintainability and reduce code duplication, you can refactor these three tests (test_multi_hot_output_shape, test_count_output_shape, test_tf_idf_output_shape) into a single private helper method that is called by the individual test cases. this consolidates the common setup and assertion logic.",
    "line_number": 155,
    "enriched": "File: keras/src/layers/preprocessing/integer_lookup_test.py\nCode: @@ -104,3 +104,52 @@ def test_tf_data_compatibility(self):\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(4).map(layer)\n         output = next(iter(ds)).numpy()\n         self.assertAllClose(output, np.array([2, 3, 4, 0]))\n+\n+    def test_one_hot_output_with_higher_rank_input(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        layer = layers.IntegerLookup(\n+            vocabulary=vocabulary, output_mode=\"one_hot\"\n+        )\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 2, 4))\n+        expected_output = np.array(\n+            [\n+                [[0, 1, 0, 0], [0, 0, 1, 0]],\n+                [[0, 0, 0, 1], [1, 0, 0, 0]],\n+            ]\n+        )\n+        self.assertAllClose(output_data, expected_output)\n+        output_data_3d = layer(np.expand_dims(input_data, axis=0))\n+        self.assertEqual(output_data_3d.shape, (1, 2, 2, 4))\n+        self.assertAllClose(\n+            output_data_3d, np.expand_dims(expected_output, axis=0)\n+        )\n+\n+    def test_multi_hot_output_shape(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        layer = layers.IntegerLookup(\n+            vocabulary=vocabulary, output_mode=\"multi_hot\"\n+        )\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 4))\n+\n+    def test_count_output_shape(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        layer = layers.IntegerLookup(vocabulary=vocabulary, output_mode=\"count\")\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 4))\n+\n+    def test_tf_idf_output_shape(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        idf_weights = [1.0, 1.0, 1.0]\n+        layer = layers.IntegerLookup(\n+            vocabulary=vocabulary,\n+            idf_weights=idf_weights,\n+            output_mode=\"tf_idf\",\n+        )\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 4))\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nTo improve maintainability and reduce code duplication, you can refactor these three tests (`test_multi_hot_output_shape`, `test_count_output_shape`, `test_tf_idf_output_shape`) into a single private helper method that is called by the individual test cases. This consolidates the common setup and assertion logic.\n\n```suggestion\n    def _test_output_shape_for_mode(self, output_mode):\n        input_data = np.array([[1, 2], [3, 0]])\n        vocabulary = [1, 2, 3]\n        kwargs = {\"vocabulary\": vocabulary, \"output_mode\": output_mode}\n        if output_mode == \"tf_idf\":\n            kwargs[\"idf_weights\"] = [1.0, 1.0, 1.0]\n        layer = layers.IntegerLookup(**kwargs)\n        output_data = layer(input_data)\n        self.assertEqual(output_data.shape, (2, 4))\n\n    def test_multi_hot_output_shape(self):\n        self._test_output_shape_for_mode(\"multi_hot\")\n\n    def test_count_output_shape(self):\n        self._test_output_shape_for_mode(\"count\")\n\n    def test_tf_idf_output_shape(self):\n        self._test_output_shape_for_mode(\"tf_idf\")\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/layers/preprocessing/integer_lookup_test.py",
    "pr_number": 21625,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2309280219,
    "comment_created_at": "2025-08-29T06:30:29Z"
  },
  {
    "code": "@@ -1531,6 +1531,44 @@ def full_like(x, fill_value, dtype=None):\n     return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n+def gcd(x1, x2):\n+    x1 = tf.convert_to_tensor(x1)\n+    x2 = tf.convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    if not x1.dtype.is_integer:\n+        raise TypeError(\"Arguments to gcd must be integers.\")\n+\n+    target_shape = tf.broadcast_static_shape(x1.shape, x2.shape)\n+    x1 = tf.broadcast_to(x1, target_shape)\n+    x2 = tf.broadcast_to(x2, target_shape)\n+\n+    def cond(a, b):\n+        return tf.reduce_any(b != 0)\n+\n+    def body(a, b):\n+        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n+        a, b = (\n+            tf.where(tf.not_equal(b, 0), b, a),\n+            tf.where(\n+                tf.not_equal(b, 0),\n+                tf.math.floormod(a, b_safe),\n+                tf.zeros_like(b),\n+            ),\n+        )\n+        return (tf.where(a < b, b, a), tf.where(a < b, a, b))",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe sorting of a and b at the end of the body function is redundant. the euclidean algorithm, which this function implements, works correctly without requiring a >= b at each step, especially when using floormod. if a < b, the first iteration of a, b = b, a % b effectively swaps them. removing this sorting step simplifies the code and improves performance by avoiding two tf.where operations in each loop iteration.",
    "line_number": 1562,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1531,6 +1531,44 @@ def full_like(x, fill_value, dtype=None):\n     return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n+def gcd(x1, x2):\n+    x1 = tf.convert_to_tensor(x1)\n+    x2 = tf.convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    if not x1.dtype.is_integer:\n+        raise TypeError(\"Arguments to gcd must be integers.\")\n+\n+    target_shape = tf.broadcast_static_shape(x1.shape, x2.shape)\n+    x1 = tf.broadcast_to(x1, target_shape)\n+    x2 = tf.broadcast_to(x2, target_shape)\n+\n+    def cond(a, b):\n+        return tf.reduce_any(b != 0)\n+\n+    def body(a, b):\n+        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n+        a, b = (\n+            tf.where(tf.not_equal(b, 0), b, a),\n+            tf.where(\n+                tf.not_equal(b, 0),\n+                tf.math.floormod(a, b_safe),\n+                tf.zeros_like(b),\n+            ),\n+        )\n+        return (tf.where(a < b, b, a), tf.where(a < b, a, b))\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe sorting of `a` and `b` at the end of the `body` function is redundant. The Euclidean algorithm, which this function implements, works correctly without requiring `a >= b` at each step, especially when using `floormod`. If `a < b`, the first iteration of `a, b = b, a % b` effectively swaps them. Removing this sorting step simplifies the code and improves performance by avoiding two `tf.where` operations in each loop iteration.\n\n```suggestion\n    def body(a, b):\n        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n        a, b = (\n            tf.where(tf.not_equal(b, 0), b, a),\n            tf.where(\n                tf.not_equal(b, 0),\n                tf.math.floormod(a, b_safe),\n                tf.zeros_like(b),\n            ),\n        )\n        return a, b\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21623,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2307550199,
    "comment_created_at": "2025-08-28T14:13:56Z"
  },
  {
    "code": "@@ -1531,6 +1531,44 @@ def full_like(x, fill_value, dtype=None):\n     return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n+def gcd(x1, x2):\n+    x1 = tf.convert_to_tensor(x1)\n+    x2 = tf.convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    if not x1.dtype.is_integer:\n+        raise TypeError(\"Arguments to gcd must be integers.\")\n+\n+    target_shape = tf.broadcast_static_shape(x1.shape, x2.shape)\n+    x1 = tf.broadcast_to(x1, target_shape)\n+    x2 = tf.broadcast_to(x2, target_shape)\n+\n+    def cond(a, b):\n+        return tf.reduce_any(b != 0)\n+\n+    def body(a, b):\n+        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n+        a, b = (\n+            tf.where(tf.not_equal(b, 0), b, a),\n+            tf.where(\n+                tf.not_equal(b, 0),\n+                tf.math.floormod(a, b_safe),\n+                tf.zeros_like(b),\n+            ),\n+        )\n+        return tf.maximum(a, b), tf.minimum(a, b)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe tf.maximum and tf.minimum calls here are redundant. the a and b variables have already been updated with the next values for the euclidean algorithm on the preceding lines. the algorithm works correctly without requiring a >= b. you can simplify this by directly returning the updated a and b, which improves readability and may offer a minor performance improvement.",
    "line_number": 1562,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1531,6 +1531,44 @@ def full_like(x, fill_value, dtype=None):\n     return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n+def gcd(x1, x2):\n+    x1 = tf.convert_to_tensor(x1)\n+    x2 = tf.convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    if not x1.dtype.is_integer:\n+        raise TypeError(\"Arguments to gcd must be integers.\")\n+\n+    target_shape = tf.broadcast_static_shape(x1.shape, x2.shape)\n+    x1 = tf.broadcast_to(x1, target_shape)\n+    x2 = tf.broadcast_to(x2, target_shape)\n+\n+    def cond(a, b):\n+        return tf.reduce_any(b != 0)\n+\n+    def body(a, b):\n+        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n+        a, b = (\n+            tf.where(tf.not_equal(b, 0), b, a),\n+            tf.where(\n+                tf.not_equal(b, 0),\n+                tf.math.floormod(a, b_safe),\n+                tf.zeros_like(b),\n+            ),\n+        )\n+        return tf.maximum(a, b), tf.minimum(a, b)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe `tf.maximum` and `tf.minimum` calls here are redundant. The `a` and `b` variables have already been updated with the next values for the Euclidean algorithm on the preceding lines. The algorithm works correctly without requiring `a >= b`. You can simplify this by directly returning the updated `a` and `b`, which improves readability and may offer a minor performance improvement.\n\n```suggestion\n        return a, b\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21622,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2307504359,
    "comment_created_at": "2025-08-28T13:57:32Z"
  },
  {
    "code": "@@ -950,10 +958,14 @@ def map_coordinates(\n         )\n     else:\n         padded = np.pad(inputs, padding, mode=pad_mode)\n+\n+    # `scipy.ndimage.map_coordinates` lacks support for float16 and bfloat16.\n+    if backend.is_float_dtype(padded.dtype):\n+        padded = padded.astype(\"float32\")",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe current implementation for handling dtypes in map_coordinates unconditionally casts any float type to float32. this could lead to a loss of precision if the input is float64. other functions in this file, like affine_transform, were updated to use backend.result_type(input_dtype, \"float32\") to determine the computation dtype. this correctly preserves float64 while upcasting lower precision floats. for consistency and to avoid potential precision loss, map_coordinates should adopt the same approach.\n\npython\n    if backend.is_float_dtype(padded.dtype):\n        compute_dtype = backend.result_type(padded.dtype, \"float32\")\n        padded = padded.astype(compute_dtype)",
    "line_number": 964,
    "enriched": "File: keras/src/backend/numpy/image.py\nCode: @@ -950,10 +958,14 @@ def map_coordinates(\n         )\n     else:\n         padded = np.pad(inputs, padding, mode=pad_mode)\n+\n+    # `scipy.ndimage.map_coordinates` lacks support for float16 and bfloat16.\n+    if backend.is_float_dtype(padded.dtype):\n+        padded = padded.astype(\"float32\")\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe current implementation for handling dtypes in `map_coordinates` unconditionally casts any float type to `float32`. This could lead to a loss of precision if the input is `float64`. Other functions in this file, like `affine_transform`, were updated to use `backend.result_type(input_dtype, \"float32\")` to determine the computation dtype. This correctly preserves `float64` while upcasting lower precision floats. For consistency and to avoid potential precision loss, `map_coordinates` should adopt the same approach.\n\n```python\n    if backend.is_float_dtype(padded.dtype):\n        compute_dtype = backend.result_type(padded.dtype, \"float32\")\n        padded = padded.astype(compute_dtype)\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/numpy/image.py",
    "pr_number": 21621,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2307490737,
    "comment_created_at": "2025-08-28T13:52:40Z"
  },
  {
    "code": "@@ -1551,9 +1550,22 @@ def cond(a, b):\n         return tf.reduce_any(b != 0)\n \n     def body(a, b):\n-        return b, tf.math.floormod(a, b)\n+        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n+        a, b = (\n+            tf.where(tf.not_equal(b, 0), b, a),\n+            tf.where(\n+                tf.not_equal(b, 0),\n+                tf.math.floormod(a, b_safe),\n+                tf.zeros_like(b),\n+            ),\n+        )\n+        return (tf.where(a < b, b, a), tf.where(a < b, a, b))",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nfor improved readability and to make the intent of sorting the pair (a, b) more explicit, you could use tf.maximum and tf.minimum here. the current implementation is correct, but this change would make the code slightly more self-documenting.",
    "line_number": 1562,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1551,9 +1550,22 @@ def cond(a, b):\n         return tf.reduce_any(b != 0)\n \n     def body(a, b):\n-        return b, tf.math.floormod(a, b)\n+        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n+        a, b = (\n+            tf.where(tf.not_equal(b, 0), b, a),\n+            tf.where(\n+                tf.not_equal(b, 0),\n+                tf.math.floormod(a, b_safe),\n+                tf.zeros_like(b),\n+            ),\n+        )\n+        return (tf.where(a < b, b, a), tf.where(a < b, a, b))\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nFor improved readability and to make the intent of sorting the pair `(a, b)` more explicit, you could use `tf.maximum` and `tf.minimum` here. The current implementation is correct, but this change would make the code slightly more self-documenting.\n\n```suggestion\n        return (tf.maximum(a, b), tf.minimum(a, b))\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21620,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2307460007,
    "comment_created_at": "2025-08-28T13:41:40Z"
  },
  {
    "code": "@@ -0,0 +1,402 @@\n+# Style Guide\n+\n+## General Principles\n+\n+APIs added should be:\n+- Scalable\n+- Robust\n+- Backwards compatible\n+- Future-proof\n+- Interoperable\n+- Must work across all backends\n+\n+---\n+\n+# Keras API design guidelines\n+\n+These guidelines are meant to help focus design discussions and help us create d\n+elightful developer experiences.",
    "comment": "word is cut \"delightful\"",
    "line_number": 18,
    "enriched": "File: .gemini/styleguide.md\nCode: @@ -0,0 +1,402 @@\n+# Style Guide\n+\n+## General Principles\n+\n+APIs added should be:\n+- Scalable\n+- Robust\n+- Backwards compatible\n+- Future-proof\n+- Interoperable\n+- Must work across all backends\n+\n+---\n+\n+# Keras API design guidelines\n+\n+These guidelines are meant to help focus design discussions and help us create d\n+elightful developer experiences.\nComment: word is cut \"delightful\"",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": ".gemini/styleguide.md",
    "pr_number": 21619,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2305328453,
    "comment_created_at": "2025-08-27T21:24:42Z"
  },
  {
    "code": "@@ -1531,6 +1531,32 @@ def full_like(x, fill_value, dtype=None):\n     return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n+def gcd(x1, x2):\n+    x1 = tf.convert_to_tensor(x1)\n+    x2 = tf.convert_to_tensor(x2)\n+\n+    if x1.dtype.size < x2.dtype.size:\n+        x1 = tf.cast(x1, x2.dtype)\n+    elif x2.dtype.size < x1.dtype.size:\n+        x2 = tf.cast(x2, x1.dtype)\n+\n+    if not x1.dtype.is_integer:\n+        raise TypeError(\"Arguments to gcd must be integers.\")\n+\n+    target_shape = tf.broadcast_static_shape(x1.shape, x2.shape)\n+    x1 = tf.broadcast_to(x1, target_shape)\n+    x2 = tf.broadcast_to(x2, target_shape)\n+\n+    def cond(a, b):\n+        return tf.reduce_any(b != 0)\n+\n+    def body(a, b):\n+        return b, tf.math.floormod(a, b)\n+\n+    gcd_val, _ = tf.while_loop(cond, body, [tf.abs(x1), tf.abs(x2)])\n+    return gcd_val",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe custom implementation of gcd using tf.while_loop is susceptible to division-by-zero errors and is overly complex. the tf.math.floormod(a, b) call within the loop body will fail if any element in b becomes zero during the execution of the euclidean algorithm, which can happen for certain inputs. a simpler and more robust approach is to use the native tf.math.gcd function, which correctly handles edge cases like gcd(x, 0) and is optimized for performance. this also makes the implementation consistent with other backends that use their native gcd functions. the suggested implementation also aligns with the type promotion logic used in other binary operations in this file.",
    "line_number": 1557,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1531,6 +1531,32 @@ def full_like(x, fill_value, dtype=None):\n     return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n+def gcd(x1, x2):\n+    x1 = tf.convert_to_tensor(x1)\n+    x2 = tf.convert_to_tensor(x2)\n+\n+    if x1.dtype.size < x2.dtype.size:\n+        x1 = tf.cast(x1, x2.dtype)\n+    elif x2.dtype.size < x1.dtype.size:\n+        x2 = tf.cast(x2, x1.dtype)\n+\n+    if not x1.dtype.is_integer:\n+        raise TypeError(\"Arguments to gcd must be integers.\")\n+\n+    target_shape = tf.broadcast_static_shape(x1.shape, x2.shape)\n+    x1 = tf.broadcast_to(x1, target_shape)\n+    x2 = tf.broadcast_to(x2, target_shape)\n+\n+    def cond(a, b):\n+        return tf.reduce_any(b != 0)\n+\n+    def body(a, b):\n+        return b, tf.math.floormod(a, b)\n+\n+    gcd_val, _ = tf.while_loop(cond, body, [tf.abs(x1), tf.abs(x2)])\n+    return gcd_val\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe custom implementation of `gcd` using `tf.while_loop` is susceptible to division-by-zero errors and is overly complex. The `tf.math.floormod(a, b)` call within the loop body will fail if any element in `b` becomes zero during the execution of the Euclidean algorithm, which can happen for certain inputs. A simpler and more robust approach is to use the native `tf.math.gcd` function, which correctly handles edge cases like `gcd(x, 0)` and is optimized for performance. This also makes the implementation consistent with other backends that use their native `gcd` functions. The suggested implementation also aligns with the type promotion logic used in other binary operations in this file.\n\n```suggestion\ndef gcd(x1, x2):\n    x1 = convert_to_tensor(x1)\n    x2 = convert_to_tensor(x2)\n    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n    if \"int\" not in standardize_dtype(dtype):\n        raise TypeError(\"Arguments to gcd must be integers.\")\n\n    # tf.math.gcd does not support int8\n    promoted_dtype = dtype\n    if standardize_dtype(promoted_dtype) == \"int8\":\n        promoted_dtype = \"int16\"\n\n    x1 = cast(x1, promoted_dtype)\n    x2 = cast(x2, promoted_dtype)\n    return tf.math.gcd(x1, x2)\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21616,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2303560883,
    "comment_created_at": "2025-08-27T10:48:04Z"
  },
  {
    "code": "@@ -130,8 +130,33 @@ def __new__(cls, *args, **kwargs):\n                 vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n \n         # Generate a config to be returned by default by `get_config()`.\n-        arg_names = inspect.getfullargspec(cls.__init__).args\n-        kwargs.update(dict(zip(arg_names[1 : len(args) + 1], args)))\n+        signature = inspect.signature(cls.__init__)\n+        argspec = inspect.getfullargspec(cls.__init__)\n+\n+        try:\n+            bound_parameters = signature.bind(None, *args, **kwargs)\n+            # Include default values in the config.\n+            bound_parameters.apply_defaults()\n+            # Extract all arguments as a dictionary.\n+            kwargs = bound_parameters.arguments\n+            # Expand variable kwargs argument.\n+            kwargs |= kwargs.pop(argspec.varkw, {})",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nif cls.__init__ does not have a variable keyword argument (**kwargs), argspec.varkw will be none. in this case, kwargs.pop(none, {}) will raise a keyerror, causing a crash. you should guard this operation with a check to ensure argspec.varkw is not none.",
    "line_number": 143,
    "enriched": "File: keras/src/ops/operation.py\nCode: @@ -130,8 +130,33 @@ def __new__(cls, *args, **kwargs):\n                 vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n \n         # Generate a config to be returned by default by `get_config()`.\n-        arg_names = inspect.getfullargspec(cls.__init__).args\n-        kwargs.update(dict(zip(arg_names[1 : len(args) + 1], args)))\n+        signature = inspect.signature(cls.__init__)\n+        argspec = inspect.getfullargspec(cls.__init__)\n+\n+        try:\n+            bound_parameters = signature.bind(None, *args, **kwargs)\n+            # Include default values in the config.\n+            bound_parameters.apply_defaults()\n+            # Extract all arguments as a dictionary.\n+            kwargs = bound_parameters.arguments\n+            # Expand variable kwargs argument.\n+            kwargs |= kwargs.pop(argspec.varkw, {})\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nIf `cls.__init__` does not have a variable keyword argument (`**kwargs`), `argspec.varkw` will be `None`. In this case, `kwargs.pop(None, {})` will raise a `KeyError`, causing a crash. You should guard this operation with a check to ensure `argspec.varkw` is not `None`.\n\n```suggestion\n            if argspec.varkw is not None:\n                kwargs |= kwargs.pop(argspec.varkw, {})\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/ops/operation.py",
    "pr_number": 21615,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2301689611,
    "comment_created_at": "2025-08-26T17:41:33Z"
  },
  {
    "code": "@@ -382,13 +384,24 @@ def get_config(self):\n         return {**base_config, **config}\n \n     def call(self, inputs):\n-        if isinstance(inputs, (tf.Tensor, tf.RaggedTensor, tf.SparseTensor)):\n-            tf_inputs = True\n+        original_torch = False\n+        if(backend.backend() == \"torch\" and isinstance(inputs, torch.Tensor)):\n+            original_torch = True\n+            numpy_inputs = inputs.detach().cpu().numpy()\n+            inputs_for_processing = tf.convert_to_tensor(numpy_inputs)\n+        elif isinstance(inputs, (np.ndarray, list, tuple)):\n+            if backend.backend() == \"torch\":\n+                original_torch = True\n+            inputs_for_processing = tf.convert_to_tensor(inputs)\n         else:\n-            tf_inputs = False\n-            if not isinstance(inputs, (np.ndarray, list, tuple)):\n-                inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = super().call(inputs)\n-        if not tf_inputs:\n-            outputs = backend_utils.convert_tf_tensor(outputs)\n-        return outputs\n+            inputs_for_processing = inputs\n+        output = super().call(inputs_for_processing)\n+        if not original_torch:\n+            return output\n+        numpy_outputs = output.numpy()\n+        if self.invert:\n+            return [n.decode(self.encoding) for n in numpy_outputs]\n+            # This returns a list[str] to make it equivalent to the torch implementation of this.\n+            # See : https://docs.pytorch.org/text/stable/_modules/torchtext/vocab/vocab.html#Vocab.lookup_tokens\n+        else:\n+            return torch.from_numpy(numpy_outputs)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe logic in the call method can be refactored for better readability and to fix an unreachable comment. the current implementation has complex conditional logic for handling different input types and backends. a clearer structure would be to first handle input conversion, then call the parent method, and finally handle output conversion if necessary. this also resolves the issue of having a comment after a return statement, which is unreachable code.\n\nhere is a suggested refactoring that improves clarity while preserving the intended logic:\n\npython\n    def call(self, inputs):\n        is_torch_backend = backend.backend() == \"torch\"\n\n        inputs_for_processing = inputs\n        if is_torch_backend and isinstance(inputs, torch.tensor):\n            inputs_for_processing = tf.convert_to_tensor(\n                inputs.detach().cpu().numpy()\n            )\n        elif isinstance(inputs, (np.ndarray, list, tuple)):\n            inputs_for_processing = tf.convert_to_tensor(inputs)\n\n        output = super().call(inputs_for_processing)\n\n        if is_torch_backend and isinstance(\n            inputs, (torch.tensor, np.ndarray, list, tuple)\n        ):\n            numpy_outputs = output.numpy()\n            if self.invert:\n                # this returns a list[str] to make it equivalent to the torch\n                # implementation of this.\n                # see : https://docs.pytorch.org/text/stable/_modules/torchtext/vocab/vocab.html#vocab.lookup_tokens\n                return [n.decode(self.encoding) for n in numpy_outputs]\n            else:\n                return torch.from_numpy(numpy_outputs)\n        return output",
    "line_number": 407,
    "enriched": "File: keras/src/layers/preprocessing/string_lookup.py\nCode: @@ -382,13 +384,24 @@ def get_config(self):\n         return {**base_config, **config}\n \n     def call(self, inputs):\n-        if isinstance(inputs, (tf.Tensor, tf.RaggedTensor, tf.SparseTensor)):\n-            tf_inputs = True\n+        original_torch = False\n+        if(backend.backend() == \"torch\" and isinstance(inputs, torch.Tensor)):\n+            original_torch = True\n+            numpy_inputs = inputs.detach().cpu().numpy()\n+            inputs_for_processing = tf.convert_to_tensor(numpy_inputs)\n+        elif isinstance(inputs, (np.ndarray, list, tuple)):\n+            if backend.backend() == \"torch\":\n+                original_torch = True\n+            inputs_for_processing = tf.convert_to_tensor(inputs)\n         else:\n-            tf_inputs = False\n-            if not isinstance(inputs, (np.ndarray, list, tuple)):\n-                inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = super().call(inputs)\n-        if not tf_inputs:\n-            outputs = backend_utils.convert_tf_tensor(outputs)\n-        return outputs\n+            inputs_for_processing = inputs\n+        output = super().call(inputs_for_processing)\n+        if not original_torch:\n+            return output\n+        numpy_outputs = output.numpy()\n+        if self.invert:\n+            return [n.decode(self.encoding) for n in numpy_outputs]\n+            # This returns a list[str] to make it equivalent to the torch implementation of this.\n+            # See : https://docs.pytorch.org/text/stable/_modules/torchtext/vocab/vocab.html#Vocab.lookup_tokens\n+        else:\n+            return torch.from_numpy(numpy_outputs)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe logic in the `call` method can be refactored for better readability and to fix an unreachable comment. The current implementation has complex conditional logic for handling different input types and backends. A clearer structure would be to first handle input conversion, then call the parent method, and finally handle output conversion if necessary. This also resolves the issue of having a comment after a `return` statement, which is unreachable code.\n\nHere is a suggested refactoring that improves clarity while preserving the intended logic:\n\n```python\n    def call(self, inputs):\n        is_torch_backend = backend.backend() == \"torch\"\n\n        inputs_for_processing = inputs\n        if is_torch_backend and isinstance(inputs, torch.Tensor):\n            inputs_for_processing = tf.convert_to_tensor(\n                inputs.detach().cpu().numpy()\n            )\n        elif isinstance(inputs, (np.ndarray, list, tuple)):\n            inputs_for_processing = tf.convert_to_tensor(inputs)\n\n        output = super().call(inputs_for_processing)\n\n        if is_torch_backend and isinstance(\n            inputs, (torch.Tensor, np.ndarray, list, tuple)\n        ):\n            numpy_outputs = output.numpy()\n            if self.invert:\n                # This returns a list[str] to make it equivalent to the torch\n                # implementation of this.\n                # See : https://docs.pytorch.org/text/stable/_modules/torchtext/vocab/vocab.html#Vocab.lookup_tokens\n                return [n.decode(self.encoding) for n in numpy_outputs]\n            else:\n                return torch.from_numpy(numpy_outputs)\n        return output\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/layers/preprocessing/string_lookup.py",
    "pr_number": 21614,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2298651756,
    "comment_created_at": "2025-08-25T17:09:11Z"
  },
  {
    "code": "@@ -997,9 +997,153 @@ def less_equal(x1, x2):\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n-    raise NotImplementedError(\n-        \"`linspace` is not supported with openvino backend\"\n+    start = get_ov_output(start)\n+    stop = get_ov_output(stop)\n+\n+    if hasattr(num, \"output\") or isinstance(num, OpenVINOKerasTensor):\n+        num_tensor = get_ov_output(num)\n+        try:\n+            if num_tensor.get_node().get_type_name() == \"Constant\":\n+                num_value = num_tensor.get_node().get_vector()[0]\n+                num = int(num_value)\n+            else:\n+                raise NotImplementedError(\n+                    \"Dynamic num values not fully supported\"\n+                )\n+        except:\n+            raise NotImplementedError(\"Could not extract num value from tensor\")\n+    else:\n+        num = int(num)\n+    if dtype is None:\n+        start_type = start.get_element_type()\n+        stop_type = stop.get_element_type()\n+        if start_type.is_real() or stop_type.is_real():\n+            output_type = OPENVINO_DTYPES[config.floatx()]\n+        else:\n+            output_type = OPENVINO_DTYPES[config.floatx()]\n+    else:\n+        output_type = OPENVINO_DTYPES[dtype]\n+\n+    start = ov_opset.convert(start, output_type).output(0)\n+    stop = ov_opset.convert(stop, output_type).output(0)\n+    if num <= 0:\n+        empty_shape = ov_opset.constant([0], Type.i32).output(0)\n+        result = ov_opset.broadcast(\n+            ov_opset.constant(0.0, output_type).output(0), empty_shape\n+        ).output(0)\n+        if retstep:\n+            return OpenVINOKerasTensor(result), None\n+        return OpenVINOKerasTensor(result)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthis implementation deviates from numpy's linspace behavior for num <= 0.\n1. numpy raises a valueerror for num < 0, but this implementation returns an empty tensor.\n2. for num = 0 with retstep=true, numpy returns nan for the step, whereas this returns none.\n\nto align with numpy's behavior, you should add a check for negative num and handle the retstep case for num=0 correctly.\n\npython\n    if num < 0:\n        raise valueerror(\"number of samples, num, must be non-negative.\")\n    if num == 0:\n        empty_shape = ov_opset.constant([0], type.i32).output(0)\n        result = ov_opset.broadcast(\n            ov_opset.constant(0.0, output_type).output(0), empty_shape\n        ).output(0)\n        if retstep:\n            nan_step = ov_opset.constant(np.nan, output_type).output(0)\n            return openvinokerastensor(result), openvinokerastensor(nan_step)\n        return openvinokerastensor(result)",
    "line_number": 1036,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -997,9 +997,153 @@ def less_equal(x1, x2):\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n-    raise NotImplementedError(\n-        \"`linspace` is not supported with openvino backend\"\n+    start = get_ov_output(start)\n+    stop = get_ov_output(stop)\n+\n+    if hasattr(num, \"output\") or isinstance(num, OpenVINOKerasTensor):\n+        num_tensor = get_ov_output(num)\n+        try:\n+            if num_tensor.get_node().get_type_name() == \"Constant\":\n+                num_value = num_tensor.get_node().get_vector()[0]\n+                num = int(num_value)\n+            else:\n+                raise NotImplementedError(\n+                    \"Dynamic num values not fully supported\"\n+                )\n+        except:\n+            raise NotImplementedError(\"Could not extract num value from tensor\")\n+    else:\n+        num = int(num)\n+    if dtype is None:\n+        start_type = start.get_element_type()\n+        stop_type = stop.get_element_type()\n+        if start_type.is_real() or stop_type.is_real():\n+            output_type = OPENVINO_DTYPES[config.floatx()]\n+        else:\n+            output_type = OPENVINO_DTYPES[config.floatx()]\n+    else:\n+        output_type = OPENVINO_DTYPES[dtype]\n+\n+    start = ov_opset.convert(start, output_type).output(0)\n+    stop = ov_opset.convert(stop, output_type).output(0)\n+    if num <= 0:\n+        empty_shape = ov_opset.constant([0], Type.i32).output(0)\n+        result = ov_opset.broadcast(\n+            ov_opset.constant(0.0, output_type).output(0), empty_shape\n+        ).output(0)\n+        if retstep:\n+            return OpenVINOKerasTensor(result), None\n+        return OpenVINOKerasTensor(result)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThis implementation deviates from NumPy's `linspace` behavior for `num <= 0`.\n1. NumPy raises a `ValueError` for `num < 0`, but this implementation returns an empty tensor.\n2. For `num = 0` with `retstep=True`, NumPy returns `nan` for the step, whereas this returns `None`.\n\nTo align with NumPy's behavior, you should add a check for negative `num` and handle the `retstep` case for `num=0` correctly.\n\n```python\n    if num < 0:\n        raise ValueError(\"Number of samples, `num`, must be non-negative.\")\n    if num == 0:\n        empty_shape = ov_opset.constant([0], Type.i32).output(0)\n        result = ov_opset.broadcast(\n            ov_opset.constant(0.0, output_type).output(0), empty_shape\n        ).output(0)\n        if retstep:\n            nan_step = ov_opset.constant(np.nan, output_type).output(0)\n            return OpenVINOKerasTensor(result), OpenVINOKerasTensor(nan_step)\n        return OpenVINOKerasTensor(result)\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21613,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2296798847,
    "comment_created_at": "2025-08-24T20:37:35Z"
  },
  {
    "code": "@@ -892,10 +900,11 @@ def compute_homography_matrix(start_points, end_points):\n         axis=-1,\n     )\n     target_vector = np.expand_dims(target_vector, axis=-1)\n-\n+    coefficient_matrix = coefficient_matrix.astype(compute_dtype)\n+    target_vector = target_vector.astype(compute_dtype)\n     homography_matrix = np.linalg.solve(coefficient_matrix, target_vector)\n     homography_matrix = np.reshape(homography_matrix, [-1, 8])\n-\n+    homography_matrix.astype(dtype)\n     return homography_matrix",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe result of homography_matrix.astype(dtype) is not assigned to any variable, so this line has no effect. the function will return the homography_matrix with compute_dtype instead of the intended original dtype. you should return the result of the cast directly.",
    "line_number": 908,
    "enriched": "File: keras/src/backend/numpy/image.py\nCode: @@ -892,10 +900,11 @@ def compute_homography_matrix(start_points, end_points):\n         axis=-1,\n     )\n     target_vector = np.expand_dims(target_vector, axis=-1)\n-\n+    coefficient_matrix = coefficient_matrix.astype(compute_dtype)\n+    target_vector = target_vector.astype(compute_dtype)\n     homography_matrix = np.linalg.solve(coefficient_matrix, target_vector)\n     homography_matrix = np.reshape(homography_matrix, [-1, 8])\n-\n+    homography_matrix.astype(dtype)\n     return homography_matrix\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe result of `homography_matrix.astype(dtype)` is not assigned to any variable, so this line has no effect. The function will return the `homography_matrix` with `compute_dtype` instead of the intended original `dtype`. You should return the result of the cast directly.\n\n```suggestion\n    return homography_matrix.astype(dtype)\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/numpy/image.py",
    "pr_number": 21612,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2296581654,
    "comment_created_at": "2025-08-24T09:12:55Z"
  },
  {
    "code": "@@ -146,11 +146,14 @@ def compute_mask(self, inputs, mask=None):\n                 masks.append(ops.ones_like(input_i, dtype=\"bool\"))\n             elif mask_i.ndim < input_i.ndim:\n                 # Mask is smaller than the input, expand it\n-                masks.append(\n-                    ops.broadcast_to(\n-                        ops.expand_dims(mask_i, axis=-1), ops.shape(input_i)\n-                    )\n-                )\n+                mask_i = ops.expand_dims(mask_i, axis=-1)\n+                # Broadcast mask shape to match in a way where we capture the\n+                # input as a symbolic input in the op graph.\n+                mask_dtype = mask_i.dtype\n+                mask_i = ops.cast(mask_i, \"int32\")\n+                mask_i = mask_i + ops.zeros_like(input_i, \"int32\")\n+                mask_i = ops.cast(mask_i, dtype=mask_dtype)\n+                masks.append(mask_i)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe current implementation for broadcasting the mask works, but it's a bit verbose with the casting to int32 and back. a more direct and idiomatic way to achieve this for boolean tensors is to use ops.logical_or with a tensor of zeros. this avoids the type casting and makes the intent clearer.",
    "line_number": 154,
    "enriched": "File: keras/src/layers/merging/concatenate.py\nCode: @@ -146,11 +146,14 @@ def compute_mask(self, inputs, mask=None):\n                 masks.append(ops.ones_like(input_i, dtype=\"bool\"))\n             elif mask_i.ndim < input_i.ndim:\n                 # Mask is smaller than the input, expand it\n-                masks.append(\n-                    ops.broadcast_to(\n-                        ops.expand_dims(mask_i, axis=-1), ops.shape(input_i)\n-                    )\n-                )\n+                mask_i = ops.expand_dims(mask_i, axis=-1)\n+                # Broadcast mask shape to match in a way where we capture the\n+                # input as a symbolic input in the op graph.\n+                mask_dtype = mask_i.dtype\n+                mask_i = ops.cast(mask_i, \"int32\")\n+                mask_i = mask_i + ops.zeros_like(input_i, \"int32\")\n+                mask_i = ops.cast(mask_i, dtype=mask_dtype)\n+                masks.append(mask_i)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe current implementation for broadcasting the mask works, but it's a bit verbose with the casting to `int32` and back. A more direct and idiomatic way to achieve this for boolean tensors is to use `ops.logical_or` with a tensor of zeros. This avoids the type casting and makes the intent clearer.\n\n```suggestion\n                mask_i = ops.expand_dims(mask_i, axis=-1)\n                # Broadcast mask to the same shape as the input by using\n                # `ops.logical_or` with a zero tensor of the target shape. This\n                # correctly handles symbolic tensors.\n                mask_i = ops.logical_or(mask_i, ops.zeros_like(input_i, dtype=\"bool\"))\n                masks.append(mask_i)\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/layers/merging/concatenate.py",
    "pr_number": 21611,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2296371945,
    "comment_created_at": "2025-08-23T23:20:09Z"
  },
  {
    "code": "@@ -98,35 +98,64 @@ def quantization_mode(self):\n         return self.default_policy.quantization_mode\n \n     def __getitem__(self, key):\n-        \"\"\"Retrieves the corresponding `DTypePolicy` by the string key.\n+        \"\"\"Retrieves a `DTypePolicy` by its key, with regex fallback logic.\n \n-        When there isn't an exact match, all the existing keys in the map\n-        will be treated as a regex and map against the input key again. When\n-        there are multiple matches for the regex, an `ValueError` will be\n-        raised. Returns `self.default_policy` if there isn't any match found.\n+        This method first attempts an exact key match. If no exact match is\n+        found, it treats the keys stored in the map as regular expression\n+        patterns and searches for a match against the input `key`.\n+\n+        A regex match is only considered valid if it meets two conditions:\n+        - It must match exactly one pattern in the policy map.\n+        - The policy associated with that pattern must not be for quantization.\n \n         Args:\n-            key: String key to query a `DTypePolicy`.\n+            key: The string key to query for a `DTypePolicy`.\n \n         Returns:\n-            Corresponding `DTypePolicy` based on the query.\n+            The corresponding `DTypePolicy`. If no valid match is found\n+            (either exact or regex), this method returns `self.default_policy`.\n+\n+        Raises:\n+            ValueError: If the `key` matches more than one regex pattern.\n         \"\"\"\n+        # 1. Check for an exact match.\n         if key in self._policy_map:\n             return self._policy_map[key]\n \n-        matching_keys = []\n-        for k in self._policy_map:\n-            if re.search(k, key):\n-                matching_keys.append(k)\n+        # 2. If no exact match is found, treat the keys in the map as regular\n+        # expression patterns and search for a match against the input `key`.\n+        matching_keys = [\n+            pattern for pattern in self._policy_map if re.search(pattern, key)\n+        ]\n+\n+        # 3. Handle cases based on the number of matches found.\n         if len(matching_keys) > 1:\n             raise ValueError(\n                 f\"Path '{key}' matches multiple dtype policy \"\n                 f\"specification keys: {matching_keys}. Please make \"\n                 \"sure each path only matches at most \"\n                 \"one dtype policy specification key in the DTypePolicyMap.\"\n             )\n-        elif len(matching_keys) == 1:\n-            return self._policy_map[matching_keys[0]]\n+\n+        if len(matching_keys) == 1:\n+            policy = self._policy_map[matching_keys[0]]\n+\n+            # A single regex match is only considered valid if the resulting\n+            # policy is non-quantized.\n+            # This logic specifically handles how KerasHub serializes quantized\n+            # models. In that workflow, every quantized layer is stored in the\n+            # policy map with its exact path, ensuring it's found by the direct\n+            # lookup at the start of this method.\n+            #\n+            #  For example, it stops a general key like \"attention/query\" which\n+            # represents a quantization-compatible layer from incorrectly\n+            # matching a different, incompatible layer like\n+            # \"attention/query_norm\"\n+            if not policy.quantization_mode:",
    "comment": "i'm not sure i'm following the exact bug here, but this seems like we are losing a lot of flexibility with the api here. the hope was this could also be used as a more granular dtype setting api for non quantized cases, say running at different precisions for different layers.\r\n\r\n\r\ndtype_policy_map = dtypepolicymap()\r\ndtype_policy_map[\"layer/dense_0\"] = dtypepolicy(\"float32\")\r\ndtype_policy_map[\"layer/dense_1\"] = dtypepolicy(\"bfloat16\")\r\n\r\n\r\nwon't we lose that ability with this change? is there another way we can fix whatever the issue is here?",
    "line_number": 154,
    "enriched": "File: keras/src/dtype_policies/dtype_policy_map.py\nCode: @@ -98,35 +98,64 @@ def quantization_mode(self):\n         return self.default_policy.quantization_mode\n \n     def __getitem__(self, key):\n-        \"\"\"Retrieves the corresponding `DTypePolicy` by the string key.\n+        \"\"\"Retrieves a `DTypePolicy` by its key, with regex fallback logic.\n \n-        When there isn't an exact match, all the existing keys in the map\n-        will be treated as a regex and map against the input key again. When\n-        there are multiple matches for the regex, an `ValueError` will be\n-        raised. Returns `self.default_policy` if there isn't any match found.\n+        This method first attempts an exact key match. If no exact match is\n+        found, it treats the keys stored in the map as regular expression\n+        patterns and searches for a match against the input `key`.\n+\n+        A regex match is only considered valid if it meets two conditions:\n+        - It must match exactly one pattern in the policy map.\n+        - The policy associated with that pattern must not be for quantization.\n \n         Args:\n-            key: String key to query a `DTypePolicy`.\n+            key: The string key to query for a `DTypePolicy`.\n \n         Returns:\n-            Corresponding `DTypePolicy` based on the query.\n+            The corresponding `DTypePolicy`. If no valid match is found\n+            (either exact or regex), this method returns `self.default_policy`.\n+\n+        Raises:\n+            ValueError: If the `key` matches more than one regex pattern.\n         \"\"\"\n+        # 1. Check for an exact match.\n         if key in self._policy_map:\n             return self._policy_map[key]\n \n-        matching_keys = []\n-        for k in self._policy_map:\n-            if re.search(k, key):\n-                matching_keys.append(k)\n+        # 2. If no exact match is found, treat the keys in the map as regular\n+        # expression patterns and search for a match against the input `key`.\n+        matching_keys = [\n+            pattern for pattern in self._policy_map if re.search(pattern, key)\n+        ]\n+\n+        # 3. Handle cases based on the number of matches found.\n         if len(matching_keys) > 1:\n             raise ValueError(\n                 f\"Path '{key}' matches multiple dtype policy \"\n                 f\"specification keys: {matching_keys}. Please make \"\n                 \"sure each path only matches at most \"\n                 \"one dtype policy specification key in the DTypePolicyMap.\"\n             )\n-        elif len(matching_keys) == 1:\n-            return self._policy_map[matching_keys[0]]\n+\n+        if len(matching_keys) == 1:\n+            policy = self._policy_map[matching_keys[0]]\n+\n+            # A single regex match is only considered valid if the resulting\n+            # policy is non-quantized.\n+            # This logic specifically handles how KerasHub serializes quantized\n+            # models. In that workflow, every quantized layer is stored in the\n+            # policy map with its exact path, ensuring it's found by the direct\n+            # lookup at the start of this method.\n+            #\n+            #  For example, it stops a general key like \"attention/query\" which\n+            # represents a quantization-compatible layer from incorrectly\n+            # matching a different, incompatible layer like\n+            # \"attention/query_norm\"\n+            if not policy.quantization_mode:\nComment: I'm not sure I'm following the exact bug here, but this seems like we are losing a lot of flexibility with the API here. The hope was this could also be used as a more granular dtype setting API for non quantized cases, say running at different precisions for different layers.\r\n\r\n```\r\ndtype_policy_map = DTypePolicyMap()\r\ndtype_policy_map[\"layer/dense_0\"] = DTypePolicy(\"float32\")\r\ndtype_policy_map[\"layer/dense_1\"] = DTypePolicy(\"bfloat16\")\r\n```\r\n\r\nWon't we lose that ability with this change? Is there another way we can fix whatever the issue is here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "keras/src/dtype_policies/dtype_policy_map.py",
    "pr_number": 21608,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2294432443,
    "comment_created_at": "2025-08-22T18:48:05Z"
  },
  {
    "code": "@@ -1559,6 +1559,22 @@ def hstack(xs):\n     return tf.concat(xs, axis=1)\n \n \n+def hypot(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    if dtype in [\"int8\", \"int16\", \"int32\", \"uint8\", \"uint16\", \"uint32\"]:\n+        dtype = config.floatx()\n+    elif dtype in [\"int64\"]:\n+        dtype = \"float64\"\n+\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    return tf.math.sqrt(tf.math.square(x1) + tf.math.square(x2))",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe direct implementation tf.math.sqrt(tf.math.square(x1) + tf.math.square(x2)) is numerically unstable. it can lead to overflow for large input values or underflow for very small values, resulting in inf or 0 respectively.\n\nfor example, hypot(1e20, 1) should be 1e20, but this implementation would compute sqrt((1e20)**2 + 1**2) = sqrt(inf + 1) = inf.\n\na more stable implementation should be used to avoid this. a common way to implement hypot stably is to factor out the maximum absolute value to prevent intermediate overflow.",
    "line_number": 1575,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1559,6 +1559,22 @@ def hstack(xs):\n     return tf.concat(xs, axis=1)\n \n \n+def hypot(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    if dtype in [\"int8\", \"int16\", \"int32\", \"uint8\", \"uint16\", \"uint32\"]:\n+        dtype = config.floatx()\n+    elif dtype in [\"int64\"]:\n+        dtype = \"float64\"\n+\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    return tf.math.sqrt(tf.math.square(x1) + tf.math.square(x2))\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe direct implementation `tf.math.sqrt(tf.math.square(x1) + tf.math.square(x2))` is numerically unstable. It can lead to overflow for large input values or underflow for very small values, resulting in `inf` or `0` respectively.\n\nFor example, `hypot(1e20, 1)` should be `1e20`, but this implementation would compute `sqrt((1e20)**2 + 1**2) = sqrt(inf + 1) = inf`.\n\nA more stable implementation should be used to avoid this. A common way to implement `hypot` stably is to factor out the maximum absolute value to prevent intermediate overflow.\n\n```suggestion\n    x1_abs = tf.abs(x1)\n    x2_abs = tf.abs(x2)\n    max_val = tf.maximum(x1_abs, x2_abs)\n    min_val = tf.minimum(x1_abs, x2_abs)\n    # Using divide_no_nan to handle the case where max_val is 0.\n    ratio = tf.math.divide_no_nan(min_val, max_val)\n    return max_val * tf.sqrt(1.0 + tf.square(ratio))\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21606,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2290885940,
    "comment_created_at": "2025-08-21T12:21:31Z"
  },
  {
    "code": "@@ -205,6 +205,23 @@ def test_pass_initial_state(self):\n             output,\n         )\n \n+    def test_pass_return_state(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        initial_state = np.arange(4).reshape((2, 2)).astype(\"float32\")\n+        layer = layers.GRU(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            return_state=True,\n+        )\n+        output, state = layer(sequence, initial_state=initial_state)\n+        self.assertAllClose(\n+            np.array([[0.23774096, 0.33508456], [0.83659905, 1.0227708]]),\n+            output,\n+        )\n+        self.assertEqual(state.shape, (2, 2))",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis is a good test case for return_state=true. to make it more comprehensive, consider a few enhancements:\n\n1.  **assert output is state**: when return_sequences=false, the final output of a gru layer is its final state. you can add an assertion to verify this: self.assertallclose(output, state). this is a stronger check than just verifying the shape.\n\n2.  **test go_backwards=true**: the go_backwards argument can affect state handling. it would be beneficial to add a test case for go_backwards=true to ensure correctness in that scenario as well.\n\nhere is a suggested implementation combining these points:\n\npython\n    def test_pass_return_state(self):\n        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n        initial_state = np.arange(4).reshape((2, 2)).astype(\"float32\")\n\n        # test with go_backwards=false\n        layer = layers.gru(\n            2,\n            kernel_initializer=initializers.constant(0.01),\n            recurrent_initializer=initializers.constant(0.02),\n            bias_initializer=initializers.constant(0.03),\n            return_state=true,\n        )\n        output, state = layer(sequence, initial_state=initial_state)\n        self.assertallclose(\n            np.array([[0.23774096, 0.33508456], [0.83659905, 1.0227708]]),\n            output,\n        )\n        self.assertallclose(output, state)\n\n        # test with go_backwards=true\n        layer = layers.gru(\n            2,\n            kernel_initializer=initializers.constant(0.01),\n            recurrent_initializer=initializers.constant(0.02),\n            bias_initializer=initializers.constant(0.03),\n            return_state=true,\n            go_backwards=true,\n        )\n        output, state = layer(sequence, initial_state=initial_state)\n        self.assertallclose(\n            np.array([[0.13486053, 0.23261218], [0.78257304, 0.9691353]]),\n            output,\n        )\n        self.assertallclose(output, state)",
    "line_number": 223,
    "enriched": "File: keras/src/layers/rnn/gru_test.py\nCode: @@ -205,6 +205,23 @@ def test_pass_initial_state(self):\n             output,\n         )\n \n+    def test_pass_return_state(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        initial_state = np.arange(4).reshape((2, 2)).astype(\"float32\")\n+        layer = layers.GRU(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            return_state=True,\n+        )\n+        output, state = layer(sequence, initial_state=initial_state)\n+        self.assertAllClose(\n+            np.array([[0.23774096, 0.33508456], [0.83659905, 1.0227708]]),\n+            output,\n+        )\n+        self.assertEqual(state.shape, (2, 2))\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis is a good test case for `return_state=True`. To make it more comprehensive, consider a few enhancements:\n\n1.  **Assert output is state**: When `return_sequences=False`, the final output of a GRU layer is its final state. You can add an assertion to verify this: `self.assertAllClose(output, state)`. This is a stronger check than just verifying the shape.\n\n2.  **Test `go_backwards=True`**: The `go_backwards` argument can affect state handling. It would be beneficial to add a test case for `go_backwards=True` to ensure correctness in that scenario as well.\n\nHere is a suggested implementation combining these points:\n\n```python\n    def test_pass_return_state(self):\n        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n        initial_state = np.arange(4).reshape((2, 2)).astype(\"float32\")\n\n        # Test with go_backwards=False\n        layer = layers.GRU(\n            2,\n            kernel_initializer=initializers.Constant(0.01),\n            recurrent_initializer=initializers.Constant(0.02),\n            bias_initializer=initializers.Constant(0.03),\n            return_state=True,\n        )\n        output, state = layer(sequence, initial_state=initial_state)\n        self.assertAllClose(\n            np.array([[0.23774096, 0.33508456], [0.83659905, 1.0227708]]),\n            output,\n        )\n        self.assertAllClose(output, state)\n\n        # Test with go_backwards=True\n        layer = layers.GRU(\n            2,\n            kernel_initializer=initializers.Constant(0.01),\n            recurrent_initializer=initializers.Constant(0.02),\n            bias_initializer=initializers.Constant(0.03),\n            return_state=True,\n            go_backwards=True,\n        )\n        output, state = layer(sequence, initial_state=initial_state)\n        self.assertAllClose(\n            np.array([[0.13486053, 0.23261218], [0.78257304, 0.9691353]]),\n            output,\n        )\n        self.assertAllClose(output, state)\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/layers/rnn/gru_test.py",
    "pr_number": 21603,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2289565597,
    "comment_created_at": "2025-08-21T00:09:11Z"
  },
  {
    "code": "@@ -158,8 +158,13 @@ def test_saving_lambda(self):\n \n         temp_filepath = os.path.join(self.get_temp_dir(), \"lambda_model.h5\")\n         legacy_h5_format.save_model_to_hdf5(model, temp_filepath)\n-        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\n \n+        with self.assertRaisesRegex(ValueError, \"Deserializing it is unsafe\"):\n+            legacy_h5_format.load_model_from_hdf5(temp_filepath)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe regex \"deserializing it is unsafe\" does not match the valueerror raised when deserializing a lambda in safe mode. the actual error message is \"requested the deserialization of a lambda object. this carries a potential risk of arbitrary code execution...\". a more appropriate regex, like \"arbitrary code execution\", should be used to correctly test for this exception.",
    "line_number": 163,
    "enriched": "File: keras/src/legacy/saving/legacy_h5_format_test.py\nCode: @@ -158,8 +158,13 @@ def test_saving_lambda(self):\n \n         temp_filepath = os.path.join(self.get_temp_dir(), \"lambda_model.h5\")\n         legacy_h5_format.save_model_to_hdf5(model, temp_filepath)\n-        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\n \n+        with self.assertRaisesRegex(ValueError, \"Deserializing it is unsafe\"):\n+            legacy_h5_format.load_model_from_hdf5(temp_filepath)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe regex `\"Deserializing it is unsafe\"` does not match the `ValueError` raised when deserializing a lambda in safe mode. The actual error message is \"Requested the deserialization of a `lambda` object. This carries a potential risk of arbitrary code execution...\". A more appropriate regex, like `\"arbitrary code execution\"`, should be used to correctly test for this exception.\n\n```suggestion\n        with self.assertRaisesRegex(ValueError, \"arbitrary code execution\"):\n            legacy_h5_format.load_model_from_hdf5(temp_filepath)\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/legacy/saving/legacy_h5_format_test.py",
    "pr_number": 21602,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2289196194,
    "comment_created_at": "2025-08-20T20:12:47Z"
  },
  {
    "code": "@@ -1097,9 +1097,52 @@ def median(x, axis=None, keepdims=False):\n \n \n def meshgrid(*x, indexing=\"xy\"):\n-    raise NotImplementedError(\n-        \"`meshgrid` is not supported with openvino backend\"\n-    )\n+    if len(x) < 2:\n+        raise ValueError(\"meshgrid requires at least 2 input arrays\")\n+    if indexing not in (\"xy\", \"ij\"):\n+        raise ValueError(\"indexing must be either 'xy' or 'ij'\")\n+\n+    tensors = [get_ov_output(xi) for xi in x]\n+    n = len(tensors)\n+\n+    shapes = [ov_opset.shape_of(t, Type.i64).output(0) for t in tensors]   # each is [Ni]\n+    one = ov_opset.constant([1], Type.i64).output(0)\n+\n+    if indexing == \"xy\" and n >= 2:\n+        out_shape = ov_opset.concat([shapes[1], shapes[0]] + shapes[2:], axis=0).output(0)\n+    else:\n+        out_shape = ov_opset.concat(shapes, axis=0).output(0)\n+\n+    outputs = []\n+    for i, t in enumerate(tensors):\n+        parts = []\n+        for axis in range(n):\n+            if indexing == \"xy\" and n >= 2:\n+                if i == 0:\n+                    if axis == 0:\n+                        parts.append(one)\n+                    elif axis == 1:\n+                        parts.append(shapes[0])\n+                    else:\n+                        parts.append(one if axis != i else shapes[i])  \n+                elif i == 1:\n+                    if axis == 0:\n+                        parts.append(shapes[1])\n+                    elif axis == 1:\n+                        parts.append(one)\n+                    else:\n+                        parts.append(one if axis != i else shapes[i])  \n+                else:\n+                    parts.append(shapes[i] if axis == i else one)\n+            else:\n+                parts.append(shapes[i] if axis == i else one)\n+\n+        reshape_shape = ov_opset.concat(parts, axis=0).output(0)\n+        reshaped = ov_opset.reshape(t, reshape_shape, False).output(0)\n+        broadcasted = ov_opset.broadcast(reshaped, out_shape).output(0)\n+        outputs.append(OpenVINOKerasTensor(broadcasted))\n+",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe logic for constructing the reshape shapes is quite complex and can be simplified for better readability and maintainability.\n\nyou can determine the reshape_parts directly without the nested for loop over axes. also, the and n >= 2 check is redundant since it's already handled at the beginning of the function.\n\nhere's a suggested refactoring:\n\npython\n    if indexing == \"xy\":\n        # for 'xy' indexing, the first two dimensions of the output grid are swapped.\n        shape_list = [shapes[1], shapes[0]] + shapes[2:]\n        out_shape = ov_opset.concat(shape_list, axis=0).output(0)\n    else:  # 'ij' indexing\n        out_shape = ov_opset.concat(shapes, axis=0).output(0)\n\n    outputs = []\n    for i, t in enumerate(tensors):\n        # reshape each tensor to a 1d tensor of its own size along one axis\n        # and size 1 along all other axes.\n        reshape_parts = [one] * n\n        if indexing == \"xy\":\n            if i == 0:\n                # first output varies along second axis\n                reshape_parts[1] = shapes[0]\n            elif i == 1:\n                # second output varies along first axis\n                reshape_parts[0] = shapes[1]\n            else:\n                reshape_parts[i] = shapes[i]\n        else:  # 'ij' indexing\n            reshape_parts[i] = shapes[i]\n\n        reshape_shape = ov_opset.concat(reshape_parts, axis=0).output(0)\n        reshaped = ov_opset.reshape(t, reshape_shape, false).output(0)\n        broadcasted = ov_opset.broadcast(reshaped, out_shape).output(0)\n        outputs.append(openvinokerastensor(broadcasted))",
    "line_number": 1144,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -1097,9 +1097,52 @@ def median(x, axis=None, keepdims=False):\n \n \n def meshgrid(*x, indexing=\"xy\"):\n-    raise NotImplementedError(\n-        \"`meshgrid` is not supported with openvino backend\"\n-    )\n+    if len(x) < 2:\n+        raise ValueError(\"meshgrid requires at least 2 input arrays\")\n+    if indexing not in (\"xy\", \"ij\"):\n+        raise ValueError(\"indexing must be either 'xy' or 'ij'\")\n+\n+    tensors = [get_ov_output(xi) for xi in x]\n+    n = len(tensors)\n+\n+    shapes = [ov_opset.shape_of(t, Type.i64).output(0) for t in tensors]   # each is [Ni]\n+    one = ov_opset.constant([1], Type.i64).output(0)\n+\n+    if indexing == \"xy\" and n >= 2:\n+        out_shape = ov_opset.concat([shapes[1], shapes[0]] + shapes[2:], axis=0).output(0)\n+    else:\n+        out_shape = ov_opset.concat(shapes, axis=0).output(0)\n+\n+    outputs = []\n+    for i, t in enumerate(tensors):\n+        parts = []\n+        for axis in range(n):\n+            if indexing == \"xy\" and n >= 2:\n+                if i == 0:\n+                    if axis == 0:\n+                        parts.append(one)\n+                    elif axis == 1:\n+                        parts.append(shapes[0])\n+                    else:\n+                        parts.append(one if axis != i else shapes[i])  \n+                elif i == 1:\n+                    if axis == 0:\n+                        parts.append(shapes[1])\n+                    elif axis == 1:\n+                        parts.append(one)\n+                    else:\n+                        parts.append(one if axis != i else shapes[i])  \n+                else:\n+                    parts.append(shapes[i] if axis == i else one)\n+            else:\n+                parts.append(shapes[i] if axis == i else one)\n+\n+        reshape_shape = ov_opset.concat(parts, axis=0).output(0)\n+        reshaped = ov_opset.reshape(t, reshape_shape, False).output(0)\n+        broadcasted = ov_opset.broadcast(reshaped, out_shape).output(0)\n+        outputs.append(OpenVINOKerasTensor(broadcasted))\n+\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe logic for constructing the reshape shapes is quite complex and can be simplified for better readability and maintainability.\n\nYou can determine the `reshape_parts` directly without the nested `for` loop over axes. Also, the `and n >= 2` check is redundant since it's already handled at the beginning of the function.\n\nHere's a suggested refactoring:\n\n```python\n    if indexing == \"xy\":\n        # For 'xy' indexing, the first two dimensions of the output grid are swapped.\n        shape_list = [shapes[1], shapes[0]] + shapes[2:]\n        out_shape = ov_opset.concat(shape_list, axis=0).output(0)\n    else:  # 'ij' indexing\n        out_shape = ov_opset.concat(shapes, axis=0).output(0)\n\n    outputs = []\n    for i, t in enumerate(tensors):\n        # Reshape each tensor to a 1D tensor of its own size along one axis\n        # and size 1 along all other axes.\n        reshape_parts = [one] * n\n        if indexing == \"xy\":\n            if i == 0:\n                # First output varies along second axis\n                reshape_parts[1] = shapes[0]\n            elif i == 1:\n                # Second output varies along first axis\n                reshape_parts[0] = shapes[1]\n            else:\n                reshape_parts[i] = shapes[i]\n        else:  # 'ij' indexing\n            reshape_parts[i] = shapes[i]\n\n        reshape_shape = ov_opset.concat(reshape_parts, axis=0).output(0)\n        reshaped = ov_opset.reshape(t, reshape_shape, False).output(0)\n        broadcasted = ov_opset.broadcast(reshaped, out_shape).output(0)\n        outputs.append(OpenVINOKerasTensor(broadcasted))\n\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21600,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2286972804,
    "comment_created_at": "2025-08-20T04:49:49Z"
  },
  {
    "code": "@@ -0,0 +1,159 @@\n+import keras.src.backend\n+from keras.src import tree\n+from keras.src.layers.layer import Layer\n+from keras.src.random.seed_generator import SeedGenerator\n+from keras.src.utils import backend_utils\n+from keras.src.utils import jax_utils\n+from keras.src.utils import tracking\n+\n+\n+class DataLayer(Layer):\n+    \"\"\"Layer designed for safe use in `tf.data` or `grain` pipeline.\n+\n+    This layer overrides the `__call__` method to ensure that the correct\n+    backend is used and that computation is performed on the CPU.\n+\n+    The `call()` method in subclasses should use `self.backend` ops. If\n+    randomness is needed, define both `seed` and `generator` in `__init__` and\n+    retrieve the running seed using `self._get_seed_generator()`. If the layer\n+    has weights in `__init__` or `build()`, use `convert_weight()` to ensure\n+    they are in the correct backend.\n+\n+    **Note:** This layer and its subclasses only support a single input tensor.\n+\n+    Examples:\n+\n+    **Custom `DataLayer` subclass:**\n+\n+    ```python\n+    from keras.src.layers.preprocessing.data_layer import DataLayer\n+    from keras.src.random import SeedGenerator",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe example code in the docstring uses ops on line 36 without importing it. to make the example runnable, please add from keras.src import ops.",
    "line_number": 30,
    "enriched": "File: keras/src/layers/preprocessing/data_layer.py\nCode: @@ -0,0 +1,159 @@\n+import keras.src.backend\n+from keras.src import tree\n+from keras.src.layers.layer import Layer\n+from keras.src.random.seed_generator import SeedGenerator\n+from keras.src.utils import backend_utils\n+from keras.src.utils import jax_utils\n+from keras.src.utils import tracking\n+\n+\n+class DataLayer(Layer):\n+    \"\"\"Layer designed for safe use in `tf.data` or `grain` pipeline.\n+\n+    This layer overrides the `__call__` method to ensure that the correct\n+    backend is used and that computation is performed on the CPU.\n+\n+    The `call()` method in subclasses should use `self.backend` ops. If\n+    randomness is needed, define both `seed` and `generator` in `__init__` and\n+    retrieve the running seed using `self._get_seed_generator()`. If the layer\n+    has weights in `__init__` or `build()`, use `convert_weight()` to ensure\n+    they are in the correct backend.\n+\n+    **Note:** This layer and its subclasses only support a single input tensor.\n+\n+    Examples:\n+\n+    **Custom `DataLayer` subclass:**\n+\n+    ```python\n+    from keras.src.layers.preprocessing.data_layer import DataLayer\n+    from keras.src.random import SeedGenerator\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe example code in the docstring uses `ops` on line 36 without importing it. To make the example runnable, please add `from keras.src import ops`.\n\n```suggestion\n    from keras.src import ops\n    from keras.src.layers.preprocessing.data_layer import DataLayer\n    from keras.src.random import SeedGenerator\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/layers/preprocessing/data_layer.py",
    "pr_number": 21598,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2285505805,
    "comment_created_at": "2025-08-19T14:44:07Z"
  },
  {
    "code": "@@ -1105,6 +1106,18 @@ def append_batch_axis(t):\n         y = tree.map_structure(append_batch_axis, y)\n         return y\n \n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"function\": self.function})\n+        return config\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        config[\"function\"] = serialization_lib.deserialize_keras_object(\n+            config[\"function\"]\n+        )\n+        return cls(**config)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nto prevent potential side effects for callers of this method, it's a good practice to avoid modifying the config dictionary in-place. creating a copy ensures that the original dictionary passed to this method remains unchanged. this pattern is also used elsewhere in the codebase, for instance in operation.from_config.",
    "line_number": 1120,
    "enriched": "File: keras/src/ops/core.py\nCode: @@ -1105,6 +1106,18 @@ def append_batch_axis(t):\n         y = tree.map_structure(append_batch_axis, y)\n         return y\n \n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"function\": self.function})\n+        return config\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        config[\"function\"] = serialization_lib.deserialize_keras_object(\n+            config[\"function\"]\n+        )\n+        return cls(**config)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nTo prevent potential side effects for callers of this method, it's a good practice to avoid modifying the `config` dictionary in-place. Creating a copy ensures that the original dictionary passed to this method remains unchanged. This pattern is also used elsewhere in the codebase, for instance in `Operation.from_config`.\n\n```suggestion\n    @classmethod\n    def from_config(cls, config):\n        config = config.copy()\n        config[\"function\"] = serialization_lib.deserialize_keras_object(\n            config[\"function\"]\n        )\n        return cls(**config)\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/ops/core.py",
    "pr_number": 21597,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2283532208,
    "comment_created_at": "2025-08-18T21:25:08Z"
  },
  {
    "code": "@@ -9110,7 +9100,7 @@ def test_angle(self, dtype):\n         x = knp.ones((1,), dtype=dtype)\n         x_jax = jnp.ones((1,), dtype=dtype)\n         expected_dtype = standardize_dtype(jnp.angle(x_jax).dtype)\n-        if dtype == \"int64\":\n+        if dtype == \"bool\" or is_int_dtype(dtype):\n             expected_dtype = backend.floatx()",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthis change correctly identifies that knp.angle should return backend.floatx() for all integer and boolean dtypes to be consistent across backends and jax configurations. however, the current implementation of knp.angle in the jax backend only handles int64 specifically.\n\nthe implementation in keras/src/backend/jax/numpy.py is:\npython\ndef angle(x):\n    x = convert_to_tensor(x)\n    if standardize_dtype(x.dtype) == \"int64\":\n        dtype = config.floatx()\n    else:\n        dtype = dtypes.result_type(x.dtype, float)\n    x = cast(x, dtype)\n    return jnp.angle(x)\n\nthis will cause this test to fail for other integer types (e.g., int32) when backend.floatx() is not float32.\n\nto fix this, the implementation of angle in keras/src/backend/jax/numpy.py should be updated to handle all integer and boolean types, similar to your change here. for example:\npython\nfrom keras.src.backend.common import is_int_dtype\n\ndef angle(x):\n    x = convert_to_tensor(x)\n    x_dtype = standardize_dtype(x.dtype)\n    if is_int_dtype(x_dtype) or x_dtype == \"bool\":\n        dtype = config.floatx()\n    else:\n        dtype = dtypes.result_type(x.dtype, float)\n    x = cast(x, dtype)\n    return jnp.angle(x)\n\nadditionally, the compute_output_spec for keras.ops.numpy.angle should be updated accordingly to reflect this change for symbolic execution.",
    "line_number": 9104,
    "enriched": "File: keras/src/ops/numpy_test.py\nCode: @@ -9110,7 +9100,7 @@ def test_angle(self, dtype):\n         x = knp.ones((1,), dtype=dtype)\n         x_jax = jnp.ones((1,), dtype=dtype)\n         expected_dtype = standardize_dtype(jnp.angle(x_jax).dtype)\n-        if dtype == \"int64\":\n+        if dtype == \"bool\" or is_int_dtype(dtype):\n             expected_dtype = backend.floatx()\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThis change correctly identifies that `knp.angle` should return `backend.floatx()` for all integer and boolean dtypes to be consistent across backends and JAX configurations. However, the current implementation of `knp.angle` in the JAX backend only handles `int64` specifically.\n\nThe implementation in `keras/src/backend/jax/numpy.py` is:\n```python\ndef angle(x):\n    x = convert_to_tensor(x)\n    if standardize_dtype(x.dtype) == \"int64\":\n        dtype = config.floatx()\n    else:\n        dtype = dtypes.result_type(x.dtype, float)\n    x = cast(x, dtype)\n    return jnp.angle(x)\n```\nThis will cause this test to fail for other integer types (e.g., `int32`) when `backend.floatx()` is not `float32`.\n\nTo fix this, the implementation of `angle` in `keras/src/backend/jax/numpy.py` should be updated to handle all integer and boolean types, similar to your change here. For example:\n```python\nfrom keras.src.backend.common import is_int_dtype\n\ndef angle(x):\n    x = convert_to_tensor(x)\n    x_dtype = standardize_dtype(x.dtype)\n    if is_int_dtype(x_dtype) or x_dtype == \"bool\":\n        dtype = config.floatx()\n    else:\n        dtype = dtypes.result_type(x.dtype, float)\n    x = cast(x, dtype)\n    return jnp.angle(x)\n```\nAdditionally, the `compute_output_spec` for `keras.ops.numpy.Angle` should be updated accordingly to reflect this change for symbolic execution.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/ops/numpy_test.py",
    "pr_number": 21596,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2283063982,
    "comment_created_at": "2025-08-18T17:47:28Z"
  },
  {
    "code": "@@ -86,6 +86,21 @@ def test_attention_with_mask(self):\n         self.assertAllClose(output, [[[1.0, 1.0], [0.0, 0.0]]])\n         self.assertAllClose(scores, [[[1.0, 0.0], [1.0, 0.0]]])\n \n+    def test_attention_2D_mask_shape_mismatch(self):\n+        layer = layers.Attention()\n+        batch_size, Tq, Tv, dim = 2, 3, 3, 4",
    "comment": "please use different values for tq and tv so that the tests check that there is no mixup.",
    "line_number": 91,
    "enriched": "File: keras/src/layers/attention/attention_test.py\nCode: @@ -86,6 +86,21 @@ def test_attention_with_mask(self):\n         self.assertAllClose(output, [[[1.0, 1.0], [0.0, 0.0]]])\n         self.assertAllClose(scores, [[[1.0, 0.0], [1.0, 0.0]]])\n \n+    def test_attention_2D_mask_shape_mismatch(self):\n+        layer = layers.Attention()\n+        batch_size, Tq, Tv, dim = 2, 3, 3, 4\nComment: Please use different values for `Tq` and `Tv` so that the tests check that there is no mixup.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "keras/src/layers/attention/attention_test.py",
    "pr_number": 21595,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2285724166,
    "comment_created_at": "2025-08-19T16:08:47Z"
  },
  {
    "code": "@@ -97,31 +107,39 @@ def test_image_dataset_from_directory_no_labels(self):\n         self.assertEqual(batch_count, 2)\n         self.assertEqual(sample_count, 10)\n \n-    def test_image_dataset_from_directory_binary(self):\n+    @parameterized.named_parameters(\n+        (\"tf\", \"tf\"),\n+        (\"grain\", \"grain\"),\n+    )\n+    def test_image_dataset_from_directory_binary(self, format):\n         directory = self._prepare_directory(num_classes=2)\n         dataset = image_dataset_utils.image_dataset_from_directory(\n-            directory, batch_size=8, image_size=(18, 18), label_mode=\"int\"\n+            directory,\n+            batch_size=8,\n+            image_size=(18, 18),\n+            label_mode=\"int\",\n+            format=format,\n         )\n         if backend.config.image_data_format() == \"channels_last\":\n             output_shape = [8, 18, 18, 3]\n         else:\n             output_shape = [8, 3, 18, 18]\n         batch = next(iter(dataset))\n         self.assertLen(batch, 2)\n-        self.assertEqual(batch[0].shape, output_shape)\n-        self.assertEqual(batch[0].dtype.name, \"float32\")\n-        self.assertEqual(batch[1].shape, (8,))\n-        self.assertEqual(batch[1].dtype.name, \"int32\")\n+        self.assertEqual(list(batch[0].shape), output_shape)\n+        self.assertDType(batch[0], \"float32\")\n+        self.assertEqual(list(batch[1].shape), [8])\n+        self.assertDType(batch[1], \"int32\")\n \n         dataset = image_dataset_utils.image_dataset_from_directory(\n             directory, batch_size=8, image_size=(18, 18), label_mode=\"binary\"\n         )",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe format argument is missing in this call to image_dataset_from_directory. when running with format='grain', this will default to format='tf', so the grain path for label_mode='binary' is not being tested.",
    "line_number": 140,
    "enriched": "File: keras/src/utils/image_dataset_utils_test.py\nCode: @@ -97,31 +107,39 @@ def test_image_dataset_from_directory_no_labels(self):\n         self.assertEqual(batch_count, 2)\n         self.assertEqual(sample_count, 10)\n \n-    def test_image_dataset_from_directory_binary(self):\n+    @parameterized.named_parameters(\n+        (\"tf\", \"tf\"),\n+        (\"grain\", \"grain\"),\n+    )\n+    def test_image_dataset_from_directory_binary(self, format):\n         directory = self._prepare_directory(num_classes=2)\n         dataset = image_dataset_utils.image_dataset_from_directory(\n-            directory, batch_size=8, image_size=(18, 18), label_mode=\"int\"\n+            directory,\n+            batch_size=8,\n+            image_size=(18, 18),\n+            label_mode=\"int\",\n+            format=format,\n         )\n         if backend.config.image_data_format() == \"channels_last\":\n             output_shape = [8, 18, 18, 3]\n         else:\n             output_shape = [8, 3, 18, 18]\n         batch = next(iter(dataset))\n         self.assertLen(batch, 2)\n-        self.assertEqual(batch[0].shape, output_shape)\n-        self.assertEqual(batch[0].dtype.name, \"float32\")\n-        self.assertEqual(batch[1].shape, (8,))\n-        self.assertEqual(batch[1].dtype.name, \"int32\")\n+        self.assertEqual(list(batch[0].shape), output_shape)\n+        self.assertDType(batch[0], \"float32\")\n+        self.assertEqual(list(batch[1].shape), [8])\n+        self.assertDType(batch[1], \"int32\")\n \n         dataset = image_dataset_utils.image_dataset_from_directory(\n             directory, batch_size=8, image_size=(18, 18), label_mode=\"binary\"\n         )\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe `format` argument is missing in this call to `image_dataset_from_directory`. When running with `format='grain'`, this will default to `format='tf'`, so the grain path for `label_mode='binary'` is not being tested.\n\n```suggestion\n        dataset = image_dataset_utils.image_dataset_from_directory(\n            directory, batch_size=8, image_size=(18, 18), label_mode=\"binary\", format=format\n        )\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/utils/image_dataset_utils_test.py",
    "pr_number": 21593,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2280794580,
    "comment_created_at": "2025-08-17T09:07:04Z"
  },
  {
    "code": "@@ -834,8 +834,9 @@ def _retrieve_class_or_fn(\n                 )\n \n     raise TypeError(\n-        f\"Could not locate {obj_type} '{name}'. \"\n-        \"Make sure custom classes are decorated with \"\n-        \"`@keras.saving.register_keras_serializable()`. \"\n-        f\"Full object config: {full_config}\"\n+        f\"Could not locate {obj_type} '{name}'. Make sure custom classes and \"\n+        \"functions are decorated with \"\n+        \"`@keras.saving.register_keras_serializable()`. If they are already \"\n+        \"decorated, make sure they are all imported so that the decorator is \"\n+        f\"run before trying to load them. Full object config: {full_config}\"",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthis multi-line string construction is invalid and will cause a syntaxerror. when implicitly concatenating string literals, you cannot have an f-string following a regular string in the sequence. a simple way to fix this is to make each part of the concatenated string an f-string by adding the f prefix to each line.",
    "line_number": 841,
    "enriched": "File: keras/src/saving/serialization_lib.py\nCode: @@ -834,8 +834,9 @@ def _retrieve_class_or_fn(\n                 )\n \n     raise TypeError(\n-        f\"Could not locate {obj_type} '{name}'. \"\n-        \"Make sure custom classes are decorated with \"\n-        \"`@keras.saving.register_keras_serializable()`. \"\n-        f\"Full object config: {full_config}\"\n+        f\"Could not locate {obj_type} '{name}'. Make sure custom classes and \"\n+        \"functions are decorated with \"\n+        \"`@keras.saving.register_keras_serializable()`. If they are already \"\n+        \"decorated, make sure they are all imported so that the decorator is \"\n+        f\"run before trying to load them. Full object config: {full_config}\"\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThis multi-line string construction is invalid and will cause a `SyntaxError`. When implicitly concatenating string literals, you cannot have an f-string following a regular string in the sequence. A simple way to fix this is to make each part of the concatenated string an f-string by adding the `f` prefix to each line.\n\n```suggestion\n        f\"Could not locate {obj_type} '{name}'. Make sure custom classes and \"\n        f\"functions are decorated with \"\n        f\"`@keras.saving.register_keras_serializable()`. If they are already \"\n        f\"decorated, make sure they are all imported so that the decorator is \"\n        f\"run before trying to load them. Full object config: {full_config}\"\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/saving/serialization_lib.py",
    "pr_number": 21590,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2279927461,
    "comment_created_at": "2025-08-15T21:35:10Z"
  },
  {
    "code": "@@ -76,8 +76,9 @@ class IntegerLookup(IndexLookup):\n             If passing a file path, the file should contain one line per term\n             in the vocabulary. If this argument is set,\n             there is no need to `adapt()` the layer.\n-        vocabulary_dtype: The dtype of the vocabulary terms, for example\n-            `\"int64\"` or `\"int32\"`. Defaults to `\"int64\"`.\n+        vocabulary_dtype: The dtype of the vocabulary terms.\n+            Only `vocabulary_dtype='int64'` is supported at this time.\n+            Defaults to `\"int64\"`.",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nwhile updating the docstring is a good step to inform users about the current limitation, it would be more robust to enforce this constraint in the code. this would prevent users from accidentally using an unsupported vocabulary_dtype and encountering errors later.\n\nconsider adding a check in the __init__ method of the integerlookup layer to raise a valueerror if vocabulary_dtype is not 'int64'. for example:\n\npython\n# in integerlookup.__init__\nif vocabulary_dtype != \"int64\":\n    raise valueerror(\n        \"only vocabulary_dtype='int64' is supported at this time for \"\n        f\"integerlookup. received: {vocabulary_dtype}\"\n    )\n\n\nthis would provide immediate feedback to the user and make the layer's behavior more explicit.",
    "line_number": 81,
    "enriched": "File: keras/src/layers/preprocessing/integer_lookup.py\nCode: @@ -76,8 +76,9 @@ class IntegerLookup(IndexLookup):\n             If passing a file path, the file should contain one line per term\n             in the vocabulary. If this argument is set,\n             there is no need to `adapt()` the layer.\n-        vocabulary_dtype: The dtype of the vocabulary terms, for example\n-            `\"int64\"` or `\"int32\"`. Defaults to `\"int64\"`.\n+        vocabulary_dtype: The dtype of the vocabulary terms.\n+            Only `vocabulary_dtype='int64'` is supported at this time.\n+            Defaults to `\"int64\"`.\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nWhile updating the docstring is a good step to inform users about the current limitation, it would be more robust to enforce this constraint in the code. This would prevent users from accidentally using an unsupported `vocabulary_dtype` and encountering errors later.\n\nConsider adding a check in the `__init__` method of the `IntegerLookup` layer to raise a `ValueError` if `vocabulary_dtype` is not `'int64'`. For example:\n\n```python\n# In IntegerLookup.__init__\nif vocabulary_dtype != \"int64\":\n    raise ValueError(\n        \"Only `vocabulary_dtype='int64'` is supported at this time for \"\n        f\"IntegerLookup. Received: {vocabulary_dtype}\"\n    )\n```\n\nThis would provide immediate feedback to the user and make the layer's behavior more explicit.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/layers/preprocessing/integer_lookup.py",
    "pr_number": 21587,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2278918456,
    "comment_created_at": "2025-08-15T12:38:39Z"
  },
  {
    "code": "@@ -17,6 +18,109 @@\n from keras.src.backend.openvino.core import ov_to_keras_type\n \n \n+def diagonal(x, offset=0, axis1=0, axis2=1):\n+    x_node = ov.constant(x)  # -> ov.Node\n+    offset_const = ov_opset.constant(int(offset), dtype=\"i64\")\n+\n+    # rank & normalize axes\n+    shape = ov_opset.shape_of(x_node)  # i64 vector\n+    rank = ov_opset.shape_of(shape)  # scalar i64 (len of shape)\n+    rank_val = ov_opset.squeeze(rank)  # [] -> scalar\n+    axis1_node = ov_opset.floor_mod(\n+        ov_opset.add(ov_opset.constant(int(axis1), dtype=\"i64\"), rank_val),\n+        rank_val,\n+    )\n+    axis2_node = ov_opset.floor_mod(\n+        ov_opset.add(ov_opset.constant(int(axis2), dtype=\"i64\"), rank_val),\n+        rank_val,\n+    )",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe implementation of diagonal doesn't correctly handle tensor inputs for x, offset, axis1, and axis2. it assumes they are python scalars or numpy arrays by using ov.constant and int(), which will lead to errors when openvinokerastensor objects are passed.\n\nto fix this and align with the style of other functions in this backend, please use the get_ov_output utility to convert inputs to openvino tensors and ov_opset.convert for type casting. this will also make the new import of opset13 on line 4 unnecessary.",
    "line_number": 36,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -17,6 +18,109 @@\n from keras.src.backend.openvino.core import ov_to_keras_type\n \n \n+def diagonal(x, offset=0, axis1=0, axis2=1):\n+    x_node = ov.constant(x)  # -> ov.Node\n+    offset_const = ov_opset.constant(int(offset), dtype=\"i64\")\n+\n+    # rank & normalize axes\n+    shape = ov_opset.shape_of(x_node)  # i64 vector\n+    rank = ov_opset.shape_of(shape)  # scalar i64 (len of shape)\n+    rank_val = ov_opset.squeeze(rank)  # [] -> scalar\n+    axis1_node = ov_opset.floor_mod(\n+        ov_opset.add(ov_opset.constant(int(axis1), dtype=\"i64\"), rank_val),\n+        rank_val,\n+    )\n+    axis2_node = ov_opset.floor_mod(\n+        ov_opset.add(ov_opset.constant(int(axis2), dtype=\"i64\"), rank_val),\n+        rank_val,\n+    )\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe implementation of `diagonal` doesn't correctly handle tensor inputs for `x`, `offset`, `axis1`, and `axis2`. It assumes they are Python scalars or numpy arrays by using `ov.constant` and `int()`, which will lead to errors when `OpenVINOKerasTensor` objects are passed.\n\nTo fix this and align with the style of other functions in this backend, please use the `get_ov_output` utility to convert inputs to OpenVINO tensors and `ov_opset.convert` for type casting. This will also make the new import of `opset13` on line 4 unnecessary.\n\n```suggestion\n    x_node = get_ov_output(x)\n    offset_const = ov_opset.convert(get_ov_output(offset), Type.i64)\n\n    # rank & normalize axes\n    shape = ov_opset.shape_of(x_node)  # i64 vector\n    rank = ov_opset.shape_of(shape)  # scalar i64 (len of shape)\n    rank_val = ov_opset.squeeze(rank)  # [] -> scalar\n    axis1_node = ov_opset.floor_mod(\n        ov_opset.add(ov_opset.convert(get_ov_output(axis1), Type.i64), rank_val),\n        rank_val,\n    )\n    axis2_node = ov_opset.floor_mod(\n        ov_opset.add(ov_opset.convert(get_ov_output(axis2), Type.i64), rank_val),\n        rank_val,\n    )\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21585,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2278892607,
    "comment_created_at": "2025-08-15T12:19:23Z"
  },
  {
    "code": "@@ -242,3 +245,311 @@ def _to_backend_layout(tensor_layout):\n     partition_spec = jax.sharding.PartitionSpec(*tensor_layout.axes)\n     jax_mesh = tensor_layout.device_mesh.backend_mesh\n     return jax.sharding.NamedSharding(jax_mesh, partition_spec)\n+\n+\n+_JAX_CLASSES_DEFINED = False\n+JaxGraph = None\n+JaxShardingPlanner = None\n+JaxShardApplier = None\n+\n+\n+def _define_and_register_jax_classes():\n+    global _JAX_CLASSES_DEFINED, JaxGraph, JaxShardingPlanner, JaxShardApplier\n+    if _JAX_CLASSES_DEFINED:\n+        return\n+\n+    from keras.src.distribution.autoshard_utils import MergeableGraph\n+\n+    def parse_jaxpr(jaxpr) -> MergeableGraph:\n+        graph = MergeableGraph()\n+\n+        def same_axis(node1, node2):\n+            var1, axis1 = node1\n+            var2, axis2 = node2\n+            if var1.aval.shape[axis1] != var2.aval.shape[axis2]:\n+                return\n+            graph.merge_nodes(node1, node2)\n+\n+        def parse_dot_general(eqn):\n+            lhs, rhs = eqn.invars\n+            out = eqn.outvars[0]\n+            (lc, rc), (lb, rb) = eqn.params[\"dimension_numbers\"]\n+            for l, r in zip(lc, rc):\n+                same_axis((lhs, l), (rhs, r))\n+            o_offset = 0\n+            for l, r in zip(lb, rb):\n+                same_axis((lhs, l), (rhs, r))\n+                same_axis((lhs, l), (out, o_offset))\n+                o_offset += 1\n+            for i in range(lhs.aval.ndim):\n+                if i not in lb and i not in lc:\n+                    same_axis((lhs, i), (out, o_offset))\n+                    o_offset += 1\n+            for j in range(rhs.aval.ndim):\n+                if j not in rb and j not in rc:\n+                    same_axis((rhs, j), (out, o_offset))\n+                    o_offset += 1\n+\n+        def parse_reshape(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            in_idx, out_idx = 0, 0\n+            in_prod, out_prod = 1, 1\n+            while in_idx < invar.aval.ndim and out_idx < out.aval.ndim:\n+                if (\n+                    in_prod == out_prod\n+                    and invar.aval.shape[in_idx] == out.aval.shape[out_idx]\n+                ):\n+                    if invar.aval.shape[in_idx] > 1:\n+                        same_axis((invar, in_idx), (out, out_idx))\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    out_prod *= out.aval.shape[out_idx]\n+                    in_idx += 1\n+                    out_idx += 1\n+                elif in_prod < out_prod:\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    in_idx += 1\n+                else:\n+                    out_prod *= out.aval.shape[out_idx]\n+                    out_idx += 1\n+\n+        def parse_transpose(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            for i, j in enumerate(eqn.params[\"permutation\"]):\n+                same_axis((invar, j), (out, i))\n+\n+        def parse_elementwise_with_broadcast(eqn):\n+            out = eqn.outvars[0]\n+            for invar in eqn.invars:\n+                if invar.aval.ndim == 0:\n+                    continue\n+                for i in range(1, min(invar.aval.ndim, out.aval.ndim) + 1):\n+                    in_axis, out_axis = -i, -i\n+                    if invar.aval.shape[in_axis] == out.aval.shape[out_axis]:\n+                        same_axis(\n+                            (invar, invar.aval.ndim + in_axis),\n+                            (out, out.aval.ndim + out_axis),\n+                        )\n+\n+        for var in jaxpr.jaxpr.invars:\n+            for i, j in itertools.combinations(range(var.aval.ndim), 2):\n+                graph.add_edge((var, i), (var, j))\n+\n+        for eqn in jaxpr.eqns:\n+            for outvar in eqn.outvars:\n+                for i, j in itertools.combinations(range(outvar.aval.ndim), 2):\n+                    graph.add_edge((outvar, i), (outvar, j))\n+\n+            primitive_parsers = {\n+                \"dot_general\": parse_dot_general,\n+                \"reshape\": parse_reshape,\n+                \"transpose\": parse_transpose,\n+            }\n+            parser = primitive_parsers.get(\n+                eqn.primitive.name, parse_elementwise_with_broadcast\n+            )\n+            parser(eqn)\n+        return graph\n+\n+    def shard_model(\n+        fn,\n+        params,\n+        *inputs,\n+        min_shard_size=1,\n+        data_axis_name=\"data\",\n+        model_axis_name=\"model\",\n+    ):\n+        \"\"\"Analyzes a function via jaxpr and returns sharding assignments.\"\"\"\n+        jaxpr, abs_ret = jax.make_jaxpr(fn, return_shape=True)(params, *inputs)\n+        graph = parse_jaxpr(jaxpr)\n+\n+        params_flat, params_treedef = jax.tree.flatten(params)\n+        _, inputs_treedef = jax.tree.flatten(inputs)\n+        _, outputs_treedef = jax.tree.flatten(abs_ret)\n+\n+        seen = collections.Counter()\n+        for var in jaxpr.jaxpr.invars[: len(params_flat)]:\n+            for i in range(var.aval.ndim):\n+                if var.aval.shape[i] >= min_shard_size:\n+                    seen.update([graph.get_root((var, i))])\n+\n+        model_axis_root = max(seen, key=seen.get) if seen else None\n+\n+        data_axes_roots = []\n+        for var in jaxpr.jaxpr.invars[len(params_flat) :]:\n+            for i in range(var.aval.ndim):\n+                root = graph.get_root((var, i))\n+                if root not in seen and root not in data_axes_roots:\n+                    data_axes_roots.append(root)\n+\n+        def assign_layouts(vars_flat, is_params=False):\n+            assignments = []\n+            for var in vars_flat:\n+                layout = [None] * var.aval.ndim\n+                for i in range(var.aval.ndim):\n+                    if var.aval.shape[i] < min_shard_size:\n+                        continue\n+                    root = graph.get_root((var, i))\n+                    if (\n+                        is_params\n+                        and model_axis_root\n+                        and root == model_axis_root\n+                    ):\n+                        layout[i] = model_axis_name\n+                    elif not is_params and root in data_axes_roots:\n+                        name = data_axis_name\n+                        if len(data_axes_roots) > 1:\n+                            name += str(data_axes_roots.index(root))\n+                        layout[i] = name\n+                assignments.append(layout)\n+            return assignments\n+\n+        params_assignments = params_treedef.unflatten(\n+            assign_layouts(\n+                jaxpr.jaxpr.invars[: len(params_flat)], is_params=True\n+            )\n+        )\n+        inputs_assignments = inputs_treedef.unflatten(\n+            assign_layouts(jaxpr.jaxpr.invars[len(params_flat) :])\n+        )\n+        output_assignments = outputs_treedef.unflatten(\n+            assign_layouts(jaxpr.jaxpr.outvars)\n+        )\n+\n+        return (params_assignments, *inputs_assignments), output_assignments\n+\n+    class _JaxGraph:\n+        \"\"\"A wrapper for a JAX computation graph (jaxpr) of a Keras model.\"\"\"\n+\n+        def __init__(\n+            self,\n+            jaxpr,\n+            trainable_variables,\n+            non_trainable_variables,\n+            in_treedefs,\n+        ):\n+            self.jaxpr = jaxpr\n+            self.trainable_variables = trainable_variables\n+            self.non_trainable_variables = non_trainable_variables\n+            self.in_treedefs = in_treedefs\n+\n+        @classmethod\n+        def from_model(cls, model, *args, **kwargs):\n+            \"\"\"Creates a _JaxGraph instance by tracing the model.\"\"\"\n+\n+            def stateless_fn(\n+                trainable_vars, non_trainable_vars, f_args, f_kwargs\n+            ):\n+                return model.stateless_call(\n+                    trainable_vars, non_trainable_vars, *f_args, **f_kwargs\n+                )\n+\n+            trainable_vars = model.trainable_variables\n+            non_trainable_vars = model.non_trainable_variables\n+\n+            _, t_vars_treedef = jax.tree.flatten(trainable_vars)\n+            _, nt_vars_treedef = jax.tree.flatten(non_trainable_vars)\n+            _, args_treedef = jax.tree.flatten(args)\n+            _, kwargs_treedef = jax.tree.flatten(kwargs)\n+            in_treedefs = (\n+                t_vars_treedef,\n+                nt_vars_treedef,\n+                args_treedef,\n+                kwargs_treedef,\n+            )\n+\n+            closed_jaxpr, _ = jax.make_jaxpr(stateless_fn, return_shape=True)(\n+                trainable_vars, non_trainable_vars, args, kwargs\n+            )\n+            return cls(\n+                closed_jaxpr, trainable_vars, non_trainable_vars, in_treedefs\n+            )\n+\n+    class _JaxShardingPlanner:\n+        \"\"\"\n+        Determines the optimal sharding layout for model variables using\n+        the embedded graph-parsing engine.\n+        \"\"\"\n+\n+        def plan(self, graph, device_mesh):\n+            t_vars = graph.trainable_variables\n+            nt_vars = graph.non_trainable_variables\n+\n+            all_in_avals = [var.aval for var in graph.jaxpr.jaxpr.invars]\n+            t_vars_leaves, _ = jax.tree.flatten(t_vars)\n+            nt_vars_leaves, _ = jax.tree.flatten(nt_vars)\n+\n+            pos = len(t_vars_leaves) + len(nt_vars_leaves)\n+\n+            args_treedef = graph.in_treedefs[2]\n+            kwargs_treedef = graph.in_treedefs[3]\n+\n+            num_args_leaves = args_treedef.num_leaves\n+            args_avals = all_in_avals[pos : pos + num_args_leaves]\n+            kwargs_avals = all_in_avals[pos + num_args_leaves :]\n+\n+            args_aval_tree = jax.tree.unflatten(args_treedef, args_avals)\n+            kwargs_aval_tree = jax.tree.unflatten(kwargs_treedef, kwargs_avals)\n+\n+            dummy_args = jax.tree.map(\n+                lambda x: np.zeros(x.shape, x.dtype), args_aval_tree\n+            )\n+            dummy_kwargs = jax.tree.map(\n+                lambda x: np.zeros(x.shape, x.dtype), kwargs_aval_tree\n+            )\n+\n+            def fn_to_trace(trainable_params, *fn_args, **fn_kwargs):\n+                \"\"\"A function that executes the original jaxpr computation.\"\"\"\n+                all_leaves = (\n+                    jax.tree.leaves(trainable_params)\n+                    + jax.tree.leaves(nt_vars)\n+                    + jax.tree.leaves(fn_args)\n+                    + jax.tree.leaves(fn_kwargs)\n+                )\n+                return jax.core.eval_jaxpr(\n+                    graph.jaxpr.jaxpr, graph.jaxpr.consts, *all_leaves\n+                )\n+\n+            (param_assignments, *_) = shard_model(\n+                fn_to_trace, t_vars, dummy_args, **dummy_kwargs\n+            )",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe call to shard_model here is incorrect as it passes **dummy_kwargs, but the shard_model function signature does not accept arbitrary keyword arguments. this will raise a typeerror at runtime if dummy_kwargs is not empty. \n\nfurthermore, the jax.make_jaxpr call inside shard_model is invoked only with positional arguments (*inputs), so any keyword arguments would be ignored even if the signature was corrected. this would lead to an incorrect sharding analysis for models that use keyword arguments in their call method.\n\nto fix this, the shard_model function needs to be updated to correctly handle both *args and **kwargs throughout its implementation, including passing them to jax.make_jaxpr and correctly processing the input treedefs.",
    "line_number": 514,
    "enriched": "File: keras/src/backend/jax/distribution_lib.py\nCode: @@ -242,3 +245,311 @@ def _to_backend_layout(tensor_layout):\n     partition_spec = jax.sharding.PartitionSpec(*tensor_layout.axes)\n     jax_mesh = tensor_layout.device_mesh.backend_mesh\n     return jax.sharding.NamedSharding(jax_mesh, partition_spec)\n+\n+\n+_JAX_CLASSES_DEFINED = False\n+JaxGraph = None\n+JaxShardingPlanner = None\n+JaxShardApplier = None\n+\n+\n+def _define_and_register_jax_classes():\n+    global _JAX_CLASSES_DEFINED, JaxGraph, JaxShardingPlanner, JaxShardApplier\n+    if _JAX_CLASSES_DEFINED:\n+        return\n+\n+    from keras.src.distribution.autoshard_utils import MergeableGraph\n+\n+    def parse_jaxpr(jaxpr) -> MergeableGraph:\n+        graph = MergeableGraph()\n+\n+        def same_axis(node1, node2):\n+            var1, axis1 = node1\n+            var2, axis2 = node2\n+            if var1.aval.shape[axis1] != var2.aval.shape[axis2]:\n+                return\n+            graph.merge_nodes(node1, node2)\n+\n+        def parse_dot_general(eqn):\n+            lhs, rhs = eqn.invars\n+            out = eqn.outvars[0]\n+            (lc, rc), (lb, rb) = eqn.params[\"dimension_numbers\"]\n+            for l, r in zip(lc, rc):\n+                same_axis((lhs, l), (rhs, r))\n+            o_offset = 0\n+            for l, r in zip(lb, rb):\n+                same_axis((lhs, l), (rhs, r))\n+                same_axis((lhs, l), (out, o_offset))\n+                o_offset += 1\n+            for i in range(lhs.aval.ndim):\n+                if i not in lb and i not in lc:\n+                    same_axis((lhs, i), (out, o_offset))\n+                    o_offset += 1\n+            for j in range(rhs.aval.ndim):\n+                if j not in rb and j not in rc:\n+                    same_axis((rhs, j), (out, o_offset))\n+                    o_offset += 1\n+\n+        def parse_reshape(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            in_idx, out_idx = 0, 0\n+            in_prod, out_prod = 1, 1\n+            while in_idx < invar.aval.ndim and out_idx < out.aval.ndim:\n+                if (\n+                    in_prod == out_prod\n+                    and invar.aval.shape[in_idx] == out.aval.shape[out_idx]\n+                ):\n+                    if invar.aval.shape[in_idx] > 1:\n+                        same_axis((invar, in_idx), (out, out_idx))\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    out_prod *= out.aval.shape[out_idx]\n+                    in_idx += 1\n+                    out_idx += 1\n+                elif in_prod < out_prod:\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    in_idx += 1\n+                else:\n+                    out_prod *= out.aval.shape[out_idx]\n+                    out_idx += 1\n+\n+        def parse_transpose(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            for i, j in enumerate(eqn.params[\"permutation\"]):\n+                same_axis((invar, j), (out, i))\n+\n+        def parse_elementwise_with_broadcast(eqn):\n+            out = eqn.outvars[0]\n+            for invar in eqn.invars:\n+                if invar.aval.ndim == 0:\n+                    continue\n+                for i in range(1, min(invar.aval.ndim, out.aval.ndim) + 1):\n+                    in_axis, out_axis = -i, -i\n+                    if invar.aval.shape[in_axis] == out.aval.shape[out_axis]:\n+                        same_axis(\n+                            (invar, invar.aval.ndim + in_axis),\n+                            (out, out.aval.ndim + out_axis),\n+                        )\n+\n+        for var in jaxpr.jaxpr.invars:\n+            for i, j in itertools.combinations(range(var.aval.ndim), 2):\n+                graph.add_edge((var, i), (var, j))\n+\n+        for eqn in jaxpr.eqns:\n+            for outvar in eqn.outvars:\n+                for i, j in itertools.combinations(range(outvar.aval.ndim), 2):\n+                    graph.add_edge((outvar, i), (outvar, j))\n+\n+            primitive_parsers = {\n+                \"dot_general\": parse_dot_general,\n+                \"reshape\": parse_reshape,\n+                \"transpose\": parse_transpose,\n+            }\n+            parser = primitive_parsers.get(\n+                eqn.primitive.name, parse_elementwise_with_broadcast\n+            )\n+            parser(eqn)\n+        return graph\n+\n+    def shard_model(\n+        fn,\n+        params,\n+        *inputs,\n+        min_shard_size=1,\n+        data_axis_name=\"data\",\n+        model_axis_name=\"model\",\n+    ):\n+        \"\"\"Analyzes a function via jaxpr and returns sharding assignments.\"\"\"\n+        jaxpr, abs_ret = jax.make_jaxpr(fn, return_shape=True)(params, *inputs)\n+        graph = parse_jaxpr(jaxpr)\n+\n+        params_flat, params_treedef = jax.tree.flatten(params)\n+        _, inputs_treedef = jax.tree.flatten(inputs)\n+        _, outputs_treedef = jax.tree.flatten(abs_ret)\n+\n+        seen = collections.Counter()\n+        for var in jaxpr.jaxpr.invars[: len(params_flat)]:\n+            for i in range(var.aval.ndim):\n+                if var.aval.shape[i] >= min_shard_size:\n+                    seen.update([graph.get_root((var, i))])\n+\n+        model_axis_root = max(seen, key=seen.get) if seen else None\n+\n+        data_axes_roots = []\n+        for var in jaxpr.jaxpr.invars[len(params_flat) :]:\n+            for i in range(var.aval.ndim):\n+                root = graph.get_root((var, i))\n+                if root not in seen and root not in data_axes_roots:\n+                    data_axes_roots.append(root)\n+\n+        def assign_layouts(vars_flat, is_params=False):\n+            assignments = []\n+            for var in vars_flat:\n+                layout = [None] * var.aval.ndim\n+                for i in range(var.aval.ndim):\n+                    if var.aval.shape[i] < min_shard_size:\n+                        continue\n+                    root = graph.get_root((var, i))\n+                    if (\n+                        is_params\n+                        and model_axis_root\n+                        and root == model_axis_root\n+                    ):\n+                        layout[i] = model_axis_name\n+                    elif not is_params and root in data_axes_roots:\n+                        name = data_axis_name\n+                        if len(data_axes_roots) > 1:\n+                            name += str(data_axes_roots.index(root))\n+                        layout[i] = name\n+                assignments.append(layout)\n+            return assignments\n+\n+        params_assignments = params_treedef.unflatten(\n+            assign_layouts(\n+                jaxpr.jaxpr.invars[: len(params_flat)], is_params=True\n+            )\n+        )\n+        inputs_assignments = inputs_treedef.unflatten(\n+            assign_layouts(jaxpr.jaxpr.invars[len(params_flat) :])\n+        )\n+        output_assignments = outputs_treedef.unflatten(\n+            assign_layouts(jaxpr.jaxpr.outvars)\n+        )\n+\n+        return (params_assignments, *inputs_assignments), output_assignments\n+\n+    class _JaxGraph:\n+        \"\"\"A wrapper for a JAX computation graph (jaxpr) of a Keras model.\"\"\"\n+\n+        def __init__(\n+            self,\n+            jaxpr,\n+            trainable_variables,\n+            non_trainable_variables,\n+            in_treedefs,\n+        ):\n+            self.jaxpr = jaxpr\n+            self.trainable_variables = trainable_variables\n+            self.non_trainable_variables = non_trainable_variables\n+            self.in_treedefs = in_treedefs\n+\n+        @classmethod\n+        def from_model(cls, model, *args, **kwargs):\n+            \"\"\"Creates a _JaxGraph instance by tracing the model.\"\"\"\n+\n+            def stateless_fn(\n+                trainable_vars, non_trainable_vars, f_args, f_kwargs\n+            ):\n+                return model.stateless_call(\n+                    trainable_vars, non_trainable_vars, *f_args, **f_kwargs\n+                )\n+\n+            trainable_vars = model.trainable_variables\n+            non_trainable_vars = model.non_trainable_variables\n+\n+            _, t_vars_treedef = jax.tree.flatten(trainable_vars)\n+            _, nt_vars_treedef = jax.tree.flatten(non_trainable_vars)\n+            _, args_treedef = jax.tree.flatten(args)\n+            _, kwargs_treedef = jax.tree.flatten(kwargs)\n+            in_treedefs = (\n+                t_vars_treedef,\n+                nt_vars_treedef,\n+                args_treedef,\n+                kwargs_treedef,\n+            )\n+\n+            closed_jaxpr, _ = jax.make_jaxpr(stateless_fn, return_shape=True)(\n+                trainable_vars, non_trainable_vars, args, kwargs\n+            )\n+            return cls(\n+                closed_jaxpr, trainable_vars, non_trainable_vars, in_treedefs\n+            )\n+\n+    class _JaxShardingPlanner:\n+        \"\"\"\n+        Determines the optimal sharding layout for model variables using\n+        the embedded graph-parsing engine.\n+        \"\"\"\n+\n+        def plan(self, graph, device_mesh):\n+            t_vars = graph.trainable_variables\n+            nt_vars = graph.non_trainable_variables\n+\n+            all_in_avals = [var.aval for var in graph.jaxpr.jaxpr.invars]\n+            t_vars_leaves, _ = jax.tree.flatten(t_vars)\n+            nt_vars_leaves, _ = jax.tree.flatten(nt_vars)\n+\n+            pos = len(t_vars_leaves) + len(nt_vars_leaves)\n+\n+            args_treedef = graph.in_treedefs[2]\n+            kwargs_treedef = graph.in_treedefs[3]\n+\n+            num_args_leaves = args_treedef.num_leaves\n+            args_avals = all_in_avals[pos : pos + num_args_leaves]\n+            kwargs_avals = all_in_avals[pos + num_args_leaves :]\n+\n+            args_aval_tree = jax.tree.unflatten(args_treedef, args_avals)\n+            kwargs_aval_tree = jax.tree.unflatten(kwargs_treedef, kwargs_avals)\n+\n+            dummy_args = jax.tree.map(\n+                lambda x: np.zeros(x.shape, x.dtype), args_aval_tree\n+            )\n+            dummy_kwargs = jax.tree.map(\n+                lambda x: np.zeros(x.shape, x.dtype), kwargs_aval_tree\n+            )\n+\n+            def fn_to_trace(trainable_params, *fn_args, **fn_kwargs):\n+                \"\"\"A function that executes the original jaxpr computation.\"\"\"\n+                all_leaves = (\n+                    jax.tree.leaves(trainable_params)\n+                    + jax.tree.leaves(nt_vars)\n+                    + jax.tree.leaves(fn_args)\n+                    + jax.tree.leaves(fn_kwargs)\n+                )\n+                return jax.core.eval_jaxpr(\n+                    graph.jaxpr.jaxpr, graph.jaxpr.consts, *all_leaves\n+                )\n+\n+            (param_assignments, *_) = shard_model(\n+                fn_to_trace, t_vars, dummy_args, **dummy_kwargs\n+            )\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe call to `shard_model` here is incorrect as it passes `**dummy_kwargs`, but the `shard_model` function signature does not accept arbitrary keyword arguments. This will raise a `TypeError` at runtime if `dummy_kwargs` is not empty. \n\nFurthermore, the `jax.make_jaxpr` call inside `shard_model` is invoked only with positional arguments (`*inputs`), so any keyword arguments would be ignored even if the signature was corrected. This would lead to an incorrect sharding analysis for models that use keyword arguments in their `call` method.\n\nTo fix this, the `shard_model` function needs to be updated to correctly handle both `*args` and `**kwargs` throughout its implementation, including passing them to `jax.make_jaxpr` and correctly processing the input treedefs.",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "keras/src/backend/jax/distribution_lib.py",
    "pr_number": 21583,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2277143735,
    "comment_created_at": "2025-08-14T16:38:30Z"
  },
  {
    "code": "@@ -1638,6 +1638,14 @@ def isneginf(x):\n     return tf.math.equal(x, -tf.constant(float(\"inf\"), dtype=x.dtype))\n \n \n+def isposinf(x):\n+    x = convert_to_tensor(x)\n+    dtype_as_dtype = tf.as_dtype(x.dtype)\n+    if dtype_as_dtype.is_integer or not dtype_as_dtype.is_numeric:\n+        return tf.zeros(x.shape, tf.bool)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nusing x.shape with tf.zeros can cause an error when x has a dynamic shape (i.e., contains none in its shape tuple), as tf.zeros does not support none in the shape argument. using tf.zeros_like(x, dtype=tf.bool) is a safer alternative that correctly handles both static and dynamic shapes.",
    "line_number": 1645,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1638,6 +1638,14 @@ def isneginf(x):\n     return tf.math.equal(x, -tf.constant(float(\"inf\"), dtype=x.dtype))\n \n \n+def isposinf(x):\n+    x = convert_to_tensor(x)\n+    dtype_as_dtype = tf.as_dtype(x.dtype)\n+    if dtype_as_dtype.is_integer or not dtype_as_dtype.is_numeric:\n+        return tf.zeros(x.shape, tf.bool)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nUsing `x.shape` with `tf.zeros` can cause an error when `x` has a dynamic shape (i.e., contains `None` in its shape tuple), as `tf.zeros` does not support `None` in the shape argument. Using `tf.zeros_like(x, dtype=tf.bool)` is a safer alternative that correctly handles both static and dynamic shapes.\n\n```suggestion\n        return tf.zeros_like(x, dtype=tf.bool)\n```",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21582,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2276121577,
    "comment_created_at": "2025-08-14T09:40:59Z"
  },
  {
    "code": "@@ -843,9 +843,16 @@ def prepare_slice_index(val):\n     start = ov_opset.concat(start, axis=0).output(0)\n     stop = ov_opset.concat(stop, axis=0).output(0)\n     axes = ov_opset.constant(axes, Type.i32).output(0)\n-    return OpenVINOKerasTensor(\n-        ov_opset.slice(inputs, start, stop, step, axes).output(0)\n-    )\n+    result = ov_opset.slice(inputs, start, stop, step, axes).output(0)\n+\n+    # Apply reshape to ensure output matches expected shape\n+    # Convert None (dynamic) dimensions to -1 for OpenVINO compatibility\n+    if all(dim is None or (isinstance(dim, int) and dim >= 0) for dim in shape):\n+        reshape_pattern = [(-1 if dim is None else dim) for dim in shape]\n+        target_shape = ov_opset.constant(reshape_pattern, Type.i32).output(0)\n+        result = ov_opset.reshape(result, target_shape, False).output(0)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe condition to apply the reshape is too restrictive as it only handles cases where the shape argument is a tuple of static integers or none. this means the fix for preserving shape information doesn't apply when shape contains dynamic dimensions (i.e., tensors), which is a key use case. this makes the fix incomplete because it won't work for dynamically shaped slices.\n\na more general approach is needed to construct the target_shape for the reshape operation that handles both static and dynamic values in the shape tuple.\n\npython\n    target_shape_nodes = []\n    for dim in shape:\n        if dim is none:\n            # use -1 to represent a dynamic dimension for reshape.\n            node = ov_opset.constant([-1], type.i32).output(0)\n        else:\n            # convert dim to an openvino tensor if it's not already one.\n            node = convert_to_tensor(dim, dtype=\"int32\").output\n            # reshape's shape input must be 1d, so unsqueeze scalars.\n            if len(node.get_partial_shape()) == 0:\n                node = ov_opset.unsqueeze(\n                    node, ov_opset.constant(0, type.i32)\n                ).output(0)\n        target_shape_nodes.append(node)\n\n    # concatenate all parts to form the final shape tensor.\n    target_shape = ov_opset.concat(target_shape_nodes, axis=0).output(0)\n    result = ov_opset.reshape(result, target_shape, false).output(0)",
    "line_number": 853,
    "enriched": "File: keras/src/backend/openvino/core.py\nCode: @@ -843,9 +843,16 @@ def prepare_slice_index(val):\n     start = ov_opset.concat(start, axis=0).output(0)\n     stop = ov_opset.concat(stop, axis=0).output(0)\n     axes = ov_opset.constant(axes, Type.i32).output(0)\n-    return OpenVINOKerasTensor(\n-        ov_opset.slice(inputs, start, stop, step, axes).output(0)\n-    )\n+    result = ov_opset.slice(inputs, start, stop, step, axes).output(0)\n+\n+    # Apply reshape to ensure output matches expected shape\n+    # Convert None (dynamic) dimensions to -1 for OpenVINO compatibility\n+    if all(dim is None or (isinstance(dim, int) and dim >= 0) for dim in shape):\n+        reshape_pattern = [(-1 if dim is None else dim) for dim in shape]\n+        target_shape = ov_opset.constant(reshape_pattern, Type.i32).output(0)\n+        result = ov_opset.reshape(result, target_shape, False).output(0)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe condition to apply the reshape is too restrictive as it only handles cases where the `shape` argument is a tuple of static integers or `None`. This means the fix for preserving shape information doesn't apply when `shape` contains dynamic dimensions (i.e., tensors), which is a key use case. This makes the fix incomplete because it won't work for dynamically shaped slices.\n\nA more general approach is needed to construct the `target_shape` for the `reshape` operation that handles both static and dynamic values in the `shape` tuple.\n\n```python\n    target_shape_nodes = []\n    for dim in shape:\n        if dim is None:\n            # Use -1 to represent a dynamic dimension for reshape.\n            node = ov_opset.constant([-1], Type.i32).output(0)\n        else:\n            # Convert dim to an OpenVINO tensor if it's not already one.\n            node = convert_to_tensor(dim, dtype=\"int32\").output\n            # Reshape's shape input must be 1D, so unsqueeze scalars.\n            if len(node.get_partial_shape()) == 0:\n                node = ov_opset.unsqueeze(\n                    node, ov_opset.constant(0, Type.i32)\n                ).output(0)\n        target_shape_nodes.append(node)\n\n    # Concatenate all parts to form the final shape tensor.\n    target_shape = ov_opset.concat(target_shape_nodes, axis=0).output(0)\n    result = ov_opset.reshape(result, target_shape, False).output(0)\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/core.py",
    "pr_number": 21580,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2275163397,
    "comment_created_at": "2025-08-14T02:18:12Z"
  },
  {
    "code": "@@ -410,32 +438,11 @@ def compute_weight_mat(\n \n \n def _resize(image, shape, method, antialias):\n-    def _fill_triangle_kernel(x):\n-        return np.maximum(0, 1 - np.abs(x))\n-\n-    def _fill_keys_cubic_kernel(x):\n-        out = ((1.5 * x - 2.5) * x) * x + 1.0\n-        out = np.where(x >= 1.0, ((-0.5 * x + 2.5) * x - 4.0) * x + 2.0, out)\n-        return np.where(x >= 2.0, 0.0, out)\n-\n-    def _fill_lanczos_kernel(radius, x):\n-        y = radius * np.sin(np.pi * x) * np.sin(np.pi * x / radius)\n-        out = np.where(\n-            x > 1e-3, np.divide(y, np.where(x != 0, np.pi**2 * x**2, 1)), 1\n-        )\n-        return np.where(x > radius, 0.0, out)\n-\n     if method == \"nearest\":\n         return _resize_nearest(image, shape)\n-    elif method == \"bilinear\":\n-        kernel = _fill_triangle_kernel\n-    elif method == \"lanczos3\":\n-        kernel = lambda x: _fill_lanczos_kernel(3.0, x)\n-    elif method == \"lanczos5\":\n-        kernel = lambda x: _fill_lanczos_kernel(5.0, x)\n-    elif method == \"bicubic\":\n-        kernel = _fill_keys_cubic_kernel\n     else:\n+        kernel = _kernels.get(method, None)\n+    if kernel is None:\n         raise ValueError(\"Unknown resize method\")",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe refactoring of _resize introduces a bug. the resize operation supports interpolation methods like \"bilinear\" and \"bicubic\". after this change, _resize is called with these method names, but the _kernels dictionary uses \"linear\" and \"cubic\" as keys. this will cause _kernels.get(method, none) to return none, raising a valueerror.\n\nto fix this, you should map the interpolation method names similarly to how it's done in the new scale_and_translate function.",
    "line_number": 446,
    "enriched": "File: keras/src/backend/numpy/image.py\nCode: @@ -410,32 +438,11 @@ def compute_weight_mat(\n \n \n def _resize(image, shape, method, antialias):\n-    def _fill_triangle_kernel(x):\n-        return np.maximum(0, 1 - np.abs(x))\n-\n-    def _fill_keys_cubic_kernel(x):\n-        out = ((1.5 * x - 2.5) * x) * x + 1.0\n-        out = np.where(x >= 1.0, ((-0.5 * x + 2.5) * x - 4.0) * x + 2.0, out)\n-        return np.where(x >= 2.0, 0.0, out)\n-\n-    def _fill_lanczos_kernel(radius, x):\n-        y = radius * np.sin(np.pi * x) * np.sin(np.pi * x / radius)\n-        out = np.where(\n-            x > 1e-3, np.divide(y, np.where(x != 0, np.pi**2 * x**2, 1)), 1\n-        )\n-        return np.where(x > radius, 0.0, out)\n-\n     if method == \"nearest\":\n         return _resize_nearest(image, shape)\n-    elif method == \"bilinear\":\n-        kernel = _fill_triangle_kernel\n-    elif method == \"lanczos3\":\n-        kernel = lambda x: _fill_lanczos_kernel(3.0, x)\n-    elif method == \"lanczos5\":\n-        kernel = lambda x: _fill_lanczos_kernel(5.0, x)\n-    elif method == \"bicubic\":\n-        kernel = _fill_keys_cubic_kernel\n     else:\n+        kernel = _kernels.get(method, None)\n+    if kernel is None:\n         raise ValueError(\"Unknown resize method\")\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe refactoring of `_resize` introduces a bug. The `resize` operation supports `interpolation` methods like `\"bilinear\"` and `\"bicubic\"`. After this change, `_resize` is called with these method names, but the `_kernels` dictionary uses `\"linear\"` and `\"cubic\"` as keys. This will cause `_kernels.get(method, None)` to return `None`, raising a `ValueError`.\n\nTo fix this, you should map the interpolation method names similarly to how it's done in the new `scale_and_translate` function.\n\n```suggestion\n    else:\n        if method == \"bilinear\":\n            method = \"linear\"\n        elif method == \"bicubic\":\n            method = \"cubic\"\n        kernel = _kernels.get(method, None)\n    if kernel is None:\n        raise ValueError(f\"Unknown resize method: {method}\")\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/numpy/image.py",
    "pr_number": 21577,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2272747210,
    "comment_created_at": "2025-08-13T09:45:06Z"
  },
  {
    "code": "@@ -99,13 +99,11 @@ exclude_lines = [\n ]\n omit = [\n     \"*/*_test.py\",\n-    \"keras/src/legacy/*\",\n ]",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nto improve maintainability and reduce redundancy, you can remove this omit configuration. according to the coverage.py documentation, if an option like omit is specified under [tool.coverage.run] but not under [tool.coverage.report], the [run] configuration will be used for both execution and reporting. since the omit list here is identical to the one in [tool.coverage.run], this block is redundant and can be removed.",
    "line_number": 103,
    "enriched": "File: pyproject.toml\nCode: @@ -99,13 +99,11 @@ exclude_lines = [\n ]\n omit = [\n     \"*/*_test.py\",\n-    \"keras/src/legacy/*\",\n ]\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nTo improve maintainability and reduce redundancy, you can remove this `omit` configuration. According to the `coverage.py` documentation, if an option like `omit` is specified under `[tool.coverage.run]` but not under `[tool.coverage.report]`, the `[run]` configuration will be used for both execution and reporting. Since the `omit` list here is identical to the one in `[tool.coverage.run]`, this block is redundant and can be removed.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "pyproject.toml",
    "pr_number": 21576,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2271853894,
    "comment_created_at": "2025-08-13T01:15:12Z"
  },
  {
    "code": "@@ -123,7 +123,11 @@ def __new__(cls, *args, **kwargs):\n         if backend.backend() == \"jax\" and is_nnx_enabled():\n             from flax import nnx\n \n-            vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n+            try:\n+                vars(instance)[\"_pytree__state\"] = nnx.pytreelib.PytreeState()\n+            except AttributeError:\n+                vars(instance)[\"_object__state\"] = nnx.object.ObjectState()",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe use of a broad try...except attributeerror block can be brittle. it might catch an attributeerror from an unexpected source within the pytreestate() call, potentially masking other issues. a more explicit check for the existence of nnx.pytreelib.pytreestate using hasattr would be more robust and clearer about the intent, which is to handle different versions of the flax library.",
    "line_number": 129,
    "enriched": "File: keras/src/ops/operation.py\nCode: @@ -123,7 +123,11 @@ def __new__(cls, *args, **kwargs):\n         if backend.backend() == \"jax\" and is_nnx_enabled():\n             from flax import nnx\n \n-            vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n+            try:\n+                vars(instance)[\"_pytree__state\"] = nnx.pytreelib.PytreeState()\n+            except AttributeError:\n+                vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe use of a broad `try...except AttributeError` block can be brittle. It might catch an `AttributeError` from an unexpected source within the `PytreeState()` call, potentially masking other issues. A more explicit check for the existence of `nnx.pytreelib.PytreeState` using `hasattr` would be more robust and clearer about the intent, which is to handle different versions of the `flax` library.\n\n```suggestion\n            if hasattr(nnx, \"pytreelib\") and hasattr(nnx.pytreelib, \"PytreeState\"):\n                vars(instance)[\"_pytree__state\"] = nnx.pytreelib.PytreeState()\n            else:\n                vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/ops/operation.py",
    "pr_number": 21570,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2267694130,
    "comment_created_at": "2025-08-11T18:50:47Z"
  },
  {
    "code": "@@ -53,19 +65,23 @@ def compute_output_spec(self, inputs):\n             shape=output_shape, dtype=inputs.dtype, sparse=inputs.sparse\n         )\n \n-    def build(self, input_shape):\n-        sample_output_shape = operation_utils.compute_reshape_output_shape(\n-            input_shape[1:], self.target_shape, \"target_shape\"\n-        )\n-        self._resolved_target_shape = tuple(\n-            -1 if d is None else d for d in sample_output_shape\n-        )\n-\n     def call(self, inputs):\n+        target_shape = self.target_shape\n+        if self.need_explicit_shape_for_batch_size_None and (inputs.shape[0] is None):\n+            input_nonbatch_shape = tuple(inputs.shape[1:])\n+            if input_nonbatch_shape.count(None) == 0:\n+                # If the input shape is fully defined, we can compute the desired target_shape\n+                if True:\n+                    inp_nonbatch_size = math.prod(inputs.shape[1:])\n+                else:\n+                    inp_nonbatch_size = ops.prod(ops.shape(inputs)[1:])",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis if true: construct with an else branch appears to be leftover debugging code. the else branch is unreachable, creating dead code. given the check on line 72, input_nonbatch_shape.count(none) == 0, the non-batch dimensions are guaranteed to be fully known. therefore, the conditional is unnecessary and can be simplified for better readability and maintainability.\n\npython\n                inp_nonbatch_size = math.prod(inputs.shape[1:])",
    "line_number": 77,
    "enriched": "File: keras/src/layers/reshaping/reshape.py\nCode: @@ -53,19 +65,23 @@ def compute_output_spec(self, inputs):\n             shape=output_shape, dtype=inputs.dtype, sparse=inputs.sparse\n         )\n \n-    def build(self, input_shape):\n-        sample_output_shape = operation_utils.compute_reshape_output_shape(\n-            input_shape[1:], self.target_shape, \"target_shape\"\n-        )\n-        self._resolved_target_shape = tuple(\n-            -1 if d is None else d for d in sample_output_shape\n-        )\n-\n     def call(self, inputs):\n+        target_shape = self.target_shape\n+        if self.need_explicit_shape_for_batch_size_None and (inputs.shape[0] is None):\n+            input_nonbatch_shape = tuple(inputs.shape[1:])\n+            if input_nonbatch_shape.count(None) == 0:\n+                # If the input shape is fully defined, we can compute the desired target_shape\n+                if True:\n+                    inp_nonbatch_size = math.prod(inputs.shape[1:])\n+                else:\n+                    inp_nonbatch_size = ops.prod(ops.shape(inputs)[1:])\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis `if True:` construct with an `else` branch appears to be leftover debugging code. The `else` branch is unreachable, creating dead code. Given the check on line 72, `input_nonbatch_shape.count(None) == 0`, the non-batch dimensions are guaranteed to be fully known. Therefore, the conditional is unnecessary and can be simplified for better readability and maintainability.\n\n```python\n                inp_nonbatch_size = math.prod(inputs.shape[1:])\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/layers/reshaping/reshape.py",
    "pr_number": 21568,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2264846641,
    "comment_created_at": "2025-08-09T16:29:52Z"
  },
  {
    "code": "@@ -1303,7 +1303,43 @@ def pad(x, pad_width, mode=\"constant\", constant_values=None):\n \n \n def prod(x, axis=None, keepdims=False, dtype=None):\n-    raise NotImplementedError(\"`prod` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    \n+    # Convert to appropriate type for reduction\n+    if x_type == Type.boolean:\n+        # Convert boolean to int32 for reduction\n+        x = ov_opset.convert(x, Type.i32).output(0)\n+    \n+    if axis is None:\n+        flatten_shape = ov_opset.constant([-1], Type.i32).output(0)\n+        x = ov_opset.reshape(x, flatten_shape, False).output(0)\n+        axis = 0\n+    \n+    if isinstance(axis, tuple):\n+        axis = list(axis)\n+    axis = ov_opset.constant(axis, Type.i32).output(0)\n+    \n+    # Compute the product\n+    result = ov_opset.reduce_prod(x, axis, keepdims).output(0)\n+    \n+    # Convert to the specified dtype if provided\n+    if dtype is not None:\n+        ov_dtype = _convert_to_ov_type(dtype)\n+        result = ov_opset.convert(result, ov_dtype).output(0)\n+    else:\n+        # Handle dtype promotion rules\n+        if x_type == Type.boolean:\n+            result = ov_opset.convert(result, Type.i32).output(0)\n+        elif x_type == Type.i64:\n+            result = ov_opset.convert(result, Type.i64).output(0)\n+        elif x_type in (Type.u8, Type.u16, Type.u32):\n+            result = ov_opset.convert(result, Type.u32).output(0)\n+        elif x_type.is_integral():\n+            result = ov_opset.convert(result, Type.i32).output(0)\n+    \n+    return OpenVINOKerasTensor(result)",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe current implementation has a couple of issues:\n1.  _convert_to_ov_type is not defined, which will cause a nameerror at runtime.\n2.  the dtype promotion logic for uint64 is incorrect, as it gets converted to int32, which can lead to data loss.\n3.  there's a redundant type conversion for boolean inputs, which is inefficient.\n\ni suggest refactoring the function to handle dtype promotion before the reduction. this makes the logic cleaner, more efficient, and corrects the bugs.\n\npython\n    x = get_ov_output(x)\n\n    # if a specific dtype is requested, cast the input to that dtype.\n    if dtype is not none:\n        ov_dtype = openvino_dtypes[standardize_dtype(dtype)]\n        x = ov_opset.convert(x, ov_dtype).output(0)\n    # otherwise, apply dtype promotion rules before reduction.\n    else:\n        x_type = x.get_element_type()\n        if x_type == type.boolean:\n            x = ov_opset.convert(x, type.i32).output(0)\n        elif x_type in (type.i8, type.i16):\n            x = ov_opset.convert(x, type.i32).output(0)\n        elif x_type in (type.u8, type.u16):\n            x = ov_opset.convert(x, type.u32).output(0)\n\n    if axis is none:\n        flatten_shape = ov_opset.constant([-1], type.i32).output(0)\n        x = ov_opset.reshape(x, flatten_shape, false).output(0)\n        axis = 0\n\n    if isinstance(axis, tuple):\n        axis = list(axis)\n    axis = ov_opset.constant(axis, type.i32).output(0)\n\n    # compute the product\n    result = ov_opset.reduce_prod(x, axis, keepdims).output(0)\n\n    return openvinokerastensor(result)",
    "line_number": 1341,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -1303,7 +1303,43 @@ def pad(x, pad_width, mode=\"constant\", constant_values=None):\n \n \n def prod(x, axis=None, keepdims=False, dtype=None):\n-    raise NotImplementedError(\"`prod` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    \n+    # Convert to appropriate type for reduction\n+    if x_type == Type.boolean:\n+        # Convert boolean to int32 for reduction\n+        x = ov_opset.convert(x, Type.i32).output(0)\n+    \n+    if axis is None:\n+        flatten_shape = ov_opset.constant([-1], Type.i32).output(0)\n+        x = ov_opset.reshape(x, flatten_shape, False).output(0)\n+        axis = 0\n+    \n+    if isinstance(axis, tuple):\n+        axis = list(axis)\n+    axis = ov_opset.constant(axis, Type.i32).output(0)\n+    \n+    # Compute the product\n+    result = ov_opset.reduce_prod(x, axis, keepdims).output(0)\n+    \n+    # Convert to the specified dtype if provided\n+    if dtype is not None:\n+        ov_dtype = _convert_to_ov_type(dtype)\n+        result = ov_opset.convert(result, ov_dtype).output(0)\n+    else:\n+        # Handle dtype promotion rules\n+        if x_type == Type.boolean:\n+            result = ov_opset.convert(result, Type.i32).output(0)\n+        elif x_type == Type.i64:\n+            result = ov_opset.convert(result, Type.i64).output(0)\n+        elif x_type in (Type.u8, Type.u16, Type.u32):\n+            result = ov_opset.convert(result, Type.u32).output(0)\n+        elif x_type.is_integral():\n+            result = ov_opset.convert(result, Type.i32).output(0)\n+    \n+    return OpenVINOKerasTensor(result)\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe current implementation has a couple of issues:\n1.  `_convert_to_ov_type` is not defined, which will cause a `NameError` at runtime.\n2.  The dtype promotion logic for `uint64` is incorrect, as it gets converted to `int32`, which can lead to data loss.\n3.  There's a redundant type conversion for boolean inputs, which is inefficient.\n\nI suggest refactoring the function to handle dtype promotion before the reduction. This makes the logic cleaner, more efficient, and corrects the bugs.\n\n```python\n    x = get_ov_output(x)\n\n    # If a specific dtype is requested, cast the input to that dtype.\n    if dtype is not None:\n        ov_dtype = OPENVINO_DTYPES[standardize_dtype(dtype)]\n        x = ov_opset.convert(x, ov_dtype).output(0)\n    # Otherwise, apply dtype promotion rules before reduction.\n    else:\n        x_type = x.get_element_type()\n        if x_type == Type.boolean:\n            x = ov_opset.convert(x, Type.i32).output(0)\n        elif x_type in (Type.i8, Type.i16):\n            x = ov_opset.convert(x, Type.i32).output(0)\n        elif x_type in (Type.u8, Type.u16):\n            x = ov_opset.convert(x, Type.u32).output(0)\n\n    if axis is None:\n        flatten_shape = ov_opset.constant([-1], Type.i32).output(0)\n        x = ov_opset.reshape(x, flatten_shape, False).output(0)\n        axis = 0\n\n    if isinstance(axis, tuple):\n        axis = list(axis)\n    axis = ov_opset.constant(axis, Type.i32).output(0)\n\n    # Compute the product\n    result = ov_opset.reduce_prod(x, axis, keepdims).output(0)\n\n    return OpenVINOKerasTensor(result)\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21567,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2264557873,
    "comment_created_at": "2025-08-09T06:42:29Z"
  },
  {
    "code": "@@ -13,9 +13,9 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        python-version: ['3.10']\n+        python-version: [\"3.10\"]\n         backend: [tensorflow, jax, torch, numpy]\n-    name: Run tests\n+    name: Run tests (Python 3.10)",
    "comment": "does it work if you put ${{ matrix.python-version }} here?\n\nthe goal is to minimize the number of things to change when we upgrade python.",
    "line_number": 18,
    "enriched": "File: .github/workflows/nightly.yml\nCode: @@ -13,9 +13,9 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        python-version: ['3.10']\n+        python-version: [\"3.10\"]\n         backend: [tensorflow, jax, torch, numpy]\n-    name: Run tests\n+    name: Run tests (Python 3.10)\nComment: Does it work if you put `${{ matrix.python-version }}` here?\n\nThe goal is to minimize the number of things to change when we upgrade Python.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".github/workflows/nightly.yml",
    "pr_number": 21566,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2407545918,
    "comment_created_at": "2025-10-06T17:16:51Z"
  },
  {
    "code": "@@ -123,7 +123,7 @@ def __new__(cls, *args, **kwargs):\n         if backend.backend() == \"jax\" and is_nnx_enabled():\n             from flax import nnx\n \n-            vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n+            vars(instance)[\"_pytree__state\"] = nnx.pytreelib.PytreeState()",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe nnx.pytreelib module appears to be an internal api of flax.nnx. relying on internal apis can make the code brittle and lead to breakages when flax is updated. to improve long-term maintainability, it's preferable to use a public api for making an object a pytree, if one exists. for example, flax.nnx might provide a registration function or a base class for this purpose.",
    "line_number": 126,
    "enriched": "File: keras/src/ops/operation.py\nCode: @@ -123,7 +123,7 @@ def __new__(cls, *args, **kwargs):\n         if backend.backend() == \"jax\" and is_nnx_enabled():\n             from flax import nnx\n \n-            vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n+            vars(instance)[\"_pytree__state\"] = nnx.pytreelib.PytreeState()\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe `nnx.pytreelib` module appears to be an internal API of `flax.nnx`. Relying on internal APIs can make the code brittle and lead to breakages when `flax` is updated. To improve long-term maintainability, it's preferable to use a public API for making an object a Pytree, if one exists. For example, `flax.nnx` might provide a registration function or a base class for this purpose.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/ops/operation.py",
    "pr_number": 21565,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2263792509,
    "comment_created_at": "2025-08-08T18:57:31Z"
  },
  {
    "code": "@@ -281,6 +281,17 @@\n cb.set_ticks([-500, 0, 1000, 2000, 3000, 4000])\n plt.show()\n \n+# %%\n+#\n+# .. note::",
    "comment": "i think this note is going to be pretty mysterious to folks who don't know what the previous behaviour was.  if i were doing this, i'd just add another example to this doc that shows manipulating the colorbar scale, either with this example or another one.",
    "line_number": 286,
    "enriched": "File: galleries/users_explain/colors/colormapnorms.py\nCode: @@ -281,6 +281,17 @@\n cb.set_ticks([-500, 0, 1000, 2000, 3000, 4000])\n plt.show()\n \n+# %%\n+#\n+# .. note::\nComment: I think this note is going to be pretty mysterious to folks who don't know what the previous behaviour was.  If I were doing this, I'd just add another example to this doc that shows manipulating the colorbar scale, either with this example or another one.  ",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "galleries/users_explain/colors/colormapnorms.py",
    "pr_number": 30639,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2407999639,
    "comment_created_at": "2025-10-06T18:56:41Z"
  },
  {
    "code": "@@ -7772,13 +7772,10 @@ def hist2d(self, x, y, bins=10, range=None, density=False, weights=None,\n \n         Notes\n         -----\n-        - Currently ``hist2d`` calculates its own axis limits, and any limits",
    "comment": "i think we should leave a .. versionchanged:: notice here.",
    "line_number": 7775,
    "enriched": "File: lib/matplotlib/axes/_axes.py\nCode: @@ -7772,13 +7772,10 @@ def hist2d(self, x, y, bins=10, range=None, density=False, weights=None,\n \n         Notes\n         -----\n-        - Currently ``hist2d`` calculates its own axis limits, and any limits\nComment: I think we should leave a `.. versionchanged::` notice here.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "lib/matplotlib/axes/_axes.py",
    "pr_number": 30634,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2404371042,
    "comment_created_at": "2025-10-05T08:59:38Z"
  },
  {
    "code": "@@ -1981,7 +1981,7 @@ def test_mult_norm_call_types():\n     mn.vmin = (-2, -2)\n     mn.vmax = (2, 2)\n \n-    vals = np.arange(6).reshape((3,2))\n+    vals = np.arange(6, dtype='int64').reshape((3, 2))",
    "comment": "i assume what fails is the asserion below\r\n\r\n\r\nassert no_norm_out[1].dtype == np.dtype('int64')\r\n\r\n\r\nif so, wouldn't it be slightly better to leave vals as is and change the assert to\r\n\r\nassert no_norm_out[1].dtype == vals.dtype\r\n\r\n?",
    "line_number": 1984,
    "enriched": "File: lib/matplotlib/tests/test_colors.py\nCode: @@ -1981,7 +1981,7 @@ def test_mult_norm_call_types():\n     mn.vmin = (-2, -2)\n     mn.vmax = (2, 2)\n \n-    vals = np.arange(6).reshape((3,2))\n+    vals = np.arange(6, dtype='int64').reshape((3, 2))\nComment: I assume what fails is the asserion below\r\n\r\n```\r\nassert no_norm_out[1].dtype == np.dtype('int64')\r\n```\r\n\r\nIf so, wouldn't it be slightly better to leave `vals` as is and change the assert to\r\n```\r\nassert no_norm_out[1].dtype == vals.dtype\r\n```\r\n?",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "lib/matplotlib/tests/test_colors.py",
    "pr_number": 30629,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2402369635,
    "comment_created_at": "2025-10-03T15:20:04Z"
  },
  {
    "code": "@@ -157,33 +166,23 @@ def track(self, font: FT2Font, s: str) -> list[tuple[int, CharacterCodeType]]:\n             whole). If *subset_size* is not specified, then the subset will always be 0\n             and the character codes will be returned from the string unchanged.\n         \"\"\"\n-        font_glyphs = []\n-        char_to_font = font._get_fontmap(s)\n-        for _c, _f in char_to_font.items():\n-            charcode = ord(_c)\n-            glyph_index = _f.get_char_index(charcode)\n-            if self.subset_size != 0:\n-                subset = charcode // self.subset_size\n-                subset_charcode = charcode % self.subset_size\n-            else:\n-                subset = 0\n-                subset_charcode = charcode\n-            self.used.setdefault((_f.fname, subset), {})[subset_charcode] = glyph_index\n-            font_glyphs.append((subset, subset_charcode))\n-        return font_glyphs\n-\n-    def track_glyph(\n-            self, font: FT2Font, charcode: CharacterCodeType,\n-            glyph: GlyphIndexType) -> tuple[int, CharacterCodeType]:\n+        return [\n+            self.track_glyph(_f, ord(_c), _f.get_char_index(ord(_c)))",
    "comment": "you don't need the leading underscores here?",
    "line_number": 170,
    "enriched": "File: lib/matplotlib/backends/_backend_pdf_ps.py\nCode: @@ -157,33 +166,23 @@ def track(self, font: FT2Font, s: str) -> list[tuple[int, CharacterCodeType]]:\n             whole). If *subset_size* is not specified, then the subset will always be 0\n             and the character codes will be returned from the string unchanged.\n         \"\"\"\n-        font_glyphs = []\n-        char_to_font = font._get_fontmap(s)\n-        for _c, _f in char_to_font.items():\n-            charcode = ord(_c)\n-            glyph_index = _f.get_char_index(charcode)\n-            if self.subset_size != 0:\n-                subset = charcode // self.subset_size\n-                subset_charcode = charcode % self.subset_size\n-            else:\n-                subset = 0\n-                subset_charcode = charcode\n-            self.used.setdefault((_f.fname, subset), {})[subset_charcode] = glyph_index\n-            font_glyphs.append((subset, subset_charcode))\n-        return font_glyphs\n-\n-    def track_glyph(\n-            self, font: FT2Font, charcode: CharacterCodeType,\n-            glyph: GlyphIndexType) -> tuple[int, CharacterCodeType]:\n+        return [\n+            self.track_glyph(_f, ord(_c), _f.get_char_index(ord(_c)))\nComment: You don't need the leading underscores here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/matplotlib/backends/_backend_pdf_ps.py",
    "pr_number": 30608,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2387487798,
    "comment_created_at": "2025-09-29T10:40:55Z"
  },
  {
    "code": "@@ -190,7 +190,7 @@ def _get_col_row(self, n):\n         return col, row\n \n     n_axes = property(lambda self: len(self.axes_all))\n-    ngrids = _api.deprecated(property(lambda self: len(self.axes_all)))\n+    ngrids = _api.deprecated('3.11')(property(lambda self: len(self.axes_all)))",
    "comment": "we should check the input parameter to prevent something like this in the future.\r\n\r\ninterestingly, why did type-checking not catch this?",
    "line_number": 193,
    "enriched": "File: lib/mpl_toolkits/axes_grid1/axes_grid.py\nCode: @@ -190,7 +190,7 @@ def _get_col_row(self, n):\n         return col, row\n \n     n_axes = property(lambda self: len(self.axes_all))\n-    ngrids = _api.deprecated(property(lambda self: len(self.axes_all)))\n+    ngrids = _api.deprecated('3.11')(property(lambda self: len(self.axes_all)))\nComment: We should check the input parameter to prevent something like this in the future.\r\n\r\nInterestingly, why did type-checking not catch this?",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "lib/mpl_toolkits/axes_grid1/axes_grid.py",
    "pr_number": 30603,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2381120369,
    "comment_created_at": "2025-09-26T07:13:23Z"
  },
  {
    "code": "@@ -61,6 +61,7 @@ def convert_limits(lim, converter):\n             )\n             for name, axis in axis_map.items()\n         ]),\n+        ('Legend visible', axes.legend_ is not None),",
    "comment": "this looks unrelated?",
    "line_number": 64,
    "enriched": "File: lib/matplotlib/backends/qt_editor/figureoptions.py\nCode: @@ -61,6 +61,7 @@ def convert_limits(lim, converter):\n             )\n             for name, axis in axis_map.items()\n         ]),\n+        ('Legend visible', axes.legend_ is not None),\nComment: This looks unrelated?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/matplotlib/backends/qt_editor/figureoptions.py",
    "pr_number": 30590,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2376942175,
    "comment_created_at": "2025-09-24T20:08:59Z"
  },
  {
    "code": "@@ -285,18 +285,11 @@ def test_contourf3d_extend(fig_test, fig_ref, extend, levels):\n     # Z is in the range [0, 8]\n     Z = X**2 + Y**2\n \n-    # Manually set the over/under colors to be the end of the colormap\n-    cmap = mpl.colormaps['viridis'].copy()\n-    cmap.set_under(cmap(0))\n-    cmap.set_over(cmap(255))\n-    # Set vmin/max to be the min/max values plotted on the reference image\n-    kwargs = {'vmin': 1, 'vmax': 7, 'cmap': cmap}",
    "comment": "this cmap acrobatics is actually unneeded. if the colormap does not have extremes, as in the case of viridis, the values used to render over/under are actually the ends of the colormap. so setting them explicitly does not have an impact on the rendered image.",
    "line_number": 293,
    "enriched": "File: lib/mpl_toolkits/mplot3d/tests/test_axes3d.py\nCode: @@ -285,18 +285,11 @@ def test_contourf3d_extend(fig_test, fig_ref, extend, levels):\n     # Z is in the range [0, 8]\n     Z = X**2 + Y**2\n \n-    # Manually set the over/under colors to be the end of the colormap\n-    cmap = mpl.colormaps['viridis'].copy()\n-    cmap.set_under(cmap(0))\n-    cmap.set_over(cmap(255))\n-    # Set vmin/max to be the min/max values plotted on the reference image\n-    kwargs = {'vmin': 1, 'vmax': 7, 'cmap': cmap}\nComment: This cmap acrobatics is actually unneeded. If the colormap does not have extremes, as in the case of viridis, the values used to render over/under are actually the ends of the colormap. So setting them explicitly does not have an impact on the rendered image.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "lib/mpl_toolkits/mplot3d/tests/test_axes3d.py",
    "pr_number": 30582,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2357704841,
    "comment_created_at": "2025-09-18T06:47:28Z"
  },
  {
    "code": "@@ -148,11 +174,6 @@ def test_find_invalid(tmp_path):\n     with pytest.raises(FileNotFoundError):\n         get_font(bytes(tmp_path / 'non-existent-font-name.ttf'))\n \n-    # Not really public, but get_font doesn't expose non-filename constructor.",
    "comment": "note to future reviewers, this was removed as redundent with other test",
    "line_number": 151,
    "enriched": "File: lib/matplotlib/tests/test_font_manager.py\nCode: @@ -148,11 +174,6 @@ def test_find_invalid(tmp_path):\n     with pytest.raises(FileNotFoundError):\n         get_font(bytes(tmp_path / 'non-existent-font-name.ttf'))\n \n-    # Not really public, but get_font doesn't expose non-filename constructor.\nComment: note to future reviewers, this was removed as redundent with other test",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "lib/matplotlib/tests/test_font_manager.py",
    "pr_number": 30573,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2379935970,
    "comment_created_at": "2025-09-25T18:06:37Z"
  },
  {
    "code": "@@ -77,14 +77,13 @@ jobs:\n             pygobject-ver: '<3.52.0'\n           - os: ubuntu-24.04\n             python-version: '3.12'\n-          - os: macos-13  # This runner is on Intel chips.\n-            # merge numpy and pandas install in nighties test when this runner is dropped\n+          - os: macos-14  # This runner is on M1 (arm64) chips.\n             python-version: '3.11'\n           - os: macos-14  # This runner is on M1 (arm64) chips.\n             python-version: '3.12'\n             # https://github.com/matplotlib/matplotlib/issues/29732\n             pygobject-ver: '<3.52.0'",
    "comment": "possibly we need this limitation also on the py3.11 case to fix the test error.",
    "line_number": 85,
    "enriched": "File: .github/workflows/tests.yml\nCode: @@ -77,14 +77,13 @@ jobs:\n             pygobject-ver: '<3.52.0'\n           - os: ubuntu-24.04\n             python-version: '3.12'\n-          - os: macos-13  # This runner is on Intel chips.\n-            # merge numpy and pandas install in nighties test when this runner is dropped\n+          - os: macos-14  # This runner is on M1 (arm64) chips.\n             python-version: '3.11'\n           - os: macos-14  # This runner is on M1 (arm64) chips.\n             python-version: '3.12'\n             # https://github.com/matplotlib/matplotlib/issues/29732\n             pygobject-ver: '<3.52.0'\nComment: Possibly we need this limitation also on the py3.11 case to fix the test error.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": ".github/workflows/tests.yml",
    "pr_number": 30571,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2353809834,
    "comment_created_at": "2025-09-16T22:43:36Z"
  },
  {
    "code": "@@ -962,9 +960,9 @@ def writeFonts(self):\n             else:\n                 # a normal TrueType font\n                 _log.debug('Writing TrueType font.')\n-                glyphs = self._character_tracker.used.get(filename)\n-                if glyphs:\n-                    fonts[Fx] = self.embedTTF(filename, glyphs)\n+                charmap = self._character_tracker.used.get((filename, 0))",
    "comment": "do we need to loop and add all the partitions?",
    "line_number": 963,
    "enriched": "File: lib/matplotlib/backends/backend_pdf.py\nCode: @@ -962,9 +960,9 @@ def writeFonts(self):\n             else:\n                 # a normal TrueType font\n                 _log.debug('Writing TrueType font.')\n-                glyphs = self._character_tracker.used.get(filename)\n-                if glyphs:\n-                    fonts[Fx] = self.embedTTF(filename, glyphs)\n+                charmap = self._character_tracker.used.get((filename, 0))\nComment: do we need to loop and add all the partitions?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/matplotlib/backends/backend_pdf.py",
    "pr_number": 30566,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2353607870,
    "comment_created_at": "2025-09-16T20:41:05Z"
  },
  {
    "code": "@@ -2592,7 +2592,7 @@ def scroll_handler(event, canvas=None, toolbar=None):\n         # is required for interactive navigation.\n         return\n \n-    if event.key == \"control\":  # zoom towards the mouse position\n+    if event.key in {\"control\", \"x\", \"X\", \"y\", \"Y\"}:  # zoom towards the mouse position",
    "comment": "we should only have the lower-case versions here. \"x\" would be generated by \"shift+x\", which i'd consider a separate modifier that we don't want to bind to zoom.",
    "line_number": 2595,
    "enriched": "File: lib/matplotlib/backend_bases.py\nCode: @@ -2592,7 +2592,7 @@ def scroll_handler(event, canvas=None, toolbar=None):\n         # is required for interactive navigation.\n         return\n \n-    if event.key == \"control\":  # zoom towards the mouse position\n+    if event.key in {\"control\", \"x\", \"X\", \"y\", \"Y\"}:  # zoom towards the mouse position\nComment: ```suggestion\r\n    if event.key in {\"control\", \"x\", \"y\"}:  # zoom towards the mouse position\r\n```\r\nWe should only have the lower-case versions here. \"X\" would be generated by \"Shift+x\", which I'd consider a separate modifier that we don't want to bind to zoom.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "lib/matplotlib/backend_bases.py",
    "pr_number": 30543,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2337794299,
    "comment_created_at": "2025-09-10T20:14:05Z"
  },
  {
    "code": "@@ -127,7 +128,13 @@ def glyph_name_or_index(self):\n     def _as_unicode_or_name(self):\n         if self.font.subfont:\n             raise NotImplementedError(\"Indexing TTC fonts is not supported yet\")\n-        face = font_manager.get_font(self.font.resolve_path())\n+        path = self.font.resolve_path()\n+        if path.name.lower().endswith(\"pk\"):\n+            # PK fonts have no encoding information; report glyphs as ASCII but\n+            # with a \"?\" to indicate that this is just a guess.\n+            return (f\"{chr(self.glyph)}?\" if chr(self.glyph).isprintable() else\n+                    \"??\")",
    "comment": "should we fall back to something like f'pk{self.glyph:x}' so that they are all unique? (the pk prefix is something _like_ the uni prefix on unicode-encoded glyphs)",
    "line_number": 136,
    "enriched": "File: lib/matplotlib/dviread.py\nCode: @@ -127,7 +128,13 @@ def glyph_name_or_index(self):\n     def _as_unicode_or_name(self):\n         if self.font.subfont:\n             raise NotImplementedError(\"Indexing TTC fonts is not supported yet\")\n-        face = font_manager.get_font(self.font.resolve_path())\n+        path = self.font.resolve_path()\n+        if path.name.lower().endswith(\"pk\"):\n+            # PK fonts have no encoding information; report glyphs as ASCII but\n+            # with a \"?\" to indicate that this is just a guess.\n+            return (f\"{chr(self.glyph)}?\" if chr(self.glyph).isprintable() else\n+                    \"??\")\nComment: Should we fall back to something like `f'pk{self.glyph:x}'` so that they are all unique? (the `pk` prefix is something _like_ the `uni` prefix on Unicode-encoded glyphs)",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "lib/matplotlib/dviread.py",
    "pr_number": 30514,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2326574668,
    "comment_created_at": "2025-09-06T06:22:12Z"
  },
  {
    "code": "@@ -107,23 +107,98 @@ class CharacterTracker:\n     \"\"\"\n     Helper for font subsetting by the PDF and PS backends.\n \n-    Maintains a mapping of font paths to the set of glyphs that are being used from that\n-    font.\n-    \"\"\"\n+    Maintains a mapping of font paths to the set of characters and glyphs that are being\n+    used from that font.\n \n-    def __init__(self) -> None:\n-        self.used: dict[str, set[GlyphIndexType]] = {}\n+    Attributes\n+    ----------\n+    subset_size : int\n+        The size at which characters are grouped into subsets.\n+    used : dict[tuple[str, int], dict[CharacterCodeType, GlyphIndexType]]\n+        A dictionary of font files to character maps. The key is a font filename and\n+        subset within that font. The value is a dictionary mapping a character code to a\n+        glyph index. If *subset_size* is not set, then there will only be one subset per\n+        font filename.\n+    \"\"\"\n \n-    def track(self, font: FT2Font, s: str) -> None:\n-        \"\"\"Record that string *s* is being typeset using font *font*.\"\"\"\n+    def __init__(self, subset_size: int = 0):\n+        \"\"\"\n+        Parameters\n+        ----------\n+        subset_size : int, optional\n+            The maximum size that is supported for an embedded font. If provided, then\n+            characters will be grouped into these sized subsets.\n+        \"\"\"\n+        self.used: dict[tuple[str, int], dict[CharacterCodeType, GlyphIndexType]] = {}\n+        self.subset_size = subset_size\n+\n+    def track(self, font: FT2Font, s: str) -> list[tuple[int, CharacterCodeType]]:\n+        \"\"\"\n+        Record that string *s* is being typeset using font *font*.\n+\n+        Parameters\n+        ----------\n+        font : FT2Font\n+            A font that is being used for the provided string.\n+        s : str\n+            The string that should be marked as tracked by the provided font.\n+\n+        Returns\n+        -------\n+        list[tuple[int, CharacterCodeType]]\n+            A list of subset and character code pairs corresponding to the input string.\n+            If a *subset_size* is specified on this instance, then the character code\n+            will correspond with the given subset (and not necessarily the string as a\n+            whole). If *subset_size* is not specified, then the subset will always be 0\n+            and the character codes will be returned from the string unchanged.\n+        \"\"\"\n+        font_glyphs = []\n         char_to_font = font._get_fontmap(s)\n         for _c, _f in char_to_font.items():\n-            glyph_index = _f.get_char_index(ord(_c))\n-            self.used.setdefault(_f.fname, set()).add(glyph_index)\n-\n-    def track_glyph(self, font: FT2Font, glyph_index: GlyphIndexType) -> None:\n-        \"\"\"Record that glyph index *glyph_index* is being typeset using font *font*.\"\"\"\n-        self.used.setdefault(font.fname, set()).add(glyph_index)\n+            charcode = ord(_c)\n+            glyph_index = _f.get_char_index(charcode)\n+            if self.subset_size != 0:\n+                subset = charcode // self.subset_size\n+                subset_charcode = charcode % self.subset_size\n+            else:\n+                subset = 0\n+                subset_charcode = charcode\n+            self.used.setdefault((_f.fname, subset), {})[subset_charcode] = glyph_index\n+            font_glyphs.append((subset, subset_charcode))\n+        return font_glyphs\n+\n+    def track_glyph(\n+            self, font: FT2Font, charcode: CharacterCodeType,\n+            glyph: GlyphIndexType) -> tuple[int, CharacterCodeType]:\n+        \"\"\"\n+        Record character code *charcode* at glyph index *glyph* as using font *font*.\n+\n+        Parameters\n+        ----------\n+        font : FT2Font\n+            A font that is being used for the provided string.\n+        charcode : CharacterCodeType, optional",
    "comment": "i don't think this is \"optional\"?",
    "line_number": 180,
    "enriched": "File: lib/matplotlib/backends/_backend_pdf_ps.py\nCode: @@ -107,23 +107,98 @@ class CharacterTracker:\n     \"\"\"\n     Helper for font subsetting by the PDF and PS backends.\n \n-    Maintains a mapping of font paths to the set of glyphs that are being used from that\n-    font.\n-    \"\"\"\n+    Maintains a mapping of font paths to the set of characters and glyphs that are being\n+    used from that font.\n \n-    def __init__(self) -> None:\n-        self.used: dict[str, set[GlyphIndexType]] = {}\n+    Attributes\n+    ----------\n+    subset_size : int\n+        The size at which characters are grouped into subsets.\n+    used : dict[tuple[str, int], dict[CharacterCodeType, GlyphIndexType]]\n+        A dictionary of font files to character maps. The key is a font filename and\n+        subset within that font. The value is a dictionary mapping a character code to a\n+        glyph index. If *subset_size* is not set, then there will only be one subset per\n+        font filename.\n+    \"\"\"\n \n-    def track(self, font: FT2Font, s: str) -> None:\n-        \"\"\"Record that string *s* is being typeset using font *font*.\"\"\"\n+    def __init__(self, subset_size: int = 0):\n+        \"\"\"\n+        Parameters\n+        ----------\n+        subset_size : int, optional\n+            The maximum size that is supported for an embedded font. If provided, then\n+            characters will be grouped into these sized subsets.\n+        \"\"\"\n+        self.used: dict[tuple[str, int], dict[CharacterCodeType, GlyphIndexType]] = {}\n+        self.subset_size = subset_size\n+\n+    def track(self, font: FT2Font, s: str) -> list[tuple[int, CharacterCodeType]]:\n+        \"\"\"\n+        Record that string *s* is being typeset using font *font*.\n+\n+        Parameters\n+        ----------\n+        font : FT2Font\n+            A font that is being used for the provided string.\n+        s : str\n+            The string that should be marked as tracked by the provided font.\n+\n+        Returns\n+        -------\n+        list[tuple[int, CharacterCodeType]]\n+            A list of subset and character code pairs corresponding to the input string.\n+            If a *subset_size* is specified on this instance, then the character code\n+            will correspond with the given subset (and not necessarily the string as a\n+            whole). If *subset_size* is not specified, then the subset will always be 0\n+            and the character codes will be returned from the string unchanged.\n+        \"\"\"\n+        font_glyphs = []\n         char_to_font = font._get_fontmap(s)\n         for _c, _f in char_to_font.items():\n-            glyph_index = _f.get_char_index(ord(_c))\n-            self.used.setdefault(_f.fname, set()).add(glyph_index)\n-\n-    def track_glyph(self, font: FT2Font, glyph_index: GlyphIndexType) -> None:\n-        \"\"\"Record that glyph index *glyph_index* is being typeset using font *font*.\"\"\"\n-        self.used.setdefault(font.fname, set()).add(glyph_index)\n+            charcode = ord(_c)\n+            glyph_index = _f.get_char_index(charcode)\n+            if self.subset_size != 0:\n+                subset = charcode // self.subset_size\n+                subset_charcode = charcode % self.subset_size\n+            else:\n+                subset = 0\n+                subset_charcode = charcode\n+            self.used.setdefault((_f.fname, subset), {})[subset_charcode] = glyph_index\n+            font_glyphs.append((subset, subset_charcode))\n+        return font_glyphs\n+\n+    def track_glyph(\n+            self, font: FT2Font, charcode: CharacterCodeType,\n+            glyph: GlyphIndexType) -> tuple[int, CharacterCodeType]:\n+        \"\"\"\n+        Record character code *charcode* at glyph index *glyph* as using font *font*.\n+\n+        Parameters\n+        ----------\n+        font : FT2Font\n+            A font that is being used for the provided string.\n+        charcode : CharacterCodeType, optional\nComment: I don't think this is \"optional\"?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/matplotlib/backends/_backend_pdf_ps.py",
    "pr_number": 30512,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2350949995,
    "comment_created_at": "2025-09-16T06:04:00Z"
  },
  {
    "code": "@@ -3325,9 +3343,23 @@ def _release(self, event):\n         self.update()\n         self._active_handle = None\n         self._extents_on_press = None\n-\n         return False\n \n+    def _get_action(self):\n+        \"\"\"\n+        Return one of \"rotate\", \"move\", \"resize\", \"create\"",
    "comment": "let's make this an enum instead. an enum is safer and easier to reason about compared to a bunch of strings.",
    "line_number": 3350,
    "enriched": "File: lib/matplotlib/widgets.py\nCode: @@ -3325,9 +3343,23 @@ def _release(self, event):\n         self.update()\n         self._active_handle = None\n         self._extents_on_press = None\n-\n         return False\n \n+    def _get_action(self):\n+        \"\"\"\n+        Return one of \"rotate\", \"move\", \"resize\", \"create\"\nComment: Let's make this an Enum instead. An enum is safer and easier to reason about compared to a bunch of strings.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "lib/matplotlib/widgets.py",
    "pr_number": 30499,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2312374400,
    "comment_created_at": "2025-08-31T09:48:22Z"
  },
  {
    "code": "@@ -215,14 +215,15 @@ def test_figureoptions():\n \n \n @pytest.mark.backend('QtAgg', skip_on_importerror=True)\n-def test_save_figure_return():\n+def test_save_figure_return(tmp_path):\n+    os.chdir(tmp_path)\n     fig, ax = plt.subplots()\n     ax.imshow([[1]])\n     prop = \"matplotlib.backends.qt_compat.QtWidgets.QFileDialog.getSaveFileName\"\n     with mock.patch(prop, return_value=(\"foobar.png\", None)):",
    "comment": "perhaps it would be even safer to set the return value to an absolute directory instead of having a chdir call at the top of the test?",
    "line_number": 223,
    "enriched": "File: lib/matplotlib/tests/test_backend_qt.py\nCode: @@ -215,14 +215,15 @@ def test_figureoptions():\n \n \n @pytest.mark.backend('QtAgg', skip_on_importerror=True)\n-def test_save_figure_return():\n+def test_save_figure_return(tmp_path):\n+    os.chdir(tmp_path)\n     fig, ax = plt.subplots()\n     ax.imshow([[1]])\n     prop = \"matplotlib.backends.qt_compat.QtWidgets.QFileDialog.getSaveFileName\"\n     with mock.patch(prop, return_value=(\"foobar.png\", None)):\nComment: Perhaps it would be even safer to set the return value to an absolute directory instead of having a `chdir` call at the top of the test?",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "lib/matplotlib/tests/test_backend_qt.py",
    "pr_number": 30497,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2321923140,
    "comment_created_at": "2025-09-04T12:25:26Z"
  },
  {
    "code": "@@ -12,8 +12,8 @@\n     },\n     {\n         \"name\": \"3.9\",\n-        \"version\": \"3.9.3\",\n-        \"url\": \"https://matplotlib.org/3.9.3/\"\n+        \"version\": \"3.9.4\",\n+        \"url\": \"https://matplotlib.org/3.9.4/\"",
    "comment": "this url didn't exist, not sure why it got bumped on v3.10.x.",
    "line_number": 16,
    "enriched": "File: doc/_static/switcher.json\nCode: @@ -12,8 +12,8 @@\n     },\n     {\n         \"name\": \"3.9\",\n-        \"version\": \"3.9.3\",\n-        \"url\": \"https://matplotlib.org/3.9.3/\"\n+        \"version\": \"3.9.4\",\n+        \"url\": \"https://matplotlib.org/3.9.4/\"\nComment: This URL didn't exist, not sure why it got bumped on `v3.10.x`.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "doc/_static/switcher.json",
    "pr_number": 30491,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2311780527,
    "comment_created_at": "2025-08-30T04:44:31Z"
  },
  {
    "code": "@@ -2172,6 +2172,11 @@ def update_background(self, event):\n         # `release` can call a draw event even when `ignore` is True.\n         if not self.useblit:\n             return\n+        # Skip blitting if we are saving to disk or if the backend doesn\u2019t support it\n+        if getattr(self.canvas, \"_is_saving\", False):\n+            return",
    "comment": "this is defined in figurecanvasbase and thus available in all figurecanvas subclasses.",
    "line_number": 2177,
    "enriched": "File: lib/matplotlib/widgets.py\nCode: @@ -2172,6 +2172,11 @@ def update_background(self, event):\n         # `release` can call a draw event even when `ignore` is True.\n         if not self.useblit:\n             return\n+        # Skip blitting if we are saving to disk or if the backend doesn\u2019t support it\n+        if getattr(self.canvas, \"_is_saving\", False):\n+            return\nComment: ```suggestion\r\n        if self.canvas.is_saving():\r\n            return\r\n```\r\nThis is defined in `FigureCanvasBase` and thus available in all FigureCanvas subclasses.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "lib/matplotlib/widgets.py",
    "pr_number": 30490,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2312629143,
    "comment_created_at": "2025-08-31T21:11:23Z"
  },
  {
    "code": "@@ -517,7 +517,12 @@ def _draw_idle(self):\n             if not self._draw_pending:\n                 return\n             self._draw_pending = False\n-            if self.height() <= 0 or self.width() <= 0:\n+            try:\n+                if self.height() <= 0 or self.width() <= 0:\n+                    return\n+            except RuntimeError:\n+                # This can happen if the c++ object is already cleaned up",
    "comment": "can we check the error message to ensure we\u2019re not swallowing other potential runtimeerrors?\r\n\r\nalso, are we sure this is the only place that may access the c++ object?",
    "line_number": 524,
    "enriched": "File: lib/matplotlib/backends/backend_qt.py\nCode: @@ -517,7 +517,12 @@ def _draw_idle(self):\n             if not self._draw_pending:\n                 return\n             self._draw_pending = False\n-            if self.height() <= 0 or self.width() <= 0:\n+            try:\n+                if self.height() <= 0 or self.width() <= 0:\n+                    return\n+            except RuntimeError:\n+                # This can happen if the c++ object is already cleaned up\nComment: Can we check the error message to ensure we\u2019re not swallowing other potential RuntimeErrors?\r\n\r\nAlso, are we sure this is the only place that may access the C++ object?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/matplotlib/backends/backend_qt.py",
    "pr_number": 30484,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2309199200,
    "comment_created_at": "2025-08-29T05:42:04Z"
  },
  {
    "code": "@@ -1418,24 +1418,27 @@ def contains_branch(self, other):\n                 return True\n         return False\n \n-    def contains_branch_seperately(self, other_transform):\n+    def contains_branch_separately(self, other_transform):\n         \"\"\"\n         Return whether the given branch is a sub-tree of this transform on\n         each separate dimension.\n \n         A common use for this method is to identify if a transform is a blended\n         transform containing an Axes' data transform. e.g.::\n \n-            x_isdata, y_isdata = trans.contains_branch_seperately(ax.transData)\n+            x_isdata, y_isdata = trans.contains_branch_separately(ax.transData)\n \n         \"\"\"\n         if self.output_dims != 2:\n-            raise ValueError('contains_branch_seperately only supports '\n+            raise ValueError('contains_branch_separately only supports '\n                              'transforms with 2 output dimensions')\n         # for a non-blended transform each separate dimension is the same, so\n         # just return the appropriate shape.\n         return (self.contains_branch(other_transform), ) * 2\n \n+    # Permanent alias for backwards compatibility (historical typo)\n+    contains_branch_seperately = contains_branch_separately\n+",
    "comment": "if you make this an actual method, then the call it makes will go to the subclass automatically, and there's no need for duplicate aliases on the other classes.",
    "line_number": 1443,
    "enriched": "File: lib/matplotlib/transforms.py\nCode: @@ -1418,24 +1418,27 @@ def contains_branch(self, other):\n                 return True\n         return False\n \n-    def contains_branch_seperately(self, other_transform):\n+    def contains_branch_separately(self, other_transform):\n         \"\"\"\n         Return whether the given branch is a sub-tree of this transform on\n         each separate dimension.\n \n         A common use for this method is to identify if a transform is a blended\n         transform containing an Axes' data transform. e.g.::\n \n-            x_isdata, y_isdata = trans.contains_branch_seperately(ax.transData)\n+            x_isdata, y_isdata = trans.contains_branch_separately(ax.transData)\n \n         \"\"\"\n         if self.output_dims != 2:\n-            raise ValueError('contains_branch_seperately only supports '\n+            raise ValueError('contains_branch_separately only supports '\n                              'transforms with 2 output dimensions')\n         # for a non-blended transform each separate dimension is the same, so\n         # just return the appropriate shape.\n         return (self.contains_branch(other_transform), ) * 2\n \n+    # Permanent alias for backwards compatibility (historical typo)\n+    contains_branch_seperately = contains_branch_separately\n+\nComment: If you make this an actual method, then the call it makes will go to the subclass automatically, and there's no need for duplicate aliases on the other classes.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "lib/matplotlib/transforms.py",
    "pr_number": 30475,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2306258089,
    "comment_created_at": "2025-08-28T05:55:02Z"
  },
  {
    "code": "@@ -3,8 +3,8 @@\n \n .. _imshow_extent:\n \n-*origin* and *extent* in `~.Axes.imshow`\n-========================================\n+The *origin* and *extent* in `~.Axes.imshow`\n+============================================",
    "comment": "i wonder if origin and extent are really important to place in the title here; maybe it should have been something like \"positioning and orientation of ~.axes.imshow images\"",
    "line_number": 7,
    "enriched": "File: galleries/users_explain/artists/imshow_extent.py\nCode: @@ -3,8 +3,8 @@\n \n .. _imshow_extent:\n \n-*origin* and *extent* in `~.Axes.imshow`\n-========================================\n+The *origin* and *extent* in `~.Axes.imshow`\n+============================================\nComment: I wonder if `origin` and `extent` are really important to place in the title here; maybe it should have been something like \"Positioning and orientation of `~.Axes.imshow` images\"",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "galleries/users_explain/artists/imshow_extent.py",
    "pr_number": 30471,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2299509649,
    "comment_created_at": "2025-08-26T01:39:54Z"
  },
  {
    "code": "@@ -29,35 +29,32 @@ def hat_graph(ax, xlabels, values, group_labels):\n         The group labels displayed in the legend.\n     \"\"\"\n \n-    def label_bars(heights, rects):\n-        \"\"\"Attach a text label on top of each bar.\"\"\"\n-        for height, rect in zip(heights, rects):\n-            ax.annotate(f'{height}',\n-                        xy=(rect.get_x() + rect.get_width() / 2, height),\n-                        xytext=(0, 4),  # 4 points vertical offset.\n-                        textcoords='offset points',\n-                        ha='center', va='bottom')\n-\n     values = np.asarray(values)\n     x = np.arange(values.shape[1])\n     ax.set_xticks(x, labels=xlabels)",
    "comment": "can we remove this? i believe grouped_bar(..., tick_labels=...) already cares for this.",
    "line_number": 34,
    "enriched": "File: galleries/examples/lines_bars_and_markers/hat_graph.py\nCode: @@ -29,35 +29,32 @@ def hat_graph(ax, xlabels, values, group_labels):\n         The group labels displayed in the legend.\n     \"\"\"\n \n-    def label_bars(heights, rects):\n-        \"\"\"Attach a text label on top of each bar.\"\"\"\n-        for height, rect in zip(heights, rects):\n-            ax.annotate(f'{height}',\n-                        xy=(rect.get_x() + rect.get_width() / 2, height),\n-                        xytext=(0, 4),  # 4 points vertical offset.\n-                        textcoords='offset points',\n-                        ha='center', va='bottom')\n-\n     values = np.asarray(values)\n     x = np.arange(values.shape[1])\n     ax.set_xticks(x, labels=xlabels)\nComment: Can we remove this? I believe `grouped_bar(..., tick_labels=...)` already cares for this.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "galleries/examples/lines_bars_and_markers/hat_graph.py",
    "pr_number": 30459,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2296384900,
    "comment_created_at": "2025-08-24T00:06:41Z"
  },
  {
    "code": "@@ -7,6 +7,8 @@\n Installation\n ************\n \n+.. include:: quick_install.inc.rst\n+   :end-line: 45",
    "comment": "can we do something like:\r\n\r\nso that we don't hardcode the line?",
    "line_number": 11,
    "enriched": "File: doc/install/index.rst\nCode: @@ -7,6 +7,8 @@\n Installation\n ************\n \n+.. include:: quick_install.inc.rst\n+   :end-line: 45\nComment: Can we do something like:\r\n```suggestion\r\n   :end-before: tab-item:: other\r\n```\r\nso that we don't hardcode the line?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "doc/install/index.rst",
    "pr_number": 30451,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2292179402,
    "comment_created_at": "2025-08-21T21:32:06Z"
  },
  {
    "code": "@@ -37,12 +38,12 @@ class AbstractProvider:  # type: ignore[no-redef]\n \n \n # TODO: add python requirements to ansible-test's ansible-core distribution info and remove the hardcoded lowerbound/upperbound fallback\n-RESOLVELIB_LOWERBOUND = SemanticVersion(\"0.5.3\")\n+RESOLVELIB_LOWERBOUND = SemanticVersion(\"0.8.0\")\n RESOLVELIB_UPPERBOUND = SemanticVersion(\"2.0.0\")\n RESOLVELIB_VERSION = SemanticVersion.from_loose_version(LooseVersion(resolvelib_version))\n \n \n-class CollectionDependencyProviderBase(AbstractProvider):\n+class CollectionDependencyProvider(AbstractProvider):",
    "comment": "the # type: collectiondependencyproviderbase annotation below should be removed, since that type no longer exists and the annotation is redundant.",
    "line_number": 46,
    "enriched": "File: lib/ansible/galaxy/dependency_resolution/providers.py\nCode: @@ -37,12 +38,12 @@ class AbstractProvider:  # type: ignore[no-redef]\n \n \n # TODO: add python requirements to ansible-test's ansible-core distribution info and remove the hardcoded lowerbound/upperbound fallback\n-RESOLVELIB_LOWERBOUND = SemanticVersion(\"0.5.3\")\n+RESOLVELIB_LOWERBOUND = SemanticVersion(\"0.8.0\")\n RESOLVELIB_UPPERBOUND = SemanticVersion(\"2.0.0\")\n RESOLVELIB_VERSION = SemanticVersion.from_loose_version(LooseVersion(resolvelib_version))\n \n \n-class CollectionDependencyProviderBase(AbstractProvider):\n+class CollectionDependencyProvider(AbstractProvider):\nComment: The `# type: CollectionDependencyProviderBase` annotation below should be removed, since that type no longer exists and the annotation is redundant.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "lib/ansible/galaxy/dependency_resolution/providers.py",
    "pr_number": 85936,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2395603310,
    "comment_created_at": "2025-10-01T19:09:43Z"
  },
  {
    "code": "@@ -136,6 +133,18 @@ class AnsibleUnwantedChecker(BaseChecker):\n                                         modules_only=True),\n     }\n \n+    def __init__(self, *args, **kwargs) -> None:\n+        super().__init__(*args, **kwargs)\n+        if self.is_ansible_core:",
    "comment": "why limit this check to core?",
    "line_number": 138,
    "enriched": "File: test/lib/ansible_test/_util/controller/sanity/pylint/plugins/unwanted.py\nCode: @@ -136,6 +133,18 @@ class AnsibleUnwantedChecker(BaseChecker):\n                                         modules_only=True),\n     }\n \n+    def __init__(self, *args, **kwargs) -> None:\n+        super().__init__(*args, **kwargs)\n+        if self.is_ansible_core:\nComment: Why limit this check to core?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "test/lib/ansible_test/_util/controller/sanity/pylint/plugins/unwanted.py",
    "pr_number": 85934,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2399310267,
    "comment_created_at": "2025-10-02T15:57:56Z"
  },
  {
    "code": "@@ -165,17 +165,40 @@ def load_list_of_tasks(ds, play, block=None, role=None, task_include=None, use_h\n                     subdir = 'tasks'\n                     if use_handlers:\n                         subdir = 'handlers'\n+                    try:\n+                        include_target = templar.template(task.args['_raw_params'])\n+                    except AnsibleUndefinedVariable as ex:\n+                        raise AnsibleParserError(\n+                            message=f\"Error when evaluating variable in import path {task.args['_raw_params']!r}.\",\n+                            help_text=\"When using static imports, ensure that any variables used in their names are defined in vars/vars_files\\n\"\n+                                      \"or extra-vars passed in from the command line. Static imports cannot use variables from facts or inventory\\n\"\n+                                      \"sources like group or host vars.\",\n+                            obj=task_ds,\n+                        ) from ex\n+                    # FIXME this appears to be (almost?) duplicate code as in IncludedFile for include_tasks\n                     while parent_include is not None:\n                         if not isinstance(parent_include, TaskInclude):\n                             parent_include = parent_include._parent\n                             continue\n-                        parent_include.post_validate(templar=templar)\n-                        parent_include_dir = os.path.dirname(parent_include.args.get('_raw_params'))\n+                        if isinstance(parent_include, IncludeRole):\n+                            parent_include_dir = parent_include._role_path\n+                        else:\n+                            try:\n+                                parent_include_dir = os.path.dirname(templar.template(parent_include.args.get('_raw_params')))\n+                            except AnsibleUndefinedVariable as ex:\n+                                if not parent_include.statically_loaded:",
    "comment": "does _raw_params need templating here if it wasn't statically loaded? i'm wondering if the error handling for non-static parent includes is reachable.",
    "line_number": 189,
    "enriched": "File: lib/ansible/playbook/helpers.py\nCode: @@ -165,17 +165,40 @@ def load_list_of_tasks(ds, play, block=None, role=None, task_include=None, use_h\n                     subdir = 'tasks'\n                     if use_handlers:\n                         subdir = 'handlers'\n+                    try:\n+                        include_target = templar.template(task.args['_raw_params'])\n+                    except AnsibleUndefinedVariable as ex:\n+                        raise AnsibleParserError(\n+                            message=f\"Error when evaluating variable in import path {task.args['_raw_params']!r}.\",\n+                            help_text=\"When using static imports, ensure that any variables used in their names are defined in vars/vars_files\\n\"\n+                                      \"or extra-vars passed in from the command line. Static imports cannot use variables from facts or inventory\\n\"\n+                                      \"sources like group or host vars.\",\n+                            obj=task_ds,\n+                        ) from ex\n+                    # FIXME this appears to be (almost?) duplicate code as in IncludedFile for include_tasks\n                     while parent_include is not None:\n                         if not isinstance(parent_include, TaskInclude):\n                             parent_include = parent_include._parent\n                             continue\n-                        parent_include.post_validate(templar=templar)\n-                        parent_include_dir = os.path.dirname(parent_include.args.get('_raw_params'))\n+                        if isinstance(parent_include, IncludeRole):\n+                            parent_include_dir = parent_include._role_path\n+                        else:\n+                            try:\n+                                parent_include_dir = os.path.dirname(templar.template(parent_include.args.get('_raw_params')))\n+                            except AnsibleUndefinedVariable as ex:\n+                                if not parent_include.statically_loaded:\nComment: Does _raw_params need templating here if it wasn't statically loaded? I'm wondering if the error handling for non-static parent includes is reachable.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/ansible/playbook/helpers.py",
    "pr_number": 85877,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2376712144,
    "comment_created_at": "2025-09-24T18:32:15Z"
  },
  {
    "code": "@@ -673,6 +673,12 @@ def _fixup_perms2(self, remote_paths, remote_user=None, execute=True):\n \n         if res['rc'] == 0:\n             return remote_paths\n+        if res['rc'] == 127:",
    "comment": "most of these the user cannot do much about, this would be better as a debug:\r\n\r\nself._display.debug(f'failed to set facl {setfacl_mode}, got:{res!r}')",
    "line_number": 676,
    "enriched": "File: lib/ansible/plugins/action/__init__.py\nCode: @@ -673,6 +673,12 @@ def _fixup_perms2(self, remote_paths, remote_user=None, execute=True):\n \n         if res['rc'] == 0:\n             return remote_paths\n+        if res['rc'] == 127:\nComment: most of these the user cannot do much about, this would be better as a debug:\r\n```\r\nself._display.debug(f'Failed to set facl {setfacl_mode}, got:{res!r}')\r\n```",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "lib/ansible/plugins/action/__init__.py",
    "pr_number": 85839,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2337779248,
    "comment_created_at": "2025-09-10T20:05:43Z"
  },
  {
    "code": "@@ -162,6 +162,7 @@ def set(self, key, value):\n             except OSError as ex:\n                 display.error_as_warning(f\"Error in {self.plugin_name!r} cache plugin while trying to write to {tmpfile_path!r}.\", exception=ex)\n             try:\n+                os.close(tmpfile_handle)",
    "comment": "please create a [changelog fragment](https://docs.ansible.com/ansible/latest/community/development_process.html#creating-a-changelog-fragment). see [this fragment](https://github.com/ansible/ansible/blob/c2f528b7686c9573aa8493357219e0bbe4416dd3/changelogs/fragments/reboot-change-default-boot-command.yaml) as an example. ~also, please add integration tests for this change so that we don't regress in future?~",
    "line_number": 165,
    "enriched": "File: lib/ansible/plugins/cache/__init__.py\nCode: @@ -162,6 +162,7 @@ def set(self, key, value):\n             except OSError as ex:\n                 display.error_as_warning(f\"Error in {self.plugin_name!r} cache plugin while trying to write to {tmpfile_path!r}.\", exception=ex)\n             try:\n+                os.close(tmpfile_handle)\nComment: Please create a [changelog fragment](https://docs.ansible.com/ansible/latest/community/development_process.html#creating-a-changelog-fragment). See [this fragment](https://github.com/ansible/ansible/blob/c2f528b7686c9573aa8493357219e0bbe4416dd3/changelogs/fragments/reboot-change-default-boot-command.yaml) as an example. ~Also, please add integration tests for this change so that we don't regress in future?~",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "lib/ansible/plugins/cache/__init__.py",
    "pr_number": 85816,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2330684332,
    "comment_created_at": "2025-09-08T15:57:22Z"
  },
  {
    "code": "@@ -0,0 +1,7 @@\n+minor_changes:\n+- >-\n+  Add tech preview play argument spec validation, which can be\n+  enabled by setting the play keyword ``validate_argspec`` to ``True``.\n+  This will load a file matching the pattern <playbook_name>.meta.yml,\n+  which should contain a top dictionary \"argument_specs\", containing\n+  the play name, containing an \"options\" dictionary.",
    "comment": "iirc, for role argspec validation, the existence of an argspec file means we automatically do the validation. i think include_role and import_role have options to disable. perhaps we follow the same pattern for consistency sake? if an argspec exists and a play name matches, always do the validation. the new option could then be used to explicitly disable.",
    "line_number": 7,
    "enriched": "File: changelogs/fragments/play-argument-spec-validation.yml\nCode: @@ -0,0 +1,7 @@\n+minor_changes:\n+- >-\n+  Add tech preview play argument spec validation, which can be\n+  enabled by setting the play keyword ``validate_argspec`` to ``True``.\n+  This will load a file matching the pattern <playbook_name>.meta.yml,\n+  which should contain a top dictionary \"argument_specs\", containing\n+  the play name, containing an \"options\" dictionary.\nComment: IIRC, for role argspec validation, the existence of an argspec file means we automatically do the validation. I think `include_role` and `import_role` have options to disable. Perhaps we follow the same pattern for consistency sake? If an argspec exists and a play name matches, always do the validation. The new option could then be used to explicitly disable.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "changelogs/fragments/play-argument-spec-validation.yml",
    "pr_number": 85763,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2310544378,
    "comment_created_at": "2025-08-29T16:02:35Z"
  },
  {
    "code": "@@ -537,6 +537,8 @@ def _configure_base(self, base, conf_file, disable_gpg_check, installroot='/', s\n         conf.sslverify = sslverify\n \n         # Set installroot\n+        if os.path.isfile(installroot):",
    "comment": "we need to check specifically for a directory. we can't assume that if the path is not a regular file that it is a directory. the path can be non-existing, broken symlink, ... same below.",
    "line_number": 540,
    "enriched": "File: lib/ansible/modules/dnf.py\nCode: @@ -537,6 +537,8 @@ def _configure_base(self, base, conf_file, disable_gpg_check, installroot='/', s\n         conf.sslverify = sslverify\n \n         # Set installroot\n+        if os.path.isfile(installroot):\nComment: ```suggestion\r\n        if not os.path.isdir(installroot):\r\n```\r\n\r\nWe need to check specifically for a directory. We can't assume that if the path is not a regular file that it is a directory. The path can be non-existing, broken symlink, ... Same below.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "lib/ansible/modules/dnf.py",
    "pr_number": 85748,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2303047941,
    "comment_created_at": "2025-08-27T06:56:05Z"
  },
  {
    "code": "@@ -2568,7 +2568,7 @@ HTTPie plays exceptionally well with the following tools:\n \n Helpers to convert from other client tools:\n \n-- [CurliPie](https://curlipie.now.sh/) help convert cURL command line to HTTPie command line\n+- [CurliPie](https://pypi.org/project/curlipie/) help convert cURL command line to HTTPie command line",
    "comment": "would the web frontend be a better replacement?",
    "line_number": 2571,
    "enriched": "File: docs/README.md\nCode: @@ -2568,7 +2568,7 @@ HTTPie plays exceptionally well with the following tools:\n \n Helpers to convert from other client tools:\n \n-- [CurliPie](https://curlipie.now.sh/) help convert cURL command line to HTTPie command line\n+- [CurliPie](https://pypi.org/project/curlipie/) help convert cURL command line to HTTPie command line\nComment: would the web frontend be a better replacement?\r\n\r\n```suggestion\r\n- [CurliPie](https://curlipie.open-api.vn) \u2014 library to convert cURL commands to HTTPie\r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/README.md",
    "pr_number": 1582,
    "repo": "httpie",
    "owner": "httpie",
    "comment_id": 1723894773,
    "comment_created_at": "2024-08-20T19:44:03Z"
  },
  {
    "code": "@@ -39,7 +39,7 @@ tools:\n       install:\n         - curl -SsL https://packages.httpie.io/deb/KEY.gpg | sudo gpg --dearmor -o /usr/share/keyrings/httpie.gpg\n       # - curl -SsL -o /etc/apt/sources.list.d/httpie.list https://packages.httpie.io/deb/httpie.list\n-        - sudo echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/httpie.gpg] https://packages.httpie.io/deb ./\" > /etc/apt/sources.list.d/httpie.list\n+        - sudo sh -c 'echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/httpie.gpg] https://packages.httpie.io/deb ./\" > /etc/apt/sources.list.d/httpie.list'",
    "comment": "hi, there is also another way to do it.\r\nsh\r\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/httpie.gpg] https://packages.httpie.io/deb ./\" | sudo tee -a /etc/apt/sources.list.d/httpie.list",
    "line_number": 42,
    "enriched": "File: docs/installation/methods.yml\nCode: @@ -39,7 +39,7 @@ tools:\n       install:\n         - curl -SsL https://packages.httpie.io/deb/KEY.gpg | sudo gpg --dearmor -o /usr/share/keyrings/httpie.gpg\n       # - curl -SsL -o /etc/apt/sources.list.d/httpie.list https://packages.httpie.io/deb/httpie.list\n-        - sudo echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/httpie.gpg] https://packages.httpie.io/deb ./\" > /etc/apt/sources.list.d/httpie.list\n+        - sudo sh -c 'echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/httpie.gpg] https://packages.httpie.io/deb ./\" > /etc/apt/sources.list.d/httpie.list'\nComment: Hi, there is also another way to do it.\r\n```sh\r\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/httpie.gpg] https://packages.httpie.io/deb ./\" | sudo tee -a /etc/apt/sources.list.d/httpie.list\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "docs/installation/methods.yml",
    "pr_number": 1534,
    "repo": "httpie",
    "owner": "httpie",
    "comment_id": 1370479613,
    "comment_created_at": "2023-10-24T16:16:22Z"
  },
  {
    "code": "@@ -334,23 +334,24 @@ def test_complex_json_arguments_with_non_json(httpbin, request_type, value):\n                 },\n             },\n         ),\n-        (\n-            [\n-                r'foo\\[key\\]:=1',\n-                r'bar\\[1\\]:=2',\n-                r'baz\\[\\]:3',\n-                r'quux[key\\[escape\\]]:=4',\n-                r'quux[key 2][\\\\][\\\\\\\\][\\\\\\[\\]\\\\\\]\\\\\\[\\n\\\\]:=5',\n-            ],\n-            {\n-                'foo[key]': 1,\n-                'bar[1]': 2,\n-                'quux': {\n-                    'key[escape]': 4,\n-                    'key 2': {'\\\\': {'\\\\\\\\': {'\\\\[]\\\\]\\\\[\\\\n\\\\': 5}}},\n-                },\n-            },\n-        ),\n+        # todo: this expression leads to an invalid header name! this is a bug.",
    "comment": "fyi this test case had to be disabled. it produces an invalid header name.\r\nrequests did not say anything alarming due to the too-permissive underlying urllib3 x http.client.",
    "line_number": 337,
    "enriched": "File: tests/test_json.py\nCode: @@ -334,23 +334,24 @@ def test_complex_json_arguments_with_non_json(httpbin, request_type, value):\n                 },\n             },\n         ),\n-        (\n-            [\n-                r'foo\\[key\\]:=1',\n-                r'bar\\[1\\]:=2',\n-                r'baz\\[\\]:3',\n-                r'quux[key\\[escape\\]]:=4',\n-                r'quux[key 2][\\\\][\\\\\\\\][\\\\\\[\\]\\\\\\]\\\\\\[\\n\\\\]:=5',\n-            ],\n-            {\n-                'foo[key]': 1,\n-                'bar[1]': 2,\n-                'quux': {\n-                    'key[escape]': 4,\n-                    'key 2': {'\\\\': {'\\\\\\\\': {'\\\\[]\\\\]\\\\[\\\\n\\\\': 5}}},\n-                },\n-            },\n-        ),\n+        # todo: this expression leads to an invalid header name! this is a bug.\nComment: FYI this test case had to be disabled. It produces an invalid header name.\r\nrequests did not say anything alarming due to the too-permissive underlying urllib3 x http.client. ",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "tests/test_json.py",
    "pr_number": 1531,
    "repo": "httpie",
    "owner": "httpie",
    "comment_id": 1355616302,
    "comment_created_at": "2023-10-11T18:58:44Z"
  },
  {
    "code": "@@ -215,14 +215,14 @@ $ pacman -Syu httpie\n $ pacman -Syu\n ```\n \n-#### Single Binary Executables\n+#### Single binary executables\n \n-Have a standalone HTTPie executable, when you don't want to go through the full installatin process.\n+Have a standalone HTTPie executable when you don't want to go through the full installation process",
    "comment": "@isidentical i'm still trying to wrap my head around the phrasing here, is there something that escapes me from the structure of the methods above it, or the call-to-action should simply be anything like:\r\n\"get the standalone httpie linux executables when\u2026\" or\r\n\"use the standalone httpie linux executables when\u2026\"?",
    "line_number": 220,
    "enriched": "File: docs/README.md\nCode: @@ -215,14 +215,14 @@ $ pacman -Syu httpie\n $ pacman -Syu\n ```\n \n-#### Single Binary Executables\n+#### Single binary executables\n \n-Have a standalone HTTPie executable, when you don't want to go through the full installatin process.\n+Have a standalone HTTPie executable when you don't want to go through the full installation process\nComment: @isidentical I'm still trying to wrap my head around the phrasing here, is there something that escapes me from the structure of the methods above it, or the call-to-action should simply be anything like:\r\n`\"Get the standalone HTTPie Linux executables when\u2026\"` or\r\n`\"Use the standalone HTTPie Linux executables when\u2026\"`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/README.md",
    "pr_number": 1396,
    "repo": "httpie",
    "owner": "httpie",
    "comment_id": 869142229,
    "comment_created_at": "2022-05-10T11:52:01Z"
  },
  {
    "code": "@@ -163,7 +164,10 @@ def get_stream_type_and_kwargs(\n     if not is_stream and message_type is HTTPResponse:\n         # If this is a response, then check the headers for determining\n         # auto-streaming.\n-        is_stream = headers.get('Content-Type') == 'text/event-stream'\n+        ct_raw = headers.get('Content-Type', None)\n+        if ct_raw:\n+            ct, _ = parse_content_type_header(ct_raw)\n+            is_stream = ct == 'text/event-stream'",
    "comment": "i find expliciter names much more suitable (with the rest of the code base).",
    "line_number": 170,
    "enriched": "File: httpie/output/writer.py\nCode: @@ -163,7 +164,10 @@ def get_stream_type_and_kwargs(\n     if not is_stream and message_type is HTTPResponse:\n         # If this is a response, then check the headers for determining\n         # auto-streaming.\n-        is_stream = headers.get('Content-Type') == 'text/event-stream'\n+        ct_raw = headers.get('Content-Type', None)\n+        if ct_raw:\n+            ct, _ = parse_content_type_header(ct_raw)\n+            is_stream = ct == 'text/event-stream'\nComment: I find expliciter names much more suitable (with the rest of the code base).\r\n```suggestion\r\n        raw_content_type_header = headers.get('Content-Type', None)\r\n        if raw_content_type_header:\r\n            content_type_header, _ = parse_content_type_header(raw_content_type_header)\r\n            is_stream = (content_type_header == 'text/event-stream')\r\n ```",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "httpie/output/writer.py",
    "pr_number": 1383,
    "repo": "httpie",
    "owner": "httpie",
    "comment_id": 864224248,
    "comment_created_at": "2022-05-03T20:38:50Z"
  },
  {
    "code": "@@ -0,0 +1,41 @@\n+\"\"\"\n+This is a pure implementation of simple swap sort algorithm in Python.\n+It counts the number of smaller values and then swap the element\n+with the matching digit. It uses minimum number of swaps: \n+https://geeksforgeeks.org/minimum-number-swaps-required-sort-array/?ref=lbp\n+Each element may only occur ones.\n+\n+For doctests run following command:\n+python3 -m doctest -v swap_sort.py\n+For manual testing run:\n+python3 swap_sort.py\n+\"\"\"\n+\n+\n+def swap_sort(collection):",
    "comment": "please provide return type hint for the function: swap_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: collection",
    "line_number": 15,
    "enriched": "File: sorts/swap_sort.py\nCode: @@ -0,0 +1,41 @@\n+\"\"\"\n+This is a pure implementation of simple swap sort algorithm in Python.\n+It counts the number of smaller values and then swap the element\n+with the matching digit. It uses minimum number of swaps: \n+https://geeksforgeeks.org/minimum-number-swaps-required-sort-array/?ref=lbp\n+Each element may only occur ones.\n+\n+For doctests run following command:\n+python3 -m doctest -v swap_sort.py\n+For manual testing run:\n+python3 swap_sort.py\n+\"\"\"\n+\n+\n+def swap_sort(collection):\nComment: Please provide return type hint for the function: `swap_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `collection`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "sorts/swap_sort.py",
    "pr_number": 7356,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 997155145,
    "comment_created_at": "2022-10-17T14:44:49Z"
  },
  {
    "code": "@@ -0,0 +1,107 @@\n+class FibonacciNode:\n+    def __init__(self, key):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: key",
    "line_number": 2,
    "enriched": "File: data_structures/heap/fibonacci_heap.py\nCode: @@ -0,0 +1,107 @@\n+class FibonacciNode:\n+    def __init__(self, key):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `key`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/heap/fibonacci_heap.py",
    "pr_number": 9321,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342167956,
    "comment_created_at": "2023-10-01T17:59:13Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+import random\n+\n+\"\"\"\n+* Guess Money is a simple fun math game.\n+* In this game, you have to think of any amount.\n+* It will return the total amount that is left in your pocket.\n+* Have Fun :)\n+\"\"\"\n+\n+# Example\n+\"\"\"\n+1. Let suppose you have 100 \u20b9 in your pocket.\n+2. Borrow 100\u20b9 from your friend..\n+3. Your 100 + Friend 100 = 200\u20b9.\n+5. Let the third amount be 50\u20b9 so, add 50\u20b9 in 200\u20b9 and it become 250\u20b9.\n+6. Spend 100\u20b9 of your friend to purchase some cookies.\n+7. You have 100\u20b9 initially so, donate 100 from left amount.\n+8. You have 50\u20b9 left in your pocket .\n+\"\"\"\n+\n+# Rules\n+\"\"\"\n+1. Do not enter any amount that you are thinking.\n+2. The player just have to press enter and that's it.\n+3. If you are weak in simple calculations then take a calculator and then play.\n+\"\"\"\n+\n+\n+class Guess:\n+    def calculate_number(self) -> int:\n+        # Generate a random number between 1 and 1000\n+        return random.randint(1, 10000)\n+\n+    def print_statement(self):",
    "comment": "please provide return type hint for the function: print_statement. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 34,
    "enriched": "File: other/guess_money.py\nCode: @@ -0,0 +1,58 @@\n+import random\n+\n+\"\"\"\n+* Guess Money is a simple fun math game.\n+* In this game, you have to think of any amount.\n+* It will return the total amount that is left in your pocket.\n+* Have Fun :)\n+\"\"\"\n+\n+# Example\n+\"\"\"\n+1. Let suppose you have 100 \u20b9 in your pocket.\n+2. Borrow 100\u20b9 from your friend..\n+3. Your 100 + Friend 100 = 200\u20b9.\n+5. Let the third amount be 50\u20b9 so, add 50\u20b9 in 200\u20b9 and it become 250\u20b9.\n+6. Spend 100\u20b9 of your friend to purchase some cookies.\n+7. You have 100\u20b9 initially so, donate 100 from left amount.\n+8. You have 50\u20b9 left in your pocket .\n+\"\"\"\n+\n+# Rules\n+\"\"\"\n+1. Do not enter any amount that you are thinking.\n+2. The player just have to press enter and that's it.\n+3. If you are weak in simple calculations then take a calculator and then play.\n+\"\"\"\n+\n+\n+class Guess:\n+    def calculate_number(self) -> int:\n+        # Generate a random number between 1 and 1000\n+        return random.randint(1, 10000)\n+\n+    def print_statement(self):\nComment: Please provide return type hint for the function: `print_statement`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "other/guess_money.py",
    "pr_number": 10410,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359152222,
    "comment_created_at": "2023-10-14T05:08:59Z"
  },
  {
    "code": "@@ -0,0 +1,67 @@\n+import json\n+import socket\n+\n+\n+def request(sock: socket.socket, req_type: str, endpoint: str, headers: list[str], body: str):",
    "comment": "please provide return type hint for the function: request. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 5,
    "enriched": "File: web_programming/http_request_and_response.py\nCode: @@ -0,0 +1,67 @@\n+import json\n+import socket\n+\n+\n+def request(sock: socket.socket, req_type: str, endpoint: str, headers: list[str], body: str):\nComment: Please provide return type hint for the function: `request`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "web_programming/http_request_and_response.py",
    "pr_number": 8018,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1040905202,
    "comment_created_at": "2022-12-06T12:22:08Z"
  },
  {
    "code": "@@ -1,37 +1,26 @@\n \"\"\"\n Greatest Common Divisor.\n \n-Wikipedia reference: https://en.wikipedia.org/wiki/Greatest_common_divisor\n+Wikipedia reference: https://en.wikipedia.org/wiki/greatest_common_divisor",
    "comment": "the original url is correct: it's capitalized",
    "line_number": 4,
    "enriched": "File: maths/greatest_common_divisor.py\nCode: @@ -1,37 +1,26 @@\n \"\"\"\n Greatest Common Divisor.\n \n-Wikipedia reference: https://en.wikipedia.org/wiki/Greatest_common_divisor\n+Wikipedia reference: https://en.wikipedia.org/wiki/greatest_common_divisor\nComment: ```suggestion\r\nWikipedia reference: https://en.wikipedia.org/wiki/Greatest_common_divisor\r\n```\r\nThe original URL is correct: it's capitalized",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "maths/greatest_common_divisor.py",
    "pr_number": 9358,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349619563,
    "comment_created_at": "2023-10-08T04:14:53Z"
  },
  {
    "code": "@@ -0,0 +1,133 @@\n+\"\"\"\n+RC4 (Rivest Cipher 4) Algorithm:\n+The 'RC4' class below implements the RC4 stream cipher algorithm, also known as the Rivest Cipher 4.\n+RC4 generates a pseudorandom stream of bits (keystream) based on an initial secret key.\n+It is used for both encryption and decryption by XORing the plaintext with the keystream.\n+\n+Algorithm:\n+1. Key Scheduling: Initialize the S-box (state array) based on the secret key.\n+2. Stream Generation: Generate the keystream based on the S-box.\n+3. Encryption: XOR the plaintext with the keystream to produce the ciphertext.\n+4. Decryption: Decryption is the same as encryption, as RC4 is a symmetric cipher.\n+\n+References:\n+https://en.wikipedia.org/wiki/RC4\n+https://tools.ietf.org/html/rfc6229\n+\"\"\"\n+\n+class RC4:\n+    def __init__(self, key: bytes):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 19,
    "enriched": "File: ciphers/rivest_cipher.py\nCode: @@ -0,0 +1,133 @@\n+\"\"\"\n+RC4 (Rivest Cipher 4) Algorithm:\n+The 'RC4' class below implements the RC4 stream cipher algorithm, also known as the Rivest Cipher 4.\n+RC4 generates a pseudorandom stream of bits (keystream) based on an initial secret key.\n+It is used for both encryption and decryption by XORing the plaintext with the keystream.\n+\n+Algorithm:\n+1. Key Scheduling: Initialize the S-box (state array) based on the secret key.\n+2. Stream Generation: Generate the keystream based on the S-box.\n+3. Encryption: XOR the plaintext with the keystream to produce the ciphertext.\n+4. Decryption: Decryption is the same as encryption, as RC4 is a symmetric cipher.\n+\n+References:\n+https://en.wikipedia.org/wiki/RC4\n+https://tools.ietf.org/html/rfc6229\n+\"\"\"\n+\n+class RC4:\n+    def __init__(self, key: bytes):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "ciphers/rivest_cipher.py",
    "pr_number": 10698,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1365704151,
    "comment_created_at": "2023-10-19T15:04:00Z"
  },
  {
    "code": "@@ -0,0 +1,68 @@\n+class TreeNode:\n+    def __init__(self, val=0, left=None, right=None):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: val\n\nplease provide type hint for the parameter: left\n\nplease provide type hint for the parameter: right",
    "line_number": 2,
    "enriched": "File: data_structures/binary_tree/max_sum_BST.py\nCode: @@ -0,0 +1,68 @@\n+class TreeNode:\n+    def __init__(self, val=0, left=None, right=None):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `val`\n\nPlease provide type hint for the parameter: `left`\n\nPlease provide type hint for the parameter: `right`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/binary_tree/max_sum_BST.py",
    "pr_number": 11832,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1789141902,
    "comment_created_at": "2024-10-06T15:48:11Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+class Node:\n+    def __init__(self, info):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: info",
    "line_number": 2,
    "enriched": "File: height_of_tree/tree.py\nCode: @@ -0,0 +1,53 @@\n+class Node:\n+    def __init__(self, info):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `info`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "height_of_tree/tree.py",
    "pr_number": 13170,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2403937102,
    "comment_created_at": "2025-10-04T12:00:11Z"
  },
  {
    "code": "@@ -72,7 +72,7 @@ def __len__(self) -> int:\n         >>> len(linked_list)\n         0\n         \"\"\"\n-        return len(tuple(iter(self)))\n+        return sum(1 for _ in self)",
    "comment": "the sum will call iter(self) inside,  so this code is still o(n).",
    "line_number": 75,
    "enriched": "File: data_structures/linked_list/singly_linked_list.py\nCode: @@ -72,7 +72,7 @@ def __len__(self) -> int:\n         >>> len(linked_list)\n         0\n         \"\"\"\n-        return len(tuple(iter(self)))\n+        return sum(1 for _ in self)\nComment: The sum will call iter(self) inside,  so this code is still O(n).    ",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "data_structures/linked_list/singly_linked_list.py",
    "pr_number": 8183,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1140565997,
    "comment_created_at": "2023-03-17T18:16:50Z"
  },
  {
    "code": "@@ -1,33 +1,32 @@\n-def permute(nums: list[int]) -> list[list[int]]:\n+def permute_iterative(nums: list[int]) -> list[list[int]]:\n     \"\"\"\n     Return all permutations.\n-    >>> from itertools import permutations\n-    >>> numbers= [1,2,3]\n-    >>> all(list(nums) in permute(numbers) for nums in permutations(numbers))\n-    True\n+\n+    >>> permute_backtrack([1, 2, 3])\n+    [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 2, 1], [3, 1, 2]]\n     \"\"\"\n-    result = []\n+    result: list[list[int]] = []\n     if len(nums) == 1:\n         return [nums.copy()]",
    "comment": "i think we should make the base case 0 rather than 1. this slightly extends the function to work on an empty list input, and this matches the behavior of itertools.permutations.",
    "line_number": 10,
    "enriched": "File: data_structures/arrays/permutations.py\nCode: @@ -1,33 +1,32 @@\n-def permute(nums: list[int]) -> list[list[int]]:\n+def permute_iterative(nums: list[int]) -> list[list[int]]:\n     \"\"\"\n     Return all permutations.\n-    >>> from itertools import permutations\n-    >>> numbers= [1,2,3]\n-    >>> all(list(nums) in permute(numbers) for nums in permutations(numbers))\n-    True\n+\n+    >>> permute_backtrack([1, 2, 3])\n+    [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 2, 1], [3, 1, 2]]\n     \"\"\"\n-    result = []\n+    result: list[list[int]] = []\n     if len(nums) == 1:\n         return [nums.copy()]\nComment: ```suggestion\r\n    if len(nums) == 0:\r\n        return [[]]\r\n```\r\nI think we should make the base case 0 rather than 1. This slightly extends the function to work on an empty list input, and this matches the behavior of `itertools.permutations`.",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "data_structures/arrays/permutations.py",
    "pr_number": 9007,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1301227239,
    "comment_created_at": "2023-08-22T08:14:07Z"
  },
  {
    "code": "@@ -0,0 +1,116 @@\n+import numpy as np\n+\n+def outcome_of_rolling_n_sided_dice_k_time(n_side: int, k_time: int):",
    "comment": "please provide return type hint for the function: outcome_of_rolling_n_sided_dice_k_time. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 3,
    "enriched": "File: maths/sum_of_outcomes_for_rolling_n_sided_dice_k_time.py\nCode: @@ -0,0 +1,116 @@\n+import numpy as np\n+\n+def outcome_of_rolling_n_sided_dice_k_time(n_side: int, k_time: int):\nComment: Please provide return type hint for the function: `outcome_of_rolling_n_sided_dice_k_time`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "maths/sum_of_outcomes_for_rolling_n_sided_dice_k_time.py",
    "pr_number": 11452,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1639717574,
    "comment_created_at": "2024-06-14T12:00:35Z"
  },
  {
    "code": "@@ -0,0 +1,28 @@\n+def octal_to_binary(octal_number):",
    "comment": "please provide return type hint for the function: octal_to_binary. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: octal_number",
    "line_number": 1,
    "enriched": "File: conversions/octal_to_binary.py\nCode: @@ -0,0 +1,28 @@\n+def octal_to_binary(octal_number):\nComment: Please provide return type hint for the function: `octal_to_binary`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `octal_number`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "conversions/octal_to_binary.py",
    "pr_number": 8949,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1291241973,
    "comment_created_at": "2023-08-11T11:43:21Z"
  },
  {
    "code": "@@ -5,7 +5,7 @@\n # Wikipedia reference: https://en.wikipedia.org/wiki/Fermat%27s_little_theorem\n \n \n-def binary_exponentiation(a, n, mod):\n+def binary_exponentiation(a: int, n: float | int, mod: int) -> int:",
    "comment": "per the ruff error, just typehint it as float rather than float | int",
    "line_number": 8,
    "enriched": "File: maths/fermat_little_theorem.py\nCode: @@ -5,7 +5,7 @@\n # Wikipedia reference: https://en.wikipedia.org/wiki/Fermat%27s_little_theorem\n \n \n-def binary_exponentiation(a, n, mod):\n+def binary_exponentiation(a: int, n: float | int, mod: int) -> int:\nComment: ```suggestion\r\ndef binary_exponentiation(a: int, n: float, mod: int) -> int:\r\n```\r\nPer the ruff error, just typehint it as `float` rather than `float | int`",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "maths/fermat_little_theorem.py",
    "pr_number": 9794,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347264559,
    "comment_created_at": "2023-10-05T11:26:55Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+\"\"\"\n+LeetCode 133. Clone Graph\n+https://leetcode.com/problems/clone-graph/\n+\n+Given a reference of a node in a connected undirected graph.\n+\n+Return a deep copy (clone) of the graph.\n+\n+Each node in the graph contains a value (int) and a list (List[Node]) of its\n+neighbors.\n+\"\"\"\n+\n+\n+class Node:\n+    def __init__(self, value=0, neighbors=None):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: value\n\nplease provide type hint for the parameter: neighbors",
    "line_number": 15,
    "enriched": "File: graphs/deep-clone-graph.py\nCode: @@ -0,0 +1,60 @@\n+\"\"\"\n+LeetCode 133. Clone Graph\n+https://leetcode.com/problems/clone-graph/\n+\n+Given a reference of a node in a connected undirected graph.\n+\n+Return a deep copy (clone) of the graph.\n+\n+Each node in the graph contains a value (int) and a list (List[Node]) of its\n+neighbors.\n+\"\"\"\n+\n+\n+class Node:\n+    def __init__(self, value=0, neighbors=None):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `value`\n\nPlease provide type hint for the parameter: `neighbors`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "graphs/deep-clone-graph.py",
    "pr_number": 9765,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346678516,
    "comment_created_at": "2023-10-05T01:54:42Z"
  },
  {
    "code": "@@ -0,0 +1,63 @@\n+\"\"\"\n+This is a pure Python implementation of the binary insertion sort algorithm\n+\n+For doctests run following command:\n+python -m doctest -v binary_insertion_sort.py\n+or\n+python3 -m doctest -v binary_insertion_sort.py\n+\n+For manual testing run:\n+python binary_insertion_sort.py\n+\"\"\"\n+\n+\n+def binary_insertion_sort(collection):",
    "comment": "please provide return type hint for the function: binary_insertion_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: collection",
    "line_number": 14,
    "enriched": "File: sorts/binary_insertion_sort.py\nCode: @@ -0,0 +1,63 @@\n+\"\"\"\n+This is a pure Python implementation of the binary insertion sort algorithm\n+\n+For doctests run following command:\n+python -m doctest -v binary_insertion_sort.py\n+or\n+python3 -m doctest -v binary_insertion_sort.py\n+\n+For manual testing run:\n+python binary_insertion_sort.py\n+\"\"\"\n+\n+\n+def binary_insertion_sort(collection):\nComment: Please provide return type hint for the function: `binary_insertion_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `collection`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "sorts/binary_insertion_sort.py",
    "pr_number": 8024,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1042834811,
    "comment_created_at": "2022-12-08T01:32:41Z"
  },
  {
    "code": "@@ -0,0 +1,168 @@\n+\"\"\"\n+This program is a MNIST classifier using AlexNet. It accepts three parameters provided as a command line input. \n+The first two inputs are two digits between 0-9 which are used to train and test the classifier and the third \n+parameter controls the number of training epochs.\n+Syntax: python program.py <number> <number> <number>\n+\n+For example, to train and test AlexNet with 1 and 2 MNIST samples with 4 training epochs, the command line input should be:\n+python program.py 1 2 4\n+\n+\"\"\"\n+\n+\n+import sys\n+import torch\n+import torch.nn as nn\n+import torchvision.datasets as dset\n+import torchvision.transforms as transforms\n+from torch.autograd import Variable\n+import torch.nn.functional as F\n+import torch.optim as optim\n+\n+\n+class AlexNet(nn.Module):   \n+    def __init__(self, num=10):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: num",
    "line_number": 24,
    "enriched": "File: computer_vision/mnist_classifier.py\nCode: @@ -0,0 +1,168 @@\n+\"\"\"\n+This program is a MNIST classifier using AlexNet. It accepts three parameters provided as a command line input. \n+The first two inputs are two digits between 0-9 which are used to train and test the classifier and the third \n+parameter controls the number of training epochs.\n+Syntax: python program.py <number> <number> <number>\n+\n+For example, to train and test AlexNet with 1 and 2 MNIST samples with 4 training epochs, the command line input should be:\n+python program.py 1 2 4\n+\n+\"\"\"\n+\n+\n+import sys\n+import torch\n+import torch.nn as nn\n+import torchvision.datasets as dset\n+import torchvision.transforms as transforms\n+from torch.autograd import Variable\n+import torch.nn.functional as F\n+import torch.optim as optim\n+\n+\n+class AlexNet(nn.Module):   \n+    def __init__(self, num=10):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `num`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "computer_vision/mnist_classifier.py",
    "pr_number": 12325,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1821881978,
    "comment_created_at": "2024-10-30T05:05:17Z"
  },
  {
    "code": "@@ -4,8 +4,9 @@\n def get_index_of_rightmost_set_bit(number: int) -> int:\n     \"\"\"\n     Take in a positive integer 'number'.\n-    Returns the zero-based index of first set bit in that 'number' from right.\n-    Returns -1, If no set bit found.\n+    Return the zero-based index of the first set bit in this 'number'\n+    starting from right.\n+    Return -1, if the set bit is not found.",
    "comment": "minor formatting",
    "line_number": 9,
    "enriched": "File: bit_manipulation/index_of_rightmost_set_bit.py\nCode: @@ -4,8 +4,9 @@\n def get_index_of_rightmost_set_bit(number: int) -> int:\n     \"\"\"\n     Take in a positive integer 'number'.\n-    Returns the zero-based index of first set bit in that 'number' from right.\n-    Returns -1, If no set bit found.\n+    Return the zero-based index of the first set bit in this 'number'\n+    starting from right.\n+    Return -1, if the set bit is not found.\nComment: ```suggestion\r\n    Return the zero-based index of the first set bit in 'number' starting from right.\r\n    Return -1 if the set bit is not found.\r\n```\r\nMinor formatting",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "bit_manipulation/index_of_rightmost_set_bit.py",
    "pr_number": 9074,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1335106012,
    "comment_created_at": "2023-09-24T05:16:24Z"
  },
  {
    "code": "@@ -20,6 +77,12 @@ def __repr__(self) -> str:\n             return str(self.value)\n         return pformat({f\"{self.value}\": (self.left, self.right)}, indent=1)\n \n+    @property\n+    def is_right(self) -> bool:\n+        if self.parent and self.parent.right:\n+            return self == self.parent.right\n+        return False",
    "comment": "i think this function can be simplified like this:\r\n\r\n\r\n\r\n1) == checks if two variables have the same value, while is checks if they refer to the same object, so i think is is more appropriate here.\r\n\r\n2) we don't need to check if self.parent.right is not none: if it _is_ none, then self is self.parent.right would fail and the function would return false anyway.",
    "line_number": 84,
    "enriched": "File: data_structures/binary_tree/binary_search_tree.py\nCode: @@ -20,6 +77,12 @@ def __repr__(self) -> str:\n             return str(self.value)\n         return pformat({f\"{self.value}\": (self.left, self.right)}, indent=1)\n \n+    @property\n+    def is_right(self) -> bool:\n+        if self.parent and self.parent.right:\n+            return self == self.parent.right\n+        return False\nComment: I think this function can be simplified like this:\r\n\r\n```suggestion\r\n        return self.parent and self is self.parent.right\r\n```\r\n\r\n1) `==` checks if two variables have the same value, while `is` checks if they refer to the same object, so I think `is` is more appropriate here.\r\n\r\n2) We don't need to check if `self.parent.right` is not None: if it _is_ None, then `self is self.parent.right` would fail and the function would return False anyway.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "data_structures/binary_tree/binary_search_tree.py",
    "pr_number": 8693,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1292912236,
    "comment_created_at": "2023-08-14T02:08:35Z"
  },
  {
    "code": "@@ -0,0 +1,149 @@\n+import numpy as np\n+\n+class DecisionTree:\n+    \"\"\"\n+    Decision Tree classifier.\n+\n+    Parameters:\n+        max_depth (int): Maximum depth of the tree. If None, the tree grows until pure nodes or min_samples_split is reached.\n+\n+    Attributes:\n+        tree (tuple): The decision tree structure.\n+\n+    Examples:\n+        >>> np.random.seed(42)\n+        >>> X = np.random.rand(100, 2)\n+        >>> y = (X[:, 0] + X[:, 1] > 1).astype(int)\n+        >>> tree = DecisionTree(max_depth=3)\n+        >>> tree.fit(X, y)\n+        >>> predictions = tree.predict(np.array([[0.7, 0.3], [0.2, 0.8]]))\n+    \"\"\"\n+\n+    def __init__(self, max_depth=None):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: max_depth",
    "line_number": 22,
    "enriched": "File: machine_learning/random_forest_classifier.py\nCode: @@ -0,0 +1,149 @@\n+import numpy as np\n+\n+class DecisionTree:\n+    \"\"\"\n+    Decision Tree classifier.\n+\n+    Parameters:\n+        max_depth (int): Maximum depth of the tree. If None, the tree grows until pure nodes or min_samples_split is reached.\n+\n+    Attributes:\n+        tree (tuple): The decision tree structure.\n+\n+    Examples:\n+        >>> np.random.seed(42)\n+        >>> X = np.random.rand(100, 2)\n+        >>> y = (X[:, 0] + X[:, 1] > 1).astype(int)\n+        >>> tree = DecisionTree(max_depth=3)\n+        >>> tree.fit(X, y)\n+        >>> predictions = tree.predict(np.array([[0.7, 0.3], [0.2, 0.8]]))\n+    \"\"\"\n+\n+    def __init__(self, max_depth=None):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `max_depth`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "machine_learning/random_forest_classifier.py",
    "pr_number": 9770,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346725855,
    "comment_created_at": "2023-10-05T03:14:56Z"
  },
  {
    "code": "@@ -0,0 +1,83 @@\n+import doctest\n+\n+class Polynomial:\n+    def __init__(self, coefficients):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: coefficients",
    "line_number": 4,
    "enriched": "File: maths/polynomials/polynomial_manipulation.py\nCode: @@ -0,0 +1,83 @@\n+import doctest\n+\n+class Polynomial:\n+    def __init__(self, coefficients):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `coefficients`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "maths/polynomials/polynomial_manipulation.py",
    "pr_number": 9424,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342423495,
    "comment_created_at": "2023-10-02T08:43:12Z"
  },
  {
    "code": "@@ -90,6 +111,66 @@ def calculate_current_element(\n     current_row[current_col_idx] = above_to_left_elt + above_to_right_elt\n \n \n+def generate_pascal_triangle_optimized(num_rows: int) -> list[list[int]]:\n+    \"\"\"\n+    This function returns a matrix representing the corresponding pascal's triangle\n+    according to the given input of number of rows of Pascal's triangle to be generated.\n+    It reduces the operations done to generate a row by half\n+    by eliminating redundant calculations.\n+\n+    :param num_rows: Integer specifying the number of rows in the Pascal's triangle\n+    :return: 2-D List (matrix) representing the Pascal's triangle\n+\n+    Return the Pascal's triangle of given rows\n+    >>> generate_pascal_triangle_optimized(3)\n+    [[1], [1, 1], [1, 2, 1]]\n+    >>> generate_pascal_triangle_optimized(1)\n+    [[1]]\n+    >>> generate_pascal_triangle_optimized(0)\n+    []\n+    >>> generate_pascal_triangle_optimized(-5)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The input value of 'num_rows' should be greater than or equal to 0\n+    >>> generate_pascal_triangle_optimized(7.89)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: The input value of 'num_rows' should be 'int'\n+    \"\"\"\n+\n+    if not isinstance(num_rows, int):\n+        raise TypeError(\"The input value of 'num_rows' should be 'int'\")\n+\n+    if num_rows == 0:\n+        return []\n+    elif num_rows < 0:\n+        raise ValueError(\n+            \"The input value of 'num_rows' should be greater than or equal to 0\"\n+        )\n+\n+    result: list[list[int]] = [[1]]\n+\n+    for row_index in range(1, num_rows):\n+        temp_row = [0] + result[-1] + [0]\n+        row_length = row_index + 1\n+\n+        # It calculates the number of distinct elements in a row\n+        distinct_elements = (\n+            row_length // 2 if row_length % 2 == 0 else row_length // 2 + 1\n+        )",
    "comment": "leverage the [python builtin functions](https://docs.python.org/3/library/functions.html):",
    "line_number": 160,
    "enriched": "File: other/pascal_triangle.py\nCode: @@ -90,6 +111,66 @@ def calculate_current_element(\n     current_row[current_col_idx] = above_to_left_elt + above_to_right_elt\n \n \n+def generate_pascal_triangle_optimized(num_rows: int) -> list[list[int]]:\n+    \"\"\"\n+    This function returns a matrix representing the corresponding pascal's triangle\n+    according to the given input of number of rows of Pascal's triangle to be generated.\n+    It reduces the operations done to generate a row by half\n+    by eliminating redundant calculations.\n+\n+    :param num_rows: Integer specifying the number of rows in the Pascal's triangle\n+    :return: 2-D List (matrix) representing the Pascal's triangle\n+\n+    Return the Pascal's triangle of given rows\n+    >>> generate_pascal_triangle_optimized(3)\n+    [[1], [1, 1], [1, 2, 1]]\n+    >>> generate_pascal_triangle_optimized(1)\n+    [[1]]\n+    >>> generate_pascal_triangle_optimized(0)\n+    []\n+    >>> generate_pascal_triangle_optimized(-5)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The input value of 'num_rows' should be greater than or equal to 0\n+    >>> generate_pascal_triangle_optimized(7.89)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: The input value of 'num_rows' should be 'int'\n+    \"\"\"\n+\n+    if not isinstance(num_rows, int):\n+        raise TypeError(\"The input value of 'num_rows' should be 'int'\")\n+\n+    if num_rows == 0:\n+        return []\n+    elif num_rows < 0:\n+        raise ValueError(\n+            \"The input value of 'num_rows' should be greater than or equal to 0\"\n+        )\n+\n+    result: list[list[int]] = [[1]]\n+\n+    for row_index in range(1, num_rows):\n+        temp_row = [0] + result[-1] + [0]\n+        row_length = row_index + 1\n+\n+        # It calculates the number of distinct elements in a row\n+        distinct_elements = (\n+            row_length // 2 if row_length % 2 == 0 else row_length // 2 + 1\n+        )\nComment: Leverage the [Python builtin functions](https://docs.python.org/3/library/functions.html):\r\n```suggestion\r\n        distinct_elements = sum(divmod(row_length, 2))\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "other/pascal_triangle.py",
    "pr_number": 7901,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1009141034,
    "comment_created_at": "2022-10-31T08:24:51Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+# Huffman Coding in python\n+\n+string = 'DADBOBHEHHOOEEDDDD'\n+\n+\n+# Creating tree nodes\n+class NodeTree(object):\n+\n+    def __init__(self, left=None, right=None):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: left\n\nplease provide type hint for the parameter: right",
    "line_number": 9,
    "enriched": "File: greedy_methods/huffman_coding.py\nCode: @@ -0,0 +1,60 @@\n+# Huffman Coding in python\n+\n+string = 'DADBOBHEHHOOEEDDDD'\n+\n+\n+# Creating tree nodes\n+class NodeTree(object):\n+\n+    def __init__(self, left=None, right=None):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `left`\n\nPlease provide type hint for the parameter: `right`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "greedy_methods/huffman_coding.py",
    "pr_number": 7584,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002757625,
    "comment_created_at": "2022-10-23T19:29:14Z"
  },
  {
    "code": "@@ -0,0 +1,128 @@\n+from flask import Flask, render_template_string\n+import requests\n+\n+app = Flask(__name__)\n+\n+# Public APIs for real-time global data (you can replace or add more as needed)\n+COVID_API_URL = \"https://disease.sh/v3/covid-19/countries\"  # Get COVID-19 stats by country\n+NEWS_API_URL = \"https://gnews.io/api/v4/top-headlines?token=YOUR_API_KEY&lang=en\"  # Replace with your API key\n+\n+# Base HTML template\n+BASE_TEMPLATE = \"\"\"\n+<!DOCTYPE html>\n+<html lang=\"en\">\n+<head>\n+    <meta charset=\"UTF-8\">\n+    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n+    <title>Global Events Tracker</title>\n+    <style>\n+        body { font-family: Arial, sans-serif; margin: 0; padding: 0; }\n+        header { background-color: #007bff; color: white; padding: 15px; text-align: center; }\n+        nav a { margin: 0 15px; color: white; text-decoration: none; }\n+        nav { margin-bottom: 20px; }\n+        main { padding: 20px; }\n+        ul { list-style-type: none; padding: 0; }\n+        li { margin: 15px 0; padding: 10px; border: 1px solid #ddd; background-color: #f9f9f9; }\n+        .event-title { font-size: 1.2em; font-weight: bold; }\n+        .event-meta { font-size: 0.9em; color: #555; }\n+        .description { margin-top: 10px; }\n+    </style>\n+</head>\n+<body>\n+    <header>\n+        <h1>Global Events Tracker</h1>\n+        <nav>\n+            <a href=\"/\">Home</a>\n+            <a href=\"/covid-stats\">COVID-19 Stats</a>\n+            <a href=\"/news\">Global News</a>\n+        </nav>\n+    </header>\n+    <main>\n+        {% block content %}{% endblock %}\n+    </main>\n+</body>\n+</html>\n+\"\"\"\n+\n+# Index (Home) Template\n+INDEX_TEMPLATE = \"\"\"\n+{% extends \"base.html\" %}\n+{% block content %}\n+<h2>Welcome to the Global Events Tracker</h2>\n+<p>Track live global events like COVID-19 stats and global news headlines from reliable sources.</p>\n+<ul>\n+    <li><a href=\"/covid-stats\">View COVID-19 Global Stats</a></li>\n+    <li><a href=\"/news\">View Latest Global News</a></li>\n+</ul>\n+{% endblock %}\n+\"\"\"\n+\n+# COVID-19 Stats Template\n+COVID_TEMPLATE = \"\"\"\n+{% extends \"base.html\" %}\n+{% block content %}\n+<h2>Global COVID-19 Stats</h2>\n+<p>Real-time data from the disease.sh API</p>\n+<ul>\n+    {% for country in covid_data %}\n+        <li>\n+            <div class=\"event-title\">{{ country['country'] }}</div>\n+            <div class=\"event-meta\">Cases: {{ country['cases'] }} | Deaths: {{ country['deaths'] }} | Recovered: {{ country['recovered'] }}</div>\n+        </li>\n+    {% else %}\n+        <li>No data available</li>\n+    {% endfor %}\n+</ul>\n+{% endblock %}\n+\"\"\"\n+\n+# News Template\n+NEWS_TEMPLATE = \"\"\"\n+{% extends \"base.html\" %}\n+{% block content %}\n+<h2>Latest Global News</h2>\n+<p>Real-time news fetched from the GNews API</p>\n+<ul>\n+    {% for article in news_data %}\n+        <li>\n+            <div class=\"event-title\"><a href=\"{{ article['url'] }}\" target=\"_blank\">{{ article['title'] }}</a></div>\n+            <div class=\"event-meta\">Source: {{ article['source']['name'] }} | Published: {{ article['publishedAt'][:10] }}</div>\n+            <p class=\"description\">{{ article['description'] }}</p>\n+        </li>\n+    {% else %}\n+        <li>No news articles found</li>\n+    {% endfor %}\n+</ul>\n+{% endblock %}\n+\"\"\"\n+\n+@app.route('/')\n+def index():",
    "comment": "please provide return type hint for the function: index. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 100,
    "enriched": "File: web_programming/Global Events Tracker.py\nCode: @@ -0,0 +1,128 @@\n+from flask import Flask, render_template_string\n+import requests\n+\n+app = Flask(__name__)\n+\n+# Public APIs for real-time global data (you can replace or add more as needed)\n+COVID_API_URL = \"https://disease.sh/v3/covid-19/countries\"  # Get COVID-19 stats by country\n+NEWS_API_URL = \"https://gnews.io/api/v4/top-headlines?token=YOUR_API_KEY&lang=en\"  # Replace with your API key\n+\n+# Base HTML template\n+BASE_TEMPLATE = \"\"\"\n+<!DOCTYPE html>\n+<html lang=\"en\">\n+<head>\n+    <meta charset=\"UTF-8\">\n+    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n+    <title>Global Events Tracker</title>\n+    <style>\n+        body { font-family: Arial, sans-serif; margin: 0; padding: 0; }\n+        header { background-color: #007bff; color: white; padding: 15px; text-align: center; }\n+        nav a { margin: 0 15px; color: white; text-decoration: none; }\n+        nav { margin-bottom: 20px; }\n+        main { padding: 20px; }\n+        ul { list-style-type: none; padding: 0; }\n+        li { margin: 15px 0; padding: 10px; border: 1px solid #ddd; background-color: #f9f9f9; }\n+        .event-title { font-size: 1.2em; font-weight: bold; }\n+        .event-meta { font-size: 0.9em; color: #555; }\n+        .description { margin-top: 10px; }\n+    </style>\n+</head>\n+<body>\n+    <header>\n+        <h1>Global Events Tracker</h1>\n+        <nav>\n+            <a href=\"/\">Home</a>\n+            <a href=\"/covid-stats\">COVID-19 Stats</a>\n+            <a href=\"/news\">Global News</a>\n+        </nav>\n+    </header>\n+    <main>\n+        {% block content %}{% endblock %}\n+    </main>\n+</body>\n+</html>\n+\"\"\"\n+\n+# Index (Home) Template\n+INDEX_TEMPLATE = \"\"\"\n+{% extends \"base.html\" %}\n+{% block content %}\n+<h2>Welcome to the Global Events Tracker</h2>\n+<p>Track live global events like COVID-19 stats and global news headlines from reliable sources.</p>\n+<ul>\n+    <li><a href=\"/covid-stats\">View COVID-19 Global Stats</a></li>\n+    <li><a href=\"/news\">View Latest Global News</a></li>\n+</ul>\n+{% endblock %}\n+\"\"\"\n+\n+# COVID-19 Stats Template\n+COVID_TEMPLATE = \"\"\"\n+{% extends \"base.html\" %}\n+{% block content %}\n+<h2>Global COVID-19 Stats</h2>\n+<p>Real-time data from the disease.sh API</p>\n+<ul>\n+    {% for country in covid_data %}\n+        <li>\n+            <div class=\"event-title\">{{ country['country'] }}</div>\n+            <div class=\"event-meta\">Cases: {{ country['cases'] }} | Deaths: {{ country['deaths'] }} | Recovered: {{ country['recovered'] }}</div>\n+        </li>\n+    {% else %}\n+        <li>No data available</li>\n+    {% endfor %}\n+</ul>\n+{% endblock %}\n+\"\"\"\n+\n+# News Template\n+NEWS_TEMPLATE = \"\"\"\n+{% extends \"base.html\" %}\n+{% block content %}\n+<h2>Latest Global News</h2>\n+<p>Real-time news fetched from the GNews API</p>\n+<ul>\n+    {% for article in news_data %}\n+        <li>\n+            <div class=\"event-title\"><a href=\"{{ article['url'] }}\" target=\"_blank\">{{ article['title'] }}</a></div>\n+            <div class=\"event-meta\">Source: {{ article['source']['name'] }} | Published: {{ article['publishedAt'][:10] }}</div>\n+            <p class=\"description\">{{ article['description'] }}</p>\n+        </li>\n+    {% else %}\n+        <li>No news articles found</li>\n+    {% endfor %}\n+</ul>\n+{% endblock %}\n+\"\"\"\n+\n+@app.route('/')\n+def index():\nComment: Please provide return type hint for the function: `index`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "web_programming/Global Events Tracker.py",
    "pr_number": 11722,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1786849815,
    "comment_created_at": "2024-10-03T21:12:13Z"
  },
  {
    "code": "@@ -0,0 +1,52 @@\n+''' This file provides a function which will take  \n+a product name as input from the user,and fetch the necessary  \n+information about that kind of products from Amazon like the product \n+title,link to that product,price of the product,the ratings of \n+the product and the discount available on the product  \n+in the form of a csv file,this will help the users by improving searchability  \n+and navigability and find the right product easily and in a short period of time, \n+it will also be beneficial for performing better analysis on products'''         \n+\n+\n+\n+\n+import pandas as pd \n+from bs4 import BeautifulSoup as bs \n+import requests \n+import itertools as it    \n+\n+\n+\n+\n+def get_product_info(product = 'laptop'):                 #function that will take the product as input and return the product details as output in the form of a csv file,if no input is given,it will fetch the details of laptop by default    ",
    "comment": "please provide return type hint for the function: get_product_info. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: product",
    "line_number": 21,
    "enriched": "File: web_programming/fetch_amazon_product_data.py\nCode: @@ -0,0 +1,52 @@\n+''' This file provides a function which will take  \n+a product name as input from the user,and fetch the necessary  \n+information about that kind of products from Amazon like the product \n+title,link to that product,price of the product,the ratings of \n+the product and the discount available on the product  \n+in the form of a csv file,this will help the users by improving searchability  \n+and navigability and find the right product easily and in a short period of time, \n+it will also be beneficial for performing better analysis on products'''         \n+\n+\n+\n+\n+import pandas as pd \n+from bs4 import BeautifulSoup as bs \n+import requests \n+import itertools as it    \n+\n+\n+\n+\n+def get_product_info(product = 'laptop'):                 #function that will take the product as input and return the product details as output in the form of a csv file,if no input is given,it will fetch the details of laptop by default    \nComment: Please provide return type hint for the function: `get_product_info`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `product`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "web_programming/fetch_amazon_product_data.py",
    "pr_number": 7585,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002880769,
    "comment_created_at": "2022-10-24T04:51:45Z"
  },
  {
    "code": "@@ -0,0 +1,46 @@\n+# Adjascency List representation in Python\n+\n+\n+class AdjNode:\n+    def __init__(self, value):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: value",
    "line_number": 5,
    "enriched": "File: data_structures/linked_list/adjascency_list.py\nCode: @@ -0,0 +1,46 @@\n+# Adjascency List representation in Python\n+\n+\n+class AdjNode:\n+    def __init__(self, value):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `value`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/linked_list/adjascency_list.py",
    "pr_number": 7562,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002719001,
    "comment_created_at": "2022-10-23T14:24:31Z"
  },
  {
    "code": "@@ -0,0 +1,66 @@\n+\"\"\"\n+This is a pure Python implementation of the sleep algorithm,\n+In general, sleep sort works by starting a separate task for \n+each item to be sorted, where each task sleeps for an interval \n+corresponding to the item's sort key, then emits the item. \n+Items are then collected sequentially in time. \n+\n+More info on: https://rosettacode.org/wiki/Sorting_algorithms/Sleep_sort\n+\n+For doctests run following command:\n+python -m doctest -v sleep_sort.py\n+or\n+python3 -m doctest -v sleep_sort.py\n+For manual testing run:\n+python sleep_sort.py\n+\"\"\"\n+\n+from time import sleep\n+from threading import Timer\n+\n+def sleep_sort(collection):",
    "comment": "please provide return type hint for the function: sleep_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: collection",
    "line_number": 21,
    "enriched": "File: sorts/sleep_sort.py\nCode: @@ -0,0 +1,66 @@\n+\"\"\"\n+This is a pure Python implementation of the sleep algorithm,\n+In general, sleep sort works by starting a separate task for \n+each item to be sorted, where each task sleeps for an interval \n+corresponding to the item's sort key, then emits the item. \n+Items are then collected sequentially in time. \n+\n+More info on: https://rosettacode.org/wiki/Sorting_algorithms/Sleep_sort\n+\n+For doctests run following command:\n+python -m doctest -v sleep_sort.py\n+or\n+python3 -m doctest -v sleep_sort.py\n+For manual testing run:\n+python sleep_sort.py\n+\"\"\"\n+\n+from time import sleep\n+from threading import Timer\n+\n+def sleep_sort(collection):\nComment: Please provide return type hint for the function: `sleep_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `collection`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "sorts/sleep_sort.py",
    "pr_number": 6951,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 991633950,
    "comment_created_at": "2022-10-10T20:55:57Z"
  },
  {
    "code": "@@ -0,0 +1,138 @@\n+import random\n+import sys\n+from sympy import isprime, mod_inverse\n+\n+def generate_prime_candidate(length):",
    "comment": "please provide return type hint for the function: generate_prime_candidate. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: length",
    "line_number": 5,
    "enriched": "File: other/RSA_Algo/rsa_algo.py\nCode: @@ -0,0 +1,138 @@\n+import random\n+import sys\n+from sympy import isprime, mod_inverse\n+\n+def generate_prime_candidate(length):\nComment: Please provide return type hint for the function: `generate_prime_candidate`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `length`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "other/RSA_Algo/rsa_algo.py",
    "pr_number": 11869,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1790742856,
    "comment_created_at": "2024-10-07T19:09:51Z"
  },
  {
    "code": "@@ -0,0 +1,71 @@\n+class Vertex:\n+    def __init__(self, name):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: name",
    "line_number": 2,
    "enriched": "File: graphs/find_minimal_spanning_tree.py\nCode: @@ -0,0 +1,71 @@\n+class Vertex:\n+    def __init__(self, name):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `name`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "graphs/find_minimal_spanning_tree.py",
    "pr_number": 9873,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1348008558,
    "comment_created_at": "2023-10-05T21:35:30Z"
  },
  {
    "code": "@@ -53,7 +53,20 @@ def __repr__(self) -> str:\n         return str(self.h)\n \n     def parent_index(self, child_idx: int) -> int | None:\n-        \"\"\"return the parent index of given child\"\"\"\n+        \"\"\"\n+        return the parent index of given child\n+\n+        >>> h = Heap()\n+        >>> h.build_max_heap([103, 9, 1, 7, 11, 15, 25, 201, 209, 107, 5])",
    "comment": "let's test with zero, negative integers, floating point numbers, and strings.\r\n\r\nlet's test h.parent_index(12) which is not in the heap.\r\n\r\nwe want to test how the algorithm works but also how it fails when given bad input.",
    "line_number": 60,
    "enriched": "File: data_structures/heap/heap.py\nCode: @@ -53,7 +53,20 @@ def __repr__(self) -> str:\n         return str(self.h)\n \n     def parent_index(self, child_idx: int) -> int | None:\n-        \"\"\"return the parent index of given child\"\"\"\n+        \"\"\"\n+        return the parent index of given child\n+\n+        >>> h = Heap()\n+        >>> h.build_max_heap([103, 9, 1, 7, 11, 15, 25, 201, 209, 107, 5])\nComment: Let's test with zero, negative integers, floating point numbers, and strings.\r\n\r\nLet's test `h.parent_index(12)` which is NOT in the heap.\r\n\r\nWe want to test how the algorithm works but also how it fails when given bad input.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/heap/heap.py",
    "pr_number": 11129,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1382504684,
    "comment_created_at": "2023-11-05T04:31:18Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+\"\"\"\n+Calculate the Simple Moving Average (SMA) for a time series data.\n+https://en.wikipedia.org/wiki/Moving_average\n+\"\"\"\n+\n+\n+def simple_moving_average(data, window_size):",
    "comment": "please provide return type hint for the function: simple_moving_average. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: data\n\nplease provide type hint for the parameter: window_size",
    "line_number": 7,
    "enriched": "File: financial/simple_moving_average.py\nCode: @@ -0,0 +1,58 @@\n+\"\"\"\n+Calculate the Simple Moving Average (SMA) for a time series data.\n+https://en.wikipedia.org/wiki/Moving_average\n+\"\"\"\n+\n+\n+def simple_moving_average(data, window_size):\nComment: Please provide return type hint for the function: `simple_moving_average`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `data`\n\nPlease provide type hint for the parameter: `window_size`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "financial/simple_moving_average.py",
    "pr_number": 9291,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342133552,
    "comment_created_at": "2023-10-01T13:04:48Z"
  },
  {
    "code": "@@ -0,0 +1,62 @@\n+#Atomical Length Unit Conversion\n+\n+# REFRENCE  = https://en.wikipedia.org/wiki/Orders_of_magnitude_(length)\n+\n+def convert_to_meters(value, unit):",
    "comment": "please provide return type hint for the function: convert_to_meters. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: value\n\nplease provide type hint for the parameter: unit",
    "line_number": 5,
    "enriched": "File: conversions/Atomical_Length_conversion.py\nCode: @@ -0,0 +1,62 @@\n+#Atomical Length Unit Conversion\n+\n+# REFRENCE  = https://en.wikipedia.org/wiki/Orders_of_magnitude_(length)\n+\n+def convert_to_meters(value, unit):\nComment: Please provide return type hint for the function: `convert_to_meters`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `value`\n\nPlease provide type hint for the parameter: `unit`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "conversions/Atomical_Length_conversion.py",
    "pr_number": 10329,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1357084114,
    "comment_created_at": "2023-10-12T16:34:43Z"
  },
  {
    "code": "@@ -0,0 +1,113 @@\n+\"\"\"\n+\u2014\u2014\u2192 Introduction to Pytorch Autograd (TORCH.AUTOGRAD) :-\n+\n+>>> [torch.autograd] is Pytorch's automatic differential engine that powers neural network training.\n+\n+\u2014\u2192 Background :-\n+\n+>>> Neural network (NN's) are a collection of nested function that are executed on some input data.\n+    These functions are defined by parameters (consist of weights & biases).\n+\n+        Training a NN happens in two steps:\n+\n+        \u2192 Forward Propagation :- In forward prop, NN makes its best guess about the correct output.\n+                                It runs the input data through each of its functions to make this guess.\n+\n+        \u2192 Backward Propagation :- In backward, NN adjusts its parameters proportionate to the error in its guess.\n+                                  It does this by traveling backwards from the input, collecting derivative of the\n+                                  error with respect to the parameters of the functions(gradients), and optimizing\n+                                  the parameters using gradient descent.\n+\n+\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\n+\n+\u2014\u2014\u2192 Defining New Autograd Functions\n+\n+>>> In this example we define our own model as y = a + b * P\u2083(c + dx) instead of y = a + b*x + c*x\u00b2 + d*x\u00b3, where\n+    P\u2083(x) = (1/2)*(5*x\u00b3 - 3*x) is the legendre polynomial of degree three.\n+\"\"\"\n+\n+\n+import torch\n+import math\n+\n+\n+class LegendrePolynomial3(torch.autograd.Function):\n+    \"\"\"\n+    We can implement our own custom autograd Functions by subclassing\n+    torch.autograd.Function and implementing the forward and backward passes\n+    which operate on Tensors.\n+    \"\"\"\n+\n+    @staticmethod\n+    def forward(ctx, input):",
    "comment": "please provide return type hint for the function: forward. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: ctx\n\nplease provide type hint for the parameter: input",
    "line_number": 42,
    "enriched": "File: neural_network/legendre_function.py\nCode: @@ -0,0 +1,113 @@\n+\"\"\"\n+\u2014\u2014\u2192 Introduction to Pytorch Autograd (TORCH.AUTOGRAD) :-\n+\n+>>> [torch.autograd] is Pytorch's automatic differential engine that powers neural network training.\n+\n+\u2014\u2192 Background :-\n+\n+>>> Neural network (NN's) are a collection of nested function that are executed on some input data.\n+    These functions are defined by parameters (consist of weights & biases).\n+\n+        Training a NN happens in two steps:\n+\n+        \u2192 Forward Propagation :- In forward prop, NN makes its best guess about the correct output.\n+                                It runs the input data through each of its functions to make this guess.\n+\n+        \u2192 Backward Propagation :- In backward, NN adjusts its parameters proportionate to the error in its guess.\n+                                  It does this by traveling backwards from the input, collecting derivative of the\n+                                  error with respect to the parameters of the functions(gradients), and optimizing\n+                                  the parameters using gradient descent.\n+\n+\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\n+\n+\u2014\u2014\u2192 Defining New Autograd Functions\n+\n+>>> In this example we define our own model as y = a + b * P\u2083(c + dx) instead of y = a + b*x + c*x\u00b2 + d*x\u00b3, where\n+    P\u2083(x) = (1/2)*(5*x\u00b3 - 3*x) is the legendre polynomial of degree three.\n+\"\"\"\n+\n+\n+import torch\n+import math\n+\n+\n+class LegendrePolynomial3(torch.autograd.Function):\n+    \"\"\"\n+    We can implement our own custom autograd Functions by subclassing\n+    torch.autograd.Function and implementing the forward and backward passes\n+    which operate on Tensors.\n+    \"\"\"\n+\n+    @staticmethod\n+    def forward(ctx, input):\nComment: Please provide return type hint for the function: `forward`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `ctx`\n\nPlease provide type hint for the parameter: `input`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "neural_network/legendre_function.py",
    "pr_number": 10588,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1360558650,
    "comment_created_at": "2023-10-16T12:08:38Z"
  },
  {
    "code": "@@ -0,0 +1,29 @@\n+# Definition for singly-linked list.\n+class ListNode:\n+     def __init__(self, val=0, next=None):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: val\n\nplease provide type hint for the parameter: next",
    "line_number": 3,
    "enriched": "File: data_structures/linked_list/sort_linked_list.py\nCode: @@ -0,0 +1,29 @@\n+# Definition for singly-linked list.\n+class ListNode:\n+     def __init__(self, val=0, next=None):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `val`\n\nPlease provide type hint for the parameter: `next`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/linked_list/sort_linked_list.py",
    "pr_number": 9910,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1348527673,
    "comment_created_at": "2023-10-06T10:18:21Z"
  },
  {
    "code": "@@ -0,0 +1,228 @@\n+from typing import List\n+import heapq\n+from copy import deepcopy\n+\n+\n+class UniformCostSearch:\n+    def __init__(self, current, final, grid):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: current\n\nplease provide type hint for the parameter: final\n\nplease provide type hint for the parameter: grid",
    "line_number": 7,
    "enriched": "File: graphs/uniform_search_cost.py\nCode: @@ -0,0 +1,228 @@\n+from typing import List\n+import heapq\n+from copy import deepcopy\n+\n+\n+class UniformCostSearch:\n+    def __init__(self, current, final, grid):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `current`\n\nPlease provide type hint for the parameter: `final`\n\nPlease provide type hint for the parameter: `grid`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "graphs/uniform_search_cost.py",
    "pr_number": 8929,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1285250921,
    "comment_created_at": "2023-08-06T18:47:26Z"
  },
  {
    "code": "@@ -0,0 +1,195 @@\n+import traceback\n+\n+\"\"\"\n+    author: Samarth Tankasali\n+    date: 29.10.2023\n+    class: Brainfuck Cipher\n+\n+    This class implements the BF-cipher algorithm and provides\n+    some useful methods for interpreting instructions\n+\n+    Description:\n+    Brainfuck is an esoteric programming language created in 1993 by Urban\n+    M\u00fcller.\n+\n+    Notable for its extreme minimalism, the language consists of only eight\n+    simple commands, a data pointer and an instruction pointer. While it is\n+    fully Turing complete, it is not intended for practical use, but to\n+    challenge and amuse programmers. Brainfuck requires one to break commands\n+    into microscopic steps.\n+\n+    The language's name is a reference to the slang term brainfuck, which refers\n+    to things so complicated or unusual that they exceed the limits of one's\n+    understanding, as it was not meant or made for designing actual software but\n+    to challenge the boundaries of computer programming.\n+    (https://en.wikipedia.org/wiki/Brainfuck)\n+\"\"\"\n+\n+\n+class BFCipher:\n+    \"\"\"\n+    Brainfuck Interpreter.\n+\n+    BFCipher class processes the brainfuck instruction and returns the output\n+    after compilation.\n+\n+    Args:\n+        instruction (str): A string of the BFCipher code.\n+\n+    Examples:\n+        >>> # Code to add 5 + 2\n+        >>> instruction = \"++>+++++[<+>-]++++++++[<++++++>-]<.\"\n+        >>> print(BFCipher(instruction).bf_compiler())\n+        7\n+        >>> # Code to print \"Hello World!\"\n+        >>> instruction = \"++++++++[>++++[>++>+++>+++>+<<<<-]>+>+>->>+[<]<-]>>\"\n+        >>> instruction += \".>---.+++++++..+++.>>.<-.<.+++.------.--------.>>+\"\n+        >>> instruction += \".>++.\"\n+        >>> print(BFCipher(instruction).bf_compiler(), end = \"\")\n+        Hello World!\n+    \"\"\"\n+\n+    def __init__(self, instruction: str) -> None:\n+        \"\"\"\n+        BFCipher constructor\n+\n+        Args:\n+            instruction (str): A string of the BFCipher code.\n+        \"\"\"\n+        self.main_arr: list = [0] * 30000\n+        self.instruction_ptr: int = 0\n+        self.data_ptr: int = 0\n+        self.user_input: list = []\n+        self.loop_table: dict = {}\n+        self.output: str = \"\"\n+        if not isinstance(instruction, str):\n+            raise TypeError(\"Instruction must be a string\")\n+        else:\n+            self.instruction: str = instruction\n+\n+    def __syntax_check(self) -> None:\n+        \"\"\"\n+        Performs a syntax check of the instruction and generates the\n+        `loop_table`.\n+        \"\"\"\n+        checker_ptr: int = self.instruction_ptr\n+        bracket_stack = []\n+        while checker_ptr < len(self.instruction):\n+            if self.instruction[checker_ptr] == \"[\":\n+                bracket_stack.append(checker_ptr)\n+            elif self.instruction[checker_ptr] == \"]\":\n+                if len(bracket_stack) == 0:\n+                    raise SyntaxError(\"Incomplete closure of bracket for instruction\")\n+                loop_beginning_index = bracket_stack.pop()\n+                self.loop_table[loop_beginning_index] = checker_ptr\n+                self.loop_table[checker_ptr] = loop_beginning_index\n+            checker_ptr += 1\n+        if len(bracket_stack) > 0:\n+            raise SyntaxError(\"Incomplete closure of bracket for instruction\")\n+\n+    def __increment_data_ptr(self) -> None:\n+        \"\"\"\n+        Increment the data pointer by one (to point to the next cell to the\n+        right).\n+        \"\"\"\n+        self.data_ptr += 1\n+        if self.data_ptr > 30000:\n+            raise ValueError(\"NullValue Reference: Data pointer referencing null.\")\n+\n+    def __decrement_data_ptr(self) -> None:\n+        \"\"\"\n+        Decrement the data pointer by one (to point to the next cell to the\n+        left).\n+        \"\"\"\n+        self.data_ptr -= 1\n+        if self.data_ptr < 0:\n+            raise ValueError(\"NullValue Reference: Data pointer referencing null.\")\n+\n+    def __increment_data_value(self) -> None:\n+        \"\"\"\n+        Increment the byte at the data pointer by one.\n+        \"\"\"\n+        self.main_arr[self.data_ptr] += 1\n+        self.main_arr[self.data_ptr] = self.main_arr[self.data_ptr] & 0xFF\n+\n+    def __decrement_data_value(self) -> None:\n+        \"\"\"\n+        Decrement the byte at the data pointer by one.\n+        \"\"\"\n+        self.main_arr[self.data_ptr] -= 1\n+        self.main_arr[self.data_ptr] = self.main_arr[self.data_ptr] & 0xFF\n+\n+    def __append_bracket(self) -> None:\n+        \"\"\"\n+        If the byte at the data pointer is zero, then instead of moving the\n+        instruction pointer forward to the next command, jump it forward to\n+        the command after the matching `]` command.\n+        \"\"\"\n+        if self.main_arr[self.data_ptr] == 0:\n+            self.instruction_ptr = self.loop_table[self.instruction_ptr]\n+\n+    def __pop_bracket(self) -> None:\n+        \"\"\"\n+        If the byte at the data pointer is nonzero, then instead of moving the\n+        instruction pointer forward to the next command, jump it back to the\n+        command after the matching `[` command.\n+        \"\"\"\n+        if self.main_arr[self.data_ptr] != 0:\n+            self.instruction_ptr = self.loop_table[self.instruction_ptr]\n+\n+    def __print_output(self) -> None:\n+        \"\"\"\n+        Output the byte at the data pointer.\n+        \"\"\"\n+        self.output += chr(self.main_arr[self.data_ptr])\n+\n+    def __take_input(self) -> None:\n+        \"\"\"\n+        Accept one byte of input, storing its value in the byte at the data\n+        pointer.\n+        \"\"\"\n+        if self.user_input == []:\n+            user_input = list(input() + \"\\n\")\n+        self.main_arr[self.data_ptr] = ord(user_input.pop(0))\n+\n+    def bf_compiler(self) -> str:\n+        \"\"\"\n+        Executes the brainfuck instructions and returns appropriate output after\n+        compilation.\n+\n+        Returns:\n+            `str`: A string representing the output of the BF instructions.\n+        \"\"\"\n+        try:\n+            self.__syntax_check()\n+            while self.instruction_ptr < len(self.instruction):\n+                if self.instruction[self.instruction_ptr] == \">\":\n+                    self.__increment_data_ptr()\n+                elif self.instruction[self.instruction_ptr] == \"<\":\n+                    self.__decrement_data_ptr()\n+                elif self.instruction[self.instruction_ptr] == \"+\":\n+                    self.__increment_data_value()\n+                elif self.instruction[self.instruction_ptr] == \"-\":\n+                    self.__decrement_data_value()\n+                elif self.instruction[self.instruction_ptr] == \"[\":\n+                    self.__append_bracket()\n+                elif self.instruction[self.instruction_ptr] == \"]\":\n+                    self.__pop_bracket()\n+                elif self.instruction[self.instruction_ptr] == \".\":\n+                    self.__print_output()\n+                elif self.instruction[self.instruction_ptr] == \",\":\n+                    self.__take_input()",
    "comment": "this needs timeit benchmarks because it would be a big performance gain to perform dict lookup of instructions.",
    "line_number": 181,
    "enriched": "File: ciphers/brainfuck.py\nCode: @@ -0,0 +1,195 @@\n+import traceback\n+\n+\"\"\"\n+    author: Samarth Tankasali\n+    date: 29.10.2023\n+    class: Brainfuck Cipher\n+\n+    This class implements the BF-cipher algorithm and provides\n+    some useful methods for interpreting instructions\n+\n+    Description:\n+    Brainfuck is an esoteric programming language created in 1993 by Urban\n+    M\u00fcller.\n+\n+    Notable for its extreme minimalism, the language consists of only eight\n+    simple commands, a data pointer and an instruction pointer. While it is\n+    fully Turing complete, it is not intended for practical use, but to\n+    challenge and amuse programmers. Brainfuck requires one to break commands\n+    into microscopic steps.\n+\n+    The language's name is a reference to the slang term brainfuck, which refers\n+    to things so complicated or unusual that they exceed the limits of one's\n+    understanding, as it was not meant or made for designing actual software but\n+    to challenge the boundaries of computer programming.\n+    (https://en.wikipedia.org/wiki/Brainfuck)\n+\"\"\"\n+\n+\n+class BFCipher:\n+    \"\"\"\n+    Brainfuck Interpreter.\n+\n+    BFCipher class processes the brainfuck instruction and returns the output\n+    after compilation.\n+\n+    Args:\n+        instruction (str): A string of the BFCipher code.\n+\n+    Examples:\n+        >>> # Code to add 5 + 2\n+        >>> instruction = \"++>+++++[<+>-]++++++++[<++++++>-]<.\"\n+        >>> print(BFCipher(instruction).bf_compiler())\n+        7\n+        >>> # Code to print \"Hello World!\"\n+        >>> instruction = \"++++++++[>++++[>++>+++>+++>+<<<<-]>+>+>->>+[<]<-]>>\"\n+        >>> instruction += \".>---.+++++++..+++.>>.<-.<.+++.------.--------.>>+\"\n+        >>> instruction += \".>++.\"\n+        >>> print(BFCipher(instruction).bf_compiler(), end = \"\")\n+        Hello World!\n+    \"\"\"\n+\n+    def __init__(self, instruction: str) -> None:\n+        \"\"\"\n+        BFCipher constructor\n+\n+        Args:\n+            instruction (str): A string of the BFCipher code.\n+        \"\"\"\n+        self.main_arr: list = [0] * 30000\n+        self.instruction_ptr: int = 0\n+        self.data_ptr: int = 0\n+        self.user_input: list = []\n+        self.loop_table: dict = {}\n+        self.output: str = \"\"\n+        if not isinstance(instruction, str):\n+            raise TypeError(\"Instruction must be a string\")\n+        else:\n+            self.instruction: str = instruction\n+\n+    def __syntax_check(self) -> None:\n+        \"\"\"\n+        Performs a syntax check of the instruction and generates the\n+        `loop_table`.\n+        \"\"\"\n+        checker_ptr: int = self.instruction_ptr\n+        bracket_stack = []\n+        while checker_ptr < len(self.instruction):\n+            if self.instruction[checker_ptr] == \"[\":\n+                bracket_stack.append(checker_ptr)\n+            elif self.instruction[checker_ptr] == \"]\":\n+                if len(bracket_stack) == 0:\n+                    raise SyntaxError(\"Incomplete closure of bracket for instruction\")\n+                loop_beginning_index = bracket_stack.pop()\n+                self.loop_table[loop_beginning_index] = checker_ptr\n+                self.loop_table[checker_ptr] = loop_beginning_index\n+            checker_ptr += 1\n+        if len(bracket_stack) > 0:\n+            raise SyntaxError(\"Incomplete closure of bracket for instruction\")\n+\n+    def __increment_data_ptr(self) -> None:\n+        \"\"\"\n+        Increment the data pointer by one (to point to the next cell to the\n+        right).\n+        \"\"\"\n+        self.data_ptr += 1\n+        if self.data_ptr > 30000:\n+            raise ValueError(\"NullValue Reference: Data pointer referencing null.\")\n+\n+    def __decrement_data_ptr(self) -> None:\n+        \"\"\"\n+        Decrement the data pointer by one (to point to the next cell to the\n+        left).\n+        \"\"\"\n+        self.data_ptr -= 1\n+        if self.data_ptr < 0:\n+            raise ValueError(\"NullValue Reference: Data pointer referencing null.\")\n+\n+    def __increment_data_value(self) -> None:\n+        \"\"\"\n+        Increment the byte at the data pointer by one.\n+        \"\"\"\n+        self.main_arr[self.data_ptr] += 1\n+        self.main_arr[self.data_ptr] = self.main_arr[self.data_ptr] & 0xFF\n+\n+    def __decrement_data_value(self) -> None:\n+        \"\"\"\n+        Decrement the byte at the data pointer by one.\n+        \"\"\"\n+        self.main_arr[self.data_ptr] -= 1\n+        self.main_arr[self.data_ptr] = self.main_arr[self.data_ptr] & 0xFF\n+\n+    def __append_bracket(self) -> None:\n+        \"\"\"\n+        If the byte at the data pointer is zero, then instead of moving the\n+        instruction pointer forward to the next command, jump it forward to\n+        the command after the matching `]` command.\n+        \"\"\"\n+        if self.main_arr[self.data_ptr] == 0:\n+            self.instruction_ptr = self.loop_table[self.instruction_ptr]\n+\n+    def __pop_bracket(self) -> None:\n+        \"\"\"\n+        If the byte at the data pointer is nonzero, then instead of moving the\n+        instruction pointer forward to the next command, jump it back to the\n+        command after the matching `[` command.\n+        \"\"\"\n+        if self.main_arr[self.data_ptr] != 0:\n+            self.instruction_ptr = self.loop_table[self.instruction_ptr]\n+\n+    def __print_output(self) -> None:\n+        \"\"\"\n+        Output the byte at the data pointer.\n+        \"\"\"\n+        self.output += chr(self.main_arr[self.data_ptr])\n+\n+    def __take_input(self) -> None:\n+        \"\"\"\n+        Accept one byte of input, storing its value in the byte at the data\n+        pointer.\n+        \"\"\"\n+        if self.user_input == []:\n+            user_input = list(input() + \"\\n\")\n+        self.main_arr[self.data_ptr] = ord(user_input.pop(0))\n+\n+    def bf_compiler(self) -> str:\n+        \"\"\"\n+        Executes the brainfuck instructions and returns appropriate output after\n+        compilation.\n+\n+        Returns:\n+            `str`: A string representing the output of the BF instructions.\n+        \"\"\"\n+        try:\n+            self.__syntax_check()\n+            while self.instruction_ptr < len(self.instruction):\n+                if self.instruction[self.instruction_ptr] == \">\":\n+                    self.__increment_data_ptr()\n+                elif self.instruction[self.instruction_ptr] == \"<\":\n+                    self.__decrement_data_ptr()\n+                elif self.instruction[self.instruction_ptr] == \"+\":\n+                    self.__increment_data_value()\n+                elif self.instruction[self.instruction_ptr] == \"-\":\n+                    self.__decrement_data_value()\n+                elif self.instruction[self.instruction_ptr] == \"[\":\n+                    self.__append_bracket()\n+                elif self.instruction[self.instruction_ptr] == \"]\":\n+                    self.__pop_bracket()\n+                elif self.instruction[self.instruction_ptr] == \".\":\n+                    self.__print_output()\n+                elif self.instruction[self.instruction_ptr] == \",\":\n+                    self.__take_input()\nComment: This needs timeit benchmarks because it would be a big performance gain to perform dict lookup of instructions.\r\n```suggestion\r\ninstructions = {\r\n    \"-\": self.__decrement_data_value,\r\n    \",\": self.__take_input,\r\n    \".\": self.__print_output,\r\n    \"[\": self.__append_bracket,\r\n    \"]\": self.__pop_bracket,\r\n    \"+\": self.__increment_data_value,\r\n    \"<\": self.__decrement_data_ptr,\r\n    \">\": self.__increment_data_ptr,\r\n}\r\nif method := instructions.get(self.instruction[self.instruction_ptr])\r\n    method()\r\nelse:\r\n    msg = f\"{self.instruction[self.instruction_ptr]} is an invalid instruction\"\r\n    raise ValueError(msg)\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "ciphers/brainfuck.py",
    "pr_number": 11073,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375417674,
    "comment_created_at": "2023-10-29T11:22:56Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+class Node: \n+    def __init__(self,data): ",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: data",
    "line_number": 2,
    "enriched": "File: data_structures/linked_list/cdll.py\nCode: @@ -0,0 +1,94 @@\n+class Node: \n+    def __init__(self,data): \nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `data`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/linked_list/cdll.py",
    "pr_number": 7907,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1009267763,
    "comment_created_at": "2022-10-31T10:38:05Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+\"\"\"\n+Calculating Electric Power\n+Description : Electric power can be said to be the rate at which",
    "comment": "the provided code calculates the electric power for dc circuits based on the given input parameters, which include the potential difference (voltage), current, and resistance. \r\n\r\nyou should change pr name as dc electric power formula.",
    "line_number": 3,
    "enriched": "File: physics/electric_power.py\nCode: @@ -0,0 +1,60 @@\n+\"\"\"\n+Calculating Electric Power\n+Description : Electric power can be said to be the rate at which\nComment: The provided code calculates the electric power for DC circuits based on the given input parameters, which include the potential difference (voltage), current, and resistance. \r\n\r\nYou should change PR name as DC Electric Power formula.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "physics/electric_power.py",
    "pr_number": 8155,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1175654947,
    "comment_created_at": "2023-04-24T18:37:39Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+\"\"\"\n+Signum function\n+\n+Refer - https://en.wikipedia.org/wiki/Sign_function\n+\"\"\"\n+\n+def signum(num):",
    "comment": "please provide return type hint for the function: signum. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: num",
    "line_number": 7,
    "enriched": "File: maths/signum.py\nCode: @@ -0,0 +1,39 @@\n+\"\"\"\n+Signum function\n+\n+Refer - https://en.wikipedia.org/wiki/Sign_function\n+\"\"\"\n+\n+def signum(num):\nComment: Please provide return type hint for the function: `signum`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `num`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "maths/signum.py",
    "pr_number": 7526,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002582399,
    "comment_created_at": "2022-10-22T21:06:45Z"
  },
  {
    "code": "@@ -0,0 +1,65 @@\n+\"\"\"\n+You are given a list of segments, where each segment is represented by its left and right endpoints. \n+The task is to find the minimum number of points needed to cover all the segments. \n+If you can cover all the segments with points, return the points' positions; otherwise, return -1.\n+\n+Implement the SegmentIntersection class with the following methods:\n+\n+ 1. __init__(self, segments): The constructor that initializes the SegmentIntersection object with a list of segments.\n+\n+ 2. find_min_points(self): A method that finds the minimum number of points needed to cover\n+all the segments and returns both the count of points and their positions as a list.\n+\n+Use the following implementation notes as a guide:\n+\n+1. Sort the segments by their right endpoints.\n+2. Initialize an empty list called points to store the positions of points.\n+3. While there are segments left in the list:\n+4. Take the segment with the smallest right endpoint, x.\n+5. Add the right endpoint of x to the points list.\n+6. Remove any segments in the list that intersect with x.\n+7. Return the count of points and the list of points.\n+\"\"\"\n+\n+class SegmentIntersection:\n+    def __init__(self, segments):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: segments",
    "line_number": 25,
    "enriched": "File: greedy_methods/minimum_segment_intersection.py\nCode: @@ -0,0 +1,65 @@\n+\"\"\"\n+You are given a list of segments, where each segment is represented by its left and right endpoints. \n+The task is to find the minimum number of points needed to cover all the segments. \n+If you can cover all the segments with points, return the points' positions; otherwise, return -1.\n+\n+Implement the SegmentIntersection class with the following methods:\n+\n+ 1. __init__(self, segments): The constructor that initializes the SegmentIntersection object with a list of segments.\n+\n+ 2. find_min_points(self): A method that finds the minimum number of points needed to cover\n+all the segments and returns both the count of points and their positions as a list.\n+\n+Use the following implementation notes as a guide:\n+\n+1. Sort the segments by their right endpoints.\n+2. Initialize an empty list called points to store the positions of points.\n+3. While there are segments left in the list:\n+4. Take the segment with the smallest right endpoint, x.\n+5. Add the right endpoint of x to the points list.\n+6. Remove any segments in the list that intersect with x.\n+7. Return the count of points and the list of points.\n+\"\"\"\n+\n+class SegmentIntersection:\n+    def __init__(self, segments):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `segments`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "greedy_methods/minimum_segment_intersection.py",
    "pr_number": 10647,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1362314345,
    "comment_created_at": "2023-10-17T15:21:55Z"
  },
  {
    "code": "@@ -0,0 +1,251 @@\n+\"\"\"\n+Quantum Logic Gates which are implemented mathematically\n+and can be used as functions to build complex calculations\n+and implement different operations. The input taken is a real value\n+and imaginary value of the number and the result is output after computation.\n+\n+References :\n+https://en.wikipedia.org/wiki/Quantum_logic_gate\n+\n+Book : Mathematics Of Quantum Computing An Introduction by Wolfgang Scherer\n+\n+Glossary ;\n+input_realvalue : the magnitude of the real part of the input complex number.\n+input_imaginaryvalue : the magnitude of the imaginary part of the input complex number.\n+In cases which require 2 inputs the input is named with a suffix of 1 and 2\n+(Eg. input_realvalue_1)\n+\n+alpha : angle of rotation as represented by the block sphere.\n+iota : The exponential complex of alpha value.\n+nx_value : value of vector in X axis as represented by Hilbert space.\n+nx_value : value of vector in Y axis as represented by Hilbert space.\n+nx_value : value of vector in Z axis as represented by Hilbert space.\n+\n+* The nx,ny and nz values can also be considered as values of vectors along\n+the respective axes on the bloch sphere.\n+\n+Usage :\n+>>> paulix_gate(2,3)\n+[3 2]\n+\n+>>> pauliy_gate(5,8)\n+[0.+8.j 0.-5.j]\n+\n+>>> pauliz_gate(4,1)\n+[ 4 -1]\n+\n+>>> identity_gate(7,2)\n+9\n+\n+>>> phasefactor_of_input(4,7,45)\n+[1.39737084e+20+0.j 2.44539897e+20+0.j]\n+\n+>>> phaseshift_of_input(3,9,30)\n+[3.00000000e+00+0.j 9.61782712e+13+0.j]\n+\n+>>> hadamard_gate(5,9)\n+[ 9.89949494 -2.82842712]\n+[1.+0.j 0.+0.j 0.+0.j 7.+0.j]\n+\n+>>> controlled_not_gate_in_0ket(1,7,4,8)\n+[7 1 4 8]\n+\n+>>> controlled_not_gate(6,3,7,5)\n+[6 3 5 7]\n+\n+>>> inverted_controlled_not_gate(8,4,9,6)\n+[8 6 9 4]\n+\n+>>> controlled_phase_multiplication(3,2,5,1,10)\n+[3.00000000e+00+0.j 2.00000000e+00+0.j 1.10132329e+05+0.j\n+ 2.20264658e+04+0.j]\n+\n+>>> swap_gate(5,1,3,7)\n+[5 3 1 7]\n+\n+>>> spin_of_input(6,3,45,1,8,3)\n+[-16.93201614+10.23066476j -50.61991392 -1.46152354j]\n+\n+\"\"\"\n+\n+import cmath\n+import math\n+\n+import numpy as np\n+\n+\n+def paulix_gate(input_realvalue, input_imaginaryvalue):",
    "comment": "please provide return type hint for the function: paulix_gate. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: input_realvalue\n\nplease provide type hint for the parameter: input_imaginaryvalue",
    "line_number": 77,
    "enriched": "File: quantum/qauntum_logic_gates.py\nCode: @@ -0,0 +1,251 @@\n+\"\"\"\n+Quantum Logic Gates which are implemented mathematically\n+and can be used as functions to build complex calculations\n+and implement different operations. The input taken is a real value\n+and imaginary value of the number and the result is output after computation.\n+\n+References :\n+https://en.wikipedia.org/wiki/Quantum_logic_gate\n+\n+Book : Mathematics Of Quantum Computing An Introduction by Wolfgang Scherer\n+\n+Glossary ;\n+input_realvalue : the magnitude of the real part of the input complex number.\n+input_imaginaryvalue : the magnitude of the imaginary part of the input complex number.\n+In cases which require 2 inputs the input is named with a suffix of 1 and 2\n+(Eg. input_realvalue_1)\n+\n+alpha : angle of rotation as represented by the block sphere.\n+iota : The exponential complex of alpha value.\n+nx_value : value of vector in X axis as represented by Hilbert space.\n+nx_value : value of vector in Y axis as represented by Hilbert space.\n+nx_value : value of vector in Z axis as represented by Hilbert space.\n+\n+* The nx,ny and nz values can also be considered as values of vectors along\n+the respective axes on the bloch sphere.\n+\n+Usage :\n+>>> paulix_gate(2,3)\n+[3 2]\n+\n+>>> pauliy_gate(5,8)\n+[0.+8.j 0.-5.j]\n+\n+>>> pauliz_gate(4,1)\n+[ 4 -1]\n+\n+>>> identity_gate(7,2)\n+9\n+\n+>>> phasefactor_of_input(4,7,45)\n+[1.39737084e+20+0.j 2.44539897e+20+0.j]\n+\n+>>> phaseshift_of_input(3,9,30)\n+[3.00000000e+00+0.j 9.61782712e+13+0.j]\n+\n+>>> hadamard_gate(5,9)\n+[ 9.89949494 -2.82842712]\n+[1.+0.j 0.+0.j 0.+0.j 7.+0.j]\n+\n+>>> controlled_not_gate_in_0ket(1,7,4,8)\n+[7 1 4 8]\n+\n+>>> controlled_not_gate(6,3,7,5)\n+[6 3 5 7]\n+\n+>>> inverted_controlled_not_gate(8,4,9,6)\n+[8 6 9 4]\n+\n+>>> controlled_phase_multiplication(3,2,5,1,10)\n+[3.00000000e+00+0.j 2.00000000e+00+0.j 1.10132329e+05+0.j\n+ 2.20264658e+04+0.j]\n+\n+>>> swap_gate(5,1,3,7)\n+[5 3 1 7]\n+\n+>>> spin_of_input(6,3,45,1,8,3)\n+[-16.93201614+10.23066476j -50.61991392 -1.46152354j]\n+\n+\"\"\"\n+\n+import cmath\n+import math\n+\n+import numpy as np\n+\n+\n+def paulix_gate(input_realvalue, input_imaginaryvalue):\nComment: Please provide return type hint for the function: `paulix_gate`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `input_realvalue`\n\nPlease provide type hint for the parameter: `input_imaginaryvalue`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "quantum/qauntum_logic_gates.py",
    "pr_number": 8956,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1292927286,
    "comment_created_at": "2023-08-14T02:56:27Z"
  },
  {
    "code": "@@ -0,0 +1,74 @@\n+import doctest\n+\n+import numpy as np\n+from sklearn.datasets import load_iris\n+from sklearn.tree import DecisionTreeClassifier\n+\n+\"\"\"\"\n+Implementation of basic AdaBoost classifier on iris dataset.\n+The following classifier uses a pre-built DecisionTreeClassifier\n+from scikit-learn as a weak learner.\n+\n+AdaBoost (Adaptive Boosting) is an ensemble learning technique \n+used for classification problems. It combines multiple weak learners \n+(in this case, decision trees with maximum depth 1) to create a \n+strong classifier. The key idea behind AdaBoost is to give \n+more weight to the training instances thatare misclassified by \n+the previous weak learners, so that subsequent weak learners focus more on \n+these misclassified instances.\n+\"\"\"\n+\n+class AdaBoost:\n+    def __init__(self, n_estimators=50):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: n_estimators",
    "line_number": 22,
    "enriched": "File: machine_learning/adaboost_classifier.py\nCode: @@ -0,0 +1,74 @@\n+import doctest\n+\n+import numpy as np\n+from sklearn.datasets import load_iris\n+from sklearn.tree import DecisionTreeClassifier\n+\n+\"\"\"\"\n+Implementation of basic AdaBoost classifier on iris dataset.\n+The following classifier uses a pre-built DecisionTreeClassifier\n+from scikit-learn as a weak learner.\n+\n+AdaBoost (Adaptive Boosting) is an ensemble learning technique \n+used for classification problems. It combines multiple weak learners \n+(in this case, decision trees with maximum depth 1) to create a \n+strong classifier. The key idea behind AdaBoost is to give \n+more weight to the training instances thatare misclassified by \n+the previous weak learners, so that subsequent weak learners focus more on \n+these misclassified instances.\n+\"\"\"\n+\n+class AdaBoost:\n+    def __init__(self, n_estimators=50):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `n_estimators`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "machine_learning/adaboost_classifier.py",
    "pr_number": 10550,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359931617,
    "comment_created_at": "2023-10-15T19:17:44Z"
  },
  {
    "code": "@@ -0,0 +1,33 @@\n+def max_product_subarray(nums):",
    "comment": "please provide return type hint for the function: max_product_subarray. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: nums",
    "line_number": 1,
    "enriched": "File: Kadane_algorithm/max_product_subarray.py\nCode: @@ -0,0 +1,33 @@\n+def max_product_subarray(nums):\nComment: Please provide return type hint for the function: `max_product_subarray`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `nums`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "Kadane_algorithm/max_product_subarray.py",
    "pr_number": 8568,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1151407246,
    "comment_created_at": "2023-03-29T05:22:52Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+import numpy as np\n+\n+def matrix_factorization_svd(matrix):",
    "comment": "please provide return type hint for the function: matrix_factorization_svd. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: matrix",
    "line_number": 3,
    "enriched": "File: matrix/matrix_factorization.py\nCode: @@ -0,0 +1,48 @@\n+import numpy as np\n+\n+def matrix_factorization_svd(matrix):\nComment: Please provide return type hint for the function: `matrix_factorization_svd`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `matrix`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "matrix/matrix_factorization.py",
    "pr_number": 9496,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342915604,
    "comment_created_at": "2023-10-02T16:31:47Z"
  },
  {
    "code": "@@ -0,0 +1,73 @@\n+\"\"\"\n+Longest Common Substring Problem Statement: Given two sequences, find the length\n+of longest common substring present in both of them. A substring is\n+necessarily continuous.\n+Example:\"abcdef\", \"xabded\" have the length of longest common substring 2 (\"ab\" or \"de\").\n+\"\"\"\n+\n+\n+def longest_common_substring(t1: str, t2: str):",
    "comment": "please provide return type hint for the function: longest_common_substring. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 9,
    "enriched": "File: dynamic_programming/longest_common_substring.py\nCode: @@ -0,0 +1,73 @@\n+\"\"\"\n+Longest Common Substring Problem Statement: Given two sequences, find the length\n+of longest common substring present in both of them. A substring is\n+necessarily continuous.\n+Example:\"abcdef\", \"xabded\" have the length of longest common substring 2 (\"ab\" or \"de\").\n+\"\"\"\n+\n+\n+def longest_common_substring(t1: str, t2: str):\nComment: Please provide return type hint for the function: `longest_common_substring`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "dynamic_programming/longest_common_substring.py",
    "pr_number": 7488,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1001924134,
    "comment_created_at": "2022-10-21T15:28:07Z"
  },
  {
    "code": "@@ -1,18 +1,23 @@\n-\"\"\"\n-Description :\n-Newton's second law of motion pertains to the behavior of objects for which\n-all existing forces are not balanced.\n-The second law states that the acceleration of an object is dependent upon two variables\n-- the net force acting upon the object and the mass of the object.\n-The acceleration of an object depends directly\n-upon the net force acting upon the object,\n-and inversely upon the mass of the object.\n-As the force acting upon an object is increased,\n-the acceleration of the object is increased.\n-As the mass of an object is increased, the acceleration of the object is decreased.\n+r\"\"\"\n+Description:\n+    Newton's second law of motion pertains to the behavior of objects for which\n+    all existing forces are not balanced.\n+    The second law states that the acceleration of an object is dependent upon\n+    two variables - the net force acting upon the object and the mass of the object.\n+    The acceleration of an object depends directly\n+    upon the net force acting upon the object,\n+    and inversely upon the mass of the object.\n+    As the force acting upon an object is increased,\n+    the acceleration of the object is increased.\n+    As the mass of an object is increased, the acceleration of the object is decreased.\n+\n Source: https://www.physicsclassroom.com/class/newtlaws/Lesson-3/Newton-s-Second-Law\n-Formulation: Fnet = m \u2022 a\n-Diagrammatic Explanation:\n+\n+Formulation:\n+    .. math:: F_{net} = m \\cdot a",
    "comment": "this change helps computer readers but does hurt human readers of the code.",
    "line_number": 17,
    "enriched": "File: physics/newtons_second_law_of_motion.py\nCode: @@ -1,18 +1,23 @@\n-\"\"\"\n-Description :\n-Newton's second law of motion pertains to the behavior of objects for which\n-all existing forces are not balanced.\n-The second law states that the acceleration of an object is dependent upon two variables\n-- the net force acting upon the object and the mass of the object.\n-The acceleration of an object depends directly\n-upon the net force acting upon the object,\n-and inversely upon the mass of the object.\n-As the force acting upon an object is increased,\n-the acceleration of the object is increased.\n-As the mass of an object is increased, the acceleration of the object is decreased.\n+r\"\"\"\n+Description:\n+    Newton's second law of motion pertains to the behavior of objects for which\n+    all existing forces are not balanced.\n+    The second law states that the acceleration of an object is dependent upon\n+    two variables - the net force acting upon the object and the mass of the object.\n+    The acceleration of an object depends directly\n+    upon the net force acting upon the object,\n+    and inversely upon the mass of the object.\n+    As the force acting upon an object is increased,\n+    the acceleration of the object is increased.\n+    As the mass of an object is increased, the acceleration of the object is decreased.\n+\n Source: https://www.physicsclassroom.com/class/newtlaws/Lesson-3/Newton-s-Second-Law\n-Formulation: Fnet = m \u2022 a\n-Diagrammatic Explanation:\n+\n+Formulation:\n+    .. math:: F_{net} = m \\cdot a\nComment: This change helps computer readers but does hurt human readers of the code.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "physics/newtons_second_law_of_motion.py",
    "pr_number": 12480,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1899151777,
    "comment_created_at": "2024-12-29T15:33:17Z"
  },
  {
    "code": "@@ -0,0 +1,32 @@\n+def quick_sort(array):",
    "comment": "please provide return type hint for the function: quick_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: array",
    "line_number": 1,
    "enriched": "File: divide_and_conquer/quicksort.py\nCode: @@ -0,0 +1,32 @@\n+def quick_sort(array):\nComment: Please provide return type hint for the function: `quick_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `array`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "divide_and_conquer/quicksort.py",
    "pr_number": 11038,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375013248,
    "comment_created_at": "2023-10-27T20:23:56Z"
  },
  {
    "code": "@@ -0,0 +1,75 @@\n+\"\"\"\n+Contour Detection Using OpenCV\n+\n+This script reads an image, performs contour detection using OpenCV,\n+and saves the result with detected contours.\n+\n+Author: Anuj Mishra\n+Date: 05-10-2023\n+\"\"\"\n+\n+import cv2\n+import numpy as np\n+\n+class ContourDetector:\n+    def __init__(self, image_path):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: image_path",
    "line_number": 15,
    "enriched": "File: computer_vision/contour_detection_opencv.py\nCode: @@ -0,0 +1,75 @@\n+\"\"\"\n+Contour Detection Using OpenCV\n+\n+This script reads an image, performs contour detection using OpenCV,\n+and saves the result with detected contours.\n+\n+Author: Anuj Mishra\n+Date: 05-10-2023\n+\"\"\"\n+\n+import cv2\n+import numpy as np\n+\n+class ContourDetector:\n+    def __init__(self, image_path):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `image_path`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "computer_vision/contour_detection_opencv.py",
    "pr_number": 9781,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346895703,
    "comment_created_at": "2023-10-05T07:04:47Z"
  },
  {
    "code": "@@ -0,0 +1,77 @@\n+\"\"\"\n+Project Euler Problem 148 : https://projecteuler.net/problem=148\n+Author:\tSai Teja Manchi\n+Problem Statement:\n+We can easily verify that none of the entries in the\n+first seven rows of Pascal's triangle are divisible by 7:\n+                               1\n+                          1          1\n+                     1          2          1\n+                1          3          3          1\n+           1          4          6          4          1\n+      1          5         10         10          5          1\n+1          6         15         20         15          6          1\n+However, if we check the first one hundred rows, we will find that\n+only 2361 of the 5050 entries are not divisible by 7.\n+Find the number of entries which are not divisible by 7\n+in the first one billion (109) rows of Pascal's triangle.\n+\n+Solution:\n+We iteratively generate each row in the pascal triangle one-by-one.\n+Since Pascal's triangle is vertically-symmetric,\n+We only need to generate half of the values.\n+We then count the values which are not divisible by 7.\n+We only store the remainders(when divided by 7) in the list to reduce memory usage.\n+\n+Note: In the original problem, we need to calculate for 10^9 rows\n+      but we took 10^4 rows here by default.\n+\"\"\"\n+\n+\n+def solution(pascal_row_count: int = 10**4) -> int:",
    "comment": "it has a hardcoded value of pascal_row_count which is only 10^4. as a result, the function returns the result for the first 10^4 rows of pascal's triangle instead of the first billion rows, try this:\r\n\r\ndef solution(pascal_row_count: int = 10**9) -> int:",
    "line_number": 31,
    "enriched": "File: project_euler/problem_148/sol1.py\nCode: @@ -0,0 +1,77 @@\n+\"\"\"\n+Project Euler Problem 148 : https://projecteuler.net/problem=148\n+Author:\tSai Teja Manchi\n+Problem Statement:\n+We can easily verify that none of the entries in the\n+first seven rows of Pascal's triangle are divisible by 7:\n+                               1\n+                          1          1\n+                     1          2          1\n+                1          3          3          1\n+           1          4          6          4          1\n+      1          5         10         10          5          1\n+1          6         15         20         15          6          1\n+However, if we check the first one hundred rows, we will find that\n+only 2361 of the 5050 entries are not divisible by 7.\n+Find the number of entries which are not divisible by 7\n+in the first one billion (109) rows of Pascal's triangle.\n+\n+Solution:\n+We iteratively generate each row in the pascal triangle one-by-one.\n+Since Pascal's triangle is vertically-symmetric,\n+We only need to generate half of the values.\n+We then count the values which are not divisible by 7.\n+We only store the remainders(when divided by 7) in the list to reduce memory usage.\n+\n+Note: In the original problem, we need to calculate for 10^9 rows\n+      but we took 10^4 rows here by default.\n+\"\"\"\n+\n+\n+def solution(pascal_row_count: int = 10**4) -> int:\nComment: It has a hardcoded value of pascal_row_count which is only 10^4. As a result, the function returns the result for the first 10^4 rows of Pascal's triangle instead of the first billion rows, try this:\r\n\r\n`def solution(pascal_row_count: int = 10**9) -> int:`",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "project_euler/problem_148/sol1.py",
    "pr_number": 8662,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1174793342,
    "comment_created_at": "2023-04-24T05:32:00Z"
  },
  {
    "code": "@@ -0,0 +1,104 @@\n+\"\"\"\n+Title : Calculating the Hubble Parameter\n+\n+Description : The Hubble parameter H is the Universe expansion rate in any time.\n+In cosmology is customary to use the redshift redshift in place of time, because\n+the redshift is directily mensure in the light of galaxies moving away\n+from us.\n+\n+So, the general relation that we obtain is\n+\n+H = hubble_constant*(radiation_density*(redshift+1)**4\n+                     + matter_density*(redshift+1)**3\n+                     + curvature*(redshift+1)**2 + dark_energy)**(1/2)\n+\n+where radiation_density, matter_density, dark_energy are the relativity\n+(the percentage) energy densities that exist\n+in the Universe today. Here, matter_density is the\n+sum of the barion density and the\n+dark matter. Curvature is the curvature parameter and can be written in term\n+of the densities by the completeness\n+\n+\n+curvature = 1 - (matter_density + radiation_density + dark_energy)\n+\n+Source :\n+https://www.sciencedirect.com/topics/mathematics/hubble-parameter\n+\"\"\"\n+\n+\n+def hubble_parameter(",
    "comment": "please provide return type hint for the function: hubble_parameter. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 30,
    "enriched": "File: physics/hubble_parameter.py\nCode: @@ -0,0 +1,104 @@\n+\"\"\"\n+Title : Calculating the Hubble Parameter\n+\n+Description : The Hubble parameter H is the Universe expansion rate in any time.\n+In cosmology is customary to use the redshift redshift in place of time, because\n+the redshift is directily mensure in the light of galaxies moving away\n+from us.\n+\n+So, the general relation that we obtain is\n+\n+H = hubble_constant*(radiation_density*(redshift+1)**4\n+                     + matter_density*(redshift+1)**3\n+                     + curvature*(redshift+1)**2 + dark_energy)**(1/2)\n+\n+where radiation_density, matter_density, dark_energy are the relativity\n+(the percentage) energy densities that exist\n+in the Universe today. Here, matter_density is the\n+sum of the barion density and the\n+dark matter. Curvature is the curvature parameter and can be written in term\n+of the densities by the completeness\n+\n+\n+curvature = 1 - (matter_density + radiation_density + dark_energy)\n+\n+Source :\n+https://www.sciencedirect.com/topics/mathematics/hubble-parameter\n+\"\"\"\n+\n+\n+def hubble_parameter(\nComment: Please provide return type hint for the function: `hubble_parameter`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "physics/hubble_parameter.py",
    "pr_number": 7806,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008320634,
    "comment_created_at": "2022-10-28T18:02:47Z"
  },
  {
    "code": "@@ -0,0 +1,77 @@\n+from sympy import diff\n+from sympy.abc import x\n+",
    "comment": "i've checked your code and it seems to be free of syntax errors. however, i did notice that you have used the eval() function to convert the string function to an algebraic expression. while this approach works fine for simple expressions, it can be dangerous and lead to security vulnerabilities if the input string is not properly sanitized. it is generally recommended to use more secure methods like the ast module to parse and evaluate user input.",
    "line_number": 3,
    "enriched": "File: arithmetic_analysis/bisection_method.py\nCode: @@ -0,0 +1,77 @@\n+from sympy import diff\n+from sympy.abc import x\n+\nComment: I've checked your code and it seems to be free of syntax errors. However, I did notice that you have used the `eval()` function to convert the string function to an algebraic expression. While this approach works fine for simple expressions, it can be dangerous and lead to security vulnerabilities if the input string is not properly sanitized. It is generally recommended to use more secure methods like the `ast` module to parse and evaluate user input.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "arithmetic_analysis/bisection_method.py",
    "pr_number": 8143,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1174815655,
    "comment_created_at": "2023-04-24T05:56:30Z"
  },
  {
    "code": "@@ -0,0 +1,106 @@\n+from typing import Optional\n+\n+class ListNode:\n+    def __init__(self, val=0):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: val",
    "line_number": 4,
    "enriched": "File: data_structures/linked_list/reverse_k_group.py\nCode: @@ -0,0 +1,106 @@\n+from typing import Optional\n+\n+class ListNode:\n+    def __init__(self, val=0):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `val`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/linked_list/reverse_k_group.py",
    "pr_number": 9323,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342169363,
    "comment_created_at": "2023-10-01T18:13:23Z"
  },
  {
    "code": "@@ -0,0 +1,67 @@\n+from hashlib import sha256\n+\n+\n+def updatehash(*args):",
    "comment": "please provide return type hint for the function: updatehash. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: args",
    "line_number": 4,
    "enriched": "File: blockchain/block_description.py\nCode: @@ -0,0 +1,67 @@\n+from hashlib import sha256\n+\n+\n+def updatehash(*args):\nComment: Please provide return type hint for the function: `updatehash`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `args`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "blockchain/block_description.py",
    "pr_number": 10245,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1353070164,
    "comment_created_at": "2023-10-10T17:58:20Z"
  },
  {
    "code": "@@ -0,0 +1,66 @@\n+\n+\"\"\"\n+    Problem Statement : https://www.spoj.com/problems/HISTOGRA/\n+\"\"\"\n+\n+\n+histogram = [3, 5, 11, 7, 5, 9]\n+max_area = 25\n+\n+histogram2 = [3, 5, 1, 7, 5, 9]\n+max_area2 = 15\n+\n+def max_rectangle_area_histogram(histogram):",
    "comment": "please provide return type hint for the function: max_rectangle_area_histogram. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: histogram",
    "line_number": 13,
    "enriched": "File: data_structures/stacks/largest_rectangle_in_histogram.py\nCode: @@ -0,0 +1,66 @@\n+\n+\"\"\"\n+    Problem Statement : https://www.spoj.com/problems/HISTOGRA/\n+\"\"\"\n+\n+\n+histogram = [3, 5, 11, 7, 5, 9]\n+max_area = 25\n+\n+histogram2 = [3, 5, 1, 7, 5, 9]\n+max_area2 = 15\n+\n+def max_rectangle_area_histogram(histogram):\nComment: Please provide return type hint for the function: `max_rectangle_area_histogram`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `histogram`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/stacks/largest_rectangle_in_histogram.py",
    "pr_number": 7225,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996293684,
    "comment_created_at": "2022-10-15T11:33:26Z"
  },
  {
    "code": "@@ -0,0 +1,51 @@\n+import os\n+import requests\n+from bs4 import BeautifulSoup\n+\n+# Read the input file\n+url_list = list()\n+with open(\"url.txt\", \"r\") as f:\n+    url_list = f.read().split(\"\\n\")\n+    f.close()\n+\n+\n+# Create a directory to save the extracted articles\n+output_dir = \"extracted_articles\"\n+if not os.path.exists(output_dir):\n+    os.makedirs(output_dir)\n+\n+\n+# Function to extract article text\n+def extract_article_text(url):",
    "comment": "please provide return type hint for the function: extract_article_text. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: url",
    "line_number": 19,
    "enriched": "File: web_programming/webscraping.py\nCode: @@ -0,0 +1,51 @@\n+import os\n+import requests\n+from bs4 import BeautifulSoup\n+\n+# Read the input file\n+url_list = list()\n+with open(\"url.txt\", \"r\") as f:\n+    url_list = f.read().split(\"\\n\")\n+    f.close()\n+\n+\n+# Create a directory to save the extracted articles\n+output_dir = \"extracted_articles\"\n+if not os.path.exists(output_dir):\n+    os.makedirs(output_dir)\n+\n+\n+# Function to extract article text\n+def extract_article_text(url):\nComment: Please provide return type hint for the function: `extract_article_text`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `url`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "web_programming/webscraping.py",
    "pr_number": 10806,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367906211,
    "comment_created_at": "2023-10-22T13:26:44Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+def maxSubArraySum(arr, size):",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: maxsubarraysum\n\nplease provide return type hint for the function: maxsubarraysum. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: size",
    "line_number": 1,
    "enriched": "File: dynamic_programming/max_sub_array_sum.py\nCode: @@ -0,0 +1,42 @@\n+def maxSubArraySum(arr, size):\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `maxSubArraySum`\n\nPlease provide return type hint for the function: `maxSubArraySum`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `size`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "dynamic_programming/max_sub_array_sum.py",
    "pr_number": 8804,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1221988764,
    "comment_created_at": "2023-06-07T18:09:47Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+\"\"\"\n+This is a pure Python implementation for minimum waiting time problem using greedy\n+algorithm.\n+reference: https://www.youtube.com/watch?v=Sf3eiO12eJs\n+\n+For doctests run following command:\n+python -m doctest -v minimum_waiting_time.py\n+\n+The minimum_waiting_time function uses a greedy algorithm to calculate the minimum\n+time for queries to complete. It sorts the list in non-decreasing order, calculates\n+the waiting time for each query by multiplying its position in the list with the\n+sum of all remaining query times, and returns the total waiting time. A doctest\n+ensures that the function produces the correct output.\n+\"\"\"\n+\n+\n+def minimum_waiting_time(queries):",
    "comment": "please provide return type hint for the function: minimum_waiting_time. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: queries",
    "line_number": 17,
    "enriched": "File: greedy_methods/minimum_waiting_time.py\nCode: @@ -0,0 +1,48 @@\n+\"\"\"\n+This is a pure Python implementation for minimum waiting time problem using greedy\n+algorithm.\n+reference: https://www.youtube.com/watch?v=Sf3eiO12eJs\n+\n+For doctests run following command:\n+python -m doctest -v minimum_waiting_time.py\n+\n+The minimum_waiting_time function uses a greedy algorithm to calculate the minimum\n+time for queries to complete. It sorts the list in non-decreasing order, calculates\n+the waiting time for each query by multiplying its position in the list with the\n+sum of all remaining query times, and returns the total waiting time. A doctest\n+ensures that the function produces the correct output.\n+\"\"\"\n+\n+\n+def minimum_waiting_time(queries):\nComment: Please provide return type hint for the function: `minimum_waiting_time`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `queries`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "greedy_methods/minimum_waiting_time.py",
    "pr_number": 8701,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1181107349,
    "comment_created_at": "2023-04-29T17:41:23Z"
  },
  {
    "code": "@@ -13,6 +27,10 @@ def actual_power(a: int, b: int):\n \n def power(a: int, b: int) -> float:\n     \"\"\"\n+    :param a: The base (integer).\n+    :param b: The exponent (integer).\n+    :retuen: The result of a^b, as a float for negative exponents.",
    "comment": "fix typo",
    "line_number": 32,
    "enriched": "File: divide_and_conquer/power.py\nCode: @@ -13,6 +27,10 @@ def actual_power(a: int, b: int):\n \n def power(a: int, b: int) -> float:\n     \"\"\"\n+    :param a: The base (integer).\n+    :param b: The exponent (integer).\n+    :retuen: The result of a^b, as a float for negative exponents.\nComment: ```suggestion\r\n    :return: The result of a^b, as a float for negative exponents.\r\n```\r\nFix typo",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "divide_and_conquer/power.py",
    "pr_number": 11187,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1623185206,
    "comment_created_at": "2024-06-01T09:04:49Z"
  },
  {
    "code": "@@ -1,4 +1,4 @@\n-def bin_exp_mod(a, n, b):\n+def bin_exp_mod(a: int, n: float, b: int) -> int:",
    "comment": "n here is actually supposed to be an integer, not a float",
    "line_number": 1,
    "enriched": "File: maths/binary_exp_mod.py\nCode: @@ -1,4 +1,4 @@\n-def bin_exp_mod(a, n, b):\n+def bin_exp_mod(a: int, n: float, b: int) -> int:\nComment: ```suggestion\r\ndef bin_exp_mod(a: int, n: int, b: int) -> int:\r\n```\r\n`n` here is actually supposed to be an integer, not a float",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/binary_exp_mod.py",
    "pr_number": 9469,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342749445,
    "comment_created_at": "2023-10-02T14:13:50Z"
  },
  {
    "code": "@@ -0,0 +1,106 @@\n+\"\"\"\n+Doppler's effect\n+\n+The Doppler effect (also Doppler shift) is the change in the frequency of a wave\n+in relation to an observer who is moving relative to the source of the wave.\n+The Doppler effect is named after the physicist Christian Doppler.\n+A common example of Doppler shift is the change of pitch heard when\n+a vehicle sounding a horn approaches and recedes from an observer.\n+\n+The reason for the Doppler effect is that when the source of the waves\n+is moving towards the observer, each successive wave crest is emitted from a position\n+closer to the observer than the crest of the previous wave.\n+Therefore, each wave takes slightly less time to reach the observer\n+than the previous wave. Hence, the time between the arrivals of successive\n+wave crests at the observer is reduced, causing an increase in the frequency.\n+Similarly, if the source of waves is moving away from the observer,\n+each wave is emitted from a position farther from the observer than the previous wave,\n+so the arrival time between successive waves is increased, reducing the frequency.\n+\n+Now, if the source of waves is stationary but the observer is moving with respect\n+to the source, the transmission velocity of the waves changes\n+(ie the rate at which the observer receives waves) even if the wavelength\n+and frequency emitted from the source remain constant.\n+\n+All these results are summarized by the Doppler formula:\n+\n+    f = (f0 * (v + v0)) / (v - vs)\n+\n+where:\n+    f: frequency of the wave\n+    f0: frequency of the wave when the source is stationary\n+    v: velocity of the wave in the medium\n+    v0: velocity of the observer, positive if the observer is moving towards the source\n+    vs: velocity of the source, positive if the source is moving towards the observer\n+\n+Doppler's effect has many applications in physics and engineering,\n+such as radar, astronomy, medical imaging and seismology.\n+\n+References:\n+https://en.wikipedia.org/wiki/Doppler_effect\n+\n+Now, we will implement a function that calculates the frequency of a wave as a function\n+of the frequency of the wave when the source is stationary, the velocity of the wave\n+in the medium, the velocity of the observer and the velocity of the source.\n+\"\"\"\n+\n+\n+def doppler_effect(\n+    org_freq: float, wave_vel: float, obs_vel: float, src_vel: float\n+) -> float:\n+    \"\"\"\n+    Input Parameters:\n+    -----------------\n+    org_freq: frequency of the wave when the source is stationary\n+    wave_vel: velocity of the wave in the medium\n+    obs_vel: velocity of the observer, +ve if the observer is moving towards the source\n+    src_vel: velocity of the source, +ve if the source is moving towards the observer\n+\n+    Returns:\n+    --------\n+    f: frequency of the wave as perceived by the observer\n+\n+    Docstring Tests:\n+    >>> doppler_effect(100, 330, 10, 0) #observer moving towards the source\n+    103.03030303030303\n+    >>> doppler_effect(100, 330, -10, 0) #observer moving away from the source\n+    96.96969696969697\n+    >>> doppler_effect(100, 330, 0, 10) #source moving towards the observer\n+    103.125\n+    >>> doppler_effect(100, 330, 0, -10) #source moving away from the observer\n+    97.05882352941177\n+    >>> doppler_effect(100, 330, 10, 10) #observer and source moving towards each other\n+    106.25\n+    >>> doppler_effect(100, 330, -10, -10) #observer and source moving away\n+    94.11764705882354\n+    >>> doppler_effect(100, 330, 10, 330) #source moving at the same speed as the wave\n+    Error: Division by zero\n+    Infinite frequency implies vs = v and observer infront of the source\n+    0.0\n+    >>> doppler_effect(100, 330, 10, 340) #source moving faster than the wave\n+    Error: Non-positive frequency\n+    Non-positive frequency implies vs > v or v0 > v(in opposite direction)\n+    0.0\n+    >>> doppler_effect(100, 330, -340, 10) #observer moving faster than the wave\n+    Error: Non-positive frequency\n+    Non-positive frequency implies vs > v or v0 > v(in opposite direction)\n+    0.0\n+    \"\"\"\n+\n+    try:\n+        doppler_freq = (org_freq * (wave_vel + obs_vel)) / (wave_vel - src_vel)\n+    except ZeroDivisionError:\n+        print(\"Error: Division by zero\")\n+        print(\"Infinite frequency implies vs = v and observer infront of the source\")\n+        return 0.0",
    "comment": "contributing.md says that algorithmic functions should not print().  please let the zerodivisionerror be raised to the caller because the answer is not 0.",
    "line_number": 95,
    "enriched": "File: physics/doppler_frequency.py\nCode: @@ -0,0 +1,106 @@\n+\"\"\"\n+Doppler's effect\n+\n+The Doppler effect (also Doppler shift) is the change in the frequency of a wave\n+in relation to an observer who is moving relative to the source of the wave.\n+The Doppler effect is named after the physicist Christian Doppler.\n+A common example of Doppler shift is the change of pitch heard when\n+a vehicle sounding a horn approaches and recedes from an observer.\n+\n+The reason for the Doppler effect is that when the source of the waves\n+is moving towards the observer, each successive wave crest is emitted from a position\n+closer to the observer than the crest of the previous wave.\n+Therefore, each wave takes slightly less time to reach the observer\n+than the previous wave. Hence, the time between the arrivals of successive\n+wave crests at the observer is reduced, causing an increase in the frequency.\n+Similarly, if the source of waves is moving away from the observer,\n+each wave is emitted from a position farther from the observer than the previous wave,\n+so the arrival time between successive waves is increased, reducing the frequency.\n+\n+Now, if the source of waves is stationary but the observer is moving with respect\n+to the source, the transmission velocity of the waves changes\n+(ie the rate at which the observer receives waves) even if the wavelength\n+and frequency emitted from the source remain constant.\n+\n+All these results are summarized by the Doppler formula:\n+\n+    f = (f0 * (v + v0)) / (v - vs)\n+\n+where:\n+    f: frequency of the wave\n+    f0: frequency of the wave when the source is stationary\n+    v: velocity of the wave in the medium\n+    v0: velocity of the observer, positive if the observer is moving towards the source\n+    vs: velocity of the source, positive if the source is moving towards the observer\n+\n+Doppler's effect has many applications in physics and engineering,\n+such as radar, astronomy, medical imaging and seismology.\n+\n+References:\n+https://en.wikipedia.org/wiki/Doppler_effect\n+\n+Now, we will implement a function that calculates the frequency of a wave as a function\n+of the frequency of the wave when the source is stationary, the velocity of the wave\n+in the medium, the velocity of the observer and the velocity of the source.\n+\"\"\"\n+\n+\n+def doppler_effect(\n+    org_freq: float, wave_vel: float, obs_vel: float, src_vel: float\n+) -> float:\n+    \"\"\"\n+    Input Parameters:\n+    -----------------\n+    org_freq: frequency of the wave when the source is stationary\n+    wave_vel: velocity of the wave in the medium\n+    obs_vel: velocity of the observer, +ve if the observer is moving towards the source\n+    src_vel: velocity of the source, +ve if the source is moving towards the observer\n+\n+    Returns:\n+    --------\n+    f: frequency of the wave as perceived by the observer\n+\n+    Docstring Tests:\n+    >>> doppler_effect(100, 330, 10, 0) #observer moving towards the source\n+    103.03030303030303\n+    >>> doppler_effect(100, 330, -10, 0) #observer moving away from the source\n+    96.96969696969697\n+    >>> doppler_effect(100, 330, 0, 10) #source moving towards the observer\n+    103.125\n+    >>> doppler_effect(100, 330, 0, -10) #source moving away from the observer\n+    97.05882352941177\n+    >>> doppler_effect(100, 330, 10, 10) #observer and source moving towards each other\n+    106.25\n+    >>> doppler_effect(100, 330, -10, -10) #observer and source moving away\n+    94.11764705882354\n+    >>> doppler_effect(100, 330, 10, 330) #source moving at the same speed as the wave\n+    Error: Division by zero\n+    Infinite frequency implies vs = v and observer infront of the source\n+    0.0\n+    >>> doppler_effect(100, 330, 10, 340) #source moving faster than the wave\n+    Error: Non-positive frequency\n+    Non-positive frequency implies vs > v or v0 > v(in opposite direction)\n+    0.0\n+    >>> doppler_effect(100, 330, -340, 10) #observer moving faster than the wave\n+    Error: Non-positive frequency\n+    Non-positive frequency implies vs > v or v0 > v(in opposite direction)\n+    0.0\n+    \"\"\"\n+\n+    try:\n+        doppler_freq = (org_freq * (wave_vel + obs_vel)) / (wave_vel - src_vel)\n+    except ZeroDivisionError:\n+        print(\"Error: Division by zero\")\n+        print(\"Infinite frequency implies vs = v and observer infront of the source\")\n+        return 0.0\nComment: `CONTRIBUTING.md` says that algorithmic functions should not `print()`.  Please let the ZeroDivisionError be raised to the caller because the answer is NOT 0.\r\n```suggestion\r\n        print(\"Error: Division by zero\")\r\n        print(\"Infinite frequency implies vs = v and observer infront of the source\")\r\n        return 0.0\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "physics/doppler_frequency.py",
    "pr_number": 10776,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367747924,
    "comment_created_at": "2023-10-21T15:33:49Z"
  },
  {
    "code": "@@ -0,0 +1,51 @@\n+import heapq\n+\n+def dijkstra(graph, start):",
    "comment": "please provide return type hint for the function: dijkstra. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: graph\n\nplease provide type hint for the parameter: start",
    "line_number": 3,
    "enriched": "File: greedy_methods/dijkstra_algorithm.py\nCode: @@ -0,0 +1,51 @@\n+import heapq\n+\n+def dijkstra(graph, start):\nComment: Please provide return type hint for the function: `dijkstra`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `graph`\n\nPlease provide type hint for the parameter: `start`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "greedy_methods/dijkstra_algorithm.py",
    "pr_number": 9517,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342984457,
    "comment_created_at": "2023-10-02T17:49:53Z"
  },
  {
    "code": "@@ -0,0 +1,79 @@\n+\"\"\"\n+A Telegram bot that can remove chat that contains offensive words provided to look after from a  Group or Channel the bot is part of.\n+\n+Prerequisite:\n+1. This script needs two 3rd party packages:  pyTelegramBotAPI and python-dotenv\n+\n+2. This script needs a BOT_TOKEN that can be generated following the simple steps provided in the official guide:\n+https://core.telegram.org/bots/features#creating-a-new-bot\n+\n+\n+Testing Steps:\n+1. Create the BOT using the above official doc\n+2. Add the BOT to a channel or Group\n+3. Install the required packages and run this Python file\n+4. Send any Offensive text that you have set to filter out in the group/channel and see the response\n+\n+\n+Author: Suman Mitra\n+https://youtube.com/@LetsCodeTogether\n+https://github.com/suman2023\n+\"\"\"\n+import os\n+\n+import telebot\n+from dotenv import load_dotenv\n+\n+\"\"\"\n+load_dotenv()\n+\n+Parse a .env file and then load all the variables found as environment variables.\n+\n+Add the BOT_TOKEN variable in the .env file\n+eg. BOT_TOKEN=<the token received from Bot Father>\n+\"\"\"\n+load_dotenv()\n+\n+\n+BOT_TOKEN = os.getenv(\"BOT_TOKEN\")\n+bot = telebot.TeleBot(BOT_TOKEN)\n+\n+# Add the words to be checked for in the set.\n+offensive_words = {\"stupid\", \"shit\", \"thug\"}\n+\n+\"\"\"\n+@bot.message_handler(func=lambda message: message != None)\n+\n+Handles New incoming message of any kind - text, photo, sticker, etc.\n+As a parameter to the decorator function, it passes :class:`telebot.types.Message` object.\n+Telegram Documentation: https://core.telegram.org/bots/api#message\n+\n+\n+bot.delete_message()\n+\n+Use this method to delete a message, including service messages.\n+- If the bot is an administrator of a group, it can delete any message there.\n+Returns True on success.\n+Telegram documentation: https://core.telegram.org/bots/api#deletemessage\n+\"\"\"\n+\n+\n+@bot.message_handler(func=lambda message: message != None)\n+def remove_offensive_chat(message):",
    "comment": "please provide return type hint for the function: remove_offensive_chat. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: message",
    "line_number": 62,
    "enriched": "File: web_programming/remove_offensive_chat_telegram_bot.py\nCode: @@ -0,0 +1,79 @@\n+\"\"\"\n+A Telegram bot that can remove chat that contains offensive words provided to look after from a  Group or Channel the bot is part of.\n+\n+Prerequisite:\n+1. This script needs two 3rd party packages:  pyTelegramBotAPI and python-dotenv\n+\n+2. This script needs a BOT_TOKEN that can be generated following the simple steps provided in the official guide:\n+https://core.telegram.org/bots/features#creating-a-new-bot\n+\n+\n+Testing Steps:\n+1. Create the BOT using the above official doc\n+2. Add the BOT to a channel or Group\n+3. Install the required packages and run this Python file\n+4. Send any Offensive text that you have set to filter out in the group/channel and see the response\n+\n+\n+Author: Suman Mitra\n+https://youtube.com/@LetsCodeTogether\n+https://github.com/suman2023\n+\"\"\"\n+import os\n+\n+import telebot\n+from dotenv import load_dotenv\n+\n+\"\"\"\n+load_dotenv()\n+\n+Parse a .env file and then load all the variables found as environment variables.\n+\n+Add the BOT_TOKEN variable in the .env file\n+eg. BOT_TOKEN=<the token received from Bot Father>\n+\"\"\"\n+load_dotenv()\n+\n+\n+BOT_TOKEN = os.getenv(\"BOT_TOKEN\")\n+bot = telebot.TeleBot(BOT_TOKEN)\n+\n+# Add the words to be checked for in the set.\n+offensive_words = {\"stupid\", \"shit\", \"thug\"}\n+\n+\"\"\"\n+@bot.message_handler(func=lambda message: message != None)\n+\n+Handles New incoming message of any kind - text, photo, sticker, etc.\n+As a parameter to the decorator function, it passes :class:`telebot.types.Message` object.\n+Telegram Documentation: https://core.telegram.org/bots/api#message\n+\n+\n+bot.delete_message()\n+\n+Use this method to delete a message, including service messages.\n+- If the bot is an administrator of a group, it can delete any message there.\n+Returns True on success.\n+Telegram documentation: https://core.telegram.org/bots/api#deletemessage\n+\"\"\"\n+\n+\n+@bot.message_handler(func=lambda message: message != None)\n+def remove_offensive_chat(message):\nComment: Please provide return type hint for the function: `remove_offensive_chat`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `message`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "web_programming/remove_offensive_chat_telegram_bot.py",
    "pr_number": 9876,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1348077986,
    "comment_created_at": "2023-10-05T22:49:00Z"
  },
  {
    "code": "@@ -0,0 +1,112 @@\n+# Author: Your Name\n+# Author email: your.email@example.com\n+# Coding date: Month Year\n+# Black: True\n+\n+\"\"\"\n+    * This code implements the xxHash algorithm:\n+        https://github.com/Cyan4973/xxHash\n+\n+    * xxHash is an extremely fast non-cryptographic hash algorithm that can be used\n+      to hash data for various applications like hash tables, checksums, and more.\n+\n+    * The implemented code consists of:\n+        * A function for calculating the xxHash hash value for given data.\n+\n+    * How to use:\n+        You can use the `xxhash` function to calculate the hash value for your data.\n+\n+    * Example:\n+        data = b'Hello, World!'\n+        seed = 0\n+        hash_value = xxhash(data, seed)\n+        print(f'Hash value: {hash_value}')\n+\n+\"\"\"\n+\n+# Constants\n+PRIME32_1 = 0x9E3779B1\n+PRIME32_2 = 0x85EBCA77\n+PRIME32_3 = 0xC2B2AE3D\n+PRIME32_4 = 0x27D4EB2F\n+PRIME32_5 = 0x165667B1\n+\n+\n+def xxhash(data, seed=0):",
    "comment": "please provide return type hint for the function: xxhash. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: data\n\nplease provide type hint for the parameter: seed",
    "line_number": 35,
    "enriched": "File: hashes/xxhash.py\nCode: @@ -0,0 +1,112 @@\n+# Author: Your Name\n+# Author email: your.email@example.com\n+# Coding date: Month Year\n+# Black: True\n+\n+\"\"\"\n+    * This code implements the xxHash algorithm:\n+        https://github.com/Cyan4973/xxHash\n+\n+    * xxHash is an extremely fast non-cryptographic hash algorithm that can be used\n+      to hash data for various applications like hash tables, checksums, and more.\n+\n+    * The implemented code consists of:\n+        * A function for calculating the xxHash hash value for given data.\n+\n+    * How to use:\n+        You can use the `xxhash` function to calculate the hash value for your data.\n+\n+    * Example:\n+        data = b'Hello, World!'\n+        seed = 0\n+        hash_value = xxhash(data, seed)\n+        print(f'Hash value: {hash_value}')\n+\n+\"\"\"\n+\n+# Constants\n+PRIME32_1 = 0x9E3779B1\n+PRIME32_2 = 0x85EBCA77\n+PRIME32_3 = 0xC2B2AE3D\n+PRIME32_4 = 0x27D4EB2F\n+PRIME32_5 = 0x165667B1\n+\n+\n+def xxhash(data, seed=0):\nComment: Please provide return type hint for the function: `xxhash`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `data`\n\nPlease provide type hint for the parameter: `seed`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "hashes/xxhash.py",
    "pr_number": 11123,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1382537973,
    "comment_created_at": "2023-11-05T09:20:26Z"
  },
  {
    "code": "@@ -0,0 +1,158 @@\n+\"\"\"\n+Implementation of gradient descent algorithm using momentum\n+for minimizing cost of a linear hypothesis function.\n+\"\"\"\n+\n+import numpy as np\n+\n+# List of input, output pairs\n+train_data = (\n+    ((5, 2, 3), 15),\n+    ((6, 5, 9), 25),\n+    ((11, 12, 13), 41),\n+    ((1, 1, 1), 8),\n+    ((11, 12, 13), 41),\n+)\n+test_data = (((515, 22, 13), 555), ((61, 35, 49), 150))\n+parameter_vector = [2, 4, 1, 5]\n+m = len(train_data)\n+LEARNING_RATE = 0.009\n+MOMENTUM = 0.9\n+\n+# Initialize velocity (for momentum)\n+velocity = [0] * len(parameter_vector)\n+\n+\n+def _error(example_no, data_set=\"train\"):",
    "comment": "please provide return type hint for the function: _error. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: example_no\n\nplease provide type hint for the parameter: data_set",
    "line_number": 26,
    "enriched": "File: machine_learning/gradient_descent_momentum.py\nCode: @@ -0,0 +1,158 @@\n+\"\"\"\n+Implementation of gradient descent algorithm using momentum\n+for minimizing cost of a linear hypothesis function.\n+\"\"\"\n+\n+import numpy as np\n+\n+# List of input, output pairs\n+train_data = (\n+    ((5, 2, 3), 15),\n+    ((6, 5, 9), 25),\n+    ((11, 12, 13), 41),\n+    ((1, 1, 1), 8),\n+    ((11, 12, 13), 41),\n+)\n+test_data = (((515, 22, 13), 555), ((61, 35, 49), 150))\n+parameter_vector = [2, 4, 1, 5]\n+m = len(train_data)\n+LEARNING_RATE = 0.009\n+MOMENTUM = 0.9\n+\n+# Initialize velocity (for momentum)\n+velocity = [0] * len(parameter_vector)\n+\n+\n+def _error(example_no, data_set=\"train\"):\nComment: Please provide return type hint for the function: `_error`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `example_no`\n\nPlease provide type hint for the parameter: `data_set`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "machine_learning/gradient_descent_momentum.py",
    "pr_number": 11884,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1791875802,
    "comment_created_at": "2024-10-08T13:26:16Z"
  },
  {
    "code": "@@ -0,0 +1,102 @@\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+\n+@dataclass\n+class Node:\n+    data: int\n+    next_node: Node | None = None\n+\n+def print_linked_list(head: Node | None) -> None:\n+    \"\"\"\n+    Print the entire linked list iteratively.\n+\n+    >>> head = insert_node(None, 0)\n+    >>> head = insert_node(head, 2)\n+    >>> head = insert_node(head, 1)\n+    >>> print_linked_list(head)\n+    0->2->1\n+    >>> head = insert_node(head, 4)\n+    >>> head = insert_node(head, 5)\n+    >>> print_linked_list(head)\n+    0->2->1->4->5\n+    \"\"\"\n+    if head is None:\n+        return\n+    while head.next_node is not None:\n+        print(head.data, end=\"->\")\n+        head = head.next_node\n+    print(head.data)\n+\n+def insert_node(head: Node | None, data: int) -> Node:\n+    \"\"\"\n+    Insert a new node at the end of a linked list\n+    and return the new head.\n+\n+    >>> head = insert_node(None, 10)\n+    >>> head = insert_node(head, 9)\n+    >>> head = insert_node(head, 8)\n+    >>> print_linked_list(head)\n+    10->9->8\n+    \"\"\"\n+    new_node = Node(data)\n+    if head is None:\n+        return new_node\n+\n+    temp_node = head\n+    while temp_node.next_node:\n+        temp_node = temp_node.next_node\n+    temp_node.next_node = new_node  \n+    return head\n+\n+def remove_duplicates(head:Node | None):",
    "comment": "please provide return type hint for the function: remove_duplicates. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 52,
    "enriched": "File: data_structures/linked_list/remove_duplicates.py\nCode: @@ -0,0 +1,102 @@\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+\n+@dataclass\n+class Node:\n+    data: int\n+    next_node: Node | None = None\n+\n+def print_linked_list(head: Node | None) -> None:\n+    \"\"\"\n+    Print the entire linked list iteratively.\n+\n+    >>> head = insert_node(None, 0)\n+    >>> head = insert_node(head, 2)\n+    >>> head = insert_node(head, 1)\n+    >>> print_linked_list(head)\n+    0->2->1\n+    >>> head = insert_node(head, 4)\n+    >>> head = insert_node(head, 5)\n+    >>> print_linked_list(head)\n+    0->2->1->4->5\n+    \"\"\"\n+    if head is None:\n+        return\n+    while head.next_node is not None:\n+        print(head.data, end=\"->\")\n+        head = head.next_node\n+    print(head.data)\n+\n+def insert_node(head: Node | None, data: int) -> Node:\n+    \"\"\"\n+    Insert a new node at the end of a linked list\n+    and return the new head.\n+\n+    >>> head = insert_node(None, 10)\n+    >>> head = insert_node(head, 9)\n+    >>> head = insert_node(head, 8)\n+    >>> print_linked_list(head)\n+    10->9->8\n+    \"\"\"\n+    new_node = Node(data)\n+    if head is None:\n+        return new_node\n+\n+    temp_node = head\n+    while temp_node.next_node:\n+        temp_node = temp_node.next_node\n+    temp_node.next_node = new_node  \n+    return head\n+\n+def remove_duplicates(head:Node | None):\nComment: Please provide return type hint for the function: `remove_duplicates`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/linked_list/remove_duplicates.py",
    "pr_number": 9395,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342330333,
    "comment_created_at": "2023-10-02T06:51:50Z"
  },
  {
    "code": "@@ -0,0 +1,163 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval=None):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: initval",
    "line_number": 21,
    "enriched": "File: sorts/tree_sort_2.py\nCode: @@ -0,0 +1,163 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval=None):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `initval`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "sorts/tree_sort_2.py",
    "pr_number": 7457,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000714984,
    "comment_created_at": "2022-10-20T14:29:17Z"
  },
  {
    "code": "@@ -0,0 +1,341 @@\n+\"\"\"\n+Binary Search Tree Implementation\n+\n+This implementation provides a binary search tree (BST) with basic operations\n+including insertion, search, deletion, and in-order traversal. Each operation\n+leverages recursive helper functions.\n+\n+Binary Search Tree Implementation with Doctest Examples\n+For more information on binary search trees, please see:\n+https://en.wikipedia.org/wiki/Binary_search_tree\n+\n+To run the doctests:\n+    python -m doctest -v binary_search_tree.py\n+\"\"\"\n+\n+\n+class BSTNode:\n+    \"\"\"\n+    A node in the binary search tree.\n+\n+    Attributes\n+    ----------\n+    key : int\n+        The key value stored in the node.\n+    left : BSTNode or None\n+        The left child node.\n+    right : BSTNode or None\n+        The right child node.\n+    \"\"\"\n+\n+    def __init__(self, key: int) -> None:\n+        \"\"\"\n+        Initializes a new BST node.\n+\n+        Parameters\n+        ----------\n+        key : int\n+            The key value for the new node.\n+        \"\"\"\n+        self.key = key\n+        self.left = None\n+        self.right = None\n+\n+\n+class BinarySearchTree:\n+    \"\"\"\n+    Binary Search Tree (BST) class that supports basic operations such as\n+    insertion, search, deletion, and in-order traversal.\n+\n+    For details on BSTs, see:\n+    https://en.wikipedia.org/wiki/Binary_search_tree\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        \"\"\"\n+        Initializes an empty Binary Search Tree.\n+        \"\"\"\n+        self.root = None\n+\n+    def insert(self, key: int) -> None:\n+        \"\"\"\n+        Inserts a new key into the BST.\n+\n+        Parameters\n+        ----------\n+        key : int\n+            The key to be inserted into the BST.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> bst.insert(10)\n+        >>> bst.insert(5)\n+        >>> bst.insert(15)\n+        >>> bst.inorder_traversal()\n+        [5, 10, 15]\n+        \"\"\"\n+        if self.root is None:\n+            self.root = BSTNode(key)\n+        else:\n+            self._insert_recursive(self.root, key)\n+\n+    def _insert_recursive(self, node: BSTNode, key: int) -> None:\n+        \"\"\"\n+        Recursively inserts a new key into the subtree rooted at the given node.\n+\n+        Parameters\n+        ----------\n+        node : BSTNode\n+            The current node in the BST.\n+        key : int\n+            The key to be inserted.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> bst.root = BSTNode(10)\n+        >>> bst._insert_recursive(bst.root, 5)\n+        >>> bst._insert_recursive(bst.root, 15)\n+        >>> bst.inorder_traversal()\n+        [5, 10, 15]\n+        \"\"\"\n+        if key < node.key:\n+            if node.left is None:\n+                node.left = BSTNode(key)\n+            else:\n+                self._insert_recursive(node.left, key)\n+        else:\n+            if node.right is None:\n+                node.right = BSTNode(key)\n+            else:\n+                self._insert_recursive(node.right, key)\n+\n+    def inorder_traversal(self) -> list:\n+        \"\"\"\n+        Performs an in-order traversal of the BST and returns the keys in sorted order.\n+\n+        Returns\n+        -------\n+        list\n+            A list of keys in increasing order.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> bst.inorder_traversal()  # For an empty BST.\n+        []\n+        >>> bst.insert(10)\n+        >>> bst.insert(5)\n+        >>> bst.insert(15)\n+        >>> bst.inorder_traversal()\n+        [5, 10, 15]\n+        >>> bst.insert(7)\n+        >>> bst.inorder_traversal()\n+        [5, 7, 10, 15]\n+        \"\"\"\n+        result = []\n+        self._inorder_recursive(self.root, result)\n+        return result\n+\n+    def _inorder_recursive(self, node: BSTNode, result: list) -> None:\n+        \"\"\"\n+        Helper function for recursively performing in-order traversal by\n+        accumulating the keys in the provided list.\n+\n+        Parameters\n+        ----------\n+        node : BSTNode or None\n+            The current node being visited.\n+        result : list\n+            The list to accumulate the keys.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> # Manually build a simple BST.\n+        >>> bst.root = BSTNode(20)\n+        >>> bst.root.left = BSTNode(10)\n+        >>> bst.root.right = BSTNode(30)\n+        >>> result = []\n+        >>> bst._inorder_recursive(bst.root, result)\n+        >>> result\n+        [10, 20, 30]\n+        >>> # If the subtree is empty, the result remains unchanged.\n+        >>> result = [1, 2, 3]\n+        >>> bst._inorder_recursive(None, result)\n+        >>> result\n+        [1, 2, 3]\n+        \"\"\"\n+        if node is not None:\n+            self._inorder_recursive(node.left, result)\n+            result.append(node.key)\n+            self._inorder_recursive(node.right, result)\n+\n+    def search(self, key: int):",
    "comment": "please provide return type hint for the function: search. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 175,
    "enriched": "File: tree/binary_search_tree.py\nCode: @@ -0,0 +1,341 @@\n+\"\"\"\n+Binary Search Tree Implementation\n+\n+This implementation provides a binary search tree (BST) with basic operations\n+including insertion, search, deletion, and in-order traversal. Each operation\n+leverages recursive helper functions.\n+\n+Binary Search Tree Implementation with Doctest Examples\n+For more information on binary search trees, please see:\n+https://en.wikipedia.org/wiki/Binary_search_tree\n+\n+To run the doctests:\n+    python -m doctest -v binary_search_tree.py\n+\"\"\"\n+\n+\n+class BSTNode:\n+    \"\"\"\n+    A node in the binary search tree.\n+\n+    Attributes\n+    ----------\n+    key : int\n+        The key value stored in the node.\n+    left : BSTNode or None\n+        The left child node.\n+    right : BSTNode or None\n+        The right child node.\n+    \"\"\"\n+\n+    def __init__(self, key: int) -> None:\n+        \"\"\"\n+        Initializes a new BST node.\n+\n+        Parameters\n+        ----------\n+        key : int\n+            The key value for the new node.\n+        \"\"\"\n+        self.key = key\n+        self.left = None\n+        self.right = None\n+\n+\n+class BinarySearchTree:\n+    \"\"\"\n+    Binary Search Tree (BST) class that supports basic operations such as\n+    insertion, search, deletion, and in-order traversal.\n+\n+    For details on BSTs, see:\n+    https://en.wikipedia.org/wiki/Binary_search_tree\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        \"\"\"\n+        Initializes an empty Binary Search Tree.\n+        \"\"\"\n+        self.root = None\n+\n+    def insert(self, key: int) -> None:\n+        \"\"\"\n+        Inserts a new key into the BST.\n+\n+        Parameters\n+        ----------\n+        key : int\n+            The key to be inserted into the BST.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> bst.insert(10)\n+        >>> bst.insert(5)\n+        >>> bst.insert(15)\n+        >>> bst.inorder_traversal()\n+        [5, 10, 15]\n+        \"\"\"\n+        if self.root is None:\n+            self.root = BSTNode(key)\n+        else:\n+            self._insert_recursive(self.root, key)\n+\n+    def _insert_recursive(self, node: BSTNode, key: int) -> None:\n+        \"\"\"\n+        Recursively inserts a new key into the subtree rooted at the given node.\n+\n+        Parameters\n+        ----------\n+        node : BSTNode\n+            The current node in the BST.\n+        key : int\n+            The key to be inserted.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> bst.root = BSTNode(10)\n+        >>> bst._insert_recursive(bst.root, 5)\n+        >>> bst._insert_recursive(bst.root, 15)\n+        >>> bst.inorder_traversal()\n+        [5, 10, 15]\n+        \"\"\"\n+        if key < node.key:\n+            if node.left is None:\n+                node.left = BSTNode(key)\n+            else:\n+                self._insert_recursive(node.left, key)\n+        else:\n+            if node.right is None:\n+                node.right = BSTNode(key)\n+            else:\n+                self._insert_recursive(node.right, key)\n+\n+    def inorder_traversal(self) -> list:\n+        \"\"\"\n+        Performs an in-order traversal of the BST and returns the keys in sorted order.\n+\n+        Returns\n+        -------\n+        list\n+            A list of keys in increasing order.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> bst.inorder_traversal()  # For an empty BST.\n+        []\n+        >>> bst.insert(10)\n+        >>> bst.insert(5)\n+        >>> bst.insert(15)\n+        >>> bst.inorder_traversal()\n+        [5, 10, 15]\n+        >>> bst.insert(7)\n+        >>> bst.inorder_traversal()\n+        [5, 7, 10, 15]\n+        \"\"\"\n+        result = []\n+        self._inorder_recursive(self.root, result)\n+        return result\n+\n+    def _inorder_recursive(self, node: BSTNode, result: list) -> None:\n+        \"\"\"\n+        Helper function for recursively performing in-order traversal by\n+        accumulating the keys in the provided list.\n+\n+        Parameters\n+        ----------\n+        node : BSTNode or None\n+            The current node being visited.\n+        result : list\n+            The list to accumulate the keys.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> # Manually build a simple BST.\n+        >>> bst.root = BSTNode(20)\n+        >>> bst.root.left = BSTNode(10)\n+        >>> bst.root.right = BSTNode(30)\n+        >>> result = []\n+        >>> bst._inorder_recursive(bst.root, result)\n+        >>> result\n+        [10, 20, 30]\n+        >>> # If the subtree is empty, the result remains unchanged.\n+        >>> result = [1, 2, 3]\n+        >>> bst._inorder_recursive(None, result)\n+        >>> result\n+        [1, 2, 3]\n+        \"\"\"\n+        if node is not None:\n+            self._inorder_recursive(node.left, result)\n+            result.append(node.key)\n+            self._inorder_recursive(node.right, result)\n+\n+    def search(self, key: int):\nComment: Please provide return type hint for the function: `search`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "tree/binary_search_tree.py",
    "pr_number": 12779,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2121759500,
    "comment_created_at": "2025-06-02T17:27:26Z"
  },
  {
    "code": "@@ -0,0 +1,145 @@\n+class Node():\n+    def __init__(self, start, end):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: start\n\nplease provide type hint for the parameter: end",
    "line_number": 2,
    "enriched": "File: data_structures/binary_tree/segment_tree_node.py\nCode: @@ -0,0 +1,145 @@\n+class Node():\n+    def __init__(self, start, end):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `start`\n\nPlease provide type hint for the parameter: `end`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/binary_tree/segment_tree_node.py",
    "pr_number": 12707,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2072877721,
    "comment_created_at": "2025-05-05T05:59:15Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+def find_median(nums):",
    "comment": "please provide return type hint for the function: find_median. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: nums",
    "line_number": 1,
    "enriched": "File: data_structures/arrays/median.py\nCode: @@ -0,0 +1,35 @@\n+def find_median(nums):\nComment: Please provide return type hint for the function: `find_median`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `nums`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/arrays/median.py",
    "pr_number": 11006,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1373553614,
    "comment_created_at": "2023-10-26T18:10:26Z"
  },
  {
    "code": "@@ -0,0 +1,32 @@\n+import requests\n+\n+# Function to get geolocation data for an IP address\n+def get_ip_geolocation(ip_address):",
    "comment": "please provide return type hint for the function: get_ip_geolocation. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: ip_address",
    "line_number": 4,
    "enriched": "File: web_programming/get_ip_geolocation.py\nCode: @@ -0,0 +1,32 @@\n+import requests\n+\n+# Function to get geolocation data for an IP address\n+def get_ip_geolocation(ip_address):\nComment: Please provide return type hint for the function: `get_ip_geolocation`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `ip_address`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "web_programming/get_ip_geolocation.py",
    "pr_number": 10902,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1370224543,
    "comment_created_at": "2023-10-24T14:01:01Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+\n+\"\"\"\n+\tEvery element represents the height of the block. The width of the block can be considered 1.\n+\tFind the volume of trapped rainwater between these block.\n+\"\"\"\n+\n+height = [34, 2, 5, 2, 3, 23, 18, 23, 45]\n+answer = 162\n+\n+def max_water(height):",
    "comment": "please provide return type hint for the function: max_water. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: height",
    "line_number": 10,
    "enriched": "File: data_structures/stacks/Trapping_Rain_Water.py\nCode: @@ -0,0 +1,53 @@\n+\n+\"\"\"\n+\tEvery element represents the height of the block. The width of the block can be considered 1.\n+\tFind the volume of trapped rainwater between these block.\n+\"\"\"\n+\n+height = [34, 2, 5, 2, 3, 23, 18, 23, 45]\n+answer = 162\n+\n+def max_water(height):\nComment: Please provide return type hint for the function: `max_water`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `height`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/stacks/Trapping_Rain_Water.py",
    "pr_number": 7223,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996288650,
    "comment_created_at": "2022-10-15T10:45:49Z"
  },
  {
    "code": "@@ -0,0 +1,126 @@\n+from __future__ import annotations\n+class Node:\n+    \"\"\"\n+prints the inorder Traversal of transformed tree\n+>>> sum = 0\n+>>> root = Node(11)\n+>>> root.left = Node(2)\n+>>> root.right = Node(29)\n+>>> root.left.left = Node(1)\n+>>> root.left.right = Node(7)\n+>>> root.right.left = Node(15)\n+>>> root.right.right = Node(40)\n+>>> root.right.right.left = Node(35)\n+>>> printInorder(root)\n+1 2 7 11 15 29 35 40 \n+\n+>>> transformTree(root)\n+\n+>>> printInorder(root)\n+139 137 130 119 104 75 40 0\n+\n+\"\"\"\n+\n+    def __init__(self, number:int) -> None:\n+        self.data = number\n+        self.left = None\n+        self.right = None\n+\n+# Recursive function to transform a BST to sum tree.\n+# This function traverses the tree in reverse inorder so\n+# that we have visited all greater key nodes of the currently\n+# visited node\n+def transform_tree_util(root:Node | None) -> None:\n+    \"\"\"\n+    Transform a binary tree into a sum tree.\n+    \n+    Example:\n+    >>> root = Node(11)\n+    >>> root.left = Node(2)\n+    >>> root.right = Node(29)\n+    >>> transformTree(root)\n+    >>> root.data\n+    60\n+    >>> root.left.data\n+    31\n+    >>> root.right.data\n+    29\n+    \"\"\"\n+\n+    # Base case\n+    if (root == None):\n+        return\n+\n+    # Recur for right subtree\n+    transform_tree_util(root.right)\n+\n+    # Update sum\n+    global sum\n+    sum = sum + root.data\n+\n+    # Store old sum in the current node\n+    root.data = sum - root.data\n+\n+    # Recur for left subtree\n+    transform_tree_util(root.left)\n+\n+# A wrapper over transformTreeUtil()\n+def transform_tree(root:Node | None) -> None:\n+    \"\"\"\n+    Transform a binary tree into a sum tree.\n+    \n+    Example:\n+    >>> root = Node(11)\n+    >>> root.left = Node(2)\n+    >>> root.right = Node(29)\n+    >>> transformTree(root)\n+    >>> root.data\n+    60\n+    >>> root.left.data\n+    31\n+    >>> root.right.data\n+    29\n+    \"\"\"\n+\n+    # sum = 0 #Initialize sum\n+    transform_tree_util(root)\n+\n+# A utility function to prindorder traversal of a\n+# binary tree\n+def print_inorder(root:Node | None):",
    "comment": "please provide return type hint for the function: print_inorder. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 90,
    "enriched": "File: data_structures/binary_tree/transform_bst_sum_tree.py\nCode: @@ -0,0 +1,126 @@\n+from __future__ import annotations\n+class Node:\n+    \"\"\"\n+prints the inorder Traversal of transformed tree\n+>>> sum = 0\n+>>> root = Node(11)\n+>>> root.left = Node(2)\n+>>> root.right = Node(29)\n+>>> root.left.left = Node(1)\n+>>> root.left.right = Node(7)\n+>>> root.right.left = Node(15)\n+>>> root.right.right = Node(40)\n+>>> root.right.right.left = Node(35)\n+>>> printInorder(root)\n+1 2 7 11 15 29 35 40 \n+\n+>>> transformTree(root)\n+\n+>>> printInorder(root)\n+139 137 130 119 104 75 40 0\n+\n+\"\"\"\n+\n+    def __init__(self, number:int) -> None:\n+        self.data = number\n+        self.left = None\n+        self.right = None\n+\n+# Recursive function to transform a BST to sum tree.\n+# This function traverses the tree in reverse inorder so\n+# that we have visited all greater key nodes of the currently\n+# visited node\n+def transform_tree_util(root:Node | None) -> None:\n+    \"\"\"\n+    Transform a binary tree into a sum tree.\n+    \n+    Example:\n+    >>> root = Node(11)\n+    >>> root.left = Node(2)\n+    >>> root.right = Node(29)\n+    >>> transformTree(root)\n+    >>> root.data\n+    60\n+    >>> root.left.data\n+    31\n+    >>> root.right.data\n+    29\n+    \"\"\"\n+\n+    # Base case\n+    if (root == None):\n+        return\n+\n+    # Recur for right subtree\n+    transform_tree_util(root.right)\n+\n+    # Update sum\n+    global sum\n+    sum = sum + root.data\n+\n+    # Store old sum in the current node\n+    root.data = sum - root.data\n+\n+    # Recur for left subtree\n+    transform_tree_util(root.left)\n+\n+# A wrapper over transformTreeUtil()\n+def transform_tree(root:Node | None) -> None:\n+    \"\"\"\n+    Transform a binary tree into a sum tree.\n+    \n+    Example:\n+    >>> root = Node(11)\n+    >>> root.left = Node(2)\n+    >>> root.right = Node(29)\n+    >>> transformTree(root)\n+    >>> root.data\n+    60\n+    >>> root.left.data\n+    31\n+    >>> root.right.data\n+    29\n+    \"\"\"\n+\n+    # sum = 0 #Initialize sum\n+    transform_tree_util(root)\n+\n+# A utility function to prindorder traversal of a\n+# binary tree\n+def print_inorder(root:Node | None):\nComment: Please provide return type hint for the function: `print_inorder`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/binary_tree/transform_bst_sum_tree.py",
    "pr_number": 9777,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346811388,
    "comment_created_at": "2023-10-05T05:39:04Z"
  },
  {
    "code": "@@ -0,0 +1,145 @@\n+# Python3 program to create target string, starting from\n+# random string using Genetic Algorithm\n+\n+import random\n+\n+# Number of individuals in each generation\n+POPULATION_SIZE = 100\n+\n+# Valid genes\n+GENES = '''abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOP\n+QRSTUVWXYZ 1234567890, .-;:_!\"#%&/()=?@${[]}'''\n+\n+# Target string to be generated\n+TARGET = \"I love GeeksforGeeks\"\n+\n+class Individual(object):\n+\t'''\n+\tClass representing individual in population\n+\t'''\n+\tdef __init__(self, chromosome):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: chromosome",
    "line_number": 20,
    "enriched": "File: genetic_algorithm/approach2.py\nCode: @@ -0,0 +1,145 @@\n+# Python3 program to create target string, starting from\n+# random string using Genetic Algorithm\n+\n+import random\n+\n+# Number of individuals in each generation\n+POPULATION_SIZE = 100\n+\n+# Valid genes\n+GENES = '''abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOP\n+QRSTUVWXYZ 1234567890, .-;:_!\"#%&/()=?@${[]}'''\n+\n+# Target string to be generated\n+TARGET = \"I love GeeksforGeeks\"\n+\n+class Individual(object):\n+\t'''\n+\tClass representing individual in population\n+\t'''\n+\tdef __init__(self, chromosome):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `chromosome`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "genetic_algorithm/approach2.py",
    "pr_number": 9624,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1344476187,
    "comment_created_at": "2023-10-03T17:26:49Z"
  },
  {
    "code": "@@ -0,0 +1,64 @@\n+import math\n+\n+from geometry.shapes.shape_types.closed_shapes import ClosedShape\n+\n+\n+class Circle(ClosedShape):\n+\n+    \"\"\"\n+    a structure which represents a\n+    geometrical circle on a 2D surface\n+\n+    >>> circle_one = Circle(5)\n+    >>> circle_one.get_diameter()\n+    10\n+    >>> circle_one.perimeter()\n+    31.41592653589793\n+    >>> circle_one.is_similar(None)\n+    Traceback (most recent call last):\n+    NotImplementedError: Not Implemented\n+    >>> circle_one.split()\n+    Traceback (most recent call last):\n+    NotImplementedError: Not Implemented\n+    >>> circle_one.max_parts(54)\n+    1486.0\n+    >>> circle_one.max_parts(7)\n+    29.0\n+    >>> circle_one.max_parts(22.5)\n+    265.375\n+    >>> circle_one.max_parts(-222)\n+    -1\n+    >>> circle_one.max_parts(\"-222\")\n+    Traceback (most recent call last):\n+    TypeError: num_cuts must be a numeric value.\n+    \"\"\"\n+\n+    def __init__(self, radius):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: radius",
    "line_number": 36,
    "enriched": "File: geometry/shapes/ellipses/circle.py\nCode: @@ -0,0 +1,64 @@\n+import math\n+\n+from geometry.shapes.shape_types.closed_shapes import ClosedShape\n+\n+\n+class Circle(ClosedShape):\n+\n+    \"\"\"\n+    a structure which represents a\n+    geometrical circle on a 2D surface\n+\n+    >>> circle_one = Circle(5)\n+    >>> circle_one.get_diameter()\n+    10\n+    >>> circle_one.perimeter()\n+    31.41592653589793\n+    >>> circle_one.is_similar(None)\n+    Traceback (most recent call last):\n+    NotImplementedError: Not Implemented\n+    >>> circle_one.split()\n+    Traceback (most recent call last):\n+    NotImplementedError: Not Implemented\n+    >>> circle_one.max_parts(54)\n+    1486.0\n+    >>> circle_one.max_parts(7)\n+    29.0\n+    >>> circle_one.max_parts(22.5)\n+    265.375\n+    >>> circle_one.max_parts(-222)\n+    -1\n+    >>> circle_one.max_parts(\"-222\")\n+    Traceback (most recent call last):\n+    TypeError: num_cuts must be a numeric value.\n+    \"\"\"\n+\n+    def __init__(self, radius):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `radius`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "geometry/shapes/ellipses/circle.py",
    "pr_number": 11138,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1389545836,
    "comment_created_at": "2023-11-10T15:29:14Z"
  },
  {
    "code": "@@ -0,0 +1,123 @@\n+import numpy as np\n+\n+def parse_function(user_input):",
    "comment": "please provide return type hint for the function: parse_function. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: user_input",
    "line_number": 3,
    "enriched": "File: genetic_algorithm/function_optimization.py\nCode: @@ -0,0 +1,123 @@\n+import numpy as np\n+\n+def parse_function(user_input):\nComment: Please provide return type hint for the function: `parse_function`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `user_input`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "genetic_algorithm/function_optimization.py",
    "pr_number": 11611,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1782531054,
    "comment_created_at": "2024-10-01T10:31:31Z"
  },
  {
    "code": "@@ -10,6 +10,24 @@\n \n \n def check_bipartite(graph):\n+    \"\"\"\n+    >>> check_bipartite(\n+    ... {0: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2]}\n+    ... )\n+    True\n+    >>> check_bipartite(\n+    ... {0: [1, 2, 3], 1: [0, 2], 2: [0, 1, 3], 3: [0, 2]}\n+    ... )\n+    False\n+    >>> check_bipartite(\n+    ... {0: [4], 1: [], 2: [4], 3: [4], 4: [0, 2, 3]}\n+    ... )\n+    True\n+    >>> check_bipartite(\n+    ... {0: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2], 4: [0]}\n+    ... )\n+    False",
    "comment": "please add some negative values, floating point values, and some strings.\r\nalso,\r\n{}\r\n{7: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2], 4: [0]}\r\n{0: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2], 9: [0]}",
    "line_number": 29,
    "enriched": "File: graphs/check_bipartite_graph_bfs.py\nCode: @@ -10,6 +10,24 @@\n \n \n def check_bipartite(graph):\n+    \"\"\"\n+    >>> check_bipartite(\n+    ... {0: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2]}\n+    ... )\n+    True\n+    >>> check_bipartite(\n+    ... {0: [1, 2, 3], 1: [0, 2], 2: [0, 1, 3], 3: [0, 2]}\n+    ... )\n+    False\n+    >>> check_bipartite(\n+    ... {0: [4], 1: [], 2: [4], 3: [4], 4: [0, 2, 3]}\n+    ... )\n+    True\n+    >>> check_bipartite(\n+    ... {0: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2], 4: [0]}\n+    ... )\n+    False\nComment: Please add some negative values, floating point values, and some strings.\r\nAlso,\r\n`{}`\r\n`{7: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2], 4: [0]}`\r\n`{0: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2], 9: [0]}`\r\n",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "graphs/check_bipartite_graph_bfs.py",
    "pr_number": 10688,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1365673281,
    "comment_created_at": "2023-10-19T14:45:05Z"
  },
  {
    "code": "@@ -0,0 +1,47 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Instalation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interval_start: The start of your internal scale.\n+            - interval_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Exemples:\n+    interval_start:0\n+    interval_end:100\n+    number:50\n+    output: 50.0\n+    interval_start:6\n+    interval_end:12\n+    number:9\n+    output:50.0\n+\n+    Link: https://en.wikipedia.org/wiki/Conversion_of_units\n+\"\"\"\n+\n+\n+def absolute_conversion(interval_start: float, interval_end: float, number: float):",
    "comment": "please provide return type hint for the function: absolute_conversion. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 26,
    "enriched": "File: conversions/absolute_conversion.py\nCode: @@ -0,0 +1,47 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Instalation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interval_start: The start of your internal scale.\n+            - interval_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Exemples:\n+    interval_start:0\n+    interval_end:100\n+    number:50\n+    output: 50.0\n+    interval_start:6\n+    interval_end:12\n+    number:9\n+    output:50.0\n+\n+    Link: https://en.wikipedia.org/wiki/Conversion_of_units\n+\"\"\"\n+\n+\n+def absolute_conversion(interval_start: float, interval_end: float, number: float):\nComment: Please provide return type hint for the function: `absolute_conversion`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "conversions/absolute_conversion.py",
    "pr_number": 7253,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996340231,
    "comment_created_at": "2022-10-15T18:33:59Z"
  },
  {
    "code": "@@ -53,6 +53,40 @@ def volume_of_gas_system(moles: float, kelvin: float, pressure: float) -> float:\n     return moles * kelvin * UNIVERSAL_GAS_CONSTANT / pressure\n \n \n+def temperature_of_gas_system(moles: float, volume: float, pressure: float) -> float:\n+    \"\"\"\n+    >>> temperature_of_gas_system(2, 100, 5)\n+    30.068090996146232\n+    >>> temperature_of_gas_system(11,5009,1000)\n+    54767.66101807144\n+    >>> temperature_of_gas_system(3, -0.46, 23.5)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Invalid inputs. Enter positive value.\n+    \"\"\"\n+    if moles < 0 or volume < 0 or pressure < 0:\n+        raise ValueError(\"Invalid inputs. Enter positive value.\")\n+\n+    return (pressure * volume) / (moles * UNIVERSAL_GAS_CONSTANT)\n+\n+\n+def num_moles_of_gas_in_system(kelvin: float, volume: float, pressure: float) -> float:",
    "comment": "more consistent name?",
    "line_number": 73,
    "enriched": "File: physics/ideal_gas_law.py\nCode: @@ -53,6 +53,40 @@ def volume_of_gas_system(moles: float, kelvin: float, pressure: float) -> float:\n     return moles * kelvin * UNIVERSAL_GAS_CONSTANT / pressure\n \n \n+def temperature_of_gas_system(moles: float, volume: float, pressure: float) -> float:\n+    \"\"\"\n+    >>> temperature_of_gas_system(2, 100, 5)\n+    30.068090996146232\n+    >>> temperature_of_gas_system(11,5009,1000)\n+    54767.66101807144\n+    >>> temperature_of_gas_system(3, -0.46, 23.5)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Invalid inputs. Enter positive value.\n+    \"\"\"\n+    if moles < 0 or volume < 0 or pressure < 0:\n+        raise ValueError(\"Invalid inputs. Enter positive value.\")\n+\n+    return (pressure * volume) / (moles * UNIVERSAL_GAS_CONSTANT)\n+\n+\n+def num_moles_of_gas_in_system(kelvin: float, volume: float, pressure: float) -> float:\nComment: ```suggestion\r\ndef moles_of_gas_system(kelvin: float, volume: float, pressure: float) -> float:\r\n```\r\nMore consistent name?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "physics/ideal_gas_law.py",
    "pr_number": 8919,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1282687570,
    "comment_created_at": "2023-08-03T06:07:19Z"
  },
  {
    "code": "@@ -0,0 +1,175 @@\n+\"\"\"\n+Python program for the Fractionated Morse Cipher.\n+\n+The Fractionated Morse cipher first converts the plaintext to morse code,\n+then enciphers fixed-size blocks of morse code back to letters.\n+This procedure means plaintext letters are mixed into the ciphertext letters,\n+making it more secure than substitution ciphers.\n+\n+For more information visit - http://practicalcryptography.com/ciphers/fractionated-morse-cipher/\n+\n+\"\"\"\n+\n+\n+import string\n+\n+# Define Morse code dictionary\n+MORSE_CODE_DICT = {\n+    \"A\": \".-\",\n+    \"B\": \"-...\",\n+    \"C\": \"-.-.\",\n+    \"D\": \"-..\",\n+    \"E\": \".\",\n+    \"F\": \"..-.\",\n+    \"G\": \"--.\",\n+    \"H\": \"....\",\n+    \"I\": \"..\",\n+    \"J\": \".---\",\n+    \"K\": \"-.-\",\n+    \"L\": \".-..\",\n+    \"M\": \"--\",\n+    \"N\": \"-.\",\n+    \"O\": \"---\",\n+    \"P\": \".--.\",\n+    \"Q\": \"--.-\",\n+    \"R\": \".-.\",\n+    \"S\": \"...\",\n+    \"T\": \"-\",\n+    \"U\": \"..-\",\n+    \"V\": \"...-\",\n+    \"W\": \".--\",\n+    \"X\": \"-..-\",\n+    \"Y\": \"-.--\",\n+    \"Z\": \"--..\",\n+    \" \": \"\",\n+}\n+\n+# Define possible trigrams of Morse code\n+MORSE_COMBINATIONS = [\n+    \"...\",\n+    \"..-\",\n+    \"..x\",\n+    \".-.\",\n+    \".--\",\n+    \".-x\",\n+    \".x.\",\n+    \".x-\",\n+    \".xx\",\n+    \"-..\",\n+    \"-.-\",\n+    \"-.x\",\n+    \"--.\",\n+    \"---\",\n+    \"--x\",\n+    \"-x.\",\n+    \"-x-\",\n+    \"-xx\",\n+    \"x..\",\n+    \"x.-\",\n+    \"x.x\",\n+    \"x-.\",\n+    \"x--\",\n+    \"x-x\",\n+    \"xx.\",\n+    \"xx-\",\n+    \"xxx\",\n+]\n+\n+# Create a reverse dictionary for Morse code\n+REVERSE_DICT = {value: key for key, value in MORSE_CODE_DICT.items()}\n+\n+\n+def encode_to_morse(plaintext: str) -> str:\n+    \"\"\"Encode a plaintext message into Morse code.\n+\n+    Args:\n+        plaintext (str): The plaintext message to encode.\n+\n+    Returns:\n+        str: The Morse code representation of the plaintext message.\n+\n+    Example:\n+        >>> encode_to_morse(\"defend the east\")\n+        '-..x.x..-.x.x-.x-..xx-x....x.xx.x.-x...x-'\n+    \"\"\"\n+    return \"x\".join([MORSE_CODE_DICT.get(letter.upper(), \"\") for letter in plaintext])\n+\n+\n+def encrypt_fractionated_morse(plaintext: str, key: str) -> str:\n+    \"\"\"Encrypt a plaintext message using Fractionated Morse Cipher.\n+\n+    Args:\n+        plaintext (str): The plaintext message to encrypt.\n+        key (str): The encryption key.\n+\n+    Returns:\n+        str: The encrypted ciphertext.\n+\n+    Example:\n+        >>> encrypt_fractionated_morse(\"defend the east\",\"Roundtable\")\n+        'ESOAVVLJRSSTRX'\n+\n+    \"\"\"\n+    morse_code = encode_to_morse(plaintext)\n+    key = key.upper() + string.ascii_uppercase\n+    key = \"\".join(sorted(set(key), key=key.find))\n+\n+    # Ensure morse_code length is a multiple of 3\n+    padding_length = 3 - (len(morse_code) % 3)\n+    morse_code += \"x\" * padding_length\n+\n+    fractionated_morse_dict = {\n+        value: key for key, value in zip(key, MORSE_COMBINATIONS)\n+    }\n+    fractionated_morse_dict[\"xxx\"] = \"\"\n+    encrypted_text = \"\".join(\n+        [\n+            fractionated_morse_dict[morse_code[i : i + 3]]\n+            for i in range(0, len(morse_code), 3)\n+        ]\n+    )\n+    return encrypted_text\n+\n+\n+def decrypt_fractionated_morse(ciphertext: str, key: str) -> str:\n+    \"\"\"Decrypt a ciphertext message encrypted with Fractionated Morse Cipher.\n+\n+    Args:\n+        ciphertext (str): The ciphertext message to decrypt.\n+        key (str): The decryption key.\n+\n+    Returns:\n+        str: The decrypted plaintext message.\n+\n+    Example:\n+        >>> decrypt_fractionated_morse(\"ESOAVVLJRSSTRX\",\"Roundtable\")\n+        'DEFEND THE EAST'\n+    \"\"\"\n+    key = key.upper() + string.ascii_uppercase\n+    key = \"\".join(sorted(set(key), key=key.find))\n+\n+    inverse_fractionated_morse_dict = dict(zip(key, MORSE_COMBINATIONS))\n+    morse_code = \"\".join(\n+        [inverse_fractionated_morse_dict.get(letter, \"\") for letter in ciphertext]\n+    )\n+    decrypted_text = \"\".join(\n+        [REVERSE_DICT[code] for code in morse_code.split(\"x\")]\n+    ).strip()\n+    return decrypted_text\n+\n+\n+def main():",
    "comment": "as there is no test file in this pull request nor any test function or class in the file ciphers/fractionated_morse_cipher.py, please provide doctest for the function main\n\nplease provide return type hint for the function: main. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 161,
    "enriched": "File: ciphers/fractionated_morse_cipher.py\nCode: @@ -0,0 +1,175 @@\n+\"\"\"\n+Python program for the Fractionated Morse Cipher.\n+\n+The Fractionated Morse cipher first converts the plaintext to morse code,\n+then enciphers fixed-size blocks of morse code back to letters.\n+This procedure means plaintext letters are mixed into the ciphertext letters,\n+making it more secure than substitution ciphers.\n+\n+For more information visit - http://practicalcryptography.com/ciphers/fractionated-morse-cipher/\n+\n+\"\"\"\n+\n+\n+import string\n+\n+# Define Morse code dictionary\n+MORSE_CODE_DICT = {\n+    \"A\": \".-\",\n+    \"B\": \"-...\",\n+    \"C\": \"-.-.\",\n+    \"D\": \"-..\",\n+    \"E\": \".\",\n+    \"F\": \"..-.\",\n+    \"G\": \"--.\",\n+    \"H\": \"....\",\n+    \"I\": \"..\",\n+    \"J\": \".---\",\n+    \"K\": \"-.-\",\n+    \"L\": \".-..\",\n+    \"M\": \"--\",\n+    \"N\": \"-.\",\n+    \"O\": \"---\",\n+    \"P\": \".--.\",\n+    \"Q\": \"--.-\",\n+    \"R\": \".-.\",\n+    \"S\": \"...\",\n+    \"T\": \"-\",\n+    \"U\": \"..-\",\n+    \"V\": \"...-\",\n+    \"W\": \".--\",\n+    \"X\": \"-..-\",\n+    \"Y\": \"-.--\",\n+    \"Z\": \"--..\",\n+    \" \": \"\",\n+}\n+\n+# Define possible trigrams of Morse code\n+MORSE_COMBINATIONS = [\n+    \"...\",\n+    \"..-\",\n+    \"..x\",\n+    \".-.\",\n+    \".--\",\n+    \".-x\",\n+    \".x.\",\n+    \".x-\",\n+    \".xx\",\n+    \"-..\",\n+    \"-.-\",\n+    \"-.x\",\n+    \"--.\",\n+    \"---\",\n+    \"--x\",\n+    \"-x.\",\n+    \"-x-\",\n+    \"-xx\",\n+    \"x..\",\n+    \"x.-\",\n+    \"x.x\",\n+    \"x-.\",\n+    \"x--\",\n+    \"x-x\",\n+    \"xx.\",\n+    \"xx-\",\n+    \"xxx\",\n+]\n+\n+# Create a reverse dictionary for Morse code\n+REVERSE_DICT = {value: key for key, value in MORSE_CODE_DICT.items()}\n+\n+\n+def encode_to_morse(plaintext: str) -> str:\n+    \"\"\"Encode a plaintext message into Morse code.\n+\n+    Args:\n+        plaintext (str): The plaintext message to encode.\n+\n+    Returns:\n+        str: The Morse code representation of the plaintext message.\n+\n+    Example:\n+        >>> encode_to_morse(\"defend the east\")\n+        '-..x.x..-.x.x-.x-..xx-x....x.xx.x.-x...x-'\n+    \"\"\"\n+    return \"x\".join([MORSE_CODE_DICT.get(letter.upper(), \"\") for letter in plaintext])\n+\n+\n+def encrypt_fractionated_morse(plaintext: str, key: str) -> str:\n+    \"\"\"Encrypt a plaintext message using Fractionated Morse Cipher.\n+\n+    Args:\n+        plaintext (str): The plaintext message to encrypt.\n+        key (str): The encryption key.\n+\n+    Returns:\n+        str: The encrypted ciphertext.\n+\n+    Example:\n+        >>> encrypt_fractionated_morse(\"defend the east\",\"Roundtable\")\n+        'ESOAVVLJRSSTRX'\n+\n+    \"\"\"\n+    morse_code = encode_to_morse(plaintext)\n+    key = key.upper() + string.ascii_uppercase\n+    key = \"\".join(sorted(set(key), key=key.find))\n+\n+    # Ensure morse_code length is a multiple of 3\n+    padding_length = 3 - (len(morse_code) % 3)\n+    morse_code += \"x\" * padding_length\n+\n+    fractionated_morse_dict = {\n+        value: key for key, value in zip(key, MORSE_COMBINATIONS)\n+    }\n+    fractionated_morse_dict[\"xxx\"] = \"\"\n+    encrypted_text = \"\".join(\n+        [\n+            fractionated_morse_dict[morse_code[i : i + 3]]\n+            for i in range(0, len(morse_code), 3)\n+        ]\n+    )\n+    return encrypted_text\n+\n+\n+def decrypt_fractionated_morse(ciphertext: str, key: str) -> str:\n+    \"\"\"Decrypt a ciphertext message encrypted with Fractionated Morse Cipher.\n+\n+    Args:\n+        ciphertext (str): The ciphertext message to decrypt.\n+        key (str): The decryption key.\n+\n+    Returns:\n+        str: The decrypted plaintext message.\n+\n+    Example:\n+        >>> decrypt_fractionated_morse(\"ESOAVVLJRSSTRX\",\"Roundtable\")\n+        'DEFEND THE EAST'\n+    \"\"\"\n+    key = key.upper() + string.ascii_uppercase\n+    key = \"\".join(sorted(set(key), key=key.find))\n+\n+    inverse_fractionated_morse_dict = dict(zip(key, MORSE_COMBINATIONS))\n+    morse_code = \"\".join(\n+        [inverse_fractionated_morse_dict.get(letter, \"\") for letter in ciphertext]\n+    )\n+    decrypted_text = \"\".join(\n+        [REVERSE_DICT[code] for code in morse_code.split(\"x\")]\n+    ).strip()\n+    return decrypted_text\n+\n+\n+def main():\nComment: As there is no test file in this pull request nor any test function or class in the file `ciphers/fractionated_morse_cipher.py`, please provide doctest for the function `main`\n\nPlease provide return type hint for the function: `main`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "ciphers/fractionated_morse_cipher.py",
    "pr_number": 9442,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342521030,
    "comment_created_at": "2023-10-02T10:27:46Z"
  },
  {
    "code": "@@ -0,0 +1,44 @@\n+\"\"\"\n+This script demonstrates the implementation of the tangent hyperbolic",
    "comment": "one minor suggestion i have is to add a brief description of what the function does at the beginning of the docstring. for example:\r\n\r\n\"\"\"\r\nimplements the tangent hyperbolic or tanh function.\r\n\r\nthe function takes a vector of k real numbers as input and then applies\r\nthe tanh function to each element of the vector. the output values are\r\nmostly in the range (-1, 1).\r\n...",
    "line_number": 2,
    "enriched": "File: maths/tanh.py\nCode: @@ -0,0 +1,44 @@\n+\"\"\"\n+This script demonstrates the implementation of the tangent hyperbolic\nComment: One minor suggestion I have is to add a brief description of what the function does at the beginning of the docstring. For example:\r\n\r\n\"\"\"\r\nImplements the tangent hyperbolic or tanh function.\r\n\r\nThe function takes a vector of K real numbers as input and then applies\r\nthe tanh function to each element of the vector. The output values are\r\nmostly in the range (-1, 1).\r\n...\r\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "maths/tanh.py",
    "pr_number": 8689,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1175673209,
    "comment_created_at": "2023-04-24T18:57:54Z"
  },
  {
    "code": "@@ -0,0 +1,134 @@\n+import random\n+def pick_pivot(l):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file arithmetic_analysis/mediana_best_algorithm.py, please provide doctest for the function pick_pivot\n\nplease provide return type hint for the function: pick_pivot. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: l\n\nplease provide descriptive name for the parameter: l",
    "line_number": 2,
    "enriched": "File: arithmetic_analysis/mediana_best_algorithm.py\nCode: @@ -0,0 +1,134 @@\n+import random\n+def pick_pivot(l):\nComment: As there is no test file in this pull request nor any test function or class in the file `arithmetic_analysis/mediana_best_algorithm.py`, please provide doctest for the function `pick_pivot`\n\nPlease provide return type hint for the function: `pick_pivot`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `l`\n\nPlease provide descriptive name for the parameter: `l`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "arithmetic_analysis/mediana_best_algorithm.py",
    "pr_number": 6969,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 991742757,
    "comment_created_at": "2022-10-11T01:38:19Z"
  },
  {
    "code": "@@ -0,0 +1,67 @@\n+\"\"\"\n+Title: Bernoulli's Principle Implementation\n+\n+Description:\n+This Python script implements Bernoulli's Principle, which describes the behavior of \n+a fluid under varying conditions of pressure, velocity, and height. Bernoulli's equation\n+is applied to an incompressible, frictionless fluid to calculate the unknown variable \n+(pressure, velocity, or height) when the others are known.\n+\n+Bernoulli's Equation:\n+P1 + 0.5 * \u03c1 * v1^2 + \u03c1 * g * h1 = P2 + 0.5 * \u03c1 * v2^2 + \u03c1 * g * h2\n+\n+Where:\n+- P1, P2 are pressures at points 1 and 2 (in Pascals),\n+- v1, v2 are velocities at points 1 and 2 (in m/s),\n+- h1, h2 are heights at points 1 and 2 (in meters),\n+- \u03c1 is the fluid density (in kg/m\u00b3, default is 1000 for water),\n+- g is the acceleration due to gravity (default is 9.81 m/s\u00b2).\n+\n+The function `bernoullis_principle` calculates one unknown variable based on inputs and returns the result.\n+\"\"\"\n+\n+def bernoullis_principle(P1, v1, h1, P2=None, v2=None, h2=None, density=1000, g=9.81):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file physics/bernoullis_principle.py, please provide doctest for the function bernoullis_principle\n\nplease provide return type hint for the function: bernoullis_principle. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: p1\n\nplease provide type hint for the parameter: v1\n\nplease provide type hint for the parameter: h1\n\nplease provide type hint for the parameter: p2\n\nplease provide type hint for the parameter: v2\n\nplease provide type hint for the parameter: h2\n\nplease provide type hint for the parameter: density\n\nplease provide descriptive name for the parameter: g\n\nplease provide type hint for the parameter: g",
    "line_number": 23,
    "enriched": "File: physics/bernoullis_principle.py\nCode: @@ -0,0 +1,67 @@\n+\"\"\"\n+Title: Bernoulli's Principle Implementation\n+\n+Description:\n+This Python script implements Bernoulli's Principle, which describes the behavior of \n+a fluid under varying conditions of pressure, velocity, and height. Bernoulli's equation\n+is applied to an incompressible, frictionless fluid to calculate the unknown variable \n+(pressure, velocity, or height) when the others are known.\n+\n+Bernoulli's Equation:\n+P1 + 0.5 * \u03c1 * v1^2 + \u03c1 * g * h1 = P2 + 0.5 * \u03c1 * v2^2 + \u03c1 * g * h2\n+\n+Where:\n+- P1, P2 are pressures at points 1 and 2 (in Pascals),\n+- v1, v2 are velocities at points 1 and 2 (in m/s),\n+- h1, h2 are heights at points 1 and 2 (in meters),\n+- \u03c1 is the fluid density (in kg/m\u00b3, default is 1000 for water),\n+- g is the acceleration due to gravity (default is 9.81 m/s\u00b2).\n+\n+The function `bernoullis_principle` calculates one unknown variable based on inputs and returns the result.\n+\"\"\"\n+\n+def bernoullis_principle(P1, v1, h1, P2=None, v2=None, h2=None, density=1000, g=9.81):\nComment: As there is no test file in this pull request nor any test function or class in the file `physics/bernoullis_principle.py`, please provide doctest for the function `bernoullis_principle`\n\nPlease provide return type hint for the function: `bernoullis_principle`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `P1`\n\nPlease provide type hint for the parameter: `v1`\n\nPlease provide type hint for the parameter: `h1`\n\nPlease provide type hint for the parameter: `P2`\n\nPlease provide type hint for the parameter: `v2`\n\nPlease provide type hint for the parameter: `h2`\n\nPlease provide type hint for the parameter: `density`\n\nPlease provide descriptive name for the parameter: `g`\n\nPlease provide type hint for the parameter: `g`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "physics/bernoullis_principle.py",
    "pr_number": 11915,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1792877833,
    "comment_created_at": "2024-10-09T05:44:37Z"
  },
  {
    "code": "@@ -0,0 +1,18 @@\n+def maximumSubarraySum(arr):",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: maximumsubarraysum\n\nas there is no test file in this pull request nor any test function or class in the file data_structures/arrays/kadanealgo.py, please provide doctest for the function maximumsubarraysum\n\nplease provide return type hint for the function: maximumsubarraysum. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: arr",
    "line_number": 1,
    "enriched": "File: data_structures/arrays/KadaneAlgo.py\nCode: @@ -0,0 +1,18 @@\n+def maximumSubarraySum(arr):\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `maximumSubarraySum`\n\nAs there is no test file in this pull request nor any test function or class in the file `data_structures/arrays/KadaneAlgo.py`, please provide doctest for the function `maximumSubarraySum`\n\nPlease provide return type hint for the function: `maximumSubarraySum`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `arr`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "data_structures/arrays/KadaneAlgo.py",
    "pr_number": 11839,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1789363893,
    "comment_created_at": "2024-10-07T01:52:32Z"
  },
  {
    "code": "@@ -0,0 +1,50 @@\n+# Recursive Python3 code to sort\r\n+# an array using selection sort\r\n+\r\n+# Return minimum index\r\n+def minIndex( a , i , j ):\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file sorts/recursive selection sort.py, please provide doctest for the function minindex\n\nplease provide return type hint for the function: minindex. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: minindex\n\nplease provide type hint for the parameter: a\n\nplease provide descriptive name for the parameter: a\n\nplease provide type hint for the parameter: i\n\nplease provide descriptive name for the parameter: i\n\nplease provide type hint for the parameter: j\n\nplease provide descriptive name for the parameter: j",
    "line_number": 5,
    "enriched": "File: sorts/Recursive Selection Sort.py\nCode: @@ -0,0 +1,50 @@\n+# Recursive Python3 code to sort\r\n+# an array using selection sort\r\n+\r\n+# Return minimum index\r\n+def minIndex( a , i , j ):\r\nComment: As there is no test file in this pull request nor any test function or class in the file `sorts/Recursive Selection Sort.py`, please provide doctest for the function `minIndex`\n\nPlease provide return type hint for the function: `minIndex`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `minIndex`\n\nPlease provide type hint for the parameter: `a`\n\nPlease provide descriptive name for the parameter: `a`\n\nPlease provide type hint for the parameter: `i`\n\nPlease provide descriptive name for the parameter: `i`\n\nPlease provide type hint for the parameter: `j`\n\nPlease provide descriptive name for the parameter: `j`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "sorts/Recursive Selection Sort.py",
    "pr_number": 6886,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 990833316,
    "comment_created_at": "2022-10-09T20:06:28Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+def spiral_order(matrix):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/spiralmatrix.py, please provide doctest for the function spiral_order\n\nplease provide return type hint for the function: spiral_order. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: matrix",
    "line_number": 1,
    "enriched": "File: dynamic_programming/spiralmatrix.py\nCode: @@ -0,0 +1,39 @@\n+def spiral_order(matrix):\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/spiralmatrix.py`, please provide doctest for the function `spiral_order`\n\nPlease provide return type hint for the function: `spiral_order`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `matrix`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "dynamic_programming/spiralmatrix.py",
    "pr_number": 11057,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375397540,
    "comment_created_at": "2023-10-29T09:07:26Z"
  },
  {
    "code": "@@ -39,6 +39,59 @@ def binary_cross_entropy(\n     return np.mean(bce_loss)\n \n \n+def binary_focal_cross_entropy(\n+    y_true: np.ndarray,\n+    y_pred: np.ndarray,\n+    gamma: float = 2.0,\n+    alpha: float = 0.25,\n+    epsilon: float = 1e-15,\n+) -> float:\n+    \"\"\"\n+    Calculate the mean binary focal cross-entropy (BFCE) loss between true labels\n+    and predicted probabilities.\n+\n+    BFCE loss quantifies dissimilarity between true labels (0 or 1) and predicted\n+    probabilities. It's a variation of binary cross-entropy that addresses class\n+    imbalance by focusing on hard examples.\n+\n+    BCFE = -\u03a3(alpha * (1 - y_pred)**gamma * y_true * log(y_pred)\n+                + (1 - alpha) * y_pred**gamma * (1 - y_true) * log(1 - y_pred))\n+\n+    Reference: [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf)\n+\n+    Parameters:\n+    - y_true: True binary labels (0 or 1).\n+    - y_pred: Predicted probabilities for class 1.\n+    - gamma: Focusing parameter for modulating the loss (default: 2.0).\n+    - alpha: Weighting factor for class 1 (default: 0.25).\n+    - epsilon: Small constant to avoid numerical instability.\n+\n+    >>> true_labels = np.array([0, 1, 1, 0, 1])\n+    >>> predicted_probs = np.array([0.2, 0.7, 0.9, 0.3, 0.8])\n+    >>> binary_focal_cross_entropy(true_labels, predicted_probs)\n+    0.008257977659239775\n+    >>> true_labels = np.array([0, 1, 1, 0, 1])\n+    >>> predicted_probs = np.array([0.3, 0.8, 0.9, 0.2])\n+    >>> binary_focal_cross_entropy(true_labels, predicted_probs)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Input arrays must have the same length.\n+    \"\"\"\n+    if len(y_true) != len(y_pred):\n+        raise ValueError(\"Input arrays must have the same length.\")\n+    # Clip predicted probabilities to avoid log(0) and log(1)\n+    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n+\n+    # Focal loss calculation\n+    bcfe_loss = -(\n+        alpha * (1 - y_pred) ** gamma * y_true * np.log(y_pred)\n+        + (1 - alpha) * y_pred**gamma * (1 - y_true) * np.log(1 - y_pred)\n+    )\n+\n+    # Take the mean over all samples\n+    return np.mean(bcfe_loss)",
    "comment": "for the first comment, there's nothing wrong with computing log(1), since that just evaluates to 0 with no issue. the issue is that the formula includes log(1 - x), which still may evaluate to log(0).\r\n\r\nfor the second and third comments, i feel that the code and variable names were already self-explanatory and didn't need the comments.",
    "line_number": 92,
    "enriched": "File: machine_learning/loss_functions.py\nCode: @@ -39,6 +39,59 @@ def binary_cross_entropy(\n     return np.mean(bce_loss)\n \n \n+def binary_focal_cross_entropy(\n+    y_true: np.ndarray,\n+    y_pred: np.ndarray,\n+    gamma: float = 2.0,\n+    alpha: float = 0.25,\n+    epsilon: float = 1e-15,\n+) -> float:\n+    \"\"\"\n+    Calculate the mean binary focal cross-entropy (BFCE) loss between true labels\n+    and predicted probabilities.\n+\n+    BFCE loss quantifies dissimilarity between true labels (0 or 1) and predicted\n+    probabilities. It's a variation of binary cross-entropy that addresses class\n+    imbalance by focusing on hard examples.\n+\n+    BCFE = -\u03a3(alpha * (1 - y_pred)**gamma * y_true * log(y_pred)\n+                + (1 - alpha) * y_pred**gamma * (1 - y_true) * log(1 - y_pred))\n+\n+    Reference: [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf)\n+\n+    Parameters:\n+    - y_true: True binary labels (0 or 1).\n+    - y_pred: Predicted probabilities for class 1.\n+    - gamma: Focusing parameter for modulating the loss (default: 2.0).\n+    - alpha: Weighting factor for class 1 (default: 0.25).\n+    - epsilon: Small constant to avoid numerical instability.\n+\n+    >>> true_labels = np.array([0, 1, 1, 0, 1])\n+    >>> predicted_probs = np.array([0.2, 0.7, 0.9, 0.3, 0.8])\n+    >>> binary_focal_cross_entropy(true_labels, predicted_probs)\n+    0.008257977659239775\n+    >>> true_labels = np.array([0, 1, 1, 0, 1])\n+    >>> predicted_probs = np.array([0.3, 0.8, 0.9, 0.2])\n+    >>> binary_focal_cross_entropy(true_labels, predicted_probs)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Input arrays must have the same length.\n+    \"\"\"\n+    if len(y_true) != len(y_pred):\n+        raise ValueError(\"Input arrays must have the same length.\")\n+    # Clip predicted probabilities to avoid log(0) and log(1)\n+    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n+\n+    # Focal loss calculation\n+    bcfe_loss = -(\n+        alpha * (1 - y_pred) ** gamma * y_true * np.log(y_pred)\n+        + (1 - alpha) * y_pred**gamma * (1 - y_true) * np.log(1 - y_pred)\n+    )\n+\n+    # Take the mean over all samples\n+    return np.mean(bcfe_loss)\nComment: ```suggestion\r\n    # Clip predicted probabilities to avoid log(0)\r\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\r\n\r\n    bcfe_loss = -(\r\n        alpha * (1 - y_pred) ** gamma * y_true * np.log(y_pred)\r\n        + (1 - alpha) * y_pred**gamma * (1 - y_true) * np.log(1 - y_pred)\r\n    )\r\n\r\n    return np.mean(bcfe_loss)\r\n```\r\nFor the first comment, there's nothing wrong with computing log(1), since that just evaluates to 0 with no issue. The issue is that the formula includes log(1 - x), which still may evaluate to log(0).\r\n\r\nFor the second and third comments, I feel that the code and variable names were already self-explanatory and didn't need the comments.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "machine_learning/loss_functions.py",
    "pr_number": 10674,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368142391,
    "comment_created_at": "2023-10-23T05:20:39Z"
  },
  {
    "code": "@@ -0,0 +1,28 @@\n+import numpy as np\n+def best_response_dynamics(payoff_matrix_a, payoff_matrix_b, iterations=10):",
    "comment": "please provide return type hint for the function: best_response_dynamics. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file game_theory/best_response_dynamics.py, please provide doctest for the function best_response_dynamics\n\nplease provide type hint for the parameter: payoff_matrix_a\n\nplease provide type hint for the parameter: payoff_matrix_b\n\nplease provide type hint for the parameter: iterations",
    "line_number": 2,
    "enriched": "File: game_theory/best_response_dynamics.py\nCode: @@ -0,0 +1,28 @@\n+import numpy as np\n+def best_response_dynamics(payoff_matrix_a, payoff_matrix_b, iterations=10):\nComment: Please provide return type hint for the function: `best_response_dynamics`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `game_theory/best_response_dynamics.py`, please provide doctest for the function `best_response_dynamics`\n\nPlease provide type hint for the parameter: `payoff_matrix_a`\n\nPlease provide type hint for the parameter: `payoff_matrix_b`\n\nPlease provide type hint for the parameter: `iterations`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "game_theory/best_response_dynamics.py",
    "pr_number": 11864,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1790559473,
    "comment_created_at": "2024-10-07T16:43:01Z"
  },
  {
    "code": "@@ -54,17 +54,20 @@ def hubble_parameter(\n     result : Hubble parameter in and the unit km/s/Mpc (the unit can be\n     changed if you want, just need to change the unit of the Hubble constant)\n \n-    >>> hubble_parameter(hubble_constant=68.3, radiation_density=1e-4, matter_density=-0.3, dark_energy=0.7, redshift=1)\n+    >>> hubble_parameter(hubble_constant=68.3, radiation_density=1e-4, \\",
    "comment": "pep8 says that backslashes are a bad idea in python code.",
    "line_number": 57,
    "enriched": "File: physics/hubble_parameter.py\nCode: @@ -54,17 +54,20 @@ def hubble_parameter(\n     result : Hubble parameter in and the unit km/s/Mpc (the unit can be\n     changed if you want, just need to change the unit of the Hubble constant)\n \n-    >>> hubble_parameter(hubble_constant=68.3, radiation_density=1e-4, matter_density=-0.3, dark_energy=0.7, redshift=1)\n+    >>> hubble_parameter(hubble_constant=68.3, radiation_density=1e-4, \\\nComment: PEP8 says that backslashes are a bad idea in Python code.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "physics/hubble_parameter.py",
    "pr_number": 7807,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008921694,
    "comment_created_at": "2022-10-30T20:26:26Z"
  },
  {
    "code": "@@ -0,0 +1,65 @@\n+\"\"\"\n+Principal Component Analysis (PCA) is a dimensionality reduction technique\n+commonly used in machine learning. It transforms high-dimensional data into\n+lower dimensions while retaining most of the information.\n+\n+Here,we use a dataset (Iris dataset) and apply PCA to reduce the\n+dimensionality. We compute the principal components and transform the dataset\n+into a lower-dimensional space.\n+\n+We reduce the number of columns form 4 to 2\n+\n+\"\"\"\n+\n+import numpy as np\n+from sklearn.decomposition import PCA\n+from sklearn.preprocessing import StandardScaler\n+from sklearn.datasets import load_iris\n+\n+\n+def collect_dataset():",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/principle_component_analysis.py, please provide doctest for the function collect_dataset\n\nplease provide return type hint for the function: collect_dataset. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 20,
    "enriched": "File: machine_learning/principle_component_analysis.py\nCode: @@ -0,0 +1,65 @@\n+\"\"\"\n+Principal Component Analysis (PCA) is a dimensionality reduction technique\n+commonly used in machine learning. It transforms high-dimensional data into\n+lower dimensions while retaining most of the information.\n+\n+Here,we use a dataset (Iris dataset) and apply PCA to reduce the\n+dimensionality. We compute the principal components and transform the dataset\n+into a lower-dimensional space.\n+\n+We reduce the number of columns form 4 to 2\n+\n+\"\"\"\n+\n+import numpy as np\n+from sklearn.decomposition import PCA\n+from sklearn.preprocessing import StandardScaler\n+from sklearn.datasets import load_iris\n+\n+\n+def collect_dataset():\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/principle_component_analysis.py`, please provide doctest for the function `collect_dataset`\n\nPlease provide return type hint for the function: `collect_dataset`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "machine_learning/principle_component_analysis.py",
    "pr_number": 12595,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1976290122,
    "comment_created_at": "2025-03-01T04:33:50Z"
  },
  {
    "code": "@@ -0,0 +1,27 @@\n+def best_response_dynamics(payoff_matrix_A, payoff_matrix_B, iterations=10):",
    "comment": "please provide return type hint for the function: best_response_dynamics. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file game_theory/best_response_dynamics.py, please provide doctest for the function best_response_dynamics\n\nplease provide type hint for the parameter: payoff_matrix_a\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: payoff_matrix_a\n\nplease provide type hint for the parameter: payoff_matrix_b\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: payoff_matrix_b\n\nplease provide type hint for the parameter: iterations",
    "line_number": 1,
    "enriched": "File: game_theory/best_response_dynamics.py\nCode: @@ -0,0 +1,27 @@\n+def best_response_dynamics(payoff_matrix_A, payoff_matrix_B, iterations=10):\nComment: Please provide return type hint for the function: `best_response_dynamics`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `game_theory/best_response_dynamics.py`, please provide doctest for the function `best_response_dynamics`\n\nPlease provide type hint for the parameter: `payoff_matrix_A`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `payoff_matrix_A`\n\nPlease provide type hint for the parameter: `payoff_matrix_B`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `payoff_matrix_B`\n\nPlease provide type hint for the parameter: `iterations`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "game_theory/best_response_dynamics.py",
    "pr_number": 11859,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1790458348,
    "comment_created_at": "2024-10-07T15:36:19Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+import numpy as np\n+\n+def travelling_salesman(city):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/travelling_salesman.py, please provide doctest for the function travelling_salesman\n\nplease provide return type hint for the function: travelling_salesman. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: city",
    "line_number": 3,
    "enriched": "File: dynamic_programming/travelling_salesman.py\nCode: @@ -0,0 +1,53 @@\n+import numpy as np\n+\n+def travelling_salesman(city):\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/travelling_salesman.py`, please provide doctest for the function `travelling_salesman`\n\nPlease provide return type hint for the function: `travelling_salesman`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `city`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "dynamic_programming/travelling_salesman.py",
    "pr_number": 10544,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359901657,
    "comment_created_at": "2023-10-15T15:49:20Z"
  },
  {
    "code": "@@ -0,0 +1,31 @@\n+\"\"\"\r\n+There is a robot on an m x n grid. \r\n+The robot is initially located at the top-left corner of grid the robot tries to move to the bottom-right corner. \r\n+The robot can only move either down or right at any point in time.\r\n+Return number of all  possible unique paths robot can take.\r\n+\r\n+\"\"\"\r\n+def uniquepaths(self, m, n):\r",
    "comment": "please provide return type hint for the function: uniquepaths. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file dynamic_programming/unique_paths.py, please provide doctest for the function uniquepaths\n\nplease provide descriptive name for the parameter: m\n\nplease provide type hint for the parameter: m\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: n",
    "line_number": 8,
    "enriched": "File: dynamic_programming/unique_paths.py\nCode: @@ -0,0 +1,31 @@\n+\"\"\"\r\n+There is a robot on an m x n grid. \r\n+The robot is initially located at the top-left corner of grid the robot tries to move to the bottom-right corner. \r\n+The robot can only move either down or right at any point in time.\r\n+Return number of all  possible unique paths robot can take.\r\n+\r\n+\"\"\"\r\n+def uniquepaths(self, m, n):\r\nComment: Please provide return type hint for the function: `uniquepaths`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `dynamic_programming/unique_paths.py`, please provide doctest for the function `uniquepaths`\n\nPlease provide descriptive name for the parameter: `m`\n\nPlease provide type hint for the parameter: `m`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide type hint for the parameter: `n`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "dynamic_programming/unique_paths.py",
    "pr_number": 10198,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1350729477,
    "comment_created_at": "2023-10-09T19:38:12Z"
  },
  {
    "code": "@@ -0,0 +1,59 @@\n+\"\"\"\n+You are given a rows x cols matrix grid representing a field of cherries where grid[i][j] represents the number of cherries that you can collect from the (i, j) cell.\n+\n+You have two robots that can collect cherries for you:\n+\n+Robot #1 is located at the top-left corner (0, 0), and\n+Robot #2 is located at the top-right corner (0, cols - 1).\n+Return the maximum number of cherries collection using both robots by following the rules below:\n+\n+1. From a cell (i, j), robots can move to cell (i + 1, j - 1), (i + 1, j), or (i + 1, j + 1).\n+2. When any robot passes through a cell, It picks up all cherries, and the cell becomes an empty cell.\n+3. When both robots stay in the same cell, only one takes the cherries.\n+4. Both robots cannot move outside of the grid at any moment.\n+5. Both robots should reach the bottom row in grid.\n+\n+Problem Statement:- https://leetcode.com/problems/cherry-pickup-ii\n+\n+\"\"\"\n+\n+\n+from typing import List\n+from collections import defaultdict\n+\n+\n+class Solution:\n+    def cherryPickup(self, grid: List[List[int]]) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/cherry_pickup_ii.py, please provide doctest for the function cherrypickup\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: cherrypickup",
    "line_number": 26,
    "enriched": "File: dynamic_programming/cherry_pickup_ii.py\nCode: @@ -0,0 +1,59 @@\n+\"\"\"\n+You are given a rows x cols matrix grid representing a field of cherries where grid[i][j] represents the number of cherries that you can collect from the (i, j) cell.\n+\n+You have two robots that can collect cherries for you:\n+\n+Robot #1 is located at the top-left corner (0, 0), and\n+Robot #2 is located at the top-right corner (0, cols - 1).\n+Return the maximum number of cherries collection using both robots by following the rules below:\n+\n+1. From a cell (i, j), robots can move to cell (i + 1, j - 1), (i + 1, j), or (i + 1, j + 1).\n+2. When any robot passes through a cell, It picks up all cherries, and the cell becomes an empty cell.\n+3. When both robots stay in the same cell, only one takes the cherries.\n+4. Both robots cannot move outside of the grid at any moment.\n+5. Both robots should reach the bottom row in grid.\n+\n+Problem Statement:- https://leetcode.com/problems/cherry-pickup-ii\n+\n+\"\"\"\n+\n+\n+from typing import List\n+from collections import defaultdict\n+\n+\n+class Solution:\n+    def cherryPickup(self, grid: List[List[int]]) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/cherry_pickup_ii.py`, please provide doctest for the function `cherryPickup`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `cherryPickup`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "dynamic_programming/cherry_pickup_ii.py",
    "pr_number": 9932,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349013914,
    "comment_created_at": "2023-10-06T17:12:17Z"
  },
  {
    "code": "@@ -52,7 +52,7 @@ def gaussian(x, mu: float = 0.0, sigma: float = 1.0) -> int:\n     >>> gaussian(2523, mu=234234, sigma=3425)\n     0.0\n     \"\"\"\n-    return 1 / sqrt(2 * pi * sigma**2) * exp(-((x - mu) ** 2) / (2 * sigma**2))\n+    return (1 / sigma * sqrt(2 * pi)) * exp(-((x - mu) ** 2) / (2 * sigma**2))\n ",
    "comment": "edited the notation",
    "line_number": 56,
    "enriched": "File: maths/gaussian.py\nCode: @@ -52,7 +52,7 @@ def gaussian(x, mu: float = 0.0, sigma: float = 1.0) -> int:\n     >>> gaussian(2523, mu=234234, sigma=3425)\n     0.0\n     \"\"\"\n-    return 1 / sqrt(2 * pi * sigma**2) * exp(-((x - mu) ** 2) / (2 * sigma**2))\n+    return (1 / sigma * sqrt(2 * pi)) * exp(-((x - mu) ** 2) / (2 * sigma**2))\n \nComment: edited the notation ",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "maths/gaussian.py",
    "pr_number": 7645,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004629333,
    "comment_created_at": "2022-10-25T15:18:15Z"
  },
  {
    "code": "@@ -0,0 +1,50 @@\n+# Kruskal's algorithm in Python\n+",
    "comment": "add some more description and if possible, a credible source.",
    "line_number": 2,
    "enriched": "File: graphs/Kruskals.py\nCode: @@ -0,0 +1,50 @@\n+# Kruskal's algorithm in Python\n+\nComment: Add some more description and if possible, a credible source.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "graphs/Kruskals.py",
    "pr_number": 7415,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 999248322,
    "comment_created_at": "2022-10-19T10:23:27Z"
  },
  {
    "code": "@@ -21,6 +21,56 @@ def create_state_space_tree(\n     Creates a state space tree to iterate through each branch using DFS.\n     We know that each state has exactly two children.\n     It terminates when it reaches the end of the given sequence.\n+\n+        :param sequence: The input sequence for which subsequences are generated.",
    "comment": "fix indentation",
    "line_number": 25,
    "enriched": "File: backtracking/all_subsequences.py\nCode: @@ -21,6 +21,56 @@ def create_state_space_tree(\n     Creates a state space tree to iterate through each branch using DFS.\n     We know that each state has exactly two children.\n     It terminates when it reaches the end of the given sequence.\n+\n+        :param sequence: The input sequence for which subsequences are generated.\nComment: ```suggestion\r\n    :param sequence: The input sequence for which subsequences are generated.\r\n```\r\nFix indentation",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "backtracking/all_subsequences.py",
    "pr_number": 10252,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1623732622,
    "comment_created_at": "2024-06-03T02:54:24Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+# Python program for implementation of Bubble Sort\r\n+\r\n+def bubbleSort(arr):\r",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: bubblesort\n\nplease provide return type hint for the function: bubblesort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file dsa/all_sorting/bubble_sort.py, please provide doctest for the function bubblesort\n\nplease provide type hint for the parameter: arr",
    "line_number": 3,
    "enriched": "File: dsa/all_sorting/bubble_sort.py\nCode: @@ -0,0 +1,35 @@\n+# Python program for implementation of Bubble Sort\r\n+\r\n+def bubbleSort(arr):\r\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `bubbleSort`\n\nPlease provide return type hint for the function: `bubbleSort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `dsa/all_sorting/bubble_sort.py`, please provide doctest for the function `bubbleSort`\n\nPlease provide type hint for the parameter: `arr`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "dsa/all_sorting/bubble_sort.py",
    "pr_number": 10848,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368774540,
    "comment_created_at": "2023-10-23T14:29:32Z"
  },
  {
    "code": "@@ -0,0 +1,395 @@\n+\"\"\"\n+Creates a random wordsearch with eight different directions\n+that are best described as compass locations.\n+\n+@ https://en.wikipedia.org/wiki/Word_search\n+\"\"\"\n+\n+\n+from random import choice, randint, shuffle\n+\n+# The words to display on the word search -\n+# can be made dynamic by randonly selecting a certain number of\n+# words from a predefined word file, while ensuring the character\n+# count fits within the matrix size (n x m)\n+WORDS = [\"cat\", \"dog\", \"snake\", \"fish\"]\n+\n+WIDTH = 10\n+HEIGHT = 10\n+\n+\n+class WordSearch:\n+    \"\"\"\n+    >>> ws = WordSearch(WORDS, WIDTH, HEIGHT)\n+    >>> ws.board  # doctest: +ELLIPSIS\n+    [[None, ..., None], ..., [None, ..., None]]\n+    >>> ws.generate_board()\n+    \"\"\"\n+\n+    def __init__(self, words: list[str], width: int, height: int) -> None:\n+        self.words = words\n+        self.width = width\n+        self.height = height\n+\n+        # Board matrix holding each letter\n+        self.board: list[list[str | None]] = [[None] * width for _ in range(height)]\n+\n+    def insert_north(self, word: str, rows: list[int], cols: list[int]) -> None:",
    "comment": "it'd help to have the block comment explain what rows and cols are for",
    "line_number": 37,
    "enriched": "File: other/word_search.py\nCode: @@ -0,0 +1,395 @@\n+\"\"\"\n+Creates a random wordsearch with eight different directions\n+that are best described as compass locations.\n+\n+@ https://en.wikipedia.org/wiki/Word_search\n+\"\"\"\n+\n+\n+from random import choice, randint, shuffle\n+\n+# The words to display on the word search -\n+# can be made dynamic by randonly selecting a certain number of\n+# words from a predefined word file, while ensuring the character\n+# count fits within the matrix size (n x m)\n+WORDS = [\"cat\", \"dog\", \"snake\", \"fish\"]\n+\n+WIDTH = 10\n+HEIGHT = 10\n+\n+\n+class WordSearch:\n+    \"\"\"\n+    >>> ws = WordSearch(WORDS, WIDTH, HEIGHT)\n+    >>> ws.board  # doctest: +ELLIPSIS\n+    [[None, ..., None], ..., [None, ..., None]]\n+    >>> ws.generate_board()\n+    \"\"\"\n+\n+    def __init__(self, words: list[str], width: int, height: int) -> None:\n+        self.words = words\n+        self.width = width\n+        self.height = height\n+\n+        # Board matrix holding each letter\n+        self.board: list[list[str | None]] = [[None] * width for _ in range(height)]\n+\n+    def insert_north(self, word: str, rows: list[int], cols: list[int]) -> None:\nComment: It'd help to have the block comment explain what `rows` and `cols` are for",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "other/word_search.py",
    "pr_number": 8906,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1315368431,
    "comment_created_at": "2023-09-05T05:05:30Z"
  },
  {
    "code": "@@ -73,6 +73,23 @@ That's it! The plugin will run every time you commit any changes. If there are a\n pre-commit run --all-files --show-diff-on-failure\n ```\n \n+\n+ <details>\n+  <summary>Hint for windows users</summary>\n+  <br/>\n+  On Windows, the python3 command is not recognized. This can lead to the error below.",
    "comment": "windows users should use py\r\n\r\nor in a [venv](https://docs.python.org/3/library/venv.html), everyone should use python",
    "line_number": 80,
    "enriched": "File: CONTRIBUTING.md\nCode: @@ -73,6 +73,23 @@ That's it! The plugin will run every time you commit any changes. If there are a\n pre-commit run --all-files --show-diff-on-failure\n ```\n \n+\n+ <details>\n+  <summary>Hint for windows users</summary>\n+  <br/>\n+  On Windows, the python3 command is not recognized. This can lead to the error below.\nComment: Windows users should use `py`\r\n\r\nOr in a [venv](https://docs.python.org/3/library/venv.html), everyone should use `python`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "CONTRIBUTING.md",
    "pr_number": 12369,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1837141235,
    "comment_created_at": "2024-11-11T20:07:36Z"
  },
  {
    "code": "@@ -0,0 +1,170 @@\n+\"\"\"\n+Problem Statement: Given a binary perform an postorder traversal using Morris Postorder\n+traversal algorithm. (Iterative version of Postorder traversal of tree)\n+\n+https://www.geeksforgeeks.org/morris-traversal-for-postorder/\n+\"\"\"\n+\n+\n+class TreeNode:\n+    \"\"\"\n+    Class representing a node in a binary tree.\n+\n+    Attributes:\n+    -----------\n+    value : int\n+        The value stored at the node.\n+    left : TreeNode\n+        Pointer to the left child node (default is None).\n+    right : TreeNode\n+        Pointer to the right child node (default is None).\n+    \"\"\"\n+\n+    def __init__(self, value: int) -> None:\n+        self.value = value\n+        self.left: TreeNode | None = None\n+        self.right: TreeNode | None = None\n+\n+\n+class BinaryTree:\n+    \"\"\"\n+    Class representing a binary tree.\n+\n+    Methods:\n+    --------\n+    insert(value: int) -> None:\n+        Insert a value into the binary tree following binary search tree (BST) rules.\n+\n+    morris_postorder_traversal() -> List[int]:\n+        Perform postorder traversal and return list of node values.\n+\n+\n+    >>> bt = BinaryTree()\n+    >>> bt.insert(9)\n+    >>> bt.insert(6)\n+    >>> bt.insert(10)\n+    >>> bt.insert(3)\n+    >>> bt.insert(7)\n+    >>> bt.insert(12)\n+    >>> bt.insert(2)\n+    >>> bt.insert(5)\n+    >>> bt.insert(4)\n+    >>> bt.morris_postorder_traversal()\n+    [2, 4, 5, 3, 7, 6, 12, 10, 9]\n+\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        self.root: TreeNode | None = None\n+\n+    def insert(self, value: int) -> None:\n+        \"\"\"\n+        Insert a value into the binary tree.\n+\n+        Parameters:\n+        -----------\n+        value : int\n+            The value to be inserted into the binary tree.\n+        \"\"\"\n+        if self.root is None:\n+            self.root = TreeNode(value)\n+        else:\n+            self._insert_recursive(self.root, value)\n+\n+    def _insert_recursive(self, node: TreeNode, value: int) -> None:\n+        \"\"\"\n+        Helper function to insert a value recursively into the tree.\n+\n+        Parameters:\n+        -----------\n+        node : TreeNode\n+            The current node in the binary tree.\n+        value : int\n+            The value to be inserted.\n+        \"\"\"\n+        if value < node.value:\n+            if node.left is None:\n+                node.left = TreeNode(value)\n+            else:\n+                self._insert_recursive(node.left, value)\n+        elif node.right is None:\n+            node.right = TreeNode(value)\n+        else:\n+            self._insert_recursive(node.right, value)\n+\n+    def _successor(self, node: TreeNode) -> TreeNode:\n+        \"\"\"\n+        Helper Function to return successor of the given node in a binary tree\n+\n+        Parameters:\n+        -----------\n+        node : TreeNode\n+            A node in the binary tree.\n+\n+        Returns:\n+        --------\n+        TreeNode:\n+            The successor of the node passed in the parameter\n+        \"\"\"",
    "comment": "see my comment on your previous pr: https://github.com/thealgorithms/python/pull/11774#discussion_r1788653204",
    "line_number": 108,
    "enriched": "File: data_structures/binary_tree/morris_postorder_traversal.py\nCode: @@ -0,0 +1,170 @@\n+\"\"\"\n+Problem Statement: Given a binary perform an postorder traversal using Morris Postorder\n+traversal algorithm. (Iterative version of Postorder traversal of tree)\n+\n+https://www.geeksforgeeks.org/morris-traversal-for-postorder/\n+\"\"\"\n+\n+\n+class TreeNode:\n+    \"\"\"\n+    Class representing a node in a binary tree.\n+\n+    Attributes:\n+    -----------\n+    value : int\n+        The value stored at the node.\n+    left : TreeNode\n+        Pointer to the left child node (default is None).\n+    right : TreeNode\n+        Pointer to the right child node (default is None).\n+    \"\"\"\n+\n+    def __init__(self, value: int) -> None:\n+        self.value = value\n+        self.left: TreeNode | None = None\n+        self.right: TreeNode | None = None\n+\n+\n+class BinaryTree:\n+    \"\"\"\n+    Class representing a binary tree.\n+\n+    Methods:\n+    --------\n+    insert(value: int) -> None:\n+        Insert a value into the binary tree following binary search tree (BST) rules.\n+\n+    morris_postorder_traversal() -> List[int]:\n+        Perform postorder traversal and return list of node values.\n+\n+\n+    >>> bt = BinaryTree()\n+    >>> bt.insert(9)\n+    >>> bt.insert(6)\n+    >>> bt.insert(10)\n+    >>> bt.insert(3)\n+    >>> bt.insert(7)\n+    >>> bt.insert(12)\n+    >>> bt.insert(2)\n+    >>> bt.insert(5)\n+    >>> bt.insert(4)\n+    >>> bt.morris_postorder_traversal()\n+    [2, 4, 5, 3, 7, 6, 12, 10, 9]\n+\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        self.root: TreeNode | None = None\n+\n+    def insert(self, value: int) -> None:\n+        \"\"\"\n+        Insert a value into the binary tree.\n+\n+        Parameters:\n+        -----------\n+        value : int\n+            The value to be inserted into the binary tree.\n+        \"\"\"\n+        if self.root is None:\n+            self.root = TreeNode(value)\n+        else:\n+            self._insert_recursive(self.root, value)\n+\n+    def _insert_recursive(self, node: TreeNode, value: int) -> None:\n+        \"\"\"\n+        Helper function to insert a value recursively into the tree.\n+\n+        Parameters:\n+        -----------\n+        node : TreeNode\n+            The current node in the binary tree.\n+        value : int\n+            The value to be inserted.\n+        \"\"\"\n+        if value < node.value:\n+            if node.left is None:\n+                node.left = TreeNode(value)\n+            else:\n+                self._insert_recursive(node.left, value)\n+        elif node.right is None:\n+            node.right = TreeNode(value)\n+        else:\n+            self._insert_recursive(node.right, value)\n+\n+    def _successor(self, node: TreeNode) -> TreeNode:\n+        \"\"\"\n+        Helper Function to return successor of the given node in a binary tree\n+\n+        Parameters:\n+        -----------\n+        node : TreeNode\n+            A node in the binary tree.\n+\n+        Returns:\n+        --------\n+        TreeNode:\n+            The successor of the node passed in the parameter\n+        \"\"\"\nComment: ```suggestion\r\n        Parameters:\r\n        -----------\r\n        node: A node in the binary tree.\r\n\r\n        Returns:\r\n        --------\r\n        The successor of the node passed in the parameter\r\n        \"\"\"\r\n```\r\nSee my comment on your previous PR: https://github.com/TheAlgorithms/Python/pull/11774#discussion_r1788653204",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "data_structures/binary_tree/morris_postorder_traversal.py",
    "pr_number": 11775,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1788653708,
    "comment_created_at": "2024-10-05T17:54:36Z"
  },
  {
    "code": "@@ -0,0 +1,12 @@\n+print(\n+    \"INSTRUCTIONS: USE 'and' / 'or' / 'not' when entering equation. For booleans use True/False or 1/0\"\n+)\n+\n+print()\n+while True:\n+    try:\n+        eq = input(\"Enter equation: \")\n+\n+        print(eval(eq))",
    "comment": "please read the file contributing.md to understand our definition of an algorithm and our requirements for tests and type hints.\r\n\r\nalso, https://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html",
    "line_number": 10,
    "enriched": "File: boolean_algebra/boolean algebra calculator.py\nCode: @@ -0,0 +1,12 @@\n+print(\n+    \"INSTRUCTIONS: USE 'and' / 'or' / 'not' when entering equation. For booleans use True/False or 1/0\"\n+)\n+\n+print()\n+while True:\n+    try:\n+        eq = input(\"Enter equation: \")\n+\n+        print(eval(eq))\nComment: Please read the file `CONTRIBUTING.md` to understand our definition of an algorithm and our requirements for tests and type hints.\r\n\r\nAlso, https://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "boolean_algebra/boolean algebra calculator.py",
    "pr_number": 7077,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994289201,
    "comment_created_at": "2022-10-13T07:57:44Z"
  },
  {
    "code": "@@ -1,40 +1,49 @@\n name: sphinx\n \n on:\n-  # Triggers the workflow on push or pull request events but only for the \"master\" branch\n   push:\n     branches: [\"master\"]\n   pull_request:\n     branches: [\"master\"]\n-  # Or manually from the Actions tab\n   workflow_dispatch:\n \n-# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages\n permissions:\n   contents: read\n   pages: write\n   id-token: write\n \n-# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.\n-# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.\n concurrency:\n   group: \"pages\"\n   cancel-in-progress: false\n \n jobs:\n   build_docs:\n-    runs-on: ubuntu-24.04-arm\n+    runs-on: ubuntu-latest\n     steps:\n-      - uses: actions/checkout@v5\n-      - uses: astral-sh/setup-uv@v6\n-      - uses: actions/setup-python@v5\n+      - name: Checkout repository\n+        uses: actions/checkout@v5",
    "comment": "actions/checkout@v5 tells the reader more than checkout repository does.",
    "line_number": 24,
    "enriched": "File: .github/workflows/sphinx.yml\nCode: @@ -1,40 +1,49 @@\n name: sphinx\n \n on:\n-  # Triggers the workflow on push or pull request events but only for the \"master\" branch\n   push:\n     branches: [\"master\"]\n   pull_request:\n     branches: [\"master\"]\n-  # Or manually from the Actions tab\n   workflow_dispatch:\n \n-# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages\n permissions:\n   contents: read\n   pages: write\n   id-token: write\n \n-# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.\n-# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.\n concurrency:\n   group: \"pages\"\n   cancel-in-progress: false\n \n jobs:\n   build_docs:\n-    runs-on: ubuntu-24.04-arm\n+    runs-on: ubuntu-latest\n     steps:\n-      - uses: actions/checkout@v5\n-      - uses: astral-sh/setup-uv@v6\n-      - uses: actions/setup-python@v5\n+      - name: Checkout repository\n+        uses: actions/checkout@v5\nComment: `actions/checkout@v5` tells the reader more than `Checkout repository` does.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": ".github/workflows/sphinx.yml",
    "pr_number": 12945,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2314586625,
    "comment_created_at": "2025-09-01T21:32:24Z"
  },
  {
    "code": "@@ -12,8 +12,55 @@ addopts = [\n omit = [\".env/*\"]\n sort = \"Cover\"\n \n-#[report]\n-#sort = Cover\n-#omit =\n-#    .env/*\n-#    backtracking/*\n+[tool.codespell]\n+ignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,mater,secant,som,sur,tim,zar\"\n+skip = \"./.*,*.json,ciphers/prehistoric_men.txt,project_euler/problem_022/p022_names.txt,pyproject.toml,strings/dictionary.txt,strings/words.txt\"\n+\n+[tool.ruff]\n+ignore = [",
    "comment": "it would be useful for posterity to specify why these rules are being ignored.",
    "line_number": 20,
    "enriched": "File: pyproject.toml\nCode: @@ -12,8 +12,55 @@ addopts = [\n omit = [\".env/*\"]\n sort = \"Cover\"\n \n-#[report]\n-#sort = Cover\n-#omit =\n-#    .env/*\n-#    backtracking/*\n+[tool.codespell]\n+ignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,mater,secant,som,sur,tim,zar\"\n+skip = \"./.*,*.json,ciphers/prehistoric_men.txt,project_euler/problem_022/p022_names.txt,pyproject.toml,strings/dictionary.txt,strings/words.txt\"\n+\n+[tool.ruff]\n+ignore = [\nComment: It would be useful for posterity to specify why these rules are being ignored.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "pyproject.toml",
    "pr_number": 8178,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1134994748,
    "comment_created_at": "2023-03-14T05:36:36Z"
  },
  {
    "code": "@@ -4,101 +4,84 @@\n def solve_maze(maze: list[list[int]]) -> bool:\n     \"\"\"\n     This method solves the \"rat in maze\" problem.\n-    In this problem we have some n by n matrix, a start point and an end point.\n-    We want to go from the start to the end. In this matrix zeroes represent walls\n-    and ones paths we can use.\n-    Parameters :\n-        maze(2D matrix) : maze\n+    In this problem, we have an n by n matrix, a start point, and an end point.\n+    We want to go from the start to the end. In this matrix, ones represent walls,\n+    and zeros represent paths we can use.\n+\n+    Parameters:\n+        maze (2D matrix): The maze where 1 represents walls, and 0 represents paths.\n+\n     Returns:\n-        Return: True if the maze has a solution or False if it does not.\n+        bool: True if a solution exists, False otherwise.\n+\n     >>> maze = [[0, 1, 0, 1, 1],\n     ...         [0, 0, 0, 0, 0],\n     ...         [1, 0, 1, 0, 1],\n     ...         [0, 0, 1, 0, 0],\n     ...         [1, 0, 0, 1, 0]]\n     >>> solve_maze(maze)\n+    True\n+    Path:",
    "comment": "changing this code is modifying what the doctest is looking for.  as discussed in contributing.md you can run these tests on your local computer by doing.\r\n% python3 -m doctest -v backtracking/rat_in_maze.py\r\n\r\nhttps://github.com/thealgorithms/python/actions/runs/6284643240/job/17066208866?pr=9082#step:6:709\r\n\r\n=================================== failures ===================================\r\n________________ [doctest] backtracking.rat_in_maze.solve_maze _________________\r\n013 \r\n014     returns:\r\n015         bool: true if a solution exists, false otherwise.\r\n016 \r\n017     >>> maze = [[0, 1, 0, 1, 1],\r\n018     ...         [0, 0, 0, 0, 0],\r\n019     ...         [1, 0, 1, 0, 1],\r\n020     ...         [0, 0, 1, 0, 0],\r\n021     ...         [1, 0, 0, 1, 0]]\r\n022     >>> solve_maze(maze)\r\ndifferences (unified diff with -expected +actual):\r\n    @@ -1,3 +1,2 @@\r\n    -true\r\n     path:\r\n     [1, 0, 0, 0, 0]\r\n    @@ -6,2 +5,3 @@\r\n     [0, 0, 0, 1, 1]\r\n     [0, 0, 0, 0, 1]\r\n    +true",
    "line_number": 24,
    "enriched": "File: backtracking/rat_in_maze.py\nCode: @@ -4,101 +4,84 @@\n def solve_maze(maze: list[list[int]]) -> bool:\n     \"\"\"\n     This method solves the \"rat in maze\" problem.\n-    In this problem we have some n by n matrix, a start point and an end point.\n-    We want to go from the start to the end. In this matrix zeroes represent walls\n-    and ones paths we can use.\n-    Parameters :\n-        maze(2D matrix) : maze\n+    In this problem, we have an n by n matrix, a start point, and an end point.\n+    We want to go from the start to the end. In this matrix, ones represent walls,\n+    and zeros represent paths we can use.\n+\n+    Parameters:\n+        maze (2D matrix): The maze where 1 represents walls, and 0 represents paths.\n+\n     Returns:\n-        Return: True if the maze has a solution or False if it does not.\n+        bool: True if a solution exists, False otherwise.\n+\n     >>> maze = [[0, 1, 0, 1, 1],\n     ...         [0, 0, 0, 0, 0],\n     ...         [1, 0, 1, 0, 1],\n     ...         [0, 0, 1, 0, 0],\n     ...         [1, 0, 0, 1, 0]]\n     >>> solve_maze(maze)\n+    True\n+    Path:\nComment: Changing this code is modifying what the doctest is looking for.  As discussed in `CONTRIBUTING.md` you can run these tests on your local computer by doing.\r\n% `python3 -m doctest -v backtracking/rat_in_maze.py`\r\n\r\nhttps://github.com/TheAlgorithms/Python/actions/runs/6284643240/job/17066208866?pr=9082#step:6:709\r\n```\r\n=================================== FAILURES ===================================\r\n________________ [doctest] backtracking.rat_in_maze.solve_maze _________________\r\n013 \r\n014     Returns:\r\n015         bool: True if a solution exists, False otherwise.\r\n016 \r\n017     >>> maze = [[0, 1, 0, 1, 1],\r\n018     ...         [0, 0, 0, 0, 0],\r\n019     ...         [1, 0, 1, 0, 1],\r\n020     ...         [0, 0, 1, 0, 0],\r\n021     ...         [1, 0, 0, 1, 0]]\r\n022     >>> solve_maze(maze)\r\nDifferences (unified diff with -expected +actual):\r\n    @@ -1,3 +1,2 @@\r\n    -True\r\n     Path:\r\n     [1, 0, 0, 0, 0]\r\n    @@ -6,2 +5,3 @@\r\n     [0, 0, 0, 1, 1]\r\n     [0, 0, 0, 0, 1]\r\n    +True\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "backtracking/rat_in_maze.py",
    "pr_number": 9082,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1335133316,
    "comment_created_at": "2023-09-24T06:46:27Z"
  },
  {
    "code": "@@ -266,7 +266,7 @@ def get(self, key: T) -> U | None:\n         self.miss += 1\n         return None\n \n-    def set(self, key: T, value: U) -> None:\n+    def set_key(self, key: T, value: U) -> None:",
    "comment": "set() is still called in the doctests.",
    "line_number": 269,
    "enriched": "File: other/lru_cache.py\nCode: @@ -266,7 +266,7 @@ def get(self, key: T) -> U | None:\n         self.miss += 1\n         return None\n \n-    def set(self, key: T, value: U) -> None:\n+    def set_key(self, key: T, value: U) -> None:\nComment: `set()` is still called in the doctests.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "other/lru_cache.py",
    "pr_number": 7105,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994538317,
    "comment_created_at": "2022-10-13T11:45:48Z"
  },
  {
    "code": "@@ -3,14 +3,16 @@\n predictive analysis. The idea is pretty simple: we have a dataset and we have\n features associated with it. Features should be chosen very cautiously\n as they determine how much our model will be able to make future predictions.\n-We try to set the weight of these features, over many iterations, so that they best\n-fit our dataset. In this particular code, I had used a CSGO dataset (ADR vs\n-Rating). We try to best fit a line through dataset and estimate the parameters.\n+We try to set the weight of these features, using \"sum of rectangular area\n+over sum of square area\" method which is a direct method.\n+In this particular code, I had used a CSGO dataset (ADR vs Rating).\n+We try to best fit a line through dataset and estimate the parameters.\n \"\"\"\n import numpy as np\n import requests\n \n \n+# Function to collect the CSGO dataset",
    "comment": "this comment is unnecessary because we already have a docstring for the function.",
    "line_number": 15,
    "enriched": "File: machine_learning/linear_regression.py\nCode: @@ -3,14 +3,16 @@\n predictive analysis. The idea is pretty simple: we have a dataset and we have\n features associated with it. Features should be chosen very cautiously\n as they determine how much our model will be able to make future predictions.\n-We try to set the weight of these features, over many iterations, so that they best\n-fit our dataset. In this particular code, I had used a CSGO dataset (ADR vs\n-Rating). We try to best fit a line through dataset and estimate the parameters.\n+We try to set the weight of these features, using \"sum of rectangular area\n+over sum of square area\" method which is a direct method.\n+In this particular code, I had used a CSGO dataset (ADR vs Rating).\n+We try to best fit a line through dataset and estimate the parameters.\n \"\"\"\n import numpy as np\n import requests\n \n \n+# Function to collect the CSGO dataset\nComment: ```suggestion\r\n```\r\nThis comment is unnecessary because we already have a docstring for the function.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "machine_learning/linear_regression.py",
    "pr_number": 9141,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1341999071,
    "comment_created_at": "2023-09-30T19:07:44Z"
  },
  {
    "code": "@@ -0,0 +1,111 @@\n+def encrypt(plaintext, key) -> str:\n+    \"\"\"\n+    Function that encrypt a given plaintext (string)\n+    and key (string), returning the encrypted ciphertext\n+\n+    @params\n+    plaintext - a normal text to be encrypted (string)\n+    key - a small text or word to start the replacing (sFtring)\n+\n+    @return\n+    A string with the ciphertext\n+",
    "comment": "typically, we want the parameter names to be self-documenting so this isn't really necessary",
    "line_number": 12,
    "enriched": "File: ciphers/autoclave.py\nCode: @@ -0,0 +1,111 @@\n+def encrypt(plaintext, key) -> str:\n+    \"\"\"\n+    Function that encrypt a given plaintext (string)\n+    and key (string), returning the encrypted ciphertext\n+\n+    @params\n+    plaintext - a normal text to be encrypted (string)\n+    key - a small text or word to start the replacing (sFtring)\n+\n+    @return\n+    A string with the ciphertext\n+\nComment: ```suggestion\n```\nTypically, we want the parameter names to be self-documenting so this isn't really necessary ",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "ciphers/autoclave.py",
    "pr_number": 8029,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1048752085,
    "comment_created_at": "2022-12-14T17:19:28Z"
  },
  {
    "code": "@@ -11,11 +11,16 @@ def find_missing_number(nums: list[int]) -> int:\n     Example:\n         >>> find_missing_number([0, 1, 3, 4])\n         2\n+        >>> find_missing_number([1, 3, 4, 5, 6])\n+        2\n+        >>> find_missing_number([6, 5, 4, 2, 1])\n+        3\n     \"\"\"\n-    n = len(nums)\n-    missing_number = n\n+    low = min(nums[0], nums[-1])\n+    high = max(nums[0], nums[-1])",
    "comment": "this wouldn't work if the numbers are out of order. try adding this doctest and you'll see what i mean:\r\npy\r\n>>> find_missing_number([6, 1, 5, 3, 4])\r\n2",
    "line_number": 20,
    "enriched": "File: bit_manipulation/missing_number.py\nCode: @@ -11,11 +11,16 @@ def find_missing_number(nums: list[int]) -> int:\n     Example:\n         >>> find_missing_number([0, 1, 3, 4])\n         2\n+        >>> find_missing_number([1, 3, 4, 5, 6])\n+        2\n+        >>> find_missing_number([6, 5, 4, 2, 1])\n+        3\n     \"\"\"\n-    n = len(nums)\n-    missing_number = n\n+    low = min(nums[0], nums[-1])\n+    high = max(nums[0], nums[-1])\nComment: This wouldn't work if the numbers are out of order. Try adding this doctest and you'll see what I mean:\r\n```py\r\n>>> find_missing_number([6, 1, 5, 3, 4])\r\n2\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "bit_manipulation/missing_number.py",
    "pr_number": 10361,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359234336,
    "comment_created_at": "2023-10-14T08:11:50Z"
  },
  {
    "code": "@@ -0,0 +1,146 @@\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\"\"\"\"\n+ FuzzySet class for triangular fuzzy sets\n+ Author: Shreya123714\n+ Source: https://en.wikipedia.org/wiki/Fuzzy_set\n+\"\"\"\n+\n+\n+class FuzzySet:",
    "comment": "please make this a https://docs.python.org/3/library/dataclasses.html",
    "line_number": 11,
    "enriched": "File: fuzzy_logic/fuzzy_operations.py\nCode: @@ -0,0 +1,146 @@\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\"\"\"\"\n+ FuzzySet class for triangular fuzzy sets\n+ Author: Shreya123714\n+ Source: https://en.wikipedia.org/wiki/Fuzzy_set\n+\"\"\"\n+\n+\n+class FuzzySet:\nComment: Please make this a https://docs.python.org/3/library/dataclasses.html",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "fuzzy_logic/fuzzy_operations.py",
    "pr_number": 11036,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375008262,
    "comment_created_at": "2023-10-27T20:16:37Z"
  },
  {
    "code": "@@ -0,0 +1,36 @@\n+# The function returns maximum circular contiguous sum in a[]\n+def maxCircularSum(arr: list[int], n: int) -> int:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: maxcircularsum\n\nas there is no test file in this pull request nor any test function or class in the file dynamic_programming/maximum_circular_subarray.py, please provide doctest for the function maxcircularsum\n\nplease provide descriptive name for the parameter: n",
    "line_number": 2,
    "enriched": "File: dynamic_programming/maximum_circular_subarray.py\nCode: @@ -0,0 +1,36 @@\n+# The function returns maximum circular contiguous sum in a[]\n+def maxCircularSum(arr: list[int], n: int) -> int:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `maxCircularSum`\n\nAs there is no test file in this pull request nor any test function or class in the file `dynamic_programming/maximum_circular_subarray.py`, please provide doctest for the function `maxCircularSum`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "dynamic_programming/maximum_circular_subarray.py",
    "pr_number": 7098,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994366283,
    "comment_created_at": "2022-10-13T09:04:57Z"
  },
  {
    "code": "@@ -0,0 +1,14 @@\n+''' This code finds the greatest prime number that is smaller than or equal to the input by user '''\n+def nextSmallPrime(n):",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: nextsmallprime\n\nplease provide return type hint for the function: nextsmallprime. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file maths/next_small_prime.py, please provide doctest for the function nextsmallprime\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: n",
    "line_number": 2,
    "enriched": "File: maths/Next_Small_Prime.py\nCode: @@ -0,0 +1,14 @@\n+''' This code finds the greatest prime number that is smaller than or equal to the input by user '''\n+def nextSmallPrime(n):\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `nextSmallPrime`\n\nPlease provide return type hint for the function: `nextSmallPrime`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `maths/Next_Small_Prime.py`, please provide doctest for the function `nextSmallPrime`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide type hint for the parameter: `n`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/Next_Small_Prime.py",
    "pr_number": 9693,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1345709003,
    "comment_created_at": "2023-10-04T12:23:02Z"
  },
  {
    "code": "@@ -1,14 +1,41 @@\n def print_dist(dist, v):\n+    \"\"\"\n+    Print vertex distances.\n+\n+    Parameters:\n+    dist (list): A list of distances.\n+    v (int): The number of vertices.",
    "comment": "putting the type hints into the function signature allows [mypy](https://mypy.readthedocs.io) to check them in our tests.  please do not repeat them in the docstring below because readers will get confused if one is changed but the other is not.",
    "line_number": 7,
    "enriched": "File: graphs/dijkstra_2.py\nCode: @@ -1,14 +1,41 @@\n def print_dist(dist, v):\n+    \"\"\"\n+    Print vertex distances.\n+\n+    Parameters:\n+    dist (list): A list of distances.\n+    v (int): The number of vertices.\nComment: Putting the type hints into the function signature allows [mypy](https://mypy.readthedocs.io) to check them in our tests.  Please do not repeat them in the docstring below because readers will get confused if one is changed but the other is not.\r\n```suggestion\r\ndef print_dist(dist: list[float], v: int):\r\n    \"\"\"\r\n    Print vertex distances.\r\n\r\n    Parameters:\r\n    dist: A list of distances.\r\n    v: The number of vertices.\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "graphs/dijkstra_2.py",
    "pr_number": 9967,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1351582320,
    "comment_created_at": "2023-10-10T06:16:40Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+# A class to represent a disjoint set\n+class DisjointSet:\n+\tparent = {}\n+\n+\t# Find the root of the set in which element `k` belongs\n+\tdef Find(self, k):",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: find\n\nas there is no test file in this pull request nor any test function or class in the file linear_algebra/src/operations.py, please provide doctest for the function find\n\nplease provide return type hint for the function: find. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide descriptive name for the parameter: k\n\nplease provide type hint for the parameter: k",
    "line_number": 6,
    "enriched": "File: linear_algebra/src/operations.py\nCode: @@ -0,0 +1,53 @@\n+# A class to represent a disjoint set\n+class DisjointSet:\n+\tparent = {}\n+\n+\t# Find the root of the set in which element `k` belongs\n+\tdef Find(self, k):\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `Find`\n\nAs there is no test file in this pull request nor any test function or class in the file `linear_algebra/src/operations.py`, please provide doctest for the function `Find`\n\nPlease provide return type hint for the function: `Find`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide descriptive name for the parameter: `k`\n\nPlease provide type hint for the parameter: `k`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "linear_algebra/src/operations.py",
    "pr_number": 7641,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004562840,
    "comment_created_at": "2022-10-25T14:26:36Z"
  },
  {
    "code": "@@ -0,0 +1,168 @@\n+\"\"\"\n+Problem Statement: Given a binary perform an preorder traversal using Morris Preorder\n+traversal algorithm. (Iterative version of Preorder traversal of tree)\n+\n+https://www.geeksforgeeks.org/morris-traversal-for-preorder/\n+\"\"\"\n+\n+\n+class TreeNode:\n+    \"\"\"\n+    Class representing a node in a binary tree.\n+\n+    Attributes:\n+    -----------\n+    value : int\n+        The value stored at the node.\n+    left : TreeNode\n+        Pointer to the left child node (default is None).\n+    right : TreeNode\n+        Pointer to the right child node (default is None).\n+    \"\"\"\n+\n+    def __init__(self, value: int) -> None:\n+        self.value = value\n+        self.left: TreeNode | None = None\n+        self.right: TreeNode | None = None\n+\n+\n+class BinaryTree:\n+    \"\"\"\n+    Class representing a binary tree.\n+\n+    Methods:\n+    --------\n+    insert(value: int) -> None:\n+        Insert a value into the binary tree following binary search tree (BST) rules.\n+\n+    morris_preorder_traversal() -> List[int]:\n+        Perform preorder traversal and return list of node values.\n+\n+",
    "comment": "i don't think it's necessary to specify the methods in the docstring because the methods themselves should already be documented anyway.",
    "line_number": 41,
    "enriched": "File: data_structures/binary_tree/morris_preorder_traversal.py\nCode: @@ -0,0 +1,168 @@\n+\"\"\"\n+Problem Statement: Given a binary perform an preorder traversal using Morris Preorder\n+traversal algorithm. (Iterative version of Preorder traversal of tree)\n+\n+https://www.geeksforgeeks.org/morris-traversal-for-preorder/\n+\"\"\"\n+\n+\n+class TreeNode:\n+    \"\"\"\n+    Class representing a node in a binary tree.\n+\n+    Attributes:\n+    -----------\n+    value : int\n+        The value stored at the node.\n+    left : TreeNode\n+        Pointer to the left child node (default is None).\n+    right : TreeNode\n+        Pointer to the right child node (default is None).\n+    \"\"\"\n+\n+    def __init__(self, value: int) -> None:\n+        self.value = value\n+        self.left: TreeNode | None = None\n+        self.right: TreeNode | None = None\n+\n+\n+class BinaryTree:\n+    \"\"\"\n+    Class representing a binary tree.\n+\n+    Methods:\n+    --------\n+    insert(value: int) -> None:\n+        Insert a value into the binary tree following binary search tree (BST) rules.\n+\n+    morris_preorder_traversal() -> List[int]:\n+        Perform preorder traversal and return list of node values.\n+\n+\nComment: ```suggestion\r\n```\r\nI don't think it's necessary to specify the methods in the docstring because the methods themselves should already be documented anyway.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "data_structures/binary_tree/morris_preorder_traversal.py",
    "pr_number": 11776,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1788654072,
    "comment_created_at": "2024-10-05T17:56:47Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+# Python3 Program for Floyd Warshall Algorithm\n+\n+# Number of vertices in the graph\n+V = 4\n+\n+# Define infinity as the large\n+# enough value. This value will be\n+# used for vertices not connected to each other\n+INF = 99999\n+\n+# Solves all pair shortest path\n+# via Floyd Warshall Algorithm\n+\n+\n+def floydWarshall(graph):",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: floydwarshall\n\nas there is no test file in this pull request nor any test function or class in the file graphs/floyd_warshall.py, please provide doctest for the function floydwarshall\n\nplease provide return type hint for the function: floydwarshall. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: graph",
    "line_number": 15,
    "enriched": "File: graphs/floyd_warshall.py\nCode: @@ -0,0 +1,94 @@\n+# Python3 Program for Floyd Warshall Algorithm\n+\n+# Number of vertices in the graph\n+V = 4\n+\n+# Define infinity as the large\n+# enough value. This value will be\n+# used for vertices not connected to each other\n+INF = 99999\n+\n+# Solves all pair shortest path\n+# via Floyd Warshall Algorithm\n+\n+\n+def floydWarshall(graph):\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `floydWarshall`\n\nAs there is no test file in this pull request nor any test function or class in the file `graphs/floyd_warshall.py`, please provide doctest for the function `floydWarshall`\n\nPlease provide return type hint for the function: `floydWarshall`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `graph`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "graphs/floyd_warshall.py",
    "pr_number": 10658,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1362792372,
    "comment_created_at": "2023-10-17T21:17:17Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+def find_missing_number(nums):\n+    \"\"\"\n+    Finds the missing number in a list of consecutive integers.\n+\n+    Args:\n+        nums (List[int]): A list of integers.\n+\n+    Returns:\n+        int: The missing number.",
    "comment": "[mypy](mypy.readthedocs.io) can test data types in the function signature, but not in the comments.\r\n\r\ndo not repeat the datatypes in both places because readers will be confused if one is changed and the other is not.",
    "line_number": 9,
    "enriched": "File: bit_manipulation/missing_number.py\nCode: @@ -0,0 +1,21 @@\n+def find_missing_number(nums):\n+    \"\"\"\n+    Finds the missing number in a list of consecutive integers.\n+\n+    Args:\n+        nums (List[int]): A list of integers.\n+\n+    Returns:\n+        int: The missing number.\nComment: [Mypy](mypy.readthedocs.io) can test data types in the function signature, but not in the comments.\r\n\r\nDo not repeat the datatypes in both places because readers will be confused if one is changed and the other is not.\r\n\r\n```suggestion\r\ndef find_missing_number(nums: list[int]) -> int:\r\n    \"\"\"\r\n    Finds the missing number in a list of consecutive integers.\r\n\r\n    Args:\r\n        nums: A list of integers.\r\n\r\n    Returns:\r\n        The missing number.\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "bit_manipulation/missing_number.py",
    "pr_number": 9203,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342120483,
    "comment_created_at": "2023-10-01T11:23:39Z"
  },
  {
    "code": "@@ -0,0 +1,90 @@\n+Gravitational Search Algorithm\n+Reference: https://www.sciencedirect.com/science/article/abs/pii/S0020025509001200",
    "comment": "an error occurred while parsing the file: machine_learning/gravitational_search_algorithm.py\npython\ntraceback (most recent call last):\n  file \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 146, in parse\n    reports = lint_file(\n              ^^^^^^^^^^\nlibcst._exceptions.parsersyntaxerror: syntax error @ 2:22.\nparser error: error at 1:21: expected one of !=, %, &, (, *, **, +, ,, -, ., /, //, :, ;, <, <<, <=, =, ==, >, >=, >>, @, newline, [, ^, and, if, in, is, not, or, |\n\nreference: https://www.sciencedirect.com/science/article/abs/pii/s0020025509001200\n                     ^",
    "line_number": 2,
    "enriched": "File: machine_learning/gravitational_search_algorithm.py\nCode: @@ -0,0 +1,90 @@\n+Gravitational Search Algorithm\n+Reference: https://www.sciencedirect.com/science/article/abs/pii/S0020025509001200\nComment: An error occurred while parsing the file: `machine_learning/gravitational_search_algorithm.py`\n```python\nTraceback (most recent call last):\n  File \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 146, in parse\n    reports = lint_file(\n              ^^^^^^^^^^\nlibcst._exceptions.ParserSyntaxError: Syntax Error @ 2:22.\nparser error: error at 1:21: expected one of !=, %, &, (, *, **, +, ,, -, ., /, //, :, ;, <, <<, <=, =, ==, >, >=, >>, @, NEWLINE, [, ^, and, if, in, is, not, or, |\n\nReference: https://www.sciencedirect.com/science/article/abs/pii/S0020025509001200\n                     ^\n\n```",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "machine_learning/gravitational_search_algorithm.py",
    "pr_number": 9676,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1345282864,
    "comment_created_at": "2023-10-04T06:51:12Z"
  },
  {
    "code": "@@ -32,17 +45,18 @@ def frac_knapsack(vl, wt, w, n):\n     TypeError: unsupported operand type(s) for /: 'str' and 'int'\n     \"\"\"\n \n+    # sort in descending order of value/weight ratio\n     r = sorted(zip(vl, wt), key=lambda x: x[0] / x[1], reverse=True)",
    "comment": "please add this and fix the tests to match.\r\n\r\nhttps://docs.astral.sh/ruff/rules/zip-without-explicit-strict/",
    "line_number": 49,
    "enriched": "File: greedy_methods/fractional_knapsack.py\nCode: @@ -32,17 +45,18 @@ def frac_knapsack(vl, wt, w, n):\n     TypeError: unsupported operand type(s) for /: 'str' and 'int'\n     \"\"\"\n \n+    # sort in descending order of value/weight ratio\n     r = sorted(zip(vl, wt), key=lambda x: x[0] / x[1], reverse=True)\nComment: Please add this and fix the tests to match.\r\n```suggestion\r\n    r = sorted(zip(vl, wt), key=lambda x: x[0] / x[1], reverse=True, strict=True)\r\n```\r\nhttps://docs.astral.sh/ruff/rules/zip-without-explicit-strict/",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "greedy_methods/fractional_knapsack.py",
    "pr_number": 10888,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1370441006,
    "comment_created_at": "2023-10-24T15:51:00Z"
  },
  {
    "code": "@@ -11,10 +11,8 @@\n * [ ] This pull request is all my own work -- I have not plagiarized.\n * [ ] I know that pull requests will not be merged if they fail the automated tests.\n * [ ] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.\n-* [ ] All new Python files are placed inside an existing directory.\n-* [ ] All filenames are in all lowercase characters with no spaces or dashes.\n * [ ] All functions and variable names follow Python naming conventions.\n * [ ] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).\n * [ ] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.\n-* [ ] All new algorithms have a URL in its comments that points to Wikipedia or other similar explanation.\n+* [ ] All new algorithms have a URL in their comments that point to Wikipedia or another similar explanation.",
    "comment": "perhaps this\r\n\r\nor \r\n\r\nthese make a bit more sense in my opinion",
    "line_number": 17,
    "enriched": "File: .github/pull_request_template.md\nCode: @@ -11,10 +11,8 @@\n * [ ] This pull request is all my own work -- I have not plagiarized.\n * [ ] I know that pull requests will not be merged if they fail the automated tests.\n * [ ] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.\n-* [ ] All new Python files are placed inside an existing directory.\n-* [ ] All filenames are in all lowercase characters with no spaces or dashes.\n * [ ] All functions and variable names follow Python naming conventions.\n * [ ] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).\n * [ ] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.\n-* [ ] All new algorithms have a URL in its comments that points to Wikipedia or other similar explanation.\n+* [ ] All new algorithms have a URL in their comments that point to Wikipedia or another similar explanation.\nComment: \r\nPerhaps this\r\n```suggestion\r\n* [ ] All new algorithms have a docstring containing a URL that point to Wikipedia or another similar explanation.\r\n```\r\nor \r\n```suggestion\r\n* [ ] All new algorithms include at least one URL that points to Wikipedia or another similar explanation.\r\n```\r\nThese make a bit more sense in my opinion",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": ".github/pull_request_template.md",
    "pr_number": 7794,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008297769,
    "comment_created_at": "2022-10-28T17:32:24Z"
  },
  {
    "code": "@@ -1,106 +1,83 @@\n import math\n \n \n-def convert(number: int) -> str:\n-    \"\"\"\n-    Given a number return the number in words.\n+def int_to_en(num):\n+    d = {\n+        0: \"zero\",\n+        1: \"one\",\n+        2: \"two\",\n+        3: \"three\",\n+        4: \"four\",\n+        5: \"five\",\n+        6: \"six\",\n+        7: \"seven\",\n+        8: \"eight\",\n+        9: \"nine\",\n+        10: \"ten\",\n+        11: \"eleven\",\n+        12: \"twelve\",\n+        13: \"thirteen\",\n+        14: \"fourteen\",\n+        15: \"fifteen\",\n+        16: \"sixteen\",\n+        17: \"seventeen\",\n+        18: \"eighteen\",\n+        19: \"nineteen\",\n+        20: \"twenty\",\n+        30: \"thirty\",\n+        40: \"forty\",\n+        50: \"fifty\",\n+        60: \"sixty\",\n+        70: \"seventy\",\n+        80: \"eighty\",\n+        90: \"ninety\",\n+    }\n+    k = 1000\n+    m = k * 1000\n+    b = m * 1000\n+    t = b * 1000\n \n-    >>> convert(123)\n-    'OneHundred,TwentyThree'",
    "comment": "please do not remove doctests.  please add a doctest for -8.",
    "line_number": 9,
    "enriched": "File: web_programming/convert_number_to_words.py\nCode: @@ -1,106 +1,83 @@\n import math\n \n \n-def convert(number: int) -> str:\n-    \"\"\"\n-    Given a number return the number in words.\n+def int_to_en(num):\n+    d = {\n+        0: \"zero\",\n+        1: \"one\",\n+        2: \"two\",\n+        3: \"three\",\n+        4: \"four\",\n+        5: \"five\",\n+        6: \"six\",\n+        7: \"seven\",\n+        8: \"eight\",\n+        9: \"nine\",\n+        10: \"ten\",\n+        11: \"eleven\",\n+        12: \"twelve\",\n+        13: \"thirteen\",\n+        14: \"fourteen\",\n+        15: \"fifteen\",\n+        16: \"sixteen\",\n+        17: \"seventeen\",\n+        18: \"eighteen\",\n+        19: \"nineteen\",\n+        20: \"twenty\",\n+        30: \"thirty\",\n+        40: \"forty\",\n+        50: \"fifty\",\n+        60: \"sixty\",\n+        70: \"seventy\",\n+        80: \"eighty\",\n+        90: \"ninety\",\n+    }\n+    k = 1000\n+    m = k * 1000\n+    b = m * 1000\n+    t = b * 1000\n \n-    >>> convert(123)\n-    'OneHundred,TwentyThree'\nComment: Please do not remove doctests.  Please add a doctest for `-8`.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "web_programming/convert_number_to_words.py",
    "pr_number": 8890,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1273149439,
    "comment_created_at": "2023-07-25T07:57:19Z"
  },
  {
    "code": "@@ -6,9 +6,20 @@\n \n def next_greatest_element_slow(arr: list[float]) -> list[float]:\n     \"\"\"\n-    Get the Next Greatest Element (NGE) for all elements in a list.\n-    Maximum element present after the current one which is also greater than the\n-    current one.\n+    Get the Next Greatest Element (NGE) for each element in the array\n+    by checking all subsequent elements to find the next greater one.\n+\n+    This is a brute-force implementation, and it has a time complexity\n+    of O(n^2), where n is the size of the array.\n+\n+    Args:\n+        arr (list[float]): List of numbers for which the NGE is calculated.\n+\n+    Returns:\n+        list[float]: List containing the next greatest elements. If no\n+        greater element is found, -1 is placed in the result.",
    "comment": "if the type of the variable ever changes for any reason, we'd need to remember to update the type hint in two different places. as such, in this repo we prefer to avoid specifying the variable types in the docstrings.",
    "line_number": 20,
    "enriched": "File: data_structures/stacks/next_greater_element.py\nCode: @@ -6,9 +6,20 @@\n \n def next_greatest_element_slow(arr: list[float]) -> list[float]:\n     \"\"\"\n-    Get the Next Greatest Element (NGE) for all elements in a list.\n-    Maximum element present after the current one which is also greater than the\n-    current one.\n+    Get the Next Greatest Element (NGE) for each element in the array\n+    by checking all subsequent elements to find the next greater one.\n+\n+    This is a brute-force implementation, and it has a time complexity\n+    of O(n^2), where n is the size of the array.\n+\n+    Args:\n+        arr (list[float]): List of numbers for which the NGE is calculated.\n+\n+    Returns:\n+        list[float]: List containing the next greatest elements. If no\n+        greater element is found, -1 is placed in the result.\nComment: ```suggestion\r\n    Args:\r\n        arr: List of numbers for which the NGE is calculated.\r\n\r\n    Returns:\r\n        List containing the next greatest elements. If no\r\n        greater element is found, -1 is placed in the result.\r\n```\r\nIf the type of the variable ever changes for any reason, we'd need to remember to update the type hint in two different places. As such, in this repo we prefer to avoid specifying the variable types in the docstrings.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "data_structures/stacks/next_greater_element.py",
    "pr_number": 11685,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1785442421,
    "comment_created_at": "2024-10-03T00:07:49Z"
  },
  {
    "code": "@@ -0,0 +1,28 @@\n+\"\"\"\n+wiki: https://en.wikipedia.org/wiki/Heterogram_(literature)#Isograms\n+\"\"\"\n+\n+\n+def check_isogram(string: str) -> bool:",
    "comment": "this is a slightly more self-documenting name for a function that returns a bool.\r\n\r\n\r\nalso, please add a test for true123.  should that return true or raise a valueerror?",
    "line_number": 6,
    "enriched": "File: strings/check_isogram.py\nCode: @@ -0,0 +1,28 @@\n+\"\"\"\n+wiki: https://en.wikipedia.org/wiki/Heterogram_(literature)#Isograms\n+\"\"\"\n+\n+\n+def check_isogram(string: str) -> bool:\nComment: This is a slightly more self-documenting name for a function that returns a bool.\r\n```suggestion\r\ndef is_isogram(string: str) -> bool:\r\n```\r\n\r\nAlso, please add a test for `true123`.  Should that return True or raise a ValueError?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "strings/check_isogram.py",
    "pr_number": 7608,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004945882,
    "comment_created_at": "2022-10-25T20:42:01Z"
  },
  {
    "code": "@@ -0,0 +1,129 @@\n+from __future__ import annotations\n+class Node:\n+    \"\"\"\n+prints the inorder Traversal of transformed tree\n+>>> sum = 0\n+>>> root = Node(11)\n+>>> root.left = Node(2)\n+>>> root.right = Node(29)\n+>>> root.left.left = Node(1)\n+>>> root.left.right = Node(7)\n+>>> root.right.left = Node(15)\n+>>> root.right.right = Node(40)\n+>>> root.right.right.left = Node(35)\n+>>> printInorder(root)\n+1 2 7 11 15 29 35 40 \n+\n+>>> transformTree(root)\n+\n+>>> printInorder(root)\n+139 137 130 119 104 75 40 0\n+\n+\"\"\"\n+\n+    def __init__(self, x) -> None:",
    "comment": "please provide descriptive name for the parameter: x\n\nplease provide type hint for the parameter: x",
    "line_number": 24,
    "enriched": "File: data_structures/binary_tree/transform_bst_sum_tree.py\nCode: @@ -0,0 +1,129 @@\n+from __future__ import annotations\n+class Node:\n+    \"\"\"\n+prints the inorder Traversal of transformed tree\n+>>> sum = 0\n+>>> root = Node(11)\n+>>> root.left = Node(2)\n+>>> root.right = Node(29)\n+>>> root.left.left = Node(1)\n+>>> root.left.right = Node(7)\n+>>> root.right.left = Node(15)\n+>>> root.right.right = Node(40)\n+>>> root.right.right.left = Node(35)\n+>>> printInorder(root)\n+1 2 7 11 15 29 35 40 \n+\n+>>> transformTree(root)\n+\n+>>> printInorder(root)\n+139 137 130 119 104 75 40 0\n+\n+\"\"\"\n+\n+    def __init__(self, x) -> None:\nComment: Please provide descriptive name for the parameter: `x`\n\nPlease provide type hint for the parameter: `x`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "data_structures/binary_tree/transform_bst_sum_tree.py",
    "pr_number": 9772,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346786764,
    "comment_created_at": "2023-10-05T05:03:54Z"
  },
  {
    "code": "@@ -9,7 +9,7 @@ def permute(nums: list[int]) -> list[list[int]]:\n     \"\"\"\n     result = []\n     if len(nums) == 1:\n-        return [nums.copy()]\n+        return [nums.copy()] #returns a copy of list nums",
    "comment": "this is self explanatory",
    "line_number": 12,
    "enriched": "File: data_structures/arrays/permutations.py\nCode: @@ -9,7 +9,7 @@ def permute(nums: list[int]) -> list[list[int]]:\n     \"\"\"\n     result = []\n     if len(nums) == 1:\n-        return [nums.copy()]\n+        return [nums.copy()] #returns a copy of list nums\nComment: This is self explanatory",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "data_structures/arrays/permutations.py",
    "pr_number": 8129,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1102645891,
    "comment_created_at": "2023-02-10T11:30:44Z"
  },
  {
    "code": "@@ -5,7 +5,7 @@\n import os\n import pathlib\n from types import ModuleType\n-\n+import datetime",
    "comment": "also you need to run pre-commit \r\n\r\n- install, before you can run hooks, you need to have the pre-commit package manager installed.\r\n-  >pip install pre-commit\r\n\r\nthen > pre-commit\r\n\r\n- how to expect?\r\n\r\nrun python pre-commit-#.#.#.pyz ... in place of pre-commit ...",
    "line_number": 8,
    "enriched": "File: scripts/validate_solutions.py\nCode: @@ -5,7 +5,7 @@\n import os\n import pathlib\n from types import ModuleType\n-\n+import datetime\nComment: Also you need to run` pre-commit `\r\n\r\n- Install, before you can run hooks, you need to have the pre-commit package manager installed.\r\n-  >`pip install pre-commit`\r\n\r\nthen > `pre-commit`\r\n\r\n- How to expect?\r\n\r\n`run python pre-commit-#.#.#.pyz ... in place of pre-commit ...`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "scripts/validate_solutions.py",
    "pr_number": 11351,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1550765825,
    "comment_created_at": "2024-04-04T02:00:15Z"
  },
  {
    "code": "@@ -0,0 +1,91 @@\n+\n+\n+from itertools import product\n+\n+def findPassword(chars, function, show=50, format_=\"%s\"):",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: findpassword\n\nas there is no test file in this pull request nor any test function or class in the file password_check/password tester.py, please provide doctest for the function findpassword\n\nplease provide return type hint for the function: findpassword. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: chars\n\nplease provide type hint for the parameter: function\n\nplease provide type hint for the parameter: show\n\nplease provide type hint for the parameter: format_",
    "line_number": 5,
    "enriched": "File: Password_Check/Password tester.py\nCode: @@ -0,0 +1,91 @@\n+\n+\n+from itertools import product\n+\n+def findPassword(chars, function, show=50, format_=\"%s\"):\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `findPassword`\n\nAs there is no test file in this pull request nor any test function or class in the file `Password_Check/Password tester.py`, please provide doctest for the function `findPassword`\n\nPlease provide return type hint for the function: `findPassword`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `chars`\n\nPlease provide type hint for the parameter: `function`\n\nPlease provide type hint for the parameter: `show`\n\nPlease provide type hint for the parameter: `format_`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "Password_Check/Password tester.py",
    "pr_number": 7381,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 997335988,
    "comment_created_at": "2022-10-17T17:33:00Z"
  },
  {
    "code": "@@ -1,21 +1,36 @@\n+# Enables forward annotations for type hinting",
    "comment": "we do not want to slow readers down too much.",
    "line_number": 1,
    "enriched": "File: data_structures/linked_list/circular_linked_list.py\nCode: @@ -1,21 +1,36 @@\n+# Enables forward annotations for type hinting\nComment: We do not want to slow readers down too much.\r\n```suggestion\r\n```",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "data_structures/linked_list/circular_linked_list.py",
    "pr_number": 9668,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1345270029,
    "comment_created_at": "2023-10-04T06:38:28Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+def gcd(numerator: int, denominator: int) -> int:",
    "comment": "we already have tons of implementations of greatest_common_demonintor() so please just use https://docs.python.org/3/library/math.html#math.gcd instead.",
    "line_number": 1,
    "enriched": "File: maths/numerical_analysis/proper_fractions.py\nCode: @@ -0,0 +1,42 @@\n+def gcd(numerator: int, denominator: int) -> int:\nComment: We already have tons of implementations of `greatest_common_demonintor()` so please just use https://docs.python.org/3/library/math.html#math.gcd instead.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "maths/numerical_analysis/proper_fractions.py",
    "pr_number": 11224,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1526596481,
    "comment_created_at": "2024-03-15T17:12:31Z"
  },
  {
    "code": "@@ -0,0 +1,23 @@\n+def merge_sorted_arrays(arr1: list[int], arr2: list[int]) -> list[int]:\n+    \"\"\"",
    "comment": "add description for this algo/code so that viewer can understand.",
    "line_number": 2,
    "enriched": "File: data_structures/arrays/mergesortedarrays.py\nCode: @@ -0,0 +1,23 @@\n+def merge_sorted_arrays(arr1: list[int], arr2: list[int]) -> list[int]:\n+    \"\"\"\nComment: add description for this algo/code so that viewer can understand.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "data_structures/arrays/mergesortedarrays.py",
    "pr_number": 8692,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1177998401,
    "comment_created_at": "2023-04-26T14:50:48Z"
  },
  {
    "code": "@@ -0,0 +1,62 @@\n+# https://www.geeksforgeeks.org/convert-ip-address-to-integer-and-vice-versa/\n+\n+\n+def ip_to_decimal(ip_address: str) -> int:\n+    \"\"\"\n+    Convert an IPv4 address to its decimal representation.\n+\n+    Args:\n+        ip_address (str): A string representing an IPv4 address (e.g., \"192.168.0.1\").\n+\n+    Returns:\n+        int: The decimal representation of the IP address.\n+\n+    >>> ip_to_decimal(\"192.168.0.1\")\n+    3232235521\n+    >>> ip_to_decimal(\"10.0.0.255\")\n+    167772415\n+    \"\"\"\n+\n+    ip_parts = ip_address.split(\".\")",
    "comment": "see: https://github.com/thealgorithms/python/blob/master/maths/is_ip_v4_address_valid.py",
    "line_number": 20,
    "enriched": "File: conversions/ipconversion.py\nCode: @@ -0,0 +1,62 @@\n+# https://www.geeksforgeeks.org/convert-ip-address-to-integer-and-vice-versa/\n+\n+\n+def ip_to_decimal(ip_address: str) -> int:\n+    \"\"\"\n+    Convert an IPv4 address to its decimal representation.\n+\n+    Args:\n+        ip_address (str): A string representing an IPv4 address (e.g., \"192.168.0.1\").\n+\n+    Returns:\n+        int: The decimal representation of the IP address.\n+\n+    >>> ip_to_decimal(\"192.168.0.1\")\n+    3232235521\n+    >>> ip_to_decimal(\"10.0.0.255\")\n+    167772415\n+    \"\"\"\n+\n+    ip_parts = ip_address.split(\".\")\nComment: See: https://github.com/TheAlgorithms/Python/blob/master/maths/is_ip_v4_address_valid.py\r\n```suggestion\r\n    octets = ip_address.split(\".\")\r\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "conversions/ipconversion.py",
    "pr_number": 11008,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1373828104,
    "comment_created_at": "2023-10-26T21:20:18Z"
  },
  {
    "code": "@@ -0,0 +1,45 @@\n+# Find Transition Point\r\n+# Given a sorted array containing only 0s and 1s, find the transition point. \r\n+# Example\r\n+# N = 5\r\n+# arr[] = {0,0,0,1,1}\r\n+# Output: 3\r\n+# Explanation: index 3 is the transition point where 1 begins.\r\n+\r\n+# If in case no trasistion point is found, then we return -1\r\n+\r\n+# Below are Three different Implementations.\r\n+\r\n+\r\n+def transitionPoint(arr, n):\r",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: transitionpoint\n\nplease provide return type hint for the function: transitionpoint. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file searches/find_transition_point.py, please provide doctest for the function transitionpoint\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n",
    "line_number": 14,
    "enriched": "File: searches/find_transition_point.py\nCode: @@ -0,0 +1,45 @@\n+# Find Transition Point\r\n+# Given a sorted array containing only 0s and 1s, find the transition point. \r\n+# Example\r\n+# N = 5\r\n+# arr[] = {0,0,0,1,1}\r\n+# Output: 3\r\n+# Explanation: index 3 is the transition point where 1 begins.\r\n+\r\n+# If in case no trasistion point is found, then we return -1\r\n+\r\n+# Below are Three different Implementations.\r\n+\r\n+\r\n+def transitionPoint(arr, n):\r\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `transitionPoint`\n\nPlease provide return type hint for the function: `transitionPoint`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `searches/find_transition_point.py`, please provide doctest for the function `transitionPoint`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "searches/find_transition_point.py",
    "pr_number": 7602,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1003420813,
    "comment_created_at": "2022-10-24T14:57:38Z"
  },
  {
    "code": "@@ -122,6 +122,7 @@ def decrypt_caesar_with_chi_squared(\n \n     >>> decrypt_caesar_with_chi_squared(12)\n     Traceback (most recent call last):\n+    ...",
    "comment": "please indent four spaces like the doctest docs do.",
    "line_number": 125,
    "enriched": "File: ciphers/decrypt_caesar_with_chi_squared.py\nCode: @@ -122,6 +122,7 @@ def decrypt_caesar_with_chi_squared(\n \n     >>> decrypt_caesar_with_chi_squared(12)\n     Traceback (most recent call last):\n+    ...\nComment: Please indent four spaces like the doctest docs do.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "ciphers/decrypt_caesar_with_chi_squared.py",
    "pr_number": 7558,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002719462,
    "comment_created_at": "2022-10-23T14:27:30Z"
  },
  {
    "code": "@@ -0,0 +1,52 @@\n+\"\"\"\n+This is a pure Python implementation of the Cyclic Sort algorithm.\n+\n+For doctests run following command:\n+python -m doctest -v cyclic_sort.py\n+or\n+python3 -m doctest -v cyclic_sort.py\n+For manual testing run:\n+python cyclic_sort.py\n+\"\"\"\n+\n+\n+def cyclic_sort(nums: list) -> list:\n+    \"\"\"\n+    Sorts the input list in-place using the Cyclic Sort algorithm.",
    "comment": "i'd recommend adding the input constraints.\r\n\r\nthis algorithm only works correctly when the elements range from 1 to n with no duplicates. stating this constraint on the inputs to any potential users of the algorithm can help prevent confusion should they provide a list of integers that does not fit the criteria and end up in an infinite loop or some other unexpected behaviour.",
    "line_number": 15,
    "enriched": "File: sorts/cyclic_sort.py\nCode: @@ -0,0 +1,52 @@\n+\"\"\"\n+This is a pure Python implementation of the Cyclic Sort algorithm.\n+\n+For doctests run following command:\n+python -m doctest -v cyclic_sort.py\n+or\n+python3 -m doctest -v cyclic_sort.py\n+For manual testing run:\n+python cyclic_sort.py\n+\"\"\"\n+\n+\n+def cyclic_sort(nums: list) -> list:\n+    \"\"\"\n+    Sorts the input list in-place using the Cyclic Sort algorithm.\nComment: I'd recommend adding the input constraints.\r\n\r\nThis algorithm only works correctly when the elements range from `1` to `n` with no duplicates. Stating this constraint on the inputs to any potential users of the algorithm can help prevent confusion should they provide a list of integers that does not fit the criteria and end up in an infinite loop or some other unexpected behaviour.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "sorts/cyclic_sort.py",
    "pr_number": 9256,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1792504872,
    "comment_created_at": "2024-10-08T21:01:58Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+\"\"\"\n+Author  : Siddharth Warrier\n+Date    : October 3, 2023\n+\n+Task:\n+Count the no of pairs in a given array with given sum\n+\n+Implementation notes: Using hashing\n+The idea is that we hash the array in a dictionary\n+Then go through the elements of the array\n+We subtract this with the given sum\n+and check if that is there in the array\n+We also check the edge cases like if there are multiple same elements\n+Finally we divide the count by 2\n+to avoid the same pair getting counted twice\n+\"\"\"\n+\n+\n+def pairs_with_sum(arr: list, req_sum: int) -> int:\n+    \"\"\"\n+    Return the no. of pairs with sum \"sum\"\n+\n+    >>> pairs_with_sum([1,5,7,1],6)\n+    2\n+    >>> pairs_with_sum([1,1,1,1,1,1,1,1],2)\n+    28\n+    >>> pairs_with_sum([1,7,6,2,5,4,3,1,9,8],7)\n+    4\n+    \"\"\"\n+    d: dict = {}",
    "comment": "this type is inferred. also try to use more descriptive variable names",
    "line_number": 30,
    "enriched": "File: data_structures/arrays/pairs_with_given_sum.py\nCode: @@ -0,0 +1,48 @@\n+\"\"\"\n+Author  : Siddharth Warrier\n+Date    : October 3, 2023\n+\n+Task:\n+Count the no of pairs in a given array with given sum\n+\n+Implementation notes: Using hashing\n+The idea is that we hash the array in a dictionary\n+Then go through the elements of the array\n+We subtract this with the given sum\n+and check if that is there in the array\n+We also check the edge cases like if there are multiple same elements\n+Finally we divide the count by 2\n+to avoid the same pair getting counted twice\n+\"\"\"\n+\n+\n+def pairs_with_sum(arr: list, req_sum: int) -> int:\n+    \"\"\"\n+    Return the no. of pairs with sum \"sum\"\n+\n+    >>> pairs_with_sum([1,5,7,1],6)\n+    2\n+    >>> pairs_with_sum([1,1,1,1,1,1,1,1],2)\n+    28\n+    >>> pairs_with_sum([1,7,6,2,5,4,3,1,9,8],7)\n+    4\n+    \"\"\"\n+    d: dict = {}\nComment: ```suggestion\n    d = {}\n```\nThis type is inferred. Also try to use more descriptive variable names",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "data_structures/arrays/pairs_with_given_sum.py",
    "pr_number": 10282,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1355789685,
    "comment_created_at": "2023-10-11T21:15:07Z"
  },
  {
    "code": "@@ -0,0 +1,80 @@\n+import sys\n+\n+\n+\"\"\"\n+The famous Floyd Warshall Alogirthm to\n+find the shortest distance between all\n+pairs of given vertices.\n+Wikipedia link:  https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm\n+\"\"\"\n+\n+def floyd_warshall(adjacency_matrix: list[list[int]], n:int) -> list[list[int]]:",
    "comment": "please provide descriptive name for the parameter: n",
    "line_number": 11,
    "enriched": "File: graphs/floyd_warshall.py\nCode: @@ -0,0 +1,80 @@\n+import sys\n+\n+\n+\"\"\"\n+The famous Floyd Warshall Alogirthm to\n+find the shortest distance between all\n+pairs of given vertices.\n+Wikipedia link:  https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm\n+\"\"\"\n+\n+def floyd_warshall(adjacency_matrix: list[list[int]], n:int) -> list[list[int]]:\nComment: Please provide descriptive name for the parameter: `n`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "graphs/floyd_warshall.py",
    "pr_number": 9911,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1348633402,
    "comment_created_at": "2023-10-06T12:05:47Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+\"\"\"\n+This is the implementation of inter_quartile range (IQR).\n+\n+function takes the list of numeric values as input\n+and return the IQR as output.\n+\n+Script inspired from its corresponding Wikipedia article\n+https://en.wikipedia.org/wiki/Interquartile_range\n+\"\"\"\n+\n+from typing import List\n+\n+\n+def find_median(x: List[float]) -> float:",
    "comment": "please provide descriptive name for the parameter: x",
    "line_number": 14,
    "enriched": "File: maths/inter_quartile_range.py\nCode: @@ -0,0 +1,60 @@\n+\"\"\"\n+This is the implementation of inter_quartile range (IQR).\n+\n+function takes the list of numeric values as input\n+and return the IQR as output.\n+\n+Script inspired from its corresponding Wikipedia article\n+https://en.wikipedia.org/wiki/Interquartile_range\n+\"\"\"\n+\n+from typing import List\n+\n+\n+def find_median(x: List[float]) -> float:\nComment: Please provide descriptive name for the parameter: `x`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/inter_quartile_range.py",
    "pr_number": 8734,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1199636933,
    "comment_created_at": "2023-05-20T17:15:38Z"
  },
  {
    "code": "@@ -0,0 +1,34 @@\n+def longest_valid_parenthesis(s: str) -> int:",
    "comment": "please provide descriptive name for the parameter: s",
    "line_number": 1,
    "enriched": "File: strings/longest_valid_parenthesis.py\nCode: @@ -0,0 +1,34 @@\n+def longest_valid_parenthesis(s: str) -> int:\nComment: Please provide descriptive name for the parameter: `s`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "strings/longest_valid_parenthesis.py",
    "pr_number": 6954,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 991641818,
    "comment_created_at": "2022-10-10T21:10:32Z"
  },
  {
    "code": "@@ -0,0 +1,84 @@\n+#https://webstor.srmist.edu.in/web_assets/srm_mainsite/files/2018/transient-analysis.pdf\n+\n+\"\"\"Electric circuits will be subjected to sudden changes which may be in the form of opening and closing of\n+switches or sudden changes in sources etc. Whenever such a change occurs, the circuit which was in a particular steady state condition will go to another steady state condition. Transient analysis is the analysis of the circuits during the time it changes from one steady state condition to another steady state condition\n+Source : https://webstor.srmist.edu.in/web_assets/srm_mainsite/files/2018/transient-analysis.pdf\n+\"\"\"\n+\n+from math import e,pow\n+\n+def transient_resp_RL(resistance:float, inductance:float, voltage:float, current:float, time:float) -> tuple:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: transient_resp_rl",
    "line_number": 10,
    "enriched": "File: electronics/transient_analysis.py\nCode: @@ -0,0 +1,84 @@\n+#https://webstor.srmist.edu.in/web_assets/srm_mainsite/files/2018/transient-analysis.pdf\n+\n+\"\"\"Electric circuits will be subjected to sudden changes which may be in the form of opening and closing of\n+switches or sudden changes in sources etc. Whenever such a change occurs, the circuit which was in a particular steady state condition will go to another steady state condition. Transient analysis is the analysis of the circuits during the time it changes from one steady state condition to another steady state condition\n+Source : https://webstor.srmist.edu.in/web_assets/srm_mainsite/files/2018/transient-analysis.pdf\n+\"\"\"\n+\n+from math import e,pow\n+\n+def transient_resp_RL(resistance:float, inductance:float, voltage:float, current:float, time:float) -> tuple:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `transient_resp_RL`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "electronics/transient_analysis.py",
    "pr_number": 9606,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1344251912,
    "comment_created_at": "2023-10-03T14:52:08Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+# -*- coding: utf-8 -*-\r\n+\"\"\"Project-1\r\n+\r\n+Automatically generated by Colaboratory.\r\n+\r\n+Original file is located at\r\n+    https://colab.research.google.com/drive/1uJlXbRnd1mLtAv8fp2wDPVeWG1QHKbUw\r\n+\r\n+Importing libraries\r\n+\"\"\"\r\n+\r\n+import numpy as np\r\n+import pandas as pd\r\n+from sklearn.model_selection import train_test_split\r\n+from sklearn.linear_model import LogisticRegression\r\n+from sklearn.metrics import accuracy_score\r\n+\r\n+\"\"\"Data collection + processing\"\"\"\r\n+\r\n+#loading the dataset to a panda df\r\n+sonar_data = pd.read_csv('/content/Copy of sonar data.csv' , header=None)\r\n+\r\n+sonar_data.head()\r\n+\r\n+# number of rows and column\r\n+sonar_data.shape\r\n+\r\n+sonar_data.describe()\r\n+\r\n+sonar_data[60].value_counts()\r\n+\r\n+\"\"\"M-->mine\r\n+\r\n+R-->rock\r\n+\"\"\"\r\n+\r\n+sonar_data.groupby(60).mean()\r\n+\r\n+#seperate 60th column\r\n+X=sonar_data.drop(columns=60,axis=1)##axis = 0 for rows\r\n+Y=sonar_data[60]\r\n+\r\n+print(X)\r\n+print(Y)\r\n+\r\n+\"\"\"Training and test data\"\"\"\r\n+\r\n+X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size = 0.1,stratify=Y,random_state=1)\r",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: x_train\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: x_test\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: y_train\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: y_test",
    "line_number": 48,
    "enriched": "File: rock_or_mine.py\nCode: @@ -0,0 +1,94 @@\n+# -*- coding: utf-8 -*-\r\n+\"\"\"Project-1\r\n+\r\n+Automatically generated by Colaboratory.\r\n+\r\n+Original file is located at\r\n+    https://colab.research.google.com/drive/1uJlXbRnd1mLtAv8fp2wDPVeWG1QHKbUw\r\n+\r\n+Importing libraries\r\n+\"\"\"\r\n+\r\n+import numpy as np\r\n+import pandas as pd\r\n+from sklearn.model_selection import train_test_split\r\n+from sklearn.linear_model import LogisticRegression\r\n+from sklearn.metrics import accuracy_score\r\n+\r\n+\"\"\"Data collection + processing\"\"\"\r\n+\r\n+#loading the dataset to a panda df\r\n+sonar_data = pd.read_csv('/content/Copy of sonar data.csv' , header=None)\r\n+\r\n+sonar_data.head()\r\n+\r\n+# number of rows and column\r\n+sonar_data.shape\r\n+\r\n+sonar_data.describe()\r\n+\r\n+sonar_data[60].value_counts()\r\n+\r\n+\"\"\"M-->mine\r\n+\r\n+R-->rock\r\n+\"\"\"\r\n+\r\n+sonar_data.groupby(60).mean()\r\n+\r\n+#seperate 60th column\r\n+X=sonar_data.drop(columns=60,axis=1)##axis = 0 for rows\r\n+Y=sonar_data[60]\r\n+\r\n+print(X)\r\n+print(Y)\r\n+\r\n+\"\"\"Training and test data\"\"\"\r\n+\r\n+X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size = 0.1,stratify=Y,random_state=1)\r\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `X_train`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `X_test`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `Y_train`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `Y_test`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "rock_or_mine.py",
    "pr_number": 7554,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002706351,
    "comment_created_at": "2022-10-23T12:56:26Z"
  },
  {
    "code": "@@ -0,0 +1,27 @@\n+\n+import numpy as np\n+from sklearn.model_selection import train_test_split\n+from sklearn.linear_model import LinearRegression\n+from sklearn.metrics import mean_squared_error\n+\n+# Generate some sample data\n+np.random.seed(0)\n+X = np.random.rand(100, 1)  # Feature (input)\n+y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)  # Target (output) with some noise\n+\n+# Split the data into training and testing sets\n+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: x_train\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: x_test",
    "line_number": 13,
    "enriched": "File: machine_learning/train a basic linear regression model.py\nCode: @@ -0,0 +1,27 @@\n+\n+import numpy as np\n+from sklearn.model_selection import train_test_split\n+from sklearn.linear_model import LinearRegression\n+from sklearn.metrics import mean_squared_error\n+\n+# Generate some sample data\n+np.random.seed(0)\n+X = np.random.rand(100, 1)  # Feature (input)\n+y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)  # Target (output) with some noise\n+\n+# Split the data into training and testing sets\n+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `X_train`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `X_test`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "machine_learning/train a basic linear regression model.py",
    "pr_number": 10098,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349678949,
    "comment_created_at": "2023-10-08T11:09:41Z"
  },
  {
    "code": "@@ -3,29 +3,57 @@\n \n class SegmentTree:\n     def __init__(self, a):\n-        self.N = len(a)\n+        self.A = a\n+        self.N = len(self.A)\n         self.st = [0] * (\n             4 * self.N\n         )  # approximate the overall size of segment tree with array N",
    "comment": "i gotta say that overall i am not a fan of these single variable names and uppercase variable names!!!\r\nthis looks like it was literally ported from some other language with minimal modifications.\r\n\r\na has no data type and no self-documenting name and the class version is uppercase.  if find it unreadable.",
    "line_number": 10,
    "enriched": "File: data_structures/binary_tree/segment_tree.py\nCode: @@ -3,29 +3,57 @@\n \n class SegmentTree:\n     def __init__(self, a):\n-        self.N = len(a)\n+        self.A = a\n+        self.N = len(self.A)\n         self.st = [0] * (\n             4 * self.N\n         )  # approximate the overall size of segment tree with array N\nComment: I gotta say that overall I am not a fan of these single variable names and uppercase variable names!!!\r\nThis looks like it was literally ported from some other language with minimal modifications.\r\n\r\n`a` has no data type and no self-documenting name and the class version is uppercase.  If find it unreadable.\r\n\r\n\r\n```suggestion\r\n        # approximate the overall size of the segment tree with array N\r\n        self.st = [0] * (4 * self.N)\r\n```",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "data_structures/binary_tree/segment_tree.py",
    "pr_number": 9975,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349563442,
    "comment_created_at": "2023-10-07T19:11:06Z"
  },
  {
    "code": "@@ -0,0 +1,51 @@\n+\"\"\"\n+author: Aayush Soni\n+Given n pairs of parentheses, write a function to generate all\n+combinations of well-formed parentheses.\n+Input: n = 2\n+Output: [\"(())\",\"()()\"]\n+Leetcode link: https://leetcode.com/problems/generate-parentheses/description/\n+\"\"\"\n+\n+\n+def generateParenthesis(n: int) -> list[str]:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: generateparenthesis\n\nplease provide descriptive name for the parameter: n",
    "line_number": 11,
    "enriched": "File: backtracking/generate_parentheses.py\nCode: @@ -0,0 +1,51 @@\n+\"\"\"\n+author: Aayush Soni\n+Given n pairs of parentheses, write a function to generate all\n+combinations of well-formed parentheses.\n+Input: n = 2\n+Output: [\"(())\",\"()()\"]\n+Leetcode link: https://leetcode.com/problems/generate-parentheses/description/\n+\"\"\"\n+\n+\n+def generateParenthesis(n: int) -> list[str]:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `generateParenthesis`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "backtracking/generate_parentheses.py",
    "pr_number": 10903,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1370253928,
    "comment_created_at": "2023-10-24T14:13:37Z"
  },
  {
    "code": "@@ -0,0 +1,85 @@\n+#!/usr/bin/python3            \n+#To find the python interpreter, when we execute it without python3\n+\n+\"\"\"\n+To use:\n+    ./codechef_rank.py <contest-name>\n+\"\"\"\n+\n+import requests\n+import json\n+import sys                                      #To get the passed arguments in terminal\n+\n+id= '<codechef_id>'                              #Replace with your codechef id\n+\n+\n+\n+main_url= \"https://www.codechef.com/api/rankings/\"\n+contest_name= sys.argv[1]\n+# contest_name= input(\"Enter Contest Name: \")\n+# contest_name=\"SEP221C\"\n+# print(contest_name)\n+\n+url= main_url+contest_name\n+\n+dict={}\n+headers = {\n+    'authority': 'www.codechef.com',\n+    'accept': 'application/json, text/plain, */*',\n+    'accept-language': 'en-US,en;q=0.9',\n+    # Requests sorts cookies= alphabetically\n+    'cookie': '_fbp=fb.1.1655306318354.66392898; pg_mm2_cookie_a=85654c31-1e1e-48ec-9f8b-0e71d258436a; twk_uuid_627c0f75b0d10b6f3e71c35d=%7B%22uuid%22%3A%221.H3NO5nd4jOeziibZm2mRXYSI3644LExvXsLy2XAJpeX6oKXKytIrDYkyJ5vtWMPxtdPxOmdp3K4GQf06bf07sXMb0lg4y6VSxlwRr20XGemYXnB479arAdNf5nK9agSM1Md8X1QD9MJkd5ss%22%2C%22version%22%3A3%2C%22domain%22%3A%22codechef.com%22%2C%22ts%22%3A1658326231790%7D; twk_uuid_62397c18a34c2456412c3b26=%7B%22uuid%22%3A%221.H3NMbQzCA5paUeWeHE3S0JdRNNp3WPDuQiFonJ8fJcBrCBAVhTuKUuCbnUKDAgn6QnnQCk4SSwVYleQBJh3z5Upe5NDi9N1RJ0vwflOcLcDwPvkQv50vgNeCcKHWYawcCZU7Rm3m7IMpr6Jl%22%2C%22version%22%3A3%2C%22domain%22%3A%22codechef.com%22%2C%22ts%22%3A1658577496450%7D; _gcl_au=1.1.2141587503.1663165531; FCNEC=%5B%5B%22AKsRol8OKyH-AI_Th8_GaKuT9ARaiyCq5e15MDMmxkCXSOaOOQc2UAg2I7F5xFYcIEao5-R20KCVuy_ekbbJy9ZIXib62nHbQUKxH9JgmNqqvIcNRLtM4hizhC_y2xN4sebGbvmvaBJ_J0P4e_paYSlHUuZgbAYYKg%3D%3D%22%5D%2Cnull%2C%5B%5D%5D; __gads=ID=f702a7a3ed66e141-22854ff149d70012:T=1655306346:RT=1663955229:S=ALNI_MbdnlQ1SgroV8Of5xn4KSpxKrSbKQ; SESS93b6022d778ee317bf48f7dbffe03173=db040399023abeee64d5a77e3d26f067; uid=3137718; __gpi=UID=000006ad96536525:T=1655306346:RT=1666190266:S=ALNI_MYP3iUpXkfMGriNtcMwBbc3u6ExUw; _gid=GA1.2.808286290.1666497045; _clck=17nqn5c|1|f5y|0; _gat_UA-141612136-1=1; _ga_C8RQQ7NY18=GS1.1.1666536681.133.1.1666537079.0.0.0; _ga=GA1.2.306524194.1655306320; _clsk=1qfm2v3|1666537080268|6|0|i.clarity.ms/collect',\n+    'referer': 'https://www.codechef.com/rankings/START43d?filterBy=Institution%3DMaharaja%20Agrasen%20Institute%20of%20Technology%2C%20New%20Delhi&itemsPerPage=100&order=asc&page=1&sortBy=rank',\n+    'sec-ch-ua': '\"Chromium\";v=\"106\", \"Google Chrome\";v=\"106\", \"Not;A=Brand\";v=\"99\"',\n+    'sec-ch-ua-mobile': '?1',\n+    'sec-ch-ua-platform': '\"Android\"',\n+    'sec-fetch-dest': 'empty',\n+    'sec-fetch-mode': 'cors',\n+    'sec-fetch-site': 'same-origin',\n+    'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Mobile Safari/537.36',\n+    'x-csrf-token': '9032eae6fad31fa3844a43d624be7d73fa8304fd3920bea6a5c177a8e25721be',\n+    'x-requested-with': 'XMLHttpRequest',\n+}\n+\n+params = {\n+    'filterBy': 'Institution=Maharaja Agrasen Institute of Technology, New Delhi',\n+    'itemsPerPage': '100',\n+    'order': 'asc',\n+    'page': '1',\n+    'sortBy': 'rank',\n+}\n+\n+# print(url)\n+r = requests.get(url, params=params, headers=headers).json()\n+# print(r)\n+users= r['list']\n+c=1;\n+for i in users:\n+  v= []\n+  rankVar= i['rank']",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: rankvar",
    "line_number": 59,
    "enriched": "File: web_programming/CodechefRank.py\nCode: @@ -0,0 +1,85 @@\n+#!/usr/bin/python3            \n+#To find the python interpreter, when we execute it without python3\n+\n+\"\"\"\n+To use:\n+    ./codechef_rank.py <contest-name>\n+\"\"\"\n+\n+import requests\n+import json\n+import sys                                      #To get the passed arguments in terminal\n+\n+id= '<codechef_id>'                              #Replace with your codechef id\n+\n+\n+\n+main_url= \"https://www.codechef.com/api/rankings/\"\n+contest_name= sys.argv[1]\n+# contest_name= input(\"Enter Contest Name: \")\n+# contest_name=\"SEP221C\"\n+# print(contest_name)\n+\n+url= main_url+contest_name\n+\n+dict={}\n+headers = {\n+    'authority': 'www.codechef.com',\n+    'accept': 'application/json, text/plain, */*',\n+    'accept-language': 'en-US,en;q=0.9',\n+    # Requests sorts cookies= alphabetically\n+    'cookie': '_fbp=fb.1.1655306318354.66392898; pg_mm2_cookie_a=85654c31-1e1e-48ec-9f8b-0e71d258436a; twk_uuid_627c0f75b0d10b6f3e71c35d=%7B%22uuid%22%3A%221.H3NO5nd4jOeziibZm2mRXYSI3644LExvXsLy2XAJpeX6oKXKytIrDYkyJ5vtWMPxtdPxOmdp3K4GQf06bf07sXMb0lg4y6VSxlwRr20XGemYXnB479arAdNf5nK9agSM1Md8X1QD9MJkd5ss%22%2C%22version%22%3A3%2C%22domain%22%3A%22codechef.com%22%2C%22ts%22%3A1658326231790%7D; twk_uuid_62397c18a34c2456412c3b26=%7B%22uuid%22%3A%221.H3NMbQzCA5paUeWeHE3S0JdRNNp3WPDuQiFonJ8fJcBrCBAVhTuKUuCbnUKDAgn6QnnQCk4SSwVYleQBJh3z5Upe5NDi9N1RJ0vwflOcLcDwPvkQv50vgNeCcKHWYawcCZU7Rm3m7IMpr6Jl%22%2C%22version%22%3A3%2C%22domain%22%3A%22codechef.com%22%2C%22ts%22%3A1658577496450%7D; _gcl_au=1.1.2141587503.1663165531; FCNEC=%5B%5B%22AKsRol8OKyH-AI_Th8_GaKuT9ARaiyCq5e15MDMmxkCXSOaOOQc2UAg2I7F5xFYcIEao5-R20KCVuy_ekbbJy9ZIXib62nHbQUKxH9JgmNqqvIcNRLtM4hizhC_y2xN4sebGbvmvaBJ_J0P4e_paYSlHUuZgbAYYKg%3D%3D%22%5D%2Cnull%2C%5B%5D%5D; __gads=ID=f702a7a3ed66e141-22854ff149d70012:T=1655306346:RT=1663955229:S=ALNI_MbdnlQ1SgroV8Of5xn4KSpxKrSbKQ; SESS93b6022d778ee317bf48f7dbffe03173=db040399023abeee64d5a77e3d26f067; uid=3137718; __gpi=UID=000006ad96536525:T=1655306346:RT=1666190266:S=ALNI_MYP3iUpXkfMGriNtcMwBbc3u6ExUw; _gid=GA1.2.808286290.1666497045; _clck=17nqn5c|1|f5y|0; _gat_UA-141612136-1=1; _ga_C8RQQ7NY18=GS1.1.1666536681.133.1.1666537079.0.0.0; _ga=GA1.2.306524194.1655306320; _clsk=1qfm2v3|1666537080268|6|0|i.clarity.ms/collect',\n+    'referer': 'https://www.codechef.com/rankings/START43d?filterBy=Institution%3DMaharaja%20Agrasen%20Institute%20of%20Technology%2C%20New%20Delhi&itemsPerPage=100&order=asc&page=1&sortBy=rank',\n+    'sec-ch-ua': '\"Chromium\";v=\"106\", \"Google Chrome\";v=\"106\", \"Not;A=Brand\";v=\"99\"',\n+    'sec-ch-ua-mobile': '?1',\n+    'sec-ch-ua-platform': '\"Android\"',\n+    'sec-fetch-dest': 'empty',\n+    'sec-fetch-mode': 'cors',\n+    'sec-fetch-site': 'same-origin',\n+    'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Mobile Safari/537.36',\n+    'x-csrf-token': '9032eae6fad31fa3844a43d624be7d73fa8304fd3920bea6a5c177a8e25721be',\n+    'x-requested-with': 'XMLHttpRequest',\n+}\n+\n+params = {\n+    'filterBy': 'Institution=Maharaja Agrasen Institute of Technology, New Delhi',\n+    'itemsPerPage': '100',\n+    'order': 'asc',\n+    'page': '1',\n+    'sortBy': 'rank',\n+}\n+\n+# print(url)\n+r = requests.get(url, params=params, headers=headers).json()\n+# print(r)\n+users= r['list']\n+c=1;\n+for i in users:\n+  v= []\n+  rankVar= i['rank']\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `rankVar`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "web_programming/CodechefRank.py",
    "pr_number": 7568,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002728366,
    "comment_created_at": "2022-10-23T15:31:18Z"
  },
  {
    "code": "@@ -0,0 +1,32 @@\n+\"\"\"\n+Dynamic Programming to solve Brust Balloons.py\n+Tabulation\n+\"\"\"\n+def maxCoins(self, nums: List[int]) -> int:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: maxcoins",
    "line_number": 5,
    "enriched": "File: dynamic_programming/brust_balloons.py\nCode: @@ -0,0 +1,32 @@\n+\"\"\"\n+Dynamic Programming to solve Brust Balloons.py\n+Tabulation\n+\"\"\"\n+def maxCoins(self, nums: List[int]) -> int:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `maxCoins`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "dynamic_programming/brust_balloons.py",
    "pr_number": 6904,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 991104061,
    "comment_created_at": "2022-10-10T09:48:10Z"
  },
  {
    "code": "@@ -0,0 +1,332 @@\n+import math\n+import doctest\n+\n+\n+def number_theoretic_transform(\n+    polynomial_coeffs: list, prime_modulus: int, primitive_root: int\n+) -> list:\n+    \"\"\"\n+    Compute the number-theoretic transform of a polynomial.\n+\n+    Args:\n+    - polynomial_coeffs: List of integers representing polynomial coefficients.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+    - primitive_root: Primitive root of the prime modulus.\n+\n+    Returns:\n+    - List of integers representing the polynomial in point-value form.\n+\n+    Doctest:\n+    >>> number_theoretic_transform([1, 2, 3, 4], 998244353, 3)\n+    [10, 173167434, 998244351, 825076915]\n+    >>> number_theoretic_transform([4, 3, 2, 1], 998244353, 3)\n+    [10, 825076919, 2, 173167438]\n+    \"\"\"\n+    n = len(polynomial_coeffs)\n+    assert (prime_modulus - 1) % n == 0\n+    w = pow(primitive_root, (prime_modulus - 1) // n, prime_modulus)\n+    return recursive_ntt(polynomial_coeffs, w, prime_modulus)\n+\n+\n+def recursive_ntt_transform(\n+    polynomial_coeffs_recursive: list, root_of_unity: int, prime_modulus: int\n+) -> list:\n+    \"\"\"\n+    Recursive function for number-theoretic transform.\n+\n+    Args:\n+    - polynomial_coeffs_recursive: List of integers representing polynomial coefficients during recursive steps.\n+    - root_of_unity: Primitive nth root of unity modulo the prime modulus.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+\n+    Returns:\n+    - List of integers representing the polynomial in point-value form.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> transformed = number_theoretic_transform(a, 998244353, 3)\n+    >>> recursive_ntt_transform(a, 3, 998244353)\n+    [10, 998244345, 998244351, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> recursive_ntt_transform(b, 3, 998244353)\n+    [10, 8, 2, 998244349]\n+    \"\"\"\n+    n = len(polynomial_coeffs_recursive)\n+    if n == 1:\n+        return polynomial_coeffs_recursive\n+    a0 = polynomial_coeffs_recursive[::2]\n+    a1 = polynomial_coeffs_recursive[1::2]\n+    f0 = recursive_ntt_transform(a0, root_of_unity**2 % prime_modulus, prime_modulus)\n+    f1 = recursive_ntt_transform(a1, root_of_unity**2 % prime_modulus, prime_modulus)\n+    x = 1\n+    f = [0] * n\n+    for i in range(n // 2):\n+        f[i] = (f0[i] + x * f1[i]) % prime_modulus\n+        f[i + n // 2] = (f0[i] - x * f1[i]) % prime_modulus\n+        x = x * root_of_unity % prime_modulus\n+    return f\n+\n+\n+def inverse_ntt(point_values: list, prime_modulus: int, primitive_root: int) -> list:\n+    \"\"\"\n+    Compute the inverse number-theoretic transform of a polynomial.\n+\n+    Args:\n+    - point_values: List of integers representing polynomial in point-value form.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+    - primitive_root: Primitive root of the prime modulus.\n+\n+    Returns:\n+    - List of integers representing polynomial coefficients.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> transformed = number_theoretic_transform(a, 998244353, 3)\n+    >>> inverse_ntt(transformed, 998244353, 3)\n+    [1, 2, 3, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> inverse_ntt(transformed_b, 998244353, 3)\n+    [4, 3, 2, 1]\n+    \"\"\"\n+    n = len(point_values)\n+    assert (prime_modulus - 1) % n == 0\n+    w = pow(primitive_root, (prime_modulus - 1) // n, prime_modulus)\n+    w_inv = pow(w, prime_modulus - 2, prime_modulus)\n+    return [\n+        x * pow(n, prime_modulus - 2, prime_modulus) % prime_modulus\n+        for x in recursive_inverse_ntt(point_values, w_inv, prime_modulus)\n+    ]\n+\n+\n+def recursive_inverse_transform(\n+    point_values_recursive: list, root_of_unity: int, prime_modulus: int\n+) -> list:\n+    \"\"\"\n+    Recursive function for inverse number-theoretic transform.\n+\n+    Args:\n+    - point_values_recursive: List of integers representing polynomial in point-value form during recursive steps.\n+    - root_of_unity: Primitive nth root of unity modulo the prime modulus.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+\n+    Returns:\n+    - List of integers representing polynomial coefficients.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> transformed = number_theoretic_transform(a, 998244353, 3)\n+    >>> w = pow(3, (998244353 - 1) // 4, 998244353)\n+    >>> recursive_inverse_transform(transformed, w, 998244353)\n+    [1, 2, 3, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> w_b = pow(3, (998244353 - 1) // 4, 998244353)\n+    >>> recursive_inverse_transform(transformed_b, w_b, 998244353)\n+    [4, 3, 2, 1]\n+    \"\"\"\n+    n = len(point_values_recursive)\n+    if n == 1:\n+        return point_values_recursive\n+    f0 = point_values_recursive[::2]\n+    f1 = point_values_recursive[1::2]\n+    a0 = recursive_inverse_transform(\n+        f0, root_of_unity**2 % prime_modulus, prime_modulus\n+    )\n+    a1 = recursive_inverse_transform(\n+        f1, root_of_unity**2 % prime_modulus, prime_modulus\n+    )\n+    x = 1\n+    a = [0] * n\n+    for i in range(n // 2):\n+        a[i] = (a0[i] + x * a1[i]) % prime_modulus\n+        a[i + n // 2] = (a0[i] - x * a1[i]) % prime_modulus\n+        x = x * root_of_unity % prime_modulus\n+    return a\n+\n+\n+# Helper function to compute the convolution of two polynomials in point-value form\n+def convolve_polynomials(\n+    a_point_values: list, b_point_values: list, prime_modulus: int\n+) -> list:\n+    \"\"\"\n+    Compute the convolution of two polynomials in point-value form.\n+\n+    Args:\n+    - a_point_values: List of integers representing polynomial A in point-value form.\n+    - b_point_values: List of integers representing polynomial B in point-value form.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+\n+    Returns:\n+    - List of integers representing the convolution of A and B in point-value form.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_a = number_theoretic_transform(a, 998244353, 3)\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> convolve_polynomials(transformed_a, transformed_b, 998244353)\n+    [4, 11, 20, 30, 20, 11, 4]\n+    \"\"\"\n+    return [(a * b) % prime_modulus for a, b in zip(a_point_values, b_point_values)]\n+\n+\n+def multiply_numbers(x: int, y: int, p: int, g: int) -> int:",
    "comment": "please provide descriptive name for the parameter: x\n\nplease provide descriptive name for the parameter: y\n\nplease provide descriptive name for the parameter: p\n\nplease provide descriptive name for the parameter: g",
    "line_number": 175,
    "enriched": "File: arithmetic_analysis/sch\u00f6nhage_strassen.py\nCode: @@ -0,0 +1,332 @@\n+import math\n+import doctest\n+\n+\n+def number_theoretic_transform(\n+    polynomial_coeffs: list, prime_modulus: int, primitive_root: int\n+) -> list:\n+    \"\"\"\n+    Compute the number-theoretic transform of a polynomial.\n+\n+    Args:\n+    - polynomial_coeffs: List of integers representing polynomial coefficients.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+    - primitive_root: Primitive root of the prime modulus.\n+\n+    Returns:\n+    - List of integers representing the polynomial in point-value form.\n+\n+    Doctest:\n+    >>> number_theoretic_transform([1, 2, 3, 4], 998244353, 3)\n+    [10, 173167434, 998244351, 825076915]\n+    >>> number_theoretic_transform([4, 3, 2, 1], 998244353, 3)\n+    [10, 825076919, 2, 173167438]\n+    \"\"\"\n+    n = len(polynomial_coeffs)\n+    assert (prime_modulus - 1) % n == 0\n+    w = pow(primitive_root, (prime_modulus - 1) // n, prime_modulus)\n+    return recursive_ntt(polynomial_coeffs, w, prime_modulus)\n+\n+\n+def recursive_ntt_transform(\n+    polynomial_coeffs_recursive: list, root_of_unity: int, prime_modulus: int\n+) -> list:\n+    \"\"\"\n+    Recursive function for number-theoretic transform.\n+\n+    Args:\n+    - polynomial_coeffs_recursive: List of integers representing polynomial coefficients during recursive steps.\n+    - root_of_unity: Primitive nth root of unity modulo the prime modulus.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+\n+    Returns:\n+    - List of integers representing the polynomial in point-value form.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> transformed = number_theoretic_transform(a, 998244353, 3)\n+    >>> recursive_ntt_transform(a, 3, 998244353)\n+    [10, 998244345, 998244351, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> recursive_ntt_transform(b, 3, 998244353)\n+    [10, 8, 2, 998244349]\n+    \"\"\"\n+    n = len(polynomial_coeffs_recursive)\n+    if n == 1:\n+        return polynomial_coeffs_recursive\n+    a0 = polynomial_coeffs_recursive[::2]\n+    a1 = polynomial_coeffs_recursive[1::2]\n+    f0 = recursive_ntt_transform(a0, root_of_unity**2 % prime_modulus, prime_modulus)\n+    f1 = recursive_ntt_transform(a1, root_of_unity**2 % prime_modulus, prime_modulus)\n+    x = 1\n+    f = [0] * n\n+    for i in range(n // 2):\n+        f[i] = (f0[i] + x * f1[i]) % prime_modulus\n+        f[i + n // 2] = (f0[i] - x * f1[i]) % prime_modulus\n+        x = x * root_of_unity % prime_modulus\n+    return f\n+\n+\n+def inverse_ntt(point_values: list, prime_modulus: int, primitive_root: int) -> list:\n+    \"\"\"\n+    Compute the inverse number-theoretic transform of a polynomial.\n+\n+    Args:\n+    - point_values: List of integers representing polynomial in point-value form.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+    - primitive_root: Primitive root of the prime modulus.\n+\n+    Returns:\n+    - List of integers representing polynomial coefficients.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> transformed = number_theoretic_transform(a, 998244353, 3)\n+    >>> inverse_ntt(transformed, 998244353, 3)\n+    [1, 2, 3, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> inverse_ntt(transformed_b, 998244353, 3)\n+    [4, 3, 2, 1]\n+    \"\"\"\n+    n = len(point_values)\n+    assert (prime_modulus - 1) % n == 0\n+    w = pow(primitive_root, (prime_modulus - 1) // n, prime_modulus)\n+    w_inv = pow(w, prime_modulus - 2, prime_modulus)\n+    return [\n+        x * pow(n, prime_modulus - 2, prime_modulus) % prime_modulus\n+        for x in recursive_inverse_ntt(point_values, w_inv, prime_modulus)\n+    ]\n+\n+\n+def recursive_inverse_transform(\n+    point_values_recursive: list, root_of_unity: int, prime_modulus: int\n+) -> list:\n+    \"\"\"\n+    Recursive function for inverse number-theoretic transform.\n+\n+    Args:\n+    - point_values_recursive: List of integers representing polynomial in point-value form during recursive steps.\n+    - root_of_unity: Primitive nth root of unity modulo the prime modulus.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+\n+    Returns:\n+    - List of integers representing polynomial coefficients.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> transformed = number_theoretic_transform(a, 998244353, 3)\n+    >>> w = pow(3, (998244353 - 1) // 4, 998244353)\n+    >>> recursive_inverse_transform(transformed, w, 998244353)\n+    [1, 2, 3, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> w_b = pow(3, (998244353 - 1) // 4, 998244353)\n+    >>> recursive_inverse_transform(transformed_b, w_b, 998244353)\n+    [4, 3, 2, 1]\n+    \"\"\"\n+    n = len(point_values_recursive)\n+    if n == 1:\n+        return point_values_recursive\n+    f0 = point_values_recursive[::2]\n+    f1 = point_values_recursive[1::2]\n+    a0 = recursive_inverse_transform(\n+        f0, root_of_unity**2 % prime_modulus, prime_modulus\n+    )\n+    a1 = recursive_inverse_transform(\n+        f1, root_of_unity**2 % prime_modulus, prime_modulus\n+    )\n+    x = 1\n+    a = [0] * n\n+    for i in range(n // 2):\n+        a[i] = (a0[i] + x * a1[i]) % prime_modulus\n+        a[i + n // 2] = (a0[i] - x * a1[i]) % prime_modulus\n+        x = x * root_of_unity % prime_modulus\n+    return a\n+\n+\n+# Helper function to compute the convolution of two polynomials in point-value form\n+def convolve_polynomials(\n+    a_point_values: list, b_point_values: list, prime_modulus: int\n+) -> list:\n+    \"\"\"\n+    Compute the convolution of two polynomials in point-value form.\n+\n+    Args:\n+    - a_point_values: List of integers representing polynomial A in point-value form.\n+    - b_point_values: List of integers representing polynomial B in point-value form.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+\n+    Returns:\n+    - List of integers representing the convolution of A and B in point-value form.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_a = number_theoretic_transform(a, 998244353, 3)\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> convolve_polynomials(transformed_a, transformed_b, 998244353)\n+    [4, 11, 20, 30, 20, 11, 4]\n+    \"\"\"\n+    return [(a * b) % prime_modulus for a, b in zip(a_point_values, b_point_values)]\n+\n+\n+def multiply_numbers(x: int, y: int, p: int, g: int) -> int:\nComment: Please provide descriptive name for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `y`\n\nPlease provide descriptive name for the parameter: `p`\n\nPlease provide descriptive name for the parameter: `g`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "arithmetic_analysis/sch\u00f6nhage_strassen.py",
    "pr_number": 9554,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1343478095,
    "comment_created_at": "2023-10-03T04:36:45Z"
  },
  {
    "code": "@@ -0,0 +1,55 @@\n+\"\"\"\n+Hinge Loss\n+\n+Description:\n+Compute the Hinge loss used for training SVM (Support Vector Machine).\n+\n+Formula:\n+loss = max(0, 1 - true * pred)\n+\n+Reference: https://en.wikipedia.org/wiki/Hinge_loss\n+\n+Author: Poojan Smart\n+Email: smrtpoojan@gmail.com\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def hinge_loss(y_true: np.ndarray, pred: np.ndarray) -> float:",
    "comment": "please rename for consistency",
    "line_number": 19,
    "enriched": "File: machine_learning/loss_functions/hinge_loss.py\nCode: @@ -0,0 +1,55 @@\n+\"\"\"\n+Hinge Loss\n+\n+Description:\n+Compute the Hinge loss used for training SVM (Support Vector Machine).\n+\n+Formula:\n+loss = max(0, 1 - true * pred)\n+\n+Reference: https://en.wikipedia.org/wiki/Hinge_loss\n+\n+Author: Poojan Smart\n+Email: smrtpoojan@gmail.com\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def hinge_loss(y_true: np.ndarray, pred: np.ndarray) -> float:\nComment: ```suggestion\r\ndef hinge_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\r\n```\r\nPlease rename for consistency",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "machine_learning/loss_functions/hinge_loss.py",
    "pr_number": 10628,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1361577523,
    "comment_created_at": "2023-10-17T06:40:41Z"
  },
  {
    "code": "@@ -0,0 +1,82 @@\n+import cv2\n+import matplotlib.pyplot as plt\n+\n+config_file = \"ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt\"\n+frozen_model = \"frozen_inference_graph.pb\"\n+model = cv2.dnn_DetectionModel(frozen_model, config_file)\n+\n+classLabels = []",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: classlabels",
    "line_number": 8,
    "enriched": "File: Object Detection/ObjectDection.py\nCode: @@ -0,0 +1,82 @@\n+import cv2\n+import matplotlib.pyplot as plt\n+\n+config_file = \"ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt\"\n+frozen_model = \"frozen_inference_graph.pb\"\n+model = cv2.dnn_DetectionModel(frozen_model, config_file)\n+\n+classLabels = []\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `classLabels`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "Object Detection/ObjectDection.py",
    "pr_number": 11676,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1784355394,
    "comment_created_at": "2024-10-02T11:47:52Z"
  },
  {
    "code": "@@ -0,0 +1,29 @@\n+\"\"\"\n+# Definition for a Node.\n+class Node:\n+    def __init__(self, val = 0, neighbors = None):\n+        self.val = val\n+        self.neighbors = neighbors if neighbors is not None else []\n+\"\"\"\n+# Input: adjList = [[2,4],[1,3],[2,4],[1,3]]\n+# Output: [[2,4],[1,3],[2,4],[1,3]]\n+\n+class Solution:\n+    def cloneGraph(self, node: 'Node') -> 'Node':",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: clonegraph\n\nas there is no test file in this pull request nor any test function or class in the file graphs/clone_graph.py, please provide doctest for the function clonegraph",
    "line_number": 12,
    "enriched": "File: graphs/clone_graph.py\nCode: @@ -0,0 +1,29 @@\n+\"\"\"\n+# Definition for a Node.\n+class Node:\n+    def __init__(self, val = 0, neighbors = None):\n+        self.val = val\n+        self.neighbors = neighbors if neighbors is not None else []\n+\"\"\"\n+# Input: adjList = [[2,4],[1,3],[2,4],[1,3]]\n+# Output: [[2,4],[1,3],[2,4],[1,3]]\n+\n+class Solution:\n+    def cloneGraph(self, node: 'Node') -> 'Node':\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `cloneGraph`\n\nAs there is no test file in this pull request nor any test function or class in the file `graphs/clone_graph.py`, please provide doctest for the function `cloneGraph`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "graphs/clone_graph.py",
    "pr_number": 7611,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004003931,
    "comment_created_at": "2022-10-25T05:11:45Z"
  },
  {
    "code": "@@ -0,0 +1,129 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 7, 2022\n+\n+Task:\n+You are given a tree root of a binary tree with n nodes, where each node has\n+node.data coins. There are exactly n coins in whole tree. \n+\n+In one move, we may choose two adjacent nodes and move one coin from one node\n+to another. A move may be from parent to child, or from child to parent.\n+\n+Return the minimum number of moves required to make every node have exactly one coin.\n+\n+Example 1:\n+\n+   3\n+  / \\\n+ 0   0\n+\n+Result: 2\n+\n+Example 2:\n+\n+   0\n+  / \\\n+ 3   0\n+\n+Result 3\n+\n+leetcode: https://leetcode.com/problems/distribute-coins-in-binary-tree/\n+\n+Implementation notes:\n+User depth-first search approach.\n+\n+Let n is the number of nodes in tree\n+Runtime: O(n)\n+Space: O(1)\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+from collections import namedtuple\n+\n+\n+@dataclass\n+class TreeNode:\n+    data: float\n+    left: TreeNode | None = None\n+    right: TreeNode | None = None\n+\n+CoinsDistribResult = namedtuple(\"CoinsDistribResult\", 'moves excess')\n+\n+def distributeCoins(root: TreeNode | None) -> int:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: distributecoins",
    "line_number": 54,
    "enriched": "File: data_structures/binary_tree/distribute_coins.py\nCode: @@ -0,0 +1,129 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 7, 2022\n+\n+Task:\n+You are given a tree root of a binary tree with n nodes, where each node has\n+node.data coins. There are exactly n coins in whole tree. \n+\n+In one move, we may choose two adjacent nodes and move one coin from one node\n+to another. A move may be from parent to child, or from child to parent.\n+\n+Return the minimum number of moves required to make every node have exactly one coin.\n+\n+Example 1:\n+\n+   3\n+  / \\\n+ 0   0\n+\n+Result: 2\n+\n+Example 2:\n+\n+   0\n+  / \\\n+ 3   0\n+\n+Result 3\n+\n+leetcode: https://leetcode.com/problems/distribute-coins-in-binary-tree/\n+\n+Implementation notes:\n+User depth-first search approach.\n+\n+Let n is the number of nodes in tree\n+Runtime: O(n)\n+Space: O(1)\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+from collections import namedtuple\n+\n+\n+@dataclass\n+class TreeNode:\n+    data: float\n+    left: TreeNode | None = None\n+    right: TreeNode | None = None\n+\n+CoinsDistribResult = namedtuple(\"CoinsDistribResult\", 'moves excess')\n+\n+def distributeCoins(root: TreeNode | None) -> int:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `distributeCoins`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "data_structures/binary_tree/distribute_coins.py",
    "pr_number": 7975,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1015048256,
    "comment_created_at": "2022-11-07T06:40:03Z"
  },
  {
    "code": "@@ -0,0 +1,201 @@\n+#!/usr/bin/env python\n+# coding: utf-8\n+\n+# In[5]:\n+\n+\n+import pandas as pd\n+from category_encoders import OrdinalEncoder\n+from lightgbm import LGBMRegressor\n+from sklearn.model_selection import train_test_split\n+from sklearn.ensemble import ExtraTreesRegressor\n+\n+\n+# # Building Supervized Model\n+\n+# In[10]:\n+\n+\n+from shapash.data.data_loader import data_loading\n+house_df, house_dict = data_loading('house_prices')\n+\n+\n+# In[11]:\n+\n+\n+y_df=house_df['SalePrice'].to_frame()\n+X_df=house_df[house_df.columns.difference(['SalePrice'])]",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: x_df",
    "line_number": 27,
    "enriched": "File: machine_learning/shapash.py\nCode: @@ -0,0 +1,201 @@\n+#!/usr/bin/env python\n+# coding: utf-8\n+\n+# In[5]:\n+\n+\n+import pandas as pd\n+from category_encoders import OrdinalEncoder\n+from lightgbm import LGBMRegressor\n+from sklearn.model_selection import train_test_split\n+from sklearn.ensemble import ExtraTreesRegressor\n+\n+\n+# # Building Supervized Model\n+\n+# In[10]:\n+\n+\n+from shapash.data.data_loader import data_loading\n+house_df, house_dict = data_loading('house_prices')\n+\n+\n+# In[11]:\n+\n+\n+y_df=house_df['SalePrice'].to_frame()\n+X_df=house_df[house_df.columns.difference(['SalePrice'])]\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `X_df`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "machine_learning/shapash.py",
    "pr_number": 7351,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996620543,
    "comment_created_at": "2022-10-17T05:17:07Z"
  },
  {
    "code": "@@ -0,0 +1,31 @@\n+from collections.abc import Sequence\n+\n+\n+def max_subsequence_sum(nums: Sequence[int]) -> int:\n+    \"\"\"Return the maximum possible sum amongst all non - empty subsequences.\n+\n+    Raises:\n+      ValueError: when nums is empty.\n+\n+    >>> max_subsequence_sum([1,2,3,4,-2])\n+    10\n+    >>> max_subsequence_sum([-2, -3, -1, -4, -6])\n+    -1\n+    \"\"\"\n+    if not nums:\n+        raise ValueError(\"Input sequence should not be empty\")\n+\n+    ans = nums[0]\n+    nums_len = len(nums)\n+\n+    for i in range(1, nums_len):",
    "comment": "you only use num_lens once, so this variable isn't partially necessary",
    "line_number": 21,
    "enriched": "File: other/maximum_subsequence.py\nCode: @@ -0,0 +1,31 @@\n+from collections.abc import Sequence\n+\n+\n+def max_subsequence_sum(nums: Sequence[int]) -> int:\n+    \"\"\"Return the maximum possible sum amongst all non - empty subsequences.\n+\n+    Raises:\n+      ValueError: when nums is empty.\n+\n+    >>> max_subsequence_sum([1,2,3,4,-2])\n+    10\n+    >>> max_subsequence_sum([-2, -3, -1, -4, -6])\n+    -1\n+    \"\"\"\n+    if not nums:\n+        raise ValueError(\"Input sequence should not be empty\")\n+\n+    ans = nums[0]\n+    nums_len = len(nums)\n+\n+    for i in range(1, nums_len):\nComment: ```suggestion\r\n    for i in range(1, len(nums)):\r\n```\r\nYou only use `num_lens` once, so this variable isn't partially necessary",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "other/maximum_subsequence.py",
    "pr_number": 7811,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008767920,
    "comment_created_at": "2022-10-29T23:09:09Z"
  },
  {
    "code": "@@ -0,0 +1,38 @@\n+import cv2\n+import mediapipe as mp\n+import time\n+\n+cap = cv2.VideoCapture(0)\n+\n+mpHands = mp.solutions.hands",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: mphands",
    "line_number": 7,
    "enriched": "File: computer_vision/handtracking.py\nCode: @@ -0,0 +1,38 @@\n+import cv2\n+import mediapipe as mp\n+import time\n+\n+cap = cv2.VideoCapture(0)\n+\n+mpHands = mp.solutions.hands\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `mpHands`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "computer_vision/handtracking.py",
    "pr_number": 10131,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349745277,
    "comment_created_at": "2023-10-08T16:48:29Z"
  },
  {
    "code": "@@ -0,0 +1,47 @@\n+import sys\n+\n+\n+def array_equalization(vector: list[int], k: int) -> int:",
    "comment": "self-documenting variable names, please.",
    "line_number": 4,
    "enriched": "File: matrix/matrix_equalization.py\nCode: @@ -0,0 +1,47 @@\n+import sys\n+\n+\n+def array_equalization(vector: list[int], k: int) -> int:\nComment: Self-documenting variable names, please.\r\n```suggestion\r\ndef array_equalization(vector: list[int], step_size: int) -> int:\r\n```",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "matrix/matrix_equalization.py",
    "pr_number": 11360,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1578122014,
    "comment_created_at": "2024-04-24T15:45:15Z"
  },
  {
    "code": "@@ -1,50 +1,97 @@\n \"\"\"\n Numerical integration or quadrature for a smooth function f with known values at x_i\n \n-This method is the classical approach of suming 'Equally Spaced Abscissas'\n+This method is the classical approach of summing 'Equally Spaced Abscissas'\n+\"\"\"\n \n-method 1:\n-\"extended trapezoidal rule\"\n+from collections.abc import Callable, Iterator\n \n-\"\"\"\n \n+def trapezoidal_rule(\n+    f: Callable[[float], float], boundary: list[float], steps: int\n+) -> float:\n+    \"\"\"\n+    \"extended trapezoidal rule\"\n+    int(f) = dx/2 * (f1 + 2f2 + ... + fn)\n+\n+    >>> def func(x): return x ** 2\n+    >>> abs(trapezoidal_rule(func, [0, 1], 10) - 0.335) < 1e-9\n+    True\n+\n+    >>> def func(x): return 1\n+    >>> abs(trapezoidal_rule(func, [0, 10], 100) - 10.0) < 1e-9\n+    True\n+\n+    >>> def func(x): return x\n+    >>> trapezoidal_rule(func, [0, 1], 1)\n+    0.5\n \n-def method_1(boundary, steps):\n-    # \"extended trapezoidal rule\"\n-    # int(f) = dx/2 * (f1 + 2f2 + ... + fn)\n+    >>> trapezoidal_rule(func, [], 10)  # Empty boundary list\n+    Traceback (most recent call last):\n+        ...\n+    IndexError: list index out of range\n+\n+    >>> trapezoidal_rule(func, [0, 1], 0)  # Steps as zero\n+    Traceback (most recent call last):\n+        ...\n+    ZeroDivisionError: division by zero\n+    >>> trapezoidal_rule(func, ['0', '1'], 10)  # Boundary values as strings\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: unsupported operand type(s) for -: 'str' and 'str'\n+\n+    Parameters:\n+    - f (Callable[[float], float]): The function to be integrated.\n+    - boundary (list of float): A two-element list specifying the lower and upper bounds\n+     of the integration interval.\n+    - steps (int): The number of steps (trapezoids) to divide the interval into.\n+\n+    Returns:\n+    - float: The estimated value of the integral over the specified interval.\n+    \"\"\"\n     h = (boundary[1] - boundary[0]) / steps\n     a = boundary[0]\n     b = boundary[1]\n     x_i = make_points(a, b, h)\n     y = 0.0\n     y += (h / 2.0) * f(a)\n     for i in x_i:\n-        # print(i)\n         y += h * f(i)\n     y += (h / 2.0) * f(b)\n     return y\n \n \n-def make_points(a, b, h):\n-    x = a + h\n-    while x < (b - h):\n-        yield x\n-        x = x + h\n+def make_points(a: float, b: float, h: float) -> Iterator[float]:",
    "comment": "could we rename these variables for clarity? rather than single-letter variable names, we could have something like lower_bound, upper_bound, and step_size.",
    "line_number": 64,
    "enriched": "File: maths/trapezoidal_rule.py\nCode: @@ -1,50 +1,97 @@\n \"\"\"\n Numerical integration or quadrature for a smooth function f with known values at x_i\n \n-This method is the classical approach of suming 'Equally Spaced Abscissas'\n+This method is the classical approach of summing 'Equally Spaced Abscissas'\n+\"\"\"\n \n-method 1:\n-\"extended trapezoidal rule\"\n+from collections.abc import Callable, Iterator\n \n-\"\"\"\n \n+def trapezoidal_rule(\n+    f: Callable[[float], float], boundary: list[float], steps: int\n+) -> float:\n+    \"\"\"\n+    \"extended trapezoidal rule\"\n+    int(f) = dx/2 * (f1 + 2f2 + ... + fn)\n+\n+    >>> def func(x): return x ** 2\n+    >>> abs(trapezoidal_rule(func, [0, 1], 10) - 0.335) < 1e-9\n+    True\n+\n+    >>> def func(x): return 1\n+    >>> abs(trapezoidal_rule(func, [0, 10], 100) - 10.0) < 1e-9\n+    True\n+\n+    >>> def func(x): return x\n+    >>> trapezoidal_rule(func, [0, 1], 1)\n+    0.5\n \n-def method_1(boundary, steps):\n-    # \"extended trapezoidal rule\"\n-    # int(f) = dx/2 * (f1 + 2f2 + ... + fn)\n+    >>> trapezoidal_rule(func, [], 10)  # Empty boundary list\n+    Traceback (most recent call last):\n+        ...\n+    IndexError: list index out of range\n+\n+    >>> trapezoidal_rule(func, [0, 1], 0)  # Steps as zero\n+    Traceback (most recent call last):\n+        ...\n+    ZeroDivisionError: division by zero\n+    >>> trapezoidal_rule(func, ['0', '1'], 10)  # Boundary values as strings\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: unsupported operand type(s) for -: 'str' and 'str'\n+\n+    Parameters:\n+    - f (Callable[[float], float]): The function to be integrated.\n+    - boundary (list of float): A two-element list specifying the lower and upper bounds\n+     of the integration interval.\n+    - steps (int): The number of steps (trapezoids) to divide the interval into.\n+\n+    Returns:\n+    - float: The estimated value of the integral over the specified interval.\n+    \"\"\"\n     h = (boundary[1] - boundary[0]) / steps\n     a = boundary[0]\n     b = boundary[1]\n     x_i = make_points(a, b, h)\n     y = 0.0\n     y += (h / 2.0) * f(a)\n     for i in x_i:\n-        # print(i)\n         y += h * f(i)\n     y += (h / 2.0) * f(b)\n     return y\n \n \n-def make_points(a, b, h):\n-    x = a + h\n-    while x < (b - h):\n-        yield x\n-        x = x + h\n+def make_points(a: float, b: float, h: float) -> Iterator[float]:\nComment: Could we rename these variables for clarity? Rather than single-letter variable names, we could have something like `lower_bound`, `upper_bound`, and `step_size`.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/trapezoidal_rule.py",
    "pr_number": 11491,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1705143509,
    "comment_created_at": "2024-08-06T08:39:36Z"
  },
  {
    "code": "@@ -0,0 +1,19 @@\n+ \n+\n+import math\n+def arc(angle: int, r: int) -> int:",
    "comment": "please provide descriptive name for the parameter: r",
    "line_number": 4,
    "enriched": "File: maths/arc_length.py\nCode: @@ -0,0 +1,19 @@\n+ \n+\n+import math\n+def arc(angle: int, r: int) -> int:\nComment: Please provide descriptive name for the parameter: `r`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/arc_length.py",
    "pr_number": 7609,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1003987638,
    "comment_created_at": "2022-10-25T04:34:20Z"
  },
  {
    "code": "@@ -0,0 +1,44 @@\n+def threeSum(nums: list[int]) -> list[list[int]]:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: threesum",
    "line_number": 1,
    "enriched": "File: maths/three_sum.py\nCode: @@ -0,0 +1,44 @@\n+def threeSum(nums: list[int]) -> list[list[int]]:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `threeSum`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/three_sum.py",
    "pr_number": 9177,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342071337,
    "comment_created_at": "2023-10-01T04:24:42Z"
  },
  {
    "code": "@@ -0,0 +1,29 @@\n+rect1 = {\n+    \"x\": 10,\n+    \"y\": 10,\n+    \"height\": 30,\n+    \"width\": 50\n+}\n+\n+rect2 = {\n+    \"x\": 20,\n+    \"y\": 30,\n+    \"height\": 40,\n+    \"width\": 30\n+}\n+\n+def checkCollision(rect1, rect2) -> bool:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: checkcollision\n\nplease provide type hint for the parameter: rect1\n\nplease provide type hint for the parameter: rect2",
    "line_number": 15,
    "enriched": "File: maths/collision_between_rectangles.py\nCode: @@ -0,0 +1,29 @@\n+rect1 = {\n+    \"x\": 10,\n+    \"y\": 10,\n+    \"height\": 30,\n+    \"width\": 50\n+}\n+\n+rect2 = {\n+    \"x\": 20,\n+    \"y\": 30,\n+    \"height\": 40,\n+    \"width\": 30\n+}\n+\n+def checkCollision(rect1, rect2) -> bool:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `checkCollision`\n\nPlease provide type hint for the parameter: `rect1`\n\nPlease provide type hint for the parameter: `rect2`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/collision_between_rectangles.py",
    "pr_number": 7830,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008653421,
    "comment_created_at": "2022-10-29T07:29:59Z"
  },
  {
    "code": "@@ -0,0 +1,214 @@\n+\"\"\"https://en.wikipedia.org/wiki/Advanced_Encryption_Standard\"\"\"\n+\n+Sbox = (",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: sbox",
    "line_number": 3,
    "enriched": "File: ciphers/aes128.py\nCode: @@ -0,0 +1,214 @@\n+\"\"\"https://en.wikipedia.org/wiki/Advanced_Encryption_Standard\"\"\"\n+\n+Sbox = (\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `Sbox`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "ciphers/aes128.py",
    "pr_number": 10812,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367913178,
    "comment_created_at": "2023-10-22T14:12:38Z"
  },
  {
    "code": "@@ -22,14 +22,40 @@ def __init__(self, value: T) -> None:\n \n     @property\n     def value(self) -> T:\n-        \"\"\"Return the value of the node.\"\"\"\n+        \"\"\"\n+        Return the value of the node.\n+\n+        >>> rhn = RandomizedHeapNode(10)\n+        >>> rhn.value\n+        10\n+        >>> rhn = RandomizedHeapNode(-10)\n+        >>> rhn.value\n+        -10\n+        \"\"\"\n         return self._value\n \n     @staticmethod\n     def merge(\n         root1: RandomizedHeapNode[T] | None, root2: RandomizedHeapNode[T] | None\n     ) -> RandomizedHeapNode[T] | None:\n-        \"\"\"Merge 2 nodes together.\"\"\"\n+        \"\"\"\n+        Merge 2 nodes together.\n+\n+        >>> rhn1 = RandomizedHeapNode(10)\n+        >>> rhn2 = RandomizedHeapNode(20)\n+        >>> RandomizedHeapNode.merge(rhn1,rhn2).value\n+        10\n+\n+        >>> rhn1 = RandomizedHeapNode(20)\n+        >>> rhn2 = RandomizedHeapNode(10)\n+        >>> RandomizedHeapNode.merge(rhn1,rhn2).value\n+        10\n+\n+        >>> rhn1 = RandomizedHeapNode(5)\n+        >>> rhn2 = RandomizedHeapNode(0)\n+        >>> RandomizedHeapNode.merge(rhn1,rhn2).value\n+        0",
    "comment": "minor formatting changes",
    "line_number": 57,
    "enriched": "File: data_structures/heap/randomized_heap.py\nCode: @@ -22,14 +22,40 @@ def __init__(self, value: T) -> None:\n \n     @property\n     def value(self) -> T:\n-        \"\"\"Return the value of the node.\"\"\"\n+        \"\"\"\n+        Return the value of the node.\n+\n+        >>> rhn = RandomizedHeapNode(10)\n+        >>> rhn.value\n+        10\n+        >>> rhn = RandomizedHeapNode(-10)\n+        >>> rhn.value\n+        -10\n+        \"\"\"\n         return self._value\n \n     @staticmethod\n     def merge(\n         root1: RandomizedHeapNode[T] | None, root2: RandomizedHeapNode[T] | None\n     ) -> RandomizedHeapNode[T] | None:\n-        \"\"\"Merge 2 nodes together.\"\"\"\n+        \"\"\"\n+        Merge 2 nodes together.\n+\n+        >>> rhn1 = RandomizedHeapNode(10)\n+        >>> rhn2 = RandomizedHeapNode(20)\n+        >>> RandomizedHeapNode.merge(rhn1,rhn2).value\n+        10\n+\n+        >>> rhn1 = RandomizedHeapNode(20)\n+        >>> rhn2 = RandomizedHeapNode(10)\n+        >>> RandomizedHeapNode.merge(rhn1,rhn2).value\n+        10\n+\n+        >>> rhn1 = RandomizedHeapNode(5)\n+        >>> rhn2 = RandomizedHeapNode(0)\n+        >>> RandomizedHeapNode.merge(rhn1,rhn2).value\n+        0\nComment: ```suggestion\r\n        >>> rhn1 = RandomizedHeapNode(10)\r\n        >>> rhn2 = RandomizedHeapNode(20)\r\n        >>> RandomizedHeapNode.merge(rhn1, rhn2).value\r\n        10\r\n\r\n        >>> rhn1 = RandomizedHeapNode(20)\r\n        >>> rhn2 = RandomizedHeapNode(10)\r\n        >>> RandomizedHeapNode.merge(rhn1, rhn2).value\r\n        10\r\n\r\n        >>> rhn1 = RandomizedHeapNode(5)\r\n        >>> rhn2 = RandomizedHeapNode(0)\r\n        >>> RandomizedHeapNode.merge(rhn1, rhn2).value\r\n        0\r\n```\r\nMinor formatting changes",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "data_structures/heap/randomized_heap.py",
    "pr_number": 11151,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1389866846,
    "comment_created_at": "2023-11-10T20:09:32Z"
  },
  {
    "code": "@@ -0,0 +1,43 @@\n+def dfs(grid: list(list(int)), row: int, col: int, visit: set()) -> int:",
    "comment": "is this what you mean?",
    "line_number": 1,
    "enriched": "File: matrix/count_paths.py\nCode: @@ -0,0 +1,43 @@\n+def dfs(grid: list(list(int)), row: int, col: int, visit: set()) -> int:\nComment: ```suggestion\ndef dfs(grid: list[list[int]], row: int, col: int, visit: set) -> int:\n```\nIs this what you mean?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "matrix/count_paths.py",
    "pr_number": 7532,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002595372,
    "comment_created_at": "2022-10-22T23:03:53Z"
  },
  {
    "code": "@@ -22,12 +22,12 @@\n \"\"\"\n \n \n-def combination_sum_iv(n: int, array: list[int], target: int) -> int:\n+def combination_sum_iv(array: list[int], target: int) -> int:",
    "comment": "i guess you can't commit a lot of changes in a single pr !!",
    "line_number": 25,
    "enriched": "File: dynamic_programming/combination_sum_iv.py\nCode: @@ -22,12 +22,12 @@\n \"\"\"\n \n \n-def combination_sum_iv(n: int, array: list[int], target: int) -> int:\n+def combination_sum_iv(array: list[int], target: int) -> int:\nComment: I guess you can't commit a lot of changes in a SINGLE PR !!",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "dynamic_programming/combination_sum_iv.py",
    "pr_number": 11321,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1522354138,
    "comment_created_at": "2024-03-13T01:33:51Z"
  },
  {
    "code": "@@ -130,5 +130,5 @@ omit = [\".env/*\"]\n sort = \"Cover\"\n \n [tool.codespell]\n-ignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,mater,secant,som,sur,tim,zar\"\n+ignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,manuel,mater,secant,som,sur,tim,zar\"",
    "comment": "not entirely sure how codespell handles ignored words, but would this help codespell recognize \"manuel\" as a name while still catching  \"manuel\" as a typo?",
    "line_number": 133,
    "enriched": "File: pyproject.toml\nCode: @@ -130,5 +130,5 @@ omit = [\".env/*\"]\n sort = \"Cover\"\n \n [tool.codespell]\n-ignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,mater,secant,som,sur,tim,zar\"\n+ignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,manuel,mater,secant,som,sur,tim,zar\"\nComment: ```suggestion\r\nignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,Manuel,mater,secant,som,sur,tim,zar\"\r\n```\r\nNot entirely sure how codespell handles ignored words, but would this help codespell recognize \"Manuel\" as a name while still catching  \"manuel\" as a typo?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pyproject.toml",
    "pr_number": 9543,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349559334,
    "comment_created_at": "2023-10-07T18:30:59Z"
  },
  {
    "code": "@@ -36,6 +36,8 @@\n def fetch_github_info(auth_token: str) -> dict[Any, Any]:\n     \"\"\"\n     Fetch GitHub info of a user using the requests module",
    "comment": "is that enough or do we need to add more?",
    "line_number": 38,
    "enriched": "File: web_programming/fetch_github_info.py\nCode: @@ -36,6 +36,8 @@\n def fetch_github_info(auth_token: str) -> dict[Any, Any]:\n     \"\"\"\n     Fetch GitHub info of a user using the requests module\nComment: Is that enough or do we need to add more?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "web_programming/fetch_github_info.py",
    "pr_number": 11148,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1385866284,
    "comment_created_at": "2023-11-08T02:02:05Z"
  },
  {
    "code": "@@ -27,16 +24,12 @@ jobs:\n         with:\n           python-version: 3.11 \n           allow-prereleases: true\n-\n       - name: Install dependencies\n       - run: |\n           uv sync --all-extras\n           uv pip list\n-\n       - name: Lint code\n-        run: uv run ruff check , --output-format=github\n-\n-        \n+        run: uv run ruff check , --output-format=github  ",
    "comment": "what does the comma (,) do on this line?",
    "line_number": 32,
    "enriched": "File: .github/workflows/build.yml\nCode: @@ -27,16 +24,12 @@ jobs:\n         with:\n           python-version: 3.11 \n           allow-prereleases: true\n-\n       - name: Install dependencies\n       - run: |\n           uv sync --all-extras\n           uv pip list\n-\n       - name: Lint code\n-        run: uv run ruff check , --output-format=github\n-\n-        \n+        run: uv run ruff check , --output-format=github  \nComment: What does the comma (,) do on this line?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".github/workflows/build.yml",
    "pr_number": 12913,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2295088001,
    "comment_created_at": "2025-08-23T04:05:15Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+\"\"\"\n+Title : Computing the terminal velocity of an object falling\n+        through a fluid.\n+\n+Terminal velocity is defined as the highest velocity attained by an\n+object falling through a fluid. It is observed when the sum of drag force\n+and buoyancy is equal to the downward gravity force acting on the\n+object. The acceleration of the object is zero as the net force acting on\n+the object is zero.\n+\n+Vt = ((2 * m * g)/(\u03c1 * A * Cd))^0.5\n+\n+where :\n+Vt = Terminal velocity (in m/s)\n+m = Mass of the falling object (in Kg)\n+g = Acceleration due to gravity (value taken : 9.8 m/s^2)\n+\u03c1 = Density of the fluid through which the object is falling (in Kg/m^3)\n+A = Projected area of the object (in m^2)\n+Cd = Drag coefficient (dimensionless)\n+\n+Reference : https://byjus.com/physics/derivation-of-terminal-velocity/\n+\"\"\"\n+\n+\n+def terminal_velocity(\n+    mass: float, density: float, area: float, drag_coefficient: float\n+) -> float:\n+    \"\"\"\n+    >>> terminal_velocity(1, 25, 0.6, 0.77)\n+    1.3026778945578592\n+    >>> terminal_velocity(2, 100, 0.45, 0.23)\n+    1.9461345311993645\n+    >>> terminal_velocity(5, 50, 0.2, 0.5)\n+    4.427188724235731\n+    >>> terminal_velocity(-5, 50, -0.2, -2)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: mass, density, area and the drag coefficient all need to be positive\n+    >>> terminal_velocity(3, -20, -1, 2)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: mass, density, area and the drag coefficient all need to be positive\n+    >>> terminal_velocity(-2, -1, -0.44, -1)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: mass, density, area and the drag coefficient all need to be positive\n+    \"\"\"\n+    if mass <= 0 or density <= 0 or area <= 0 or drag_coefficient <= 0:\n+        raise ValueError(\n+            \"mass, density, area and the drag coefficient all need to be positive\"\n+        )\n+    return ((2 * mass * 9.8) / (density * area * drag_coefficient)) ** 0.5",
    "comment": "instead of using a hard-coded 9.8 for the acceleration due to gravity, could you import the constant from scipy.constants? the constant value in this library is more accurate.\r\n\r\nhttps://docs.scipy.org/doc/scipy/reference/constants.html",
    "line_number": 52,
    "enriched": "File: physics/terminal_velocity.py\nCode: @@ -0,0 +1,58 @@\n+\"\"\"\n+Title : Computing the terminal velocity of an object falling\n+        through a fluid.\n+\n+Terminal velocity is defined as the highest velocity attained by an\n+object falling through a fluid. It is observed when the sum of drag force\n+and buoyancy is equal to the downward gravity force acting on the\n+object. The acceleration of the object is zero as the net force acting on\n+the object is zero.\n+\n+Vt = ((2 * m * g)/(\u03c1 * A * Cd))^0.5\n+\n+where :\n+Vt = Terminal velocity (in m/s)\n+m = Mass of the falling object (in Kg)\n+g = Acceleration due to gravity (value taken : 9.8 m/s^2)\n+\u03c1 = Density of the fluid through which the object is falling (in Kg/m^3)\n+A = Projected area of the object (in m^2)\n+Cd = Drag coefficient (dimensionless)\n+\n+Reference : https://byjus.com/physics/derivation-of-terminal-velocity/\n+\"\"\"\n+\n+\n+def terminal_velocity(\n+    mass: float, density: float, area: float, drag_coefficient: float\n+) -> float:\n+    \"\"\"\n+    >>> terminal_velocity(1, 25, 0.6, 0.77)\n+    1.3026778945578592\n+    >>> terminal_velocity(2, 100, 0.45, 0.23)\n+    1.9461345311993645\n+    >>> terminal_velocity(5, 50, 0.2, 0.5)\n+    4.427188724235731\n+    >>> terminal_velocity(-5, 50, -0.2, -2)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: mass, density, area and the drag coefficient all need to be positive\n+    >>> terminal_velocity(3, -20, -1, 2)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: mass, density, area and the drag coefficient all need to be positive\n+    >>> terminal_velocity(-2, -1, -0.44, -1)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: mass, density, area and the drag coefficient all need to be positive\n+    \"\"\"\n+    if mass <= 0 or density <= 0 or area <= 0 or drag_coefficient <= 0:\n+        raise ValueError(\n+            \"mass, density, area and the drag coefficient all need to be positive\"\n+        )\n+    return ((2 * mass * 9.8) / (density * area * drag_coefficient)) ** 0.5\nComment: Instead of using a hard-coded `9.8` for the acceleration due to gravity, could you import the constant from `scipy.constants`? The constant value in this library is more accurate.\r\n\r\nhttps://docs.scipy.org/doc/scipy/reference/constants.html",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "physics/terminal_velocity.py",
    "pr_number": 10237,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368133264,
    "comment_created_at": "2023-10-23T05:00:02Z"
  },
  {
    "code": "@@ -20,31 +20,28 @@\n class Tableau:\n     \"\"\"Operate on simplex tableaus\n \n-    >>> t = Tableau(np.array([[-1,-1,0,0,-1],[1,3,1,0,4],[3,1,0,1,4.]]), 2)\n+    >>> Tableau(np.array([[-1,-1,0,0,-1],[1,3,1,0,4],[3,1,0,1,4.]]), 2, 2)\n     Traceback (most recent call last):\n     ...\n     ValueError: RHS must be > 0\n     \"\"\"\n \n-    def __init__(self, tableau: np.ndarray, n_vars: int) -> None:\n+    def __init__(self, tableau: np.ndarray, n_vars: int, n_art_vars: int) -> None:",
    "comment": "what does art mean in this context and why does the reader have to guess?\r\nplease use real words even it if makes the variable longer.  https://word.tips/words-start-with/art",
    "line_number": 29,
    "enriched": "File: linear_programming/simplex.py\nCode: @@ -20,31 +20,28 @@\n class Tableau:\n     \"\"\"Operate on simplex tableaus\n \n-    >>> t = Tableau(np.array([[-1,-1,0,0,-1],[1,3,1,0,4],[3,1,0,1,4.]]), 2)\n+    >>> Tableau(np.array([[-1,-1,0,0,-1],[1,3,1,0,4],[3,1,0,1,4.]]), 2, 2)\n     Traceback (most recent call last):\n     ...\n     ValueError: RHS must be > 0\n     \"\"\"\n \n-    def __init__(self, tableau: np.ndarray, n_vars: int) -> None:\n+    def __init__(self, tableau: np.ndarray, n_vars: int, n_art_vars: int) -> None:\nComment: What does `art` mean in this context and why does the reader have to guess?\r\nPlease use real words even it if makes the variable longer.  https://word.tips/words-start-with/art",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "linear_programming/simplex.py",
    "pr_number": 8843,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1271280188,
    "comment_created_at": "2023-07-22T10:39:23Z"
  },
  {
    "code": "@@ -10,8 +10,8 @@\n \"\"\"\n \n \n-# Acceleration Constant on Earth (unit m/s^2)\n-g = 9.80665\n+# Acceleration Constant on Earth (unit m/s^2) imported from scipy\n+from scipy.constants import g",
    "comment": "please revert this change.  let's not create a dependency on all of scipy just to set a constant.",
    "line_number": 14,
    "enriched": "File: physics/archimedes_principle.py\nCode: @@ -10,8 +10,8 @@\n \"\"\"\n \n \n-# Acceleration Constant on Earth (unit m/s^2)\n-g = 9.80665\n+# Acceleration Constant on Earth (unit m/s^2) imported from scipy\n+from scipy.constants import g\nComment: Please revert this change.  Let's not create a dependency on all of scipy just to set a constant.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "physics/archimedes_principle.py",
    "pr_number": 10479,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359781399,
    "comment_created_at": "2023-10-15T06:01:02Z"
  },
  {
    "code": "@@ -0,0 +1,99 @@\n+\"\"\"\n+Author  : ilyas dahhou\n+Date    : Oct 7, 2023\n+\n+Task:\n+Given an input string (s) and a pattern (p), implement wildcard\n+pattern matching with support for '?' and '*' where:\n+\n+'?' matches any single character.\n+'*' matches any sequence of characters (including the empty sequence).\n+The matching should cover the entire input string (not partial).\n+\n+Implementation notes:\n+implementation Dynamic Programming up bottom approach.\n+\n+Runtime complexity:O(m * n)\n+\n+The implementation was tested on the\n+leetcode: https://leetcode.com/problems/wildcard-matching/\n+\n+\n+wildcard matching\n+Dynamic Programming: top -> down.\n+\n+\"\"\"\n+import datetime\n+\n+import pytz\n+\n+tz_maroc = pytz.timezone(\"Africa/Casablanca\")\n+\n+now_maroc = datetime.datetime.now(tz=tz_maroc)\n+\n+\n+def is_match(s: str, p: str) -> bool:",
    "comment": "please provide descriptive name for the parameter: s\n\nplease provide descriptive name for the parameter: p",
    "line_number": 35,
    "enriched": "File: dynamic_programming/wildcard_matching.py\nCode: @@ -0,0 +1,99 @@\n+\"\"\"\n+Author  : ilyas dahhou\n+Date    : Oct 7, 2023\n+\n+Task:\n+Given an input string (s) and a pattern (p), implement wildcard\n+pattern matching with support for '?' and '*' where:\n+\n+'?' matches any single character.\n+'*' matches any sequence of characters (including the empty sequence).\n+The matching should cover the entire input string (not partial).\n+\n+Implementation notes:\n+implementation Dynamic Programming up bottom approach.\n+\n+Runtime complexity:O(m * n)\n+\n+The implementation was tested on the\n+leetcode: https://leetcode.com/problems/wildcard-matching/\n+\n+\n+wildcard matching\n+Dynamic Programming: top -> down.\n+\n+\"\"\"\n+import datetime\n+\n+import pytz\n+\n+tz_maroc = pytz.timezone(\"Africa/Casablanca\")\n+\n+now_maroc = datetime.datetime.now(tz=tz_maroc)\n+\n+\n+def is_match(s: str, p: str) -> bool:\nComment: Please provide descriptive name for the parameter: `s`\n\nPlease provide descriptive name for the parameter: `p`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "dynamic_programming/wildcard_matching.py",
    "pr_number": 10052,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349568581,
    "comment_created_at": "2023-10-07T20:03:48Z"
  },
  {
    "code": "@@ -0,0 +1,82 @@\n+\"\"\"",
    "comment": "hi, you are also changing this file. can you create the different pull request for different files.",
    "line_number": 1,
    "enriched": "File: data_structures/binary_tree/floor_ceil_in_bst.py\nCode: @@ -0,0 +1,82 @@\n+\"\"\"\nComment: Hi, You are also changing this file. Can you create the different pull request for different files.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "data_structures/binary_tree/floor_ceil_in_bst.py",
    "pr_number": 10725,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367935118,
    "comment_created_at": "2023-10-22T16:44:06Z"
  },
  {
    "code": "@@ -0,0 +1,78 @@\n+from sympy.abc import x\n+from sympy import diff\n+def bisection_method(function: str, a: float, b: float) -> float:",
    "comment": "please provide descriptive name for the parameter: a\n\nplease provide descriptive name for the parameter: b",
    "line_number": 3,
    "enriched": "File: arithmetic_analysis/bisection_method.py\nCode: @@ -0,0 +1,78 @@\n+from sympy.abc import x\n+from sympy import diff\n+def bisection_method(function: str, a: float, b: float) -> float:\nComment: Please provide descriptive name for the parameter: `a`\n\nPlease provide descriptive name for the parameter: `b`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "arithmetic_analysis/bisection_method.py",
    "pr_number": 8142,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1109076375,
    "comment_created_at": "2023-02-16T22:15:31Z"
  },
  {
    "code": "@@ -0,0 +1,55 @@\n+def camel_to_snake_case(input_str: str) -> str:\r\n+    \"\"\"\r\n+    Transforms a camelCase (or PascalCase) string to snake_case\r\n+\r\n+    >>> camel_to_snake_case(\"someRandomString\")\r\n+    'some_random_string'\r\n+\r\n+    >>> camel_to_snake_case(\"SomeRandomString\")\r\n+    'some_random_string'\r\n+\r\n+    >>> camel_to_snake_case(\"123someRandom123String123\")\r\n+    '123_some_random_123_string_123'\r\n+\r\n+    >>> camel_to_snake_case(\"123SomeRandom123String123\")\r\n+    '123_some_random_123_string_123'\r\n+\r\n+    >>> camel_to_snake_case(123)\r\n+    Traceback (most recent call last):\r\n+        ...\r\n+    ValueError: Expected string as input, found <class 'int'>\r\n+\r\n+    \"\"\"\r\n+\r\n+    import re\r\n+\r\n+    # check for invalid input type\r\n+    if not isinstance(input_str, str):\r\n+        msg = f\"Expected string as input, found {type(input_str)}\"\r\n+        raise ValueError(msg)\r\n+\r\n+    # Replace all characters that are not letters or numbers with the underscore\r\n+    snake_str = re.sub(r\"[^a-zA-Z0-9]\", \"_\", input_str)\r\n+\r\n+    # Find where lowercase meets uppercase. Insert underscore between them\r\n+    snake_str = re.sub(r\"([a-z])([A-Z])\", r\"\\1_\\2\", snake_str).lower()\r\n+\r\n+    # Find the sequence of digits at the beginning\r\n+    snake_str = re.sub(r\"^(\\d+)\", r\"\\1_\", snake_str)\r\n+\r\n+    # Find the sequence of digits at the end\r\n+    snake_str = re.sub(r\"(\\d+)$\", r\"_\\1\", snake_str)\r\n+\r\n+    # Find where letter meets digits\r\n+    snake_str = re.sub(r\"([a-z])(\\d+)\", r\"\\1_\\2\", snake_str)\r\n+\r\n+    # Find where digits meet letter\r\n+    snake_str = re.sub(r\"(\\d+)([a-z])\", r\"\\1_\\2\", snake_str)\r",
    "comment": "can this be done without using a regex?",
    "line_number": 47,
    "enriched": "File: strings/camel_case_to_snake_case.py\nCode: @@ -0,0 +1,55 @@\n+def camel_to_snake_case(input_str: str) -> str:\r\n+    \"\"\"\r\n+    Transforms a camelCase (or PascalCase) string to snake_case\r\n+\r\n+    >>> camel_to_snake_case(\"someRandomString\")\r\n+    'some_random_string'\r\n+\r\n+    >>> camel_to_snake_case(\"SomeRandomString\")\r\n+    'some_random_string'\r\n+\r\n+    >>> camel_to_snake_case(\"123someRandom123String123\")\r\n+    '123_some_random_123_string_123'\r\n+\r\n+    >>> camel_to_snake_case(\"123SomeRandom123String123\")\r\n+    '123_some_random_123_string_123'\r\n+\r\n+    >>> camel_to_snake_case(123)\r\n+    Traceback (most recent call last):\r\n+        ...\r\n+    ValueError: Expected string as input, found <class 'int'>\r\n+\r\n+    \"\"\"\r\n+\r\n+    import re\r\n+\r\n+    # check for invalid input type\r\n+    if not isinstance(input_str, str):\r\n+        msg = f\"Expected string as input, found {type(input_str)}\"\r\n+        raise ValueError(msg)\r\n+\r\n+    # Replace all characters that are not letters or numbers with the underscore\r\n+    snake_str = re.sub(r\"[^a-zA-Z0-9]\", \"_\", input_str)\r\n+\r\n+    # Find where lowercase meets uppercase. Insert underscore between them\r\n+    snake_str = re.sub(r\"([a-z])([A-Z])\", r\"\\1_\\2\", snake_str).lower()\r\n+\r\n+    # Find the sequence of digits at the beginning\r\n+    snake_str = re.sub(r\"^(\\d+)\", r\"\\1_\", snake_str)\r\n+\r\n+    # Find the sequence of digits at the end\r\n+    snake_str = re.sub(r\"(\\d+)$\", r\"_\\1\", snake_str)\r\n+\r\n+    # Find where letter meets digits\r\n+    snake_str = re.sub(r\"([a-z])(\\d+)\", r\"\\1_\\2\", snake_str)\r\n+\r\n+    # Find where digits meet letter\r\n+    snake_str = re.sub(r\"(\\d+)([a-z])\", r\"\\1_\\2\", snake_str)\r\nComment: Can this be done without using a regex?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "strings/camel_case_to_snake_case.py",
    "pr_number": 9727,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346546509,
    "comment_created_at": "2023-10-04T22:35:00Z"
  },
  {
    "code": "@@ -0,0 +1,30 @@\n+def max_sum_subarray(arr: list[int], k: int) -> int:",
    "comment": "please provide descriptive name for the parameter: k",
    "line_number": 1,
    "enriched": "File: data_structures/arrays/sliding_window.py\nCode: @@ -0,0 +1,30 @@\n+def max_sum_subarray(arr: list[int], k: int) -> int:\nComment: Please provide descriptive name for the parameter: `k`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "data_structures/arrays/sliding_window.py",
    "pr_number": 11816,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1788970733,
    "comment_created_at": "2024-10-06T08:35:01Z"
  },
  {
    "code": "@@ -52,7 +52,58 @@ def decimal_to_binary(num: int) -> str:\n     return \"0b\" + \"\".join(str(e) for e in binary)\n \n \n+def decimal_to_binary_recursive(decimal: int) -> str:\n+    \"\"\"\n+    Take a positive integer value and return its binary equivalent.\n+    >>> decimal_to_binary_recursive(1000)\n+    '1111101000'\n+    >>> decimal_to_binary_recursive(\"72\")\n+    '1001000'\n+    >>> decimal_to_binary_recursive(\"number\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: invalid literal for int() with base 10: 'number'\n+    \"\"\"\n+    decimal = int(decimal)\n+    if decimal in (0, 1):  # Exit cases for the recursion\n+        return str(decimal)\n+    div, mod = divmod(decimal, 2)\n+    return decimal_to_binary_recursive(div) + str(mod)\n+\n+\n+def decimal_to_binary_recursive_main(number: str) -> str:",
    "comment": "for simplicity, can we rename the main function decimal_to_binary_recursive rather than decimal_to_binary_recursive_main? we can then rename the recursive helper function to decimal_to_binary_recursive_helper or decimal_to_binary_recursive_no_prefix or something like that",
    "line_number": 74,
    "enriched": "File: conversions/decimal_to_binary.py\nCode: @@ -52,7 +52,58 @@ def decimal_to_binary(num: int) -> str:\n     return \"0b\" + \"\".join(str(e) for e in binary)\n \n \n+def decimal_to_binary_recursive(decimal: int) -> str:\n+    \"\"\"\n+    Take a positive integer value and return its binary equivalent.\n+    >>> decimal_to_binary_recursive(1000)\n+    '1111101000'\n+    >>> decimal_to_binary_recursive(\"72\")\n+    '1001000'\n+    >>> decimal_to_binary_recursive(\"number\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: invalid literal for int() with base 10: 'number'\n+    \"\"\"\n+    decimal = int(decimal)\n+    if decimal in (0, 1):  # Exit cases for the recursion\n+        return str(decimal)\n+    div, mod = divmod(decimal, 2)\n+    return decimal_to_binary_recursive(div) + str(mod)\n+\n+\n+def decimal_to_binary_recursive_main(number: str) -> str:\nComment: For simplicity, can we rename the main function `decimal_to_binary_recursive` rather than `decimal_to_binary_recursive_main`? We can then rename the recursive helper function to `decimal_to_binary_recursive_helper` or `decimal_to_binary_recursive_no_prefix` or something like that",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "conversions/decimal_to_binary.py",
    "pr_number": 8999,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1299456574,
    "comment_created_at": "2023-08-20T23:55:50Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+import math\n+import random\n+\n+def gcd(a, b):\n+    \"\"\"Computes the greatest common divisor using Euclidean algorithm.\"\"\"\n+    while b:",
    "comment": "this line should be while b == 0:, not while b: to make sure b is zero.",
    "line_number": 6,
    "enriched": "File: algorithms/cryptography/shor_algorithm.py\nCode: @@ -0,0 +1,56 @@\n+import math\n+import random\n+\n+def gcd(a, b):\n+    \"\"\"Computes the greatest common divisor using Euclidean algorithm.\"\"\"\n+    while b:\nComment: This line should be `while b == 0:`, not `while b:` to make sure `b` is zero.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "algorithms/cryptography/shor_algorithm.py",
    "pr_number": 12545,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1954891619,
    "comment_created_at": "2025-02-13T17:01:41Z"
  },
  {
    "code": "@@ -0,0 +1,138 @@\n+from __future__ import annotations\n+\n+\n+class Node:\n+    def __init__(self, number: int) -> None:\n+        self.data = number\n+        self.left = None\n+        self.right = None\n+\n+class BinaryTree:\n+    def __init__(self, root=None) -> None:",
    "comment": "please provide type hint for the parameter: root",
    "line_number": 11,
    "enriched": "File: data_structures/binary_tree/transform_bst_sum_tree.py\nCode: @@ -0,0 +1,138 @@\n+from __future__ import annotations\n+\n+\n+class Node:\n+    def __init__(self, number: int) -> None:\n+        self.data = number\n+        self.left = None\n+        self.right = None\n+\n+class BinaryTree:\n+    def __init__(self, root=None) -> None:\nComment: Please provide type hint for the parameter: `root`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "data_structures/binary_tree/transform_bst_sum_tree.py",
    "pr_number": 10110,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349695777,
    "comment_created_at": "2023-10-08T13:15:58Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+from math import ceil, sqrt\n+from __future__ import annotations\n+\n+\n+def primeproduct(n: int, x: list = []) -> list[int]:",
    "comment": "please provide descriptive name for the parameter: n\n\nplease provide descriptive name for the parameter: x",
    "line_number": 5,
    "enriched": "File: maths/prime_factorization_fast.py\nCode: @@ -0,0 +1,61 @@\n+from math import ceil, sqrt\n+from __future__ import annotations\n+\n+\n+def primeproduct(n: int, x: list = []) -> list[int]:\nComment: Please provide descriptive name for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/prime_factorization_fast.py",
    "pr_number": 8920,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1283299337,
    "comment_created_at": "2023-08-03T14:34:14Z"
  },
  {
    "code": "@@ -0,0 +1,131 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval: int | str | float = None) -> None:\n+        \"\"\"\n+        An empty left and right branches will be created for every value inserted,\n+        to perform better in recursive methods\n+        \"\"\"\n+        self.value = initval\n+        if self.value:\n+            self.left = Tree()\n+            self.right = Tree()\n+        else:\n+            self.left = None\n+            self.right = None\n+        return\n+\n+    # Empty nodes are None valued\n+    def isempty(self) -> bool:\n+        \"\"\"\n+        Returns true if tree is empty else False\n+        >>> isempty([])\n+        True\n+        >>> isempty([1,2,3])\n+        False\n+        \"\"\"\n+        return self.value == None\n+\n+    def isleaf(self) -> bool:\n+        \"\"\"\n+        Returns true if leaf is a node\n+        Suppose in [1,2,3,4,5,6,7,8,9,10]\n+        4 had empty left and right branches.\n+        >>> isleaf(4)\n+        True\n+        \"\"\"\n+        return self.value != None and self.left.isempty() and self.right.isempty()\n+\n+    # Inorder Traversal\n+    def inorder(self) -> list:\n+        \"\"\"\n+        Returns a list sorted in increasing order from the Binary Search Tree.\n+        >>> tree_sort([23235,82107,35775,91961,4323,40556,76603,64302,27316,74372])\n+        [4323, 23235, 27316, 35775, 40556, 64302, 74372, 76603, 82107, 91961]\n+        >>> tree_sort([50,52,54,74,93,100,114,124,130,143])\n+        [50, 52, 54, 74, 93, 100, 114, 124, 130, 143]\n+        \"\"\"\n+        # Corner Case\n+        if self.isempty():\n+            return []\n+        else:\n+            return self.left.inorder() + [self.value] + self.right.inorder()\n+\n+    # Display Tree\n+    def __str__(self) -> str:\n+        \"\"\"\n+        Prints the sorted tree\n+        Suppose tree is t.inorder() = [1,2,3]\n+        >>> print(t)\n+        [1,2,3]\n+        \"\"\"\n+        return str(self.inorder())\n+\n+    # Insert new element\n+    def insert(self, v: int | str | float) -> None:",
    "comment": "please provide descriptive name for the parameter: v",
    "line_number": 82,
    "enriched": "File: sorts/tree_sort_2.py\nCode: @@ -0,0 +1,131 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval: int | str | float = None) -> None:\n+        \"\"\"\n+        An empty left and right branches will be created for every value inserted,\n+        to perform better in recursive methods\n+        \"\"\"\n+        self.value = initval\n+        if self.value:\n+            self.left = Tree()\n+            self.right = Tree()\n+        else:\n+            self.left = None\n+            self.right = None\n+        return\n+\n+    # Empty nodes are None valued\n+    def isempty(self) -> bool:\n+        \"\"\"\n+        Returns true if tree is empty else False\n+        >>> isempty([])\n+        True\n+        >>> isempty([1,2,3])\n+        False\n+        \"\"\"\n+        return self.value == None\n+\n+    def isleaf(self) -> bool:\n+        \"\"\"\n+        Returns true if leaf is a node\n+        Suppose in [1,2,3,4,5,6,7,8,9,10]\n+        4 had empty left and right branches.\n+        >>> isleaf(4)\n+        True\n+        \"\"\"\n+        return self.value != None and self.left.isempty() and self.right.isempty()\n+\n+    # Inorder Traversal\n+    def inorder(self) -> list:\n+        \"\"\"\n+        Returns a list sorted in increasing order from the Binary Search Tree.\n+        >>> tree_sort([23235,82107,35775,91961,4323,40556,76603,64302,27316,74372])\n+        [4323, 23235, 27316, 35775, 40556, 64302, 74372, 76603, 82107, 91961]\n+        >>> tree_sort([50,52,54,74,93,100,114,124,130,143])\n+        [50, 52, 54, 74, 93, 100, 114, 124, 130, 143]\n+        \"\"\"\n+        # Corner Case\n+        if self.isempty():\n+            return []\n+        else:\n+            return self.left.inorder() + [self.value] + self.right.inorder()\n+\n+    # Display Tree\n+    def __str__(self) -> str:\n+        \"\"\"\n+        Prints the sorted tree\n+        Suppose tree is t.inorder() = [1,2,3]\n+        >>> print(t)\n+        [1,2,3]\n+        \"\"\"\n+        return str(self.inorder())\n+\n+    # Insert new element\n+    def insert(self, v: int | str | float) -> None:\nComment: Please provide descriptive name for the parameter: `v`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "sorts/tree_sort_2.py",
    "pr_number": 7461,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000813400,
    "comment_created_at": "2022-10-20T15:44:44Z"
  },
  {
    "code": "@@ -1,131 +1,96 @@\n \"\"\"\n-Author  : Alexander Pantyukhin\n-Date    : November 2, 2022\n-\n-Task:\n-Given the root of a binary tree, determine if it is a valid binary search\n-tree (BST).\n+Given the root of a binary tree, determine if it is a valid binary search tree (BST).\n \n A valid binary search tree is defined as follows:\n-\n - The left subtree of a node contains only nodes with keys less than the node's key.\n - The right subtree of a node contains only nodes with keys greater than the node's key.\n - Both the left and right subtrees must also be binary search trees.\n \n-Implementation notes:\n-Depth-first search approach.\n-\n leetcode: https://leetcode.com/problems/validate-binary-search-tree/\n \n-Let n is the number of nodes in tree\n+If n is the number of nodes in the tree then:\n Runtime: O(n)\n Space: O(1)\n \"\"\"\n-\n from __future__ import annotations\n \n+from collections.abc import Iterator\n from dataclasses import dataclass\n \n \n @dataclass\n-class TreeNode:\n+class Node:\n     data: float\n-    left: TreeNode | None = None\n-    right: TreeNode | None = None\n-\n-\n-def is_binary_search_tree(root: TreeNode | None) -> bool:\n-    \"\"\"\n-    >>> is_binary_search_tree(TreeNode(data=2,\n-    ...                                left=TreeNode(data=1),\n-    ...                                right=TreeNode(data=3))\n-    ...                                )\n-    True\n-\n-    >>> is_binary_search_tree(TreeNode(data=0,\n-    ...                                left=TreeNode(data=-11),\n-    ...                                right=TreeNode(data=3))\n-    ...                                )\n-    True\n-\n-    >>> is_binary_search_tree(TreeNode(data=5,\n-    ...                                left=TreeNode(data=1),\n-    ...                                right=TreeNode(data=4, left=TreeNode(data=3)))\n-    ...                      )\n-    False\n+    left: Node | None = None\n+    right: Node | None = None\n \n-    >>> is_binary_search_tree(TreeNode(data='a',\n-    ...                                left=TreeNode(data=1),\n-    ...                                right=TreeNode(data=4, left=TreeNode(data=3)))\n-    ...                      )\n-    Traceback (most recent call last):\n-     ...\n-    ValueError: Each node should be type of TreeNode and data should be float.\n-\n-    >>> is_binary_search_tree(TreeNode(data=2,\n-    ...                                left=TreeNode([]),\n-    ...                                right=TreeNode(data=4, left=TreeNode(data=3)))\n-    ...                                )\n-    Traceback (most recent call last):\n-     ...\n-    ValueError: Each node should be type of TreeNode and data should be float.\n-    \"\"\"\n-\n-    # Validation\n-    def is_valid_tree(node: TreeNode | None) -> bool:\n+    def __iter__(self) -> Iterator[float]:\n         \"\"\"\n-        >>> is_valid_tree(None)\n-        True\n-        >>> is_valid_tree('abc')\n-        False\n-        >>> is_valid_tree(TreeNode(data='not a float'))\n-        False\n-        >>> is_valid_tree(TreeNode(data=1, left=TreeNode('123')))\n-        False\n+        >>> root = Node(data=2.1)\n+        >>> list(root)\n+        [2.1]\n+        >>> root.left=Node(data=2.0)\n+        >>> list(root)\n+        [2.0, 2.1]\n+        >>> root.right=Node(data=2.2)\n+        >>> list(root)\n+        [2.0, 2.1, 2.2]\n         \"\"\"\n-        if node is None:\n-            return True\n-\n-        if not isinstance(node, TreeNode):\n-            return False\n-\n-        try:\n-            float(node.data)\n-        except (TypeError, ValueError):\n-            return False\n-\n-        return is_valid_tree(node.left) and is_valid_tree(node.right)\n-\n-    if not is_valid_tree(root):\n-        raise ValueError(\n-            \"Each node should be type of TreeNode and data should be float.\"\n-        )\n-\n-    def is_binary_search_tree_recursive_check(\n-        node: TreeNode | None, left_bound: float, right_bound: float\n-    ) -> bool:\n+        if self.left:\n+            yield from self.left\n+        yield self.data\n+        if self.right:\n+            yield from self.right\n+\n+    @property\n+    def is_sorted(self) -> bool:",
    "comment": "the name of this function no longer matches the name of the file, and that might confuse some readers. i feel that either this function should be renamed to is_bst or the file should be renamed to is_sorted_binary_tree.",
    "line_number": 46,
    "enriched": "File: data_structures/binary_tree/is_bst.py\nCode: @@ -1,131 +1,96 @@\n \"\"\"\n-Author  : Alexander Pantyukhin\n-Date    : November 2, 2022\n-\n-Task:\n-Given the root of a binary tree, determine if it is a valid binary search\n-tree (BST).\n+Given the root of a binary tree, determine if it is a valid binary search tree (BST).\n \n A valid binary search tree is defined as follows:\n-\n - The left subtree of a node contains only nodes with keys less than the node's key.\n - The right subtree of a node contains only nodes with keys greater than the node's key.\n - Both the left and right subtrees must also be binary search trees.\n \n-Implementation notes:\n-Depth-first search approach.\n-\n leetcode: https://leetcode.com/problems/validate-binary-search-tree/\n \n-Let n is the number of nodes in tree\n+If n is the number of nodes in the tree then:\n Runtime: O(n)\n Space: O(1)\n \"\"\"\n-\n from __future__ import annotations\n \n+from collections.abc import Iterator\n from dataclasses import dataclass\n \n \n @dataclass\n-class TreeNode:\n+class Node:\n     data: float\n-    left: TreeNode | None = None\n-    right: TreeNode | None = None\n-\n-\n-def is_binary_search_tree(root: TreeNode | None) -> bool:\n-    \"\"\"\n-    >>> is_binary_search_tree(TreeNode(data=2,\n-    ...                                left=TreeNode(data=1),\n-    ...                                right=TreeNode(data=3))\n-    ...                                )\n-    True\n-\n-    >>> is_binary_search_tree(TreeNode(data=0,\n-    ...                                left=TreeNode(data=-11),\n-    ...                                right=TreeNode(data=3))\n-    ...                                )\n-    True\n-\n-    >>> is_binary_search_tree(TreeNode(data=5,\n-    ...                                left=TreeNode(data=1),\n-    ...                                right=TreeNode(data=4, left=TreeNode(data=3)))\n-    ...                      )\n-    False\n+    left: Node | None = None\n+    right: Node | None = None\n \n-    >>> is_binary_search_tree(TreeNode(data='a',\n-    ...                                left=TreeNode(data=1),\n-    ...                                right=TreeNode(data=4, left=TreeNode(data=3)))\n-    ...                      )\n-    Traceback (most recent call last):\n-     ...\n-    ValueError: Each node should be type of TreeNode and data should be float.\n-\n-    >>> is_binary_search_tree(TreeNode(data=2,\n-    ...                                left=TreeNode([]),\n-    ...                                right=TreeNode(data=4, left=TreeNode(data=3)))\n-    ...                                )\n-    Traceback (most recent call last):\n-     ...\n-    ValueError: Each node should be type of TreeNode and data should be float.\n-    \"\"\"\n-\n-    # Validation\n-    def is_valid_tree(node: TreeNode | None) -> bool:\n+    def __iter__(self) -> Iterator[float]:\n         \"\"\"\n-        >>> is_valid_tree(None)\n-        True\n-        >>> is_valid_tree('abc')\n-        False\n-        >>> is_valid_tree(TreeNode(data='not a float'))\n-        False\n-        >>> is_valid_tree(TreeNode(data=1, left=TreeNode('123')))\n-        False\n+        >>> root = Node(data=2.1)\n+        >>> list(root)\n+        [2.1]\n+        >>> root.left=Node(data=2.0)\n+        >>> list(root)\n+        [2.0, 2.1]\n+        >>> root.right=Node(data=2.2)\n+        >>> list(root)\n+        [2.0, 2.1, 2.2]\n         \"\"\"\n-        if node is None:\n-            return True\n-\n-        if not isinstance(node, TreeNode):\n-            return False\n-\n-        try:\n-            float(node.data)\n-        except (TypeError, ValueError):\n-            return False\n-\n-        return is_valid_tree(node.left) and is_valid_tree(node.right)\n-\n-    if not is_valid_tree(root):\n-        raise ValueError(\n-            \"Each node should be type of TreeNode and data should be float.\"\n-        )\n-\n-    def is_binary_search_tree_recursive_check(\n-        node: TreeNode | None, left_bound: float, right_bound: float\n-    ) -> bool:\n+        if self.left:\n+            yield from self.left\n+        yield self.data\n+        if self.right:\n+            yield from self.right\n+\n+    @property\n+    def is_sorted(self) -> bool:\nComment: The name of this function no longer matches the name of the file, and that might confuse some readers. I feel that either this function should be renamed to `is_bst` or the file should be renamed to `is_sorted_binary_tree`.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "data_structures/binary_tree/is_bst.py",
    "pr_number": 10627,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1365731163,
    "comment_created_at": "2023-10-19T15:23:47Z"
  },
  {
    "code": "@@ -22,40 +22,28 @@ def solve_linear_system(matrix: np.ndarray) -> np.ndarray:\n     >>> solution = solve_linear_system(np.column_stack((A, B)))\n     >>> np.allclose(solution, np.array([2., 3., -1.]))\n     True\n-    >>> solve_linear_system(np.array([[0, 0], [0, 0]],  dtype=float))\n-    array([nan, nan])\n+    >>> solve_linear_system(np.array([[0, 0, 0], [0, 0, 0]],  dtype=float))\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix is not correct\n     \"\"\"\n     ab = np.copy(matrix)\n     num_of_rows = ab.shape[0]\n     num_of_columns = ab.shape[1] - 1\n     x_lst: list[float] = []\n \n-    # Lead element search\n-    for column_num in range(num_of_rows):\n-        for i in range(column_num, num_of_columns):\n-            if abs(ab[i][column_num]) > abs(ab[column_num][column_num]):\n-                ab[[column_num, i]] = ab[[i, column_num]]\n-                if ab[column_num, column_num] == 0.0:\n-                    raise ValueError(\"Matrix is not correct\")\n-            else:\n-                pass\n-        if column_num != 0:\n-            for i in range(column_num, num_of_rows):\n-                ab[i, :] -= (\n-                    ab[i, column_num - 1]\n-                    / ab[column_num - 1, column_num - 1]\n-                    * ab[column_num - 1, :]\n-                )\n+    assert num_of_rows == num_of_columns\n \n-    # Upper triangular matrix\n     for column_num in range(num_of_rows):\n+        # Lead element search\n         for i in range(column_num, num_of_columns):\n             if abs(ab[i][column_num]) > abs(ab[column_num][column_num]):\n                 ab[[column_num, i]] = ab[[i, column_num]]\n-                if ab[column_num, column_num] == 0.0:\n-                    raise ValueError(\"Matrix is not correct\")\n-            else:\n-                pass\n+\n+        # Upper triangular matrix\n+        if ab[column_num, column_num] == 0.0:\n+            raise ValueError(\"Matrix is not correct\")",
    "comment": "can we have the error say \"matrix is singular\" instead? \"not correct\" is very vague. (this will need to be updated in the function docstring as well.)",
    "line_number": 45,
    "enriched": "File: linear_algebra/src/gaussian_elimination_pivoting.py\nCode: @@ -22,40 +22,28 @@ def solve_linear_system(matrix: np.ndarray) -> np.ndarray:\n     >>> solution = solve_linear_system(np.column_stack((A, B)))\n     >>> np.allclose(solution, np.array([2., 3., -1.]))\n     True\n-    >>> solve_linear_system(np.array([[0, 0], [0, 0]],  dtype=float))\n-    array([nan, nan])\n+    >>> solve_linear_system(np.array([[0, 0, 0], [0, 0, 0]],  dtype=float))\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix is not correct\n     \"\"\"\n     ab = np.copy(matrix)\n     num_of_rows = ab.shape[0]\n     num_of_columns = ab.shape[1] - 1\n     x_lst: list[float] = []\n \n-    # Lead element search\n-    for column_num in range(num_of_rows):\n-        for i in range(column_num, num_of_columns):\n-            if abs(ab[i][column_num]) > abs(ab[column_num][column_num]):\n-                ab[[column_num, i]] = ab[[i, column_num]]\n-                if ab[column_num, column_num] == 0.0:\n-                    raise ValueError(\"Matrix is not correct\")\n-            else:\n-                pass\n-        if column_num != 0:\n-            for i in range(column_num, num_of_rows):\n-                ab[i, :] -= (\n-                    ab[i, column_num - 1]\n-                    / ab[column_num - 1, column_num - 1]\n-                    * ab[column_num - 1, :]\n-                )\n+    assert num_of_rows == num_of_columns\n \n-    # Upper triangular matrix\n     for column_num in range(num_of_rows):\n+        # Lead element search\n         for i in range(column_num, num_of_columns):\n             if abs(ab[i][column_num]) > abs(ab[column_num][column_num]):\n                 ab[[column_num, i]] = ab[[i, column_num]]\n-                if ab[column_num, column_num] == 0.0:\n-                    raise ValueError(\"Matrix is not correct\")\n-            else:\n-                pass\n+\n+        # Upper triangular matrix\n+        if ab[column_num, column_num] == 0.0:\n+            raise ValueError(\"Matrix is not correct\")\nComment: Can we have the error say \"Matrix is singular\" instead? \"Not correct\" is very vague. (This will need to be updated in the function docstring as well.)",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "linear_algebra/src/gaussian_elimination_pivoting.py",
    "pr_number": 11393,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1730446141,
    "comment_created_at": "2024-08-25T22:49:23Z"
  },
  {
    "code": "@@ -0,0 +1,23 @@\n+### \u2705 **Python Code Conventions (PEP 8 Overview)**\n+\n+1. **Indentation**: Use 4 spaces per indentation level.\n+2. **Line Length**: Limit lines to 79 characters.\n+3. **Blank Lines**: Use blank lines to separate functions and classes.\n+4. **Imports**:\n+\n+   * Standard libraries first, third-party next, then local.\n+   * One import per line.\n+5. **Naming Conventions**:\n+\n+   * `snake_case` for variables and functions.\n+   * `CamelCase` for classes.\n+   * `UPPER_CASE` for constants.\n+6. **Whitespace**:\n+\n+   * Avoid extra spaces inside parentheses, brackets, or before commas.\n+7. **Comments**:\n+\n+   * Use inline comments sparingly.\n+   * Use docstrings (`\"\"\"Example\"\"\"`) for modules, functions, classes.\n+8. **Python version**:\n+   * use python 3.5.2",
    "comment": "seriously?!?",
    "line_number": 23,
    "enriched": "File: rule.txt\nCode: @@ -0,0 +1,23 @@\n+### \u2705 **Python Code Conventions (PEP 8 Overview)**\n+\n+1. **Indentation**: Use 4 spaces per indentation level.\n+2. **Line Length**: Limit lines to 79 characters.\n+3. **Blank Lines**: Use blank lines to separate functions and classes.\n+4. **Imports**:\n+\n+   * Standard libraries first, third-party next, then local.\n+   * One import per line.\n+5. **Naming Conventions**:\n+\n+   * `snake_case` for variables and functions.\n+   * `CamelCase` for classes.\n+   * `UPPER_CASE` for constants.\n+6. **Whitespace**:\n+\n+   * Avoid extra spaces inside parentheses, brackets, or before commas.\n+7. **Comments**:\n+\n+   * Use inline comments sparingly.\n+   * Use docstrings (`\"\"\"Example\"\"\"`) for modules, functions, classes.\n+8. **Python version**:\n+   * use python 3.5.2\nComment: Seriously?!?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "rule.txt",
    "pr_number": 12770,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2113013506,
    "comment_created_at": "2025-05-29T01:35:40Z"
  },
  {
    "code": "@@ -1,24 +1,32 @@\n-import sys\n import webbrowser\n+from sys import argv\n+from urllib.parse import quote\n \n-import requests\n from bs4 import BeautifulSoup\n-from fake_useragent import UserAgent\n+from requests import get\n \n if __name__ == \"__main__\":\n+    if len(argv) > 1:\n+        query = \"%20\".join(argv[1:])\n+    else:\n+        query = quote(str(input(\"Search: \")))\n+\n     print(\"Googling.....\")\n-    url = \"https://www.google.com/search?q=\" + \" \".join(sys.argv[1:])\n-    res = requests.get(url, headers={\"UserAgent\": UserAgent().random})\n-    # res.raise_for_status()\n-    with open(\"project1a.html\", \"wb\") as out_file:  # only for knowing the class\n-        for data in res.iter_content(10000):\n-            out_file.write(data)\n-    soup = BeautifulSoup(res.text, \"html.parser\")\n-    links = list(soup.select(\".eZt8xd\"))[:5]\n \n-    print(len(links))\n-    for link in links:\n-        if link.text == \"Maps\":\n-            webbrowser.open(link.get(\"href\"))\n-        else:\n-            webbrowser.open(f\"http://google.com{link.get('href')}\")\n+    url = f\"https://www.google.com/search?q={query}&num=2\"\n+\n+    res = get(\n+        url,\n+        headers={\n+            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\"",
    "comment": "tests are failing on\r\n* web_programming/crawl_google_results.py:21:89: e501 line too long (106 > 88 characters)\r\n\r\nwhy is this version an improvement on the original?",
    "line_number": 21,
    "enriched": "File: web_programming/crawl_google_results.py\nCode: @@ -1,24 +1,32 @@\n-import sys\n import webbrowser\n+from sys import argv\n+from urllib.parse import quote\n \n-import requests\n from bs4 import BeautifulSoup\n-from fake_useragent import UserAgent\n+from requests import get\n \n if __name__ == \"__main__\":\n+    if len(argv) > 1:\n+        query = \"%20\".join(argv[1:])\n+    else:\n+        query = quote(str(input(\"Search: \")))\n+\n     print(\"Googling.....\")\n-    url = \"https://www.google.com/search?q=\" + \" \".join(sys.argv[1:])\n-    res = requests.get(url, headers={\"UserAgent\": UserAgent().random})\n-    # res.raise_for_status()\n-    with open(\"project1a.html\", \"wb\") as out_file:  # only for knowing the class\n-        for data in res.iter_content(10000):\n-            out_file.write(data)\n-    soup = BeautifulSoup(res.text, \"html.parser\")\n-    links = list(soup.select(\".eZt8xd\"))[:5]\n \n-    print(len(links))\n-    for link in links:\n-        if link.text == \"Maps\":\n-            webbrowser.open(link.get(\"href\"))\n-        else:\n-            webbrowser.open(f\"http://google.com{link.get('href')}\")\n+    url = f\"https://www.google.com/search?q={query}&num=2\"\n+\n+    res = get(\n+        url,\n+        headers={\n+            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\"\nComment: Tests are failing on\r\n* `web_programming/crawl_google_results.py:21:89: E501 line too long (106 > 88 characters)`\r\n\r\nWhy is this version an improvement on the original?\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "web_programming/crawl_google_results.py",
    "pr_number": 7085,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994209224,
    "comment_created_at": "2022-10-13T06:27:47Z"
  },
  {
    "code": "@@ -1,7 +1,7 @@\n from itertools import combinations\n \n \n-def find_triplets_with_0_sum(nums: list[int]) -> list[list[int]]:\n+def find_triplets_with_0_sum(nums: list) -> list:",
    "comment": "# why?!?",
    "line_number": 4,
    "enriched": "File: data_structures/arrays/find_triplets_with_0_sum.py\nCode: @@ -1,7 +1,7 @@\n from itertools import combinations\n \n \n-def find_triplets_with_0_sum(nums: list[int]) -> list[list[int]]:\n+def find_triplets_with_0_sum(nums: list) -> list:\nComment: # Why?!?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "data_structures/arrays/find_triplets_with_0_sum.py",
    "pr_number": 11134,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1382505594,
    "comment_created_at": "2023-11-05T04:41:02Z"
  },
  {
    "code": "@@ -3,12 +3,20 @@\n \n \n def stock_price(symbol: str = \"AAPL\") -> str:\n-    url = f\"https://in.finance.yahoo.com/quote/{symbol}?s={symbol}\"\n-    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n-    class_ = \"My(6px) Pos(r) smartphone_Mt(6px)\"\n-    return soup.find(\"div\", class_=class_).find(\"span\").text\n+    url = f\"https://finance.yahoo.com/quote/{symbol}?p={symbol}\"\n+    yahoo_finance_source = requests.get(url, headers={\"USER-AGENT\": \"Mozilla/5.0\"}).text\n+    soup = BeautifulSoup(yahoo_finance_source, \"html.parser\")\n+    specific_fin_streamer_tag = soup.find(\"fin-streamer\", {\"data-test\": \"qsp-price\"})\n \n+    if specific_fin_streamer_tag:\n+        text = specific_fin_streamer_tag.get_text()\n+        return text\n+    else:\n+        print(\"No <fin-streamer> tag with the specified data-test attribute found.\")\n+    return \"Not Found\"",
    "comment": "also, why both print and return? why not just return the error message? or print the error message and return none?",
    "line_number": 16,
    "enriched": "File: web_programming/current_stock_price.py\nCode: @@ -3,12 +3,20 @@\n \n \n def stock_price(symbol: str = \"AAPL\") -> str:\n-    url = f\"https://in.finance.yahoo.com/quote/{symbol}?s={symbol}\"\n-    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n-    class_ = \"My(6px) Pos(r) smartphone_Mt(6px)\"\n-    return soup.find(\"div\", class_=class_).find(\"span\").text\n+    url = f\"https://finance.yahoo.com/quote/{symbol}?p={symbol}\"\n+    yahoo_finance_source = requests.get(url, headers={\"USER-AGENT\": \"Mozilla/5.0\"}).text\n+    soup = BeautifulSoup(yahoo_finance_source, \"html.parser\")\n+    specific_fin_streamer_tag = soup.find(\"fin-streamer\", {\"data-test\": \"qsp-price\"})\n \n+    if specific_fin_streamer_tag:\n+        text = specific_fin_streamer_tag.get_text()\n+        return text\n+    else:\n+        print(\"No <fin-streamer> tag with the specified data-test attribute found.\")\n+    return \"Not Found\"\nComment: ```suggestion\r\n    if specific_fin_streamer_tag:\r\n        text = specific_fin_streamer_tag.get_text()\r\n        return text\r\n    \r\n    print(\"No <fin-streamer> tag with the specified data-test attribute found.\")\r\n    return \"Not Found\"\r\n```\r\n\r\nAlso, why both print and return? Why not just return the error message? Or print the error message and return None?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "web_programming/current_stock_price.py",
    "pr_number": 8942,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1290839887,
    "comment_created_at": "2023-08-11T02:31:02Z"
  },
  {
    "code": "@@ -0,0 +1,76 @@\n+\"\"\"\n+Performs multiplication of two binary strings using the Karatsuba algorithm.\n+\n+Given two binary strings of equal length n, the goal is to compute\n+their product as integers.\n+\n+For example:\n+\"1100\" (12) \u00d7 \"1010\" (10) = 120\n+\n+Karatsuba's algorithm reduces the multiplication of two n-bit numbers\n+to at most three multiplications of n/2-bit numbers. It achieves\n+a time complexity of O(n^log\u20823) \u2248 O(n^1.585), which is faster\n+than the naive O(n\u00b2) approach.\n+\n+References:\n+https://en.wikipedia.org/wiki/Karatsuba_algorithm\n+\n+Example:\n+>>> karatsuba_multiply(\"1100\", \"1010\")\n+120\n+>>> karatsuba_multiply(\"10\", \"11\")\n+6\n+>>> karatsuba_multiply(\"101\", \"111\")\n+35\n+>>> karatsuba_multiply(\"0\", \"0\")\n+0\n+>>> karatsuba_multiply(\"1\", \"0\")\n+0\n+\"\"\"\n+\n+def karatsuba_multiply(x: str, y: str) -> int:",
    "comment": "please provide descriptive name for the parameter: x\n\nplease provide descriptive name for the parameter: y",
    "line_number": 31,
    "enriched": "File: divide_and_conquer/karatsuba_multiplication.py\nCode: @@ -0,0 +1,76 @@\n+\"\"\"\n+Performs multiplication of two binary strings using the Karatsuba algorithm.\n+\n+Given two binary strings of equal length n, the goal is to compute\n+their product as integers.\n+\n+For example:\n+\"1100\" (12) \u00d7 \"1010\" (10) = 120\n+\n+Karatsuba's algorithm reduces the multiplication of two n-bit numbers\n+to at most three multiplications of n/2-bit numbers. It achieves\n+a time complexity of O(n^log\u20823) \u2248 O(n^1.585), which is faster\n+than the naive O(n\u00b2) approach.\n+\n+References:\n+https://en.wikipedia.org/wiki/Karatsuba_algorithm\n+\n+Example:\n+>>> karatsuba_multiply(\"1100\", \"1010\")\n+120\n+>>> karatsuba_multiply(\"10\", \"11\")\n+6\n+>>> karatsuba_multiply(\"101\", \"111\")\n+35\n+>>> karatsuba_multiply(\"0\", \"0\")\n+0\n+>>> karatsuba_multiply(\"1\", \"0\")\n+0\n+\"\"\"\n+\n+def karatsuba_multiply(x: str, y: str) -> int:\nComment: Please provide descriptive name for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `y`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "divide_and_conquer/karatsuba_multiplication.py",
    "pr_number": 13298,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2409295433,
    "comment_created_at": "2025-10-07T04:17:37Z"
  },
  {
    "code": "@@ -11,24 +11,35 @@\n \"\"\"\n \n \n-def heapify(unsorted, index, heap_size):\n+def heapify(unsorted_list: list[int], index: int, heap_size: int) -> None:\n+    \"\"\"\n+\n+    :param unsorted_list: unsorted list containing integers numbers\n+    :param index: index\n+    :param heap_size: size of the heap\n+    :return: None\n+    \"\"\"",
    "comment": "could you add some doctests for this function?",
    "line_number": 21,
    "enriched": "File: sorts/heap_sort.py\nCode: @@ -11,24 +11,35 @@\n \"\"\"\n \n \n-def heapify(unsorted, index, heap_size):\n+def heapify(unsorted_list: list[int], index: int, heap_size: int) -> None:\n+    \"\"\"\n+\n+    :param unsorted_list: unsorted list containing integers numbers\n+    :param index: index\n+    :param heap_size: size of the heap\n+    :return: None\n+    \"\"\"\nComment: Could you add some doctests for this function?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "sorts/heap_sort.py",
    "pr_number": 9949,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359599096,
    "comment_created_at": "2023-10-14T19:06:02Z"
  },
  {
    "code": "@@ -17,32 +17,52 @@ def evaluate_postfix(postfix_notation: list) -> int:\n     9\n     >>> evaluate_postfix([\"4\", \"13\", \"5\", \"/\", \"+\"])\n     6\n+    >>> evaluate_postfix([\"2\", \"+\"])\n+    2\n+    >>> evaluate_postfix([\"5\", \"-\"])\n+    -5\n     >>> evaluate_postfix([])\n     0\n     \"\"\"\n     if not postfix_notation:\n         return 0\n \n     operations = {\"+\", \"-\", \"*\", \"/\"}\n+    unary_operations = {\"+\"}  # Unary operator(s)",
    "comment": "unary_operations is not currently being used.",
    "line_number": 31,
    "enriched": "File: data_structures/stacks/evaluate_postfix_notations.py\nCode: @@ -17,32 +17,52 @@ def evaluate_postfix(postfix_notation: list) -> int:\n     9\n     >>> evaluate_postfix([\"4\", \"13\", \"5\", \"/\", \"+\"])\n     6\n+    >>> evaluate_postfix([\"2\", \"+\"])\n+    2\n+    >>> evaluate_postfix([\"5\", \"-\"])\n+    -5\n     >>> evaluate_postfix([])\n     0\n     \"\"\"\n     if not postfix_notation:\n         return 0\n \n     operations = {\"+\", \"-\", \"*\", \"/\"}\n+    unary_operations = {\"+\"}  # Unary operator(s)\nComment: `unary_operations` is not currently being used.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "data_structures/stacks/evaluate_postfix_notations.py",
    "pr_number": 8758,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1208643279,
    "comment_created_at": "2023-05-28T20:18:14Z"
  },
  {
    "code": "@@ -0,0 +1,137 @@\n+\"\"\"\n+\n+\n+Source:\n+- https://courses.lumenlearning.com/suny-osuniversityphysics/chapter/14-6-bernoullis-equation/\n+\"\"\"\n+\n+# Acceleration Constant on Earth (unit m/s^2)\n+g = 9.80665\n+\n+\n+def bernoulli_static_equation(\n+    fluid_density: float, h: float, gravity: float = g, initial_pressure: float = 0",
    "comment": "please provide descriptive name for the parameter: h",
    "line_number": 13,
    "enriched": "File: physics/bernoullis_equation.py\nCode: @@ -0,0 +1,137 @@\n+\"\"\"\n+\n+\n+Source:\n+- https://courses.lumenlearning.com/suny-osuniversityphysics/chapter/14-6-bernoullis-equation/\n+\"\"\"\n+\n+# Acceleration Constant on Earth (unit m/s^2)\n+g = 9.80665\n+\n+\n+def bernoulli_static_equation(\n+    fluid_density: float, h: float, gravity: float = g, initial_pressure: float = 0\nComment: Please provide descriptive name for the parameter: `h`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "physics/bernoullis_equation.py",
    "pr_number": 7200,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996237299,
    "comment_created_at": "2022-10-15T02:01:06Z"
  },
  {
    "code": "@@ -1,17 +1,23 @@\n-def bubble_sort(collection):\n-    \"\"\"Pure implementation of bubble sort algorithm in Python\n-\n+#!/usr/bin/python3\n+def bubble_sort(collection: list, isascending: bool = True):\n+    \"\"\"\n+    This Is a Pure implementation of bubble sort algorithm in Python\n     :param collection: some mutable ordered collection with heterogeneous\n     comparable items inside\n-    :return: the same collection ordered by ascending\n+    :param isAccending: a boolean that determines if the output should be\\\n+    Sorted in an Ascending or Descending manner (Defaults to Acceding\\\n+    if not specified)\n+    :return: the same collection ordered by the isAccending param\n \n     Examples:\n     >>> bubble_sort([0, 5, 2, 3, 2])\n     [0, 2, 2, 3, 5]\n-    >>> bubble_sort([0, 5, 2, 3, 2]) == sorted([0, 5, 2, 3, 2])\n-    True\n-    >>> bubble_sort([]) == sorted([])",
    "comment": "why delete this test???",
    "line_number": 13,
    "enriched": "File: sorts/bubble_sort.py\nCode: @@ -1,17 +1,23 @@\n-def bubble_sort(collection):\n-    \"\"\"Pure implementation of bubble sort algorithm in Python\n-\n+#!/usr/bin/python3\n+def bubble_sort(collection: list, isascending: bool = True):\n+    \"\"\"\n+    This Is a Pure implementation of bubble sort algorithm in Python\n     :param collection: some mutable ordered collection with heterogeneous\n     comparable items inside\n-    :return: the same collection ordered by ascending\n+    :param isAccending: a boolean that determines if the output should be\\\n+    Sorted in an Ascending or Descending manner (Defaults to Acceding\\\n+    if not specified)\n+    :return: the same collection ordered by the isAccending param\n \n     Examples:\n     >>> bubble_sort([0, 5, 2, 3, 2])\n     [0, 2, 2, 3, 5]\n-    >>> bubble_sort([0, 5, 2, 3, 2]) == sorted([0, 5, 2, 3, 2])\n-    True\n-    >>> bubble_sort([]) == sorted([])\nComment: Why delete this test???",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "sorts/bubble_sort.py",
    "pr_number": 8816,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1233336002,
    "comment_created_at": "2023-06-18T16:34:56Z"
  },
  {
    "code": "@@ -0,0 +1,438 @@\n+\"\"\"\n+Created on Mon Feb 26 14:29:11 2018\n+\n+@author: Christian Bender\n+@license: MIT-license\n+\n+This module contains some useful classes and functions for dealing\n+with linear algebra in python.\n+\n+Overview:\n+\n+- class Vector\n+- function zero_vector(dimension)\n+- function unit_basis_vector(dimension, pos)\n+- function axpy(scalar, vector1, vector2)\n+- function random_vector(N, a, b)\n+- class Matrix\n+- function square_zero_matrix(N)\n+- function random_matrix(W, H, a, b)\n+\"\"\"\n+from __future__ import annotations\n+\n+import math\n+import random\n+from collections.abc import Collection\n+from typing import overload\n+\n+\n+class Vector:\n+    \"\"\"\n+    This class represents a vector of arbitrary size.\n+    You need to give the vector components.\n+\n+    Overview of the methods:\n+\n+        __init__(components: Collection[float] | None): init the vector\n+        __len__(): gets the size of the vector (number of components)\n+        __str__(): returns a string representation\n+        __add__(other: Vector): vector addition\n+        __sub__(other: Vector): vector subtraction\n+        __mul__(other: float): scalar multiplication\n+        __mul__(other: Vector): dot product\n+        copy(): copies this vector and returns it\n+        component(i): gets the i-th component (0-indexed)\n+        change_component(pos: int, value: float): changes specified component\n+        euclidean_length(): returns the euclidean length of the vector\n+        angle(other: Vector, deg: bool): returns the angle between two vectors\n+        TODO: compare-operator\n+    \"\"\"\n+\n+    def __init__(self, components: Collection[float] | None = None) -> None:\n+        \"\"\"\n+        input: components or nothing\n+        simple constructor for init the vector\n+        \"\"\"\n+        if components is None:\n+            components = []\n+        self.__components = list(components)\n+\n+    def __len__(self) -> int:\n+        \"\"\"\n+        returns the size of the vector\n+        \"\"\"\n+        return len(self.__components)\n+\n+    def __str__(self) -> str:\n+        \"\"\"\n+        returns a string representation of the vector\n+        \"\"\"\n+        return \"(\" + \",\".join(map(str, self.__components)) + \")\"\n+\n+    def __add__(self, other: Vector) -> Vector:\n+        \"\"\"\n+        input: other vector\n+        assumes: other vector has the same size\n+        returns a new vector that represents the sum.\n+        \"\"\"\n+        size = len(self)\n+        if size == len(other):\n+            result = [self.__components[i] + other.component(i) for i in range(size)]\n+            return Vector(result)\n+        else:\n+            raise Exception(\"must have the same size\")\n+\n+    def __sub__(self, other: Vector) -> Vector:\n+        \"\"\"\n+        input: other vector\n+        assumes: other vector has the same size\n+        returns a new vector that represents the difference.\n+        \"\"\"\n+        size = len(self)\n+        if size == len(other):\n+            result = [self.__components[i] - other.component(i) for i in range(size)]\n+            return Vector(result)\n+        else:  # error case\n+            raise Exception(\"must have the same size\")\n+\n+    @overload\n+    def __mul__(self, other: float) -> Vector:\n+        ...\n+\n+    @overload\n+    def __mul__(self, other: Vector) -> float:\n+        ...\n+\n+    def __mul__(self, other: float | Vector) -> float | Vector:\n+        \"\"\"\n+        mul implements the scalar multiplication\n+        and the dot-product\n+        \"\"\"\n+        if isinstance(other, float) or isinstance(other, int):\n+            ans = [c * other for c in self.__components]\n+            return Vector(ans)\n+        elif isinstance(other, Vector) and len(self) == len(other):\n+            size = len(self)\n+            prods = [self.__components[i] * other.component(i) for i in range(size)]\n+            return sum(prods)\n+        else:  # error case\n+            raise Exception(\"invalid operand!\")\n+\n+    def copy(self) -> Vector:\n+        \"\"\"\n+        copies this vector and returns it.\n+        \"\"\"\n+        return Vector(self.__components)\n+\n+    def component(self, i: int) -> float:",
    "comment": "please provide descriptive name for the parameter: i",
    "line_number": 127,
    "enriched": "File: linear_algebra/lib.py\nCode: @@ -0,0 +1,438 @@\n+\"\"\"\n+Created on Mon Feb 26 14:29:11 2018\n+\n+@author: Christian Bender\n+@license: MIT-license\n+\n+This module contains some useful classes and functions for dealing\n+with linear algebra in python.\n+\n+Overview:\n+\n+- class Vector\n+- function zero_vector(dimension)\n+- function unit_basis_vector(dimension, pos)\n+- function axpy(scalar, vector1, vector2)\n+- function random_vector(N, a, b)\n+- class Matrix\n+- function square_zero_matrix(N)\n+- function random_matrix(W, H, a, b)\n+\"\"\"\n+from __future__ import annotations\n+\n+import math\n+import random\n+from collections.abc import Collection\n+from typing import overload\n+\n+\n+class Vector:\n+    \"\"\"\n+    This class represents a vector of arbitrary size.\n+    You need to give the vector components.\n+\n+    Overview of the methods:\n+\n+        __init__(components: Collection[float] | None): init the vector\n+        __len__(): gets the size of the vector (number of components)\n+        __str__(): returns a string representation\n+        __add__(other: Vector): vector addition\n+        __sub__(other: Vector): vector subtraction\n+        __mul__(other: float): scalar multiplication\n+        __mul__(other: Vector): dot product\n+        copy(): copies this vector and returns it\n+        component(i): gets the i-th component (0-indexed)\n+        change_component(pos: int, value: float): changes specified component\n+        euclidean_length(): returns the euclidean length of the vector\n+        angle(other: Vector, deg: bool): returns the angle between two vectors\n+        TODO: compare-operator\n+    \"\"\"\n+\n+    def __init__(self, components: Collection[float] | None = None) -> None:\n+        \"\"\"\n+        input: components or nothing\n+        simple constructor for init the vector\n+        \"\"\"\n+        if components is None:\n+            components = []\n+        self.__components = list(components)\n+\n+    def __len__(self) -> int:\n+        \"\"\"\n+        returns the size of the vector\n+        \"\"\"\n+        return len(self.__components)\n+\n+    def __str__(self) -> str:\n+        \"\"\"\n+        returns a string representation of the vector\n+        \"\"\"\n+        return \"(\" + \",\".join(map(str, self.__components)) + \")\"\n+\n+    def __add__(self, other: Vector) -> Vector:\n+        \"\"\"\n+        input: other vector\n+        assumes: other vector has the same size\n+        returns a new vector that represents the sum.\n+        \"\"\"\n+        size = len(self)\n+        if size == len(other):\n+            result = [self.__components[i] + other.component(i) for i in range(size)]\n+            return Vector(result)\n+        else:\n+            raise Exception(\"must have the same size\")\n+\n+    def __sub__(self, other: Vector) -> Vector:\n+        \"\"\"\n+        input: other vector\n+        assumes: other vector has the same size\n+        returns a new vector that represents the difference.\n+        \"\"\"\n+        size = len(self)\n+        if size == len(other):\n+            result = [self.__components[i] - other.component(i) for i in range(size)]\n+            return Vector(result)\n+        else:  # error case\n+            raise Exception(\"must have the same size\")\n+\n+    @overload\n+    def __mul__(self, other: float) -> Vector:\n+        ...\n+\n+    @overload\n+    def __mul__(self, other: Vector) -> float:\n+        ...\n+\n+    def __mul__(self, other: float | Vector) -> float | Vector:\n+        \"\"\"\n+        mul implements the scalar multiplication\n+        and the dot-product\n+        \"\"\"\n+        if isinstance(other, float) or isinstance(other, int):\n+            ans = [c * other for c in self.__components]\n+            return Vector(ans)\n+        elif isinstance(other, Vector) and len(self) == len(other):\n+            size = len(self)\n+            prods = [self.__components[i] * other.component(i) for i in range(size)]\n+            return sum(prods)\n+        else:  # error case\n+            raise Exception(\"invalid operand!\")\n+\n+    def copy(self) -> Vector:\n+        \"\"\"\n+        copies this vector and returns it.\n+        \"\"\"\n+        return Vector(self.__components)\n+\n+    def component(self, i: int) -> float:\nComment: Please provide descriptive name for the parameter: `i`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "linear_algebra/lib.py",
    "pr_number": 7950,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1012201216,
    "comment_created_at": "2022-11-02T19:15:37Z"
  },
  {
    "code": "@@ -0,0 +1,74 @@\n+\"\"\"\n+Given an array of integers and an integer k, find the kth largest element in the array\n+\n+https://stackoverflow.com/questions/251781/how-to-find-the-kth-largest-element-in-an-unsorted-array-of-length-n-in-on\n+\"\"\"\n+\n+\n+def partition(arr, low, high):",
    "comment": "no type hints.\r\ncan we come up with a more self-documenting name than arr?",
    "line_number": 8,
    "enriched": "File: data_structures/arrays/kth_largest_element.py\nCode: @@ -0,0 +1,74 @@\n+\"\"\"\n+Given an array of integers and an integer k, find the kth largest element in the array\n+\n+https://stackoverflow.com/questions/251781/how-to-find-the-kth-largest-element-in-an-unsorted-array-of-length-n-in-on\n+\"\"\"\n+\n+\n+def partition(arr, low, high):\nComment: No type hints.\r\nCan we come up with a more self-documenting name than `arr`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "data_structures/arrays/kth_largest_element.py",
    "pr_number": 10687,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368616542,
    "comment_created_at": "2023-10-23T12:53:30Z"
  },
  {
    "code": "@@ -74,19 +74,12 @@ We want your work to be readable by others; therefore, we encourage you to note\n \n - We encourage the use of Python [f-strings](https://realpython.com/python-f-strings/#f-strings-a-new-and-improved-way-to-format-strings-in-python) where they make the code easier to read.\n \n-- Please consider running [__psf/black__](https://github.com/python/black) on your Python file(s) before submitting your pull request.  This is not yet a requirement but it does make your code more readable and automatically aligns it with much of [PEP 8](https://www.python.org/dev/peps/pep-0008/). There are other code formatters (autopep8, yapf) but the __black__ formatter is now hosted by the Python Software Foundation. To use it,\n-\n-  ```bash\n-  python3 -m pip install black  # only required the first time\n-  black .\n-  ```\n-\n-- All submissions will need to pass the test `flake8 . --ignore=E203,W503 --max-line-length=88` before they will be accepted so if possible, try this test locally on your Python file(s) before submitting your pull request.\n-\n-  ```bash\n-  python3 -m pip install flake8  # only required the first time\n-  flake8 . --ignore=E203,W503  --max-line-length=88 --show-source\n-  ```\n+- Do not forget that running `pre-commit` checks locally will help you to follow code quality standards.",
    "comment": "this is indecipherable for first-time contributors.  we would need a link to pre-commit instructions on how to install and initialized that tool.  i think that will be beyond the reach of many newbies.",
    "line_number": 77,
    "enriched": "File: CONTRIBUTING.md\nCode: @@ -74,19 +74,12 @@ We want your work to be readable by others; therefore, we encourage you to note\n \n - We encourage the use of Python [f-strings](https://realpython.com/python-f-strings/#f-strings-a-new-and-improved-way-to-format-strings-in-python) where they make the code easier to read.\n \n-- Please consider running [__psf/black__](https://github.com/python/black) on your Python file(s) before submitting your pull request.  This is not yet a requirement but it does make your code more readable and automatically aligns it with much of [PEP 8](https://www.python.org/dev/peps/pep-0008/). There are other code formatters (autopep8, yapf) but the __black__ formatter is now hosted by the Python Software Foundation. To use it,\n-\n-  ```bash\n-  python3 -m pip install black  # only required the first time\n-  black .\n-  ```\n-\n-- All submissions will need to pass the test `flake8 . --ignore=E203,W503 --max-line-length=88` before they will be accepted so if possible, try this test locally on your Python file(s) before submitting your pull request.\n-\n-  ```bash\n-  python3 -m pip install flake8  # only required the first time\n-  flake8 . --ignore=E203,W503  --max-line-length=88 --show-source\n-  ```\n+- Do not forget that running `pre-commit` checks locally will help you to follow code quality standards.\nComment: This is indecipherable for first-time contributors.  We would need a link to pre-commit instructions on how to install and initialized that tool.  I think that will be beyond the reach of many newbies.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "CONTRIBUTING.md",
    "pr_number": 7762,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008696288,
    "comment_created_at": "2022-10-29T12:51:22Z"
  },
  {
    "code": "@@ -0,0 +1,64 @@\n+class MaximalBipartiteMatching:\n+    \"\"\"\n+    This class implements finding maximal Bipartite matching using DFS.\n+\n+    Usage:\n+    >>> bipartite_graph = [\n+    ...     [0, 1, 1, 0, 0, 0],\n+    ...     [1, 0, 0, 1, 0, 0],\n+    ...     [0, 0, 1, 0, 0, 0],\n+    ...     [0, 0, 1, 1, 0, 0],\n+    ...     [0, 0, 0, 0, 0, 0],\n+    ...     [0, 0, 0, 0, 0, 1]",
    "comment": "we should need to add a couple more doctests ?",
    "line_number": 11,
    "enriched": "File: graphs/maximum_bipartite_matching.py\nCode: @@ -0,0 +1,64 @@\n+class MaximalBipartiteMatching:\n+    \"\"\"\n+    This class implements finding maximal Bipartite matching using DFS.\n+\n+    Usage:\n+    >>> bipartite_graph = [\n+    ...     [0, 1, 1, 0, 0, 0],\n+    ...     [1, 0, 0, 1, 0, 0],\n+    ...     [0, 0, 1, 0, 0, 0],\n+    ...     [0, 0, 1, 1, 0, 0],\n+    ...     [0, 0, 0, 0, 0, 0],\n+    ...     [0, 0, 0, 0, 0, 1]\nComment: We should need to add a couple more doctests ? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "graphs/maximum_bipartite_matching.py",
    "pr_number": 11165,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1398582910,
    "comment_created_at": "2023-11-20T02:02:40Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+\"\"\"\n+Categorical Cross-Entropy Loss\n+\n+This function calculates the Categorical Cross-Entropy Loss between true class\n+labels and predicted class probabilities.\n+\n+Formula:\n+Categorical Cross-Entropy Loss = -\u03a3(y_true * log(y_pred))\n+\n+Resources:\n+- [Wikipedia - Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def categorical_crossentropy(\n+    y_true: np.ndarray, y_pred: np.ndarray, epsilon: float = 1e-15\n+) -> float:\n+    \"\"\"\n+    Calculate Categorical Cross-Entropy Loss between true class labels and\n+    predicted class probabilities.\n+\n+    Parameters:\n+    - y_true: True class labels (one-hot encoded) as a NumPy array.\n+    - y_pred: Predicted class probabilities as a NumPy array.\n+    - epsilon: Small constant to avoid numerical instability.\n+\n+    Returns:\n+    - ce_loss: Categorical Cross-Entropy Loss as a floating-point number.\n+\n+    Example:\n+    >>> true_labels = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+    >>> pred_probs = np.array([[0.9, 0.1, 0.0], [0.2, 0.7, 0.1], [0.0, 0.1, 0.9]])\n+    >>> categorical_crossentropy(true_labels, pred_probs)\n+    0.18913199175146167\n+\n+    >>> y_true = np.array([[1, 0], [0, 1]])\n+    >>> y_pred = np.array([[0.9, 0.1, 0.0], [0.2, 0.7, 0.1]])\n+    >>> categorical_crossentropy(y_true, y_pred)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Input arrays must have the same length.\n+    \"\"\"\n+    if y_true.shape != y_pred.shape:\n+        raise ValueError(\"Input arrays must have the same length.\")\n+\n+    # Clip predicted probabilities to avoid log(0)\n+    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n+\n+    # Calculate categorical cross-entropy loss\n+    return -np.sum(y_true * np.log(y_pred)) / len(y_true)",
    "comment": "why the / len(true)? wouldn't that make it the _mean_ categorical cross-entropy loss?",
    "line_number": 52,
    "enriched": "File: machine_learning/loss_functions/categorical_cross_entropy.py\nCode: @@ -0,0 +1,58 @@\n+\"\"\"\n+Categorical Cross-Entropy Loss\n+\n+This function calculates the Categorical Cross-Entropy Loss between true class\n+labels and predicted class probabilities.\n+\n+Formula:\n+Categorical Cross-Entropy Loss = -\u03a3(y_true * log(y_pred))\n+\n+Resources:\n+- [Wikipedia - Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def categorical_crossentropy(\n+    y_true: np.ndarray, y_pred: np.ndarray, epsilon: float = 1e-15\n+) -> float:\n+    \"\"\"\n+    Calculate Categorical Cross-Entropy Loss between true class labels and\n+    predicted class probabilities.\n+\n+    Parameters:\n+    - y_true: True class labels (one-hot encoded) as a NumPy array.\n+    - y_pred: Predicted class probabilities as a NumPy array.\n+    - epsilon: Small constant to avoid numerical instability.\n+\n+    Returns:\n+    - ce_loss: Categorical Cross-Entropy Loss as a floating-point number.\n+\n+    Example:\n+    >>> true_labels = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+    >>> pred_probs = np.array([[0.9, 0.1, 0.0], [0.2, 0.7, 0.1], [0.0, 0.1, 0.9]])\n+    >>> categorical_crossentropy(true_labels, pred_probs)\n+    0.18913199175146167\n+\n+    >>> y_true = np.array([[1, 0], [0, 1]])\n+    >>> y_pred = np.array([[0.9, 0.1, 0.0], [0.2, 0.7, 0.1]])\n+    >>> categorical_crossentropy(y_true, y_pred)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Input arrays must have the same length.\n+    \"\"\"\n+    if y_true.shape != y_pred.shape:\n+        raise ValueError(\"Input arrays must have the same length.\")\n+\n+    # Clip predicted probabilities to avoid log(0)\n+    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n+\n+    # Calculate categorical cross-entropy loss\n+    return -np.sum(y_true * np.log(y_pred)) / len(y_true)\nComment: Why the `/ len(true)`? Wouldn't that make it the _mean_ categorical cross-entropy loss?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "machine_learning/loss_functions/categorical_cross_entropy.py",
    "pr_number": 10152,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349917308,
    "comment_created_at": "2023-10-09T06:56:49Z"
  },
  {
    "code": "@@ -0,0 +1,8 @@\n+def is_string_palindrome(words: str) -> bool:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/is_string_palindrome.py, please provide doctest for the function is_string_palindrome",
    "line_number": 1,
    "enriched": "File: maths/is_string_palindrome.py\nCode: @@ -0,0 +1,8 @@\n+def is_string_palindrome(words: str) -> bool:\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/is_string_palindrome.py`, please provide doctest for the function `is_string_palindrome`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/is_string_palindrome.py",
    "pr_number": 12169,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1807240459,
    "comment_created_at": "2024-10-19T08:16:58Z"
  },
  {
    "code": "@@ -0,0 +1,98 @@\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\n+@dataclass\n+class TrapezoidalFuzzySet:\n+    \"\"\"\n+    Represents and manipulates trapezoidal fuzzy sets.\n+    \n+    Attributes:\n+        name: The name or label of the fuzzy set.\n+        left_base: The left base of the trapezoid.\n+        left_peak: The top left vertex of the trapezoid.\n+        right_peak: The top right vertex of the trapezoid.\n+        right_base: The right base of the trapezoid.\n+        is_complement: Indicates if this is the complement of the fuzzy set.\n+\n+    Methods:\n+        membership(value): Calculates membership value for input 'value'.\n+        complement(): Creates left_base TrapezoidalFuzzySet instance representing\n+            the complement.\n+        plot(): Plots the membership function of the fuzzy set.\n+    \"\"\"\n+\n+    name: str\n+    left_base: float\n+    left_peak: float\n+    right_peak: float\n+    right_base: float\n+    is_complement: bool = False  # Flag for complement set\n+\n+    def membership(self, value: float) -> float:\n+        \"\"\"\n+        Calculates membership value for input 'value'. For complement sets,\n+        returns 1 - trapezoidal membership.\n+\n+        >>> TrapezoidalFuzzySet(\"Medium\", 0, 1, 2, 3).membership(1)\n+        1.0\n+        >>> TrapezoidalFuzzySet(\"Medium\", 0, 1, 2, 3).membership(0.5)\n+        0.5\n+        \"\"\"\n+        if value < self.left_base or value > self.right_base:\n+            membership_value = 0.0\n+        elif self.left_base <= value < self.left_peak:\n+            membership_value = (value - self.left_base) / (self.left_peak - self.left_base)\n+        elif self.left_peak <= value <= self.right_peak:\n+            membership_value = 1.0\n+        elif self.right_peak < value <= self.right_base:\n+            membership_value = (self.right_base - value) / (self.right_base - self.right_peak)\n+        \n+        # For complements, invert the membership value\n+        return membership_value if not self.is_complement else 1 - membership_value\n+\n+    def complement(self) -> TrapezoidalFuzzySet:\n+        \"\"\"\n+        Creates a new TrapezoidalFuzzySet instance representing the complement.\n+        \n+        >>> TrapezoidalFuzzySet(\"Medium\", 0, 1, 2, 3).complement().membership(1)\n+        0.0\n+        \"\"\"\n+        return TrapezoidalFuzzySet(f\"\u00ac{self.name}\", self.left_base, self.left_peak, self.right_peak, self.right_base,\n+                                   is_complement=not self.is_complement)\n+\n+    def plot(self) -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file fuzzy_logic/trapazoidal_fuzzyset.py, please provide doctest for the function plot",
    "line_number": 68,
    "enriched": "File: fuzzy_logic/trapazoidal_fuzzyset.py\nCode: @@ -0,0 +1,98 @@\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\n+@dataclass\n+class TrapezoidalFuzzySet:\n+    \"\"\"\n+    Represents and manipulates trapezoidal fuzzy sets.\n+    \n+    Attributes:\n+        name: The name or label of the fuzzy set.\n+        left_base: The left base of the trapezoid.\n+        left_peak: The top left vertex of the trapezoid.\n+        right_peak: The top right vertex of the trapezoid.\n+        right_base: The right base of the trapezoid.\n+        is_complement: Indicates if this is the complement of the fuzzy set.\n+\n+    Methods:\n+        membership(value): Calculates membership value for input 'value'.\n+        complement(): Creates left_base TrapezoidalFuzzySet instance representing\n+            the complement.\n+        plot(): Plots the membership function of the fuzzy set.\n+    \"\"\"\n+\n+    name: str\n+    left_base: float\n+    left_peak: float\n+    right_peak: float\n+    right_base: float\n+    is_complement: bool = False  # Flag for complement set\n+\n+    def membership(self, value: float) -> float:\n+        \"\"\"\n+        Calculates membership value for input 'value'. For complement sets,\n+        returns 1 - trapezoidal membership.\n+\n+        >>> TrapezoidalFuzzySet(\"Medium\", 0, 1, 2, 3).membership(1)\n+        1.0\n+        >>> TrapezoidalFuzzySet(\"Medium\", 0, 1, 2, 3).membership(0.5)\n+        0.5\n+        \"\"\"\n+        if value < self.left_base or value > self.right_base:\n+            membership_value = 0.0\n+        elif self.left_base <= value < self.left_peak:\n+            membership_value = (value - self.left_base) / (self.left_peak - self.left_base)\n+        elif self.left_peak <= value <= self.right_peak:\n+            membership_value = 1.0\n+        elif self.right_peak < value <= self.right_base:\n+            membership_value = (self.right_base - value) / (self.right_base - self.right_peak)\n+        \n+        # For complements, invert the membership value\n+        return membership_value if not self.is_complement else 1 - membership_value\n+\n+    def complement(self) -> TrapezoidalFuzzySet:\n+        \"\"\"\n+        Creates a new TrapezoidalFuzzySet instance representing the complement.\n+        \n+        >>> TrapezoidalFuzzySet(\"Medium\", 0, 1, 2, 3).complement().membership(1)\n+        0.0\n+        \"\"\"\n+        return TrapezoidalFuzzySet(f\"\u00ac{self.name}\", self.left_base, self.left_peak, self.right_peak, self.right_base,\n+                                   is_complement=not self.is_complement)\n+\n+    def plot(self) -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `fuzzy_logic/trapazoidal_fuzzyset.py`, please provide doctest for the function `plot`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "fuzzy_logic/trapazoidal_fuzzyset.py",
    "pr_number": 12331,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1822630102,
    "comment_created_at": "2024-10-30T13:27:04Z"
  },
  {
    "code": "@@ -0,0 +1,76 @@\n+\"\"\"\n+Word Ladder is a classic problem in computer science.\n+The problem is to transform a start word into an end word\n+by changing one letter at a time.\n+Each intermediate word must be a valid word from a given list of words.\n+The goal is to find a transformation sequence\n+from the start word to the end word.\n+\n+Wikipedia: https://en.wikipedia.org/wiki/Word_ladder\n+\"\"\"\n+\n+\n+def word_ladder_backtrack(\n+    begin_word: str, end_word: str, word_list: list[str]\n+) -> list[str]:\n+    \"\"\"\n+    Solve the Word Ladder problem using Backtracking and return\n+    the list of transformations from begin_word to end_word.\n+\n+    Parameters:\n+    begin_word (str): The word from which the transformation starts.\n+    end_word (str): The target word for transformation.\n+    word_list (list[str]): The list of valid words for transformation.\n+\n+    Returns:\n+    list[str]: The list of transformations from begin_word to end_word.\n+               Returns an empty list if there is no valid transformation.\n+\n+    Example:\n+    >>> word_ladder_backtrack(\"hit\", \"cog\", [\"hot\", \"dot\", \"dog\", \"lot\", \"log\", \"cog\"])\n+    ['hit', 'hot', 'dot', 'lot', 'log', 'cog']\n+\n+    >>> word_ladder_backtrack(\"hit\", \"cog\", [\"hot\", \"dot\", \"dog\", \"lot\", \"log\"])\n+    []\n+\n+    >>> word_ladder_backtrack(\"lead\", \"gold\", [\"load\", \"goad\", \"gold\", \"lead\", \"lord\"])\n+    ['lead', 'lead', 'load', 'goad', 'gold']\n+\n+    >>> word_ladder_backtrack(\"game\", \"code\", [\"came\", \"cage\", \"code\", \"cade\", \"gave\"])\n+    ['game', 'came', 'cade', 'code']\n+    \"\"\"\n+\n+    # Step 1: Convert the word_list to a set for faster lookup\n+    word_set = set(word_list)",
    "comment": "if word_list will be immediately converted from a list to a set, why make word_set a list at all? why not just require the input word list to be a set?",
    "line_number": 44,
    "enriched": "File: backtracking/word_ladder.py\nCode: @@ -0,0 +1,76 @@\n+\"\"\"\n+Word Ladder is a classic problem in computer science.\n+The problem is to transform a start word into an end word\n+by changing one letter at a time.\n+Each intermediate word must be a valid word from a given list of words.\n+The goal is to find a transformation sequence\n+from the start word to the end word.\n+\n+Wikipedia: https://en.wikipedia.org/wiki/Word_ladder\n+\"\"\"\n+\n+\n+def word_ladder_backtrack(\n+    begin_word: str, end_word: str, word_list: list[str]\n+) -> list[str]:\n+    \"\"\"\n+    Solve the Word Ladder problem using Backtracking and return\n+    the list of transformations from begin_word to end_word.\n+\n+    Parameters:\n+    begin_word (str): The word from which the transformation starts.\n+    end_word (str): The target word for transformation.\n+    word_list (list[str]): The list of valid words for transformation.\n+\n+    Returns:\n+    list[str]: The list of transformations from begin_word to end_word.\n+               Returns an empty list if there is no valid transformation.\n+\n+    Example:\n+    >>> word_ladder_backtrack(\"hit\", \"cog\", [\"hot\", \"dot\", \"dog\", \"lot\", \"log\", \"cog\"])\n+    ['hit', 'hot', 'dot', 'lot', 'log', 'cog']\n+\n+    >>> word_ladder_backtrack(\"hit\", \"cog\", [\"hot\", \"dot\", \"dog\", \"lot\", \"log\"])\n+    []\n+\n+    >>> word_ladder_backtrack(\"lead\", \"gold\", [\"load\", \"goad\", \"gold\", \"lead\", \"lord\"])\n+    ['lead', 'lead', 'load', 'goad', 'gold']\n+\n+    >>> word_ladder_backtrack(\"game\", \"code\", [\"came\", \"cage\", \"code\", \"cade\", \"gave\"])\n+    ['game', 'came', 'cade', 'code']\n+    \"\"\"\n+\n+    # Step 1: Convert the word_list to a set for faster lookup\n+    word_set = set(word_list)\nComment: If `word_list` will be immediately converted from a list to a set, why make `word_set` a list at all? Why not just require the input word list to be a set?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "backtracking/word_ladder.py",
    "pr_number": 11590,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1782123719,
    "comment_created_at": "2024-10-01T05:16:07Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+\"\"\"\n+CatBoost Regressor Example.\n+\n+This script demonstrates the usage of the CatBoost Regressor for a simple regression task.\n+CatBoost is a powerful gradient boosting library that handles categorical features automatically\n+and is highly efficient.\n+\n+Make sure to install CatBoost using:\n+    pip install catboost\n+\n+Contributed by: @AHuzail\n+\"\"\"\n+\n+import numpy as np\n+from sklearn.datasets import load_boston\n+from sklearn.model_selection import train_test_split\n+from sklearn.metrics import mean_squared_error\n+from catboost import CatBoostRegressor\n+\n+\n+def data_handling() -> tuple:\n+    \"\"\"\n+    Loads and handles the dataset, splitting it into features and targets.\n+\n+    The Boston dataset is used as a regression example.\n+    \n+    Returns:\n+        tuple: A tuple of (features, target), where both are numpy arrays.\n+\n+    Example:\n+    >>> features, target = data_handling()\n+    >>> features.shape\n+    (506, 13)\n+    >>> target.shape\n+    (506,)\n+    \"\"\"\n+    # Load Boston dataset (note: this dataset may be deprecated, replace if needed)\n+    boston = load_boston()\n+    features = boston.data\n+    target = boston.target\n+    return features, target\n+\n+\n+def catboost_regressor(features: np.ndarray, target: np.ndarray) -> CatBoostRegressor:\n+    \"\"\"\n+    Trains a CatBoostRegressor using the provided features and target values.\n+\n+    Args:\n+        features (np.ndarray): The input features for the regression model.\n+        target (np.ndarray): The target values for the regression model.\n+\n+    Returns:\n+        CatBoostRegressor: A trained CatBoost regressor model.\n+\n+    Example:\n+    >>> features, target = data_handling()\n+    >>> model = catboost_regressor(features, target)\n+    >>> isinstance(model, CatBoostRegressor)\n+    True\n+    \"\"\"\n+    regressor = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, verbose=0)\n+    regressor.fit(features, target)\n+    return regressor\n+\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/catboost_regressor.py, please provide doctest for the function main",
    "line_number": 66,
    "enriched": "File: machine_learning/catboost_regressor.py\nCode: @@ -0,0 +1,94 @@\n+\"\"\"\n+CatBoost Regressor Example.\n+\n+This script demonstrates the usage of the CatBoost Regressor for a simple regression task.\n+CatBoost is a powerful gradient boosting library that handles categorical features automatically\n+and is highly efficient.\n+\n+Make sure to install CatBoost using:\n+    pip install catboost\n+\n+Contributed by: @AHuzail\n+\"\"\"\n+\n+import numpy as np\n+from sklearn.datasets import load_boston\n+from sklearn.model_selection import train_test_split\n+from sklearn.metrics import mean_squared_error\n+from catboost import CatBoostRegressor\n+\n+\n+def data_handling() -> tuple:\n+    \"\"\"\n+    Loads and handles the dataset, splitting it into features and targets.\n+\n+    The Boston dataset is used as a regression example.\n+    \n+    Returns:\n+        tuple: A tuple of (features, target), where both are numpy arrays.\n+\n+    Example:\n+    >>> features, target = data_handling()\n+    >>> features.shape\n+    (506, 13)\n+    >>> target.shape\n+    (506,)\n+    \"\"\"\n+    # Load Boston dataset (note: this dataset may be deprecated, replace if needed)\n+    boston = load_boston()\n+    features = boston.data\n+    target = boston.target\n+    return features, target\n+\n+\n+def catboost_regressor(features: np.ndarray, target: np.ndarray) -> CatBoostRegressor:\n+    \"\"\"\n+    Trains a CatBoostRegressor using the provided features and target values.\n+\n+    Args:\n+        features (np.ndarray): The input features for the regression model.\n+        target (np.ndarray): The target values for the regression model.\n+\n+    Returns:\n+        CatBoostRegressor: A trained CatBoost regressor model.\n+\n+    Example:\n+    >>> features, target = data_handling()\n+    >>> model = catboost_regressor(features, target)\n+    >>> isinstance(model, CatBoostRegressor)\n+    True\n+    \"\"\"\n+    regressor = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, verbose=0)\n+    regressor.fit(features, target)\n+    return regressor\n+\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/catboost_regressor.py`, please provide doctest for the function `main`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/catboost_regressor.py",
    "pr_number": 11877,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1790935876,
    "comment_created_at": "2024-10-07T22:07:09Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+import numpy as np\n+import doctest\n+\n+\"\"\"\n+Mean Absolute Percentage Error (MAPE): \n+MAPE calculates the average of the absolute percentage differences between the\n+predicted and true values.\n+\n+MAPE = (\u03a3|y_true[i]-Y_pred[i]/y_true[i]|)/n\n+\n+https://stephenallwright.com/good-mape-score/\n+\n+\"\"\"\n+\n+\n+def mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/loss_functions/mean_absolute_percentage_error.py, please provide doctest for the function mean_absolute_percentage_error",
    "line_number": 16,
    "enriched": "File: machine_learning/loss_functions/mean_absolute_percentage_error.py\nCode: @@ -0,0 +1,48 @@\n+import numpy as np\n+import doctest\n+\n+\"\"\"\n+Mean Absolute Percentage Error (MAPE): \n+MAPE calculates the average of the absolute percentage differences between the\n+predicted and true values.\n+\n+MAPE = (\u03a3|y_true[i]-Y_pred[i]/y_true[i]|)/n\n+\n+https://stephenallwright.com/good-mape-score/\n+\n+\"\"\"\n+\n+\n+def mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/loss_functions/mean_absolute_percentage_error.py`, please provide doctest for the function `mean_absolute_percentage_error`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/loss_functions/mean_absolute_percentage_error.py",
    "pr_number": 10464,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359590241,
    "comment_created_at": "2023-10-14T18:50:56Z"
  },
  {
    "code": "@@ -0,0 +1,135 @@\n+'''\n+The forward-chaining algorithm PL-FC-ENTAILS? (KB, q) determines if a single proposition\n+symbol q\u2014the query\u2014is entailed by a knowledge base of definite clauses. It begins from\n+known facts (positive literals) in the knowledge base. \n+known facts (positive literals) in the knowledge base.\n+\n+Reference: https://dl.ebooksworld.ir/books/Artificial.Intelligence.A.Modern.Approach.4th.Edition.Peter.Norvig.%20Stuart.Russell.Pearson.9780134610993.EBooksWorld.ir.pdf\n+\n+'''\n+\n+import re\n+\n+def find_symbols_in_kb(knowledge_base: list[str]) -> dict:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file inference_engine_algorithms/forward_chaining.py, please provide doctest for the function find_symbols_in_kb",
    "line_number": 13,
    "enriched": "File: inference_engine_algorithms/forward_chaining.py\nCode: @@ -0,0 +1,135 @@\n+'''\n+The forward-chaining algorithm PL-FC-ENTAILS? (KB, q) determines if a single proposition\n+symbol q\u2014the query\u2014is entailed by a knowledge base of definite clauses. It begins from\n+known facts (positive literals) in the knowledge base. \n+known facts (positive literals) in the knowledge base.\n+\n+Reference: https://dl.ebooksworld.ir/books/Artificial.Intelligence.A.Modern.Approach.4th.Edition.Peter.Norvig.%20Stuart.Russell.Pearson.9780134610993.EBooksWorld.ir.pdf\n+\n+'''\n+\n+import re\n+\n+def find_symbols_in_kb(knowledge_base: list[str]) -> dict:\nComment: As there is no test file in this pull request nor any test function or class in the file `inference_engine_algorithms/forward_chaining.py`, please provide doctest for the function `find_symbols_in_kb`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "inference_engine_algorithms/forward_chaining.py",
    "pr_number": 11400,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1597368011,
    "comment_created_at": "2024-05-11T06:20:21Z"
  },
  {
    "code": "@@ -4,18 +4,27 @@\n def solve_maze(maze: list[list[int]]) -> bool:\n     \"\"\"\n     This method solves the \"rat in maze\" problem.\n-    In this problem we have some n by n matrix, a start point and an end point.\n-    We want to go from the start to the end. In this matrix zeroes represent walls\n-    and ones paths we can use.\n+    In this problem, we have an n by n binary matrix where each\n+    cell in the matrix contains a value 0 or 1.\n+    In this matrix, 1 represent a valid path that can be\n+    followed to reach the end of the matrix,\n+    while 0 is a dead end. The rat is allowed to move in 4 directions:",
    "comment": "i prefer wall/obstacle to \"dead end\".\r\naccording to the cambridge dictionary, a \"dead end\" is:\r\n\u00ab a [road](https://dictionary.cambridge.org/fr/dictionnaire/anglais/road) that is [closed](https://dictionary.cambridge.org/fr/dictionnaire/anglais/closed) at one end, and does not [lead](https://dictionary.cambridge.org/fr/dictionnaire/anglais/lead) [anywhere](https://dictionary.cambridge.org/fr/dictionnaire/anglais/anywhere): \u00bb\r\na \"dead end\" is already a road and doesn't as such depict very well what i see from the input.",
    "line_number": 11,
    "enriched": "File: backtracking/rat_in_maze.py\nCode: @@ -4,18 +4,27 @@\n def solve_maze(maze: list[list[int]]) -> bool:\n     \"\"\"\n     This method solves the \"rat in maze\" problem.\n-    In this problem we have some n by n matrix, a start point and an end point.\n-    We want to go from the start to the end. In this matrix zeroes represent walls\n-    and ones paths we can use.\n+    In this problem, we have an n by n binary matrix where each\n+    cell in the matrix contains a value 0 or 1.\n+    In this matrix, 1 represent a valid path that can be\n+    followed to reach the end of the matrix,\n+    while 0 is a dead end. The rat is allowed to move in 4 directions:\nComment: I prefer wall/obstacle to \"dead end\".\r\nAccording to the Cambridge Dictionary, a \"dead end\" is:\r\n\u00ab a [road](https://dictionary.cambridge.org/fr/dictionnaire/anglais/road) that is [closed](https://dictionary.cambridge.org/fr/dictionnaire/anglais/closed) at one end, and does not [lead](https://dictionary.cambridge.org/fr/dictionnaire/anglais/lead) [anywhere](https://dictionary.cambridge.org/fr/dictionnaire/anglais/anywhere): \u00bb\r\nA \"dead end\" is already a road and doesn't as such depict very well what I see from the input.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "backtracking/rat_in_maze.py",
    "pr_number": 9289,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1344269517,
    "comment_created_at": "2023-10-03T15:03:52Z"
  },
  {
    "code": "@@ -0,0 +1,136 @@\n+\"\"\"\n+A simple example uses the ant colony optimization algorithm\n+to solve the classic TSP problem",
    "comment": "what is tsp and why does the reader need to guess?",
    "line_number": 3,
    "enriched": "File: graphs/ant_colony_optimization_algorithms.py\nCode: @@ -0,0 +1,136 @@\n+\"\"\"\n+A simple example uses the ant colony optimization algorithm\n+to solve the classic TSP problem\nComment: What is TSP and why does the reader need to guess?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "graphs/ant_colony_optimization_algorithms.py",
    "pr_number": 11163,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1395453161,
    "comment_created_at": "2023-11-16T10:05:44Z"
  },
  {
    "code": "@@ -43,29 +43,19 @@ def points_to_polynomial(coordinates: list[list[int]]) -> str:\n \n     x = len(coordinates)\n \n-    count_of_line = 0\n-    matrix: list[list[float]] = []\n     # put the x and x to the power values in a matrix\n-    while count_of_line < x:\n-        count_in_line = 0\n-        a = coordinates[count_of_line][0]\n-        count_line: list[float] = []\n-        while count_in_line < x:\n-            count_line.append(a ** (x - (count_in_line + 1)))\n-            count_in_line += 1\n-        matrix.append(count_line)\n-        count_of_line += 1\n+    matrix: list[list[float]] = [\n+        [\n+            coordinates[count_of_line][0] ** (x - (count_in_line + 1))\n+            for count_in_line in range(x)\n+        ]\n+        for count_of_line in range(x)\n+    ]\n \n-    count_of_line = 0\n     # put the y values into a vector\n-    vector: list[float] = []\n-    while count_of_line < x:\n-        vector.append(coordinates[count_of_line][1])\n-        count_of_line += 1\n+    vector: list[float] = [coordinates[count_of_line][1] for count_of_line in range(x)]\n \n-    count = 0\n-\n-    while count < x:\n+    for count in range(x):\n         zahlen = 0",
    "comment": "1) could we refactor this inner while-loop to be a for-loop too? it'd probably require changing the control flow slightly, but it should be fine as long as we preserve the program logic. as a starting point, it seems to me that the reason why it's not trivial to convert this while-loop to a for-loop is because of this if-statement changing the value of zahlen inside the loop:\r\n    py\r\n    if count == zahlen:\r\n        zahlen += 1\r\n    \r\n    i think this condition is only there to ensure that zahlen != count. if my understanding is correct, then this would simply translate to a continue statement in a for-loop:\r\n    py\r\n    for zahlen in range(x):\r\n        if zahlen == count:\r\n            continue\r\n        ...\r\n    \r\n    from here, it should be pretty easy to convert this inner while-loop to a for-loop.\r\n2) let's change the variable names to be in english. nothing against german, but these names would be confusing when the rest of the repo is entirely in english.\r\nzahlen -> number or num\r\nbruch -> fraction or frac",
    "line_number": 59,
    "enriched": "File: linear_algebra/src/polynom_for_points.py\nCode: @@ -43,29 +43,19 @@ def points_to_polynomial(coordinates: list[list[int]]) -> str:\n \n     x = len(coordinates)\n \n-    count_of_line = 0\n-    matrix: list[list[float]] = []\n     # put the x and x to the power values in a matrix\n-    while count_of_line < x:\n-        count_in_line = 0\n-        a = coordinates[count_of_line][0]\n-        count_line: list[float] = []\n-        while count_in_line < x:\n-            count_line.append(a ** (x - (count_in_line + 1)))\n-            count_in_line += 1\n-        matrix.append(count_line)\n-        count_of_line += 1\n+    matrix: list[list[float]] = [\n+        [\n+            coordinates[count_of_line][0] ** (x - (count_in_line + 1))\n+            for count_in_line in range(x)\n+        ]\n+        for count_of_line in range(x)\n+    ]\n \n-    count_of_line = 0\n     # put the y values into a vector\n-    vector: list[float] = []\n-    while count_of_line < x:\n-        vector.append(coordinates[count_of_line][1])\n-        count_of_line += 1\n+    vector: list[float] = [coordinates[count_of_line][1] for count_of_line in range(x)]\n \n-    count = 0\n-\n-    while count < x:\n+    for count in range(x):\n         zahlen = 0\nComment: 1) Could we refactor this inner while-loop to be a for-loop too? It'd probably require changing the control flow slightly, but it should be fine as long as we preserve the program logic. As a starting point, it seems to me that the reason why it's not trivial to convert this while-loop to a for-loop is because of this if-statement changing the value of `zahlen` inside the loop:\r\n    ```py\r\n    if count == zahlen:\r\n        zahlen += 1\r\n    ```\r\n    I think this condition is only there to ensure that `zahlen != count`. If my understanding is correct, then this would simply translate to a `continue` statement in a for-loop:\r\n    ```py\r\n    for zahlen in range(x):\r\n        if zahlen == count:\r\n            continue\r\n        ...\r\n    ```\r\n    From here, it should be pretty easy to convert this inner while-loop to a for-loop.\r\n2) Let's change the variable names to be in English. Nothing against German, but these names would be confusing when the rest of the repo is entirely in English.\r\n`zahlen` -> `number` or `num`\r\n`bruch` -> `fraction` or `frac`\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "linear_algebra/src/polynom_for_points.py",
    "pr_number": 8605,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1278846171,
    "comment_created_at": "2023-07-31T06:35:54Z"
  },
  {
    "code": "@@ -0,0 +1,232 @@\n+# Python program for A* Search Algorithm\n+import math\n+import heapq\n+from typing import List, Tuple\n+\n+# Define the Cell class\n+\n+\n+class Cell:\n+    def __init__(self)->None:\n+      # Parent cell's row index\n+        self.parent_i = 0\n+        # Parent cell's column index\n+        self.parent_j = 0\n+        # Total cost of the cell (g + h)\n+        self.f = float(\"inf\")\n+        # Cost from start to this cell\n+        self.g = float(\"inf\")\n+        # Heuristic cost from this cell to destination\n+        self.h = 0\n+\n+\n+# Define the size of the grid\n+ROW = 9\n+COL = 10\n+\n+# Check if a cell is valid (within the grid)\n+\n+\n+def is_valid(row: int, col: int) -> bool:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/a_star_search.py, please provide doctest for the function is_valid",
    "line_number": 30,
    "enriched": "File: searches/A_star_search.py\nCode: @@ -0,0 +1,232 @@\n+# Python program for A* Search Algorithm\n+import math\n+import heapq\n+from typing import List, Tuple\n+\n+# Define the Cell class\n+\n+\n+class Cell:\n+    def __init__(self)->None:\n+      # Parent cell's row index\n+        self.parent_i = 0\n+        # Parent cell's column index\n+        self.parent_j = 0\n+        # Total cost of the cell (g + h)\n+        self.f = float(\"inf\")\n+        # Cost from start to this cell\n+        self.g = float(\"inf\")\n+        # Heuristic cost from this cell to destination\n+        self.h = 0\n+\n+\n+# Define the size of the grid\n+ROW = 9\n+COL = 10\n+\n+# Check if a cell is valid (within the grid)\n+\n+\n+def is_valid(row: int, col: int) -> bool:\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/A_star_search.py`, please provide doctest for the function `is_valid`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "searches/A_star_search.py",
    "pr_number": 11737,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1787337750,
    "comment_created_at": "2024-10-04T08:12:37Z"
  },
  {
    "code": "@@ -0,0 +1,34 @@\n+def rotated_binary_search(arr: list[int], key: int) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/rotated_binary_search.py, please provide doctest for the function rotated_binary_search",
    "line_number": 1,
    "enriched": "File: searches/rotated_binary_search.py\nCode: @@ -0,0 +1,34 @@\n+def rotated_binary_search(arr: list[int], key: int) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/rotated_binary_search.py`, please provide doctest for the function `rotated_binary_search`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "searches/rotated_binary_search.py",
    "pr_number": 11984,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1797125201,
    "comment_created_at": "2024-10-11T15:33:58Z"
  },
  {
    "code": "@@ -0,0 +1,226 @@\n+import random\n+from collections.abc import Callable, Sequence\n+from concurrent.futures import ThreadPoolExecutor\n+\n+import numpy as np\n+\n+# Parameters\n+N_POPULATION = 100  # Population size\n+N_GENERATIONS = 500  # Maximum number of generations\n+N_SELECTED = 50  # Number of parents selected for the next generation\n+MUTATION_PROBABILITY = 0.1  # Mutation probability\n+CROSSOVER_RATE = 0.8  # Probability of crossover\n+SEARCH_SPACE = (-10, 10)  # Search space for the variables\n+\n+# Random number generator\n+rng = np.random.default_rng()\n+\n+\n+class GeneticAlgorithm:\n+    def __init__(\n+        self,\n+        function: Callable[[float, float], float],\n+        bounds: Sequence[tuple[int | float, int | float]],\n+        population_size: int,\n+        generations: int,\n+        mutation_prob: float,\n+        crossover_rate: float,\n+        maximize: bool = True,\n+    ) -> None:\n+        self.function = function  # Target function to optimize\n+        self.bounds = bounds  # Search space bounds (for each variable)\n+        self.population_size = population_size\n+        self.generations = generations\n+        self.mutation_prob = mutation_prob\n+        self.crossover_rate = crossover_rate\n+        self.maximize = maximize\n+        self.dim = len(bounds)  # Dimensionality of the function (number of variables)\n+\n+        # Initialize population\n+        self.population = self.initialize_population()\n+\n+    def initialize_population(self) -> list[np.ndarray]:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file genetic_algorithm/genetic_algorithm_optimization.py, please provide doctest for the function initialize_population",
    "line_number": 42,
    "enriched": "File: genetic_algorithm/genetic_algorithm_optimization.py\nCode: @@ -0,0 +1,226 @@\n+import random\n+from collections.abc import Callable, Sequence\n+from concurrent.futures import ThreadPoolExecutor\n+\n+import numpy as np\n+\n+# Parameters\n+N_POPULATION = 100  # Population size\n+N_GENERATIONS = 500  # Maximum number of generations\n+N_SELECTED = 50  # Number of parents selected for the next generation\n+MUTATION_PROBABILITY = 0.1  # Mutation probability\n+CROSSOVER_RATE = 0.8  # Probability of crossover\n+SEARCH_SPACE = (-10, 10)  # Search space for the variables\n+\n+# Random number generator\n+rng = np.random.default_rng()\n+\n+\n+class GeneticAlgorithm:\n+    def __init__(\n+        self,\n+        function: Callable[[float, float], float],\n+        bounds: Sequence[tuple[int | float, int | float]],\n+        population_size: int,\n+        generations: int,\n+        mutation_prob: float,\n+        crossover_rate: float,\n+        maximize: bool = True,\n+    ) -> None:\n+        self.function = function  # Target function to optimize\n+        self.bounds = bounds  # Search space bounds (for each variable)\n+        self.population_size = population_size\n+        self.generations = generations\n+        self.mutation_prob = mutation_prob\n+        self.crossover_rate = crossover_rate\n+        self.maximize = maximize\n+        self.dim = len(bounds)  # Dimensionality of the function (number of variables)\n+\n+        # Initialize population\n+        self.population = self.initialize_population()\n+\n+    def initialize_population(self) -> list[np.ndarray]:\nComment: As there is no test file in this pull request nor any test function or class in the file `genetic_algorithm/genetic_algorithm_optimization.py`, please provide doctest for the function `initialize_population`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "genetic_algorithm/genetic_algorithm_optimization.py",
    "pr_number": 12059,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1799179737,
    "comment_created_at": "2024-10-14T10:28:22Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+def camel_to_snake(camel_str : str) -> str :",
    "comment": "as there is no test file in this pull request nor any test function or class in the file strings/camel_case_to_snake_case.py, please provide doctest for the function camel_to_snake",
    "line_number": 1,
    "enriched": "File: strings/camel_case_to_snake_case.py\nCode: @@ -0,0 +1,42 @@\n+def camel_to_snake(camel_str : str) -> str :\nComment: As there is no test file in this pull request nor any test function or class in the file `strings/camel_case_to_snake_case.py`, please provide doctest for the function `camel_to_snake`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "strings/camel_case_to_snake_case.py",
    "pr_number": 9828,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347539746,
    "comment_created_at": "2023-10-05T14:42:11Z"
  },
  {
    "code": "@@ -0,0 +1,71 @@\n+\"\"\"\n+Project Euler Problem 79: https://projecteuler.net/problem=79\n+\n+Passcode derivation\n+\n+A common security method used for online banking is to ask the user for three\n+random characters from a passcode. For example, if the passcode was 531278,\n+they may ask for the 2nd, 3rd, and 5th characters; the expected reply would\n+be: 317.\n+\n+The text file, keylog.txt, contains fifty successful login attempts.\n+\n+Given that the three characters are always asked for in order, analyze the file\n+to determine the shortest possible secret passcode of unknown length.\n+\"\"\"\n+\n+from collections import Counter\n+\n+\n+\n+def find_secret_passcode(logins: list[str]) -> int:\n+    \"\"\"\n+    Find the shortest possible secret passcode of unknown length.\n+\n+    :param logins: A list of successful login attempts.\n+    :type logins: list[str]\n+    :return: The shortest possible secret passcode.\n+    :rtype: int\n+\n+    >>> find_secret_passcode([\"135\", \"259\", \"235\", \"189\", \"690\", \"168\", \"120\", \"136\", \"289\", \"589\", \"160\", \"165\", \"580\", \"369\", \"250\", \"280\"])\n+    12356890\n+\n+    >>> find_secret_passcode([\"426\", \"281\", \"061\", \"819\", \"268\", \"406\", \"420\", \"428\", \"209\", \"689\", \"019\", \"421\", \"469\", \"261\", \"681\", \"201\"])\n+    4206819\n+    \"\"\"\n+    s = Counter()\n+    c = Counter()\n+    r = []\n+\n+    for login in logins:\n+        for idx, char in enumerate(login):\n+            if char not in s:\n+                r.append(char)\n+            s[char] += idx\n+            c[char] += 1\n+\n+    r.sort(key=lambda a: s[a] / c[a])",
    "comment": "please provide descriptive name for the parameter: a",
    "line_number": 47,
    "enriched": "File: project_euler/problem_079/sol2.py\nCode: @@ -0,0 +1,71 @@\n+\"\"\"\n+Project Euler Problem 79: https://projecteuler.net/problem=79\n+\n+Passcode derivation\n+\n+A common security method used for online banking is to ask the user for three\n+random characters from a passcode. For example, if the passcode was 531278,\n+they may ask for the 2nd, 3rd, and 5th characters; the expected reply would\n+be: 317.\n+\n+The text file, keylog.txt, contains fifty successful login attempts.\n+\n+Given that the three characters are always asked for in order, analyze the file\n+to determine the shortest possible secret passcode of unknown length.\n+\"\"\"\n+\n+from collections import Counter\n+\n+\n+\n+def find_secret_passcode(logins: list[str]) -> int:\n+    \"\"\"\n+    Find the shortest possible secret passcode of unknown length.\n+\n+    :param logins: A list of successful login attempts.\n+    :type logins: list[str]\n+    :return: The shortest possible secret passcode.\n+    :rtype: int\n+\n+    >>> find_secret_passcode([\"135\", \"259\", \"235\", \"189\", \"690\", \"168\", \"120\", \"136\", \"289\", \"589\", \"160\", \"165\", \"580\", \"369\", \"250\", \"280\"])\n+    12356890\n+\n+    >>> find_secret_passcode([\"426\", \"281\", \"061\", \"819\", \"268\", \"406\", \"420\", \"428\", \"209\", \"689\", \"019\", \"421\", \"469\", \"261\", \"681\", \"201\"])\n+    4206819\n+    \"\"\"\n+    s = Counter()\n+    c = Counter()\n+    r = []\n+\n+    for login in logins:\n+        for idx, char in enumerate(login):\n+            if char not in s:\n+                r.append(char)\n+            s[char] += idx\n+            c[char] += 1\n+\n+    r.sort(key=lambda a: s[a] / c[a])\nComment: Please provide descriptive name for the parameter: `a`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "project_euler/problem_079/sol2.py",
    "pr_number": 10447,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359476719,
    "comment_created_at": "2023-10-14T16:18:59Z"
  },
  {
    "code": "@@ -0,0 +1,52 @@\n+def block_sort(lst: list) -> list:\n+    \"\"\"\n+    Sorts a list using the Block Sort algorithm.\n+\n+    The Block Sort algorithm works by dividing the input list into blocks of size\n+    sqrt(n), sorting each block, and then merging the sorted blocks into a single\n+    sorted list.\n+\n+    Args:\n+        lst (List[int]): The unsorted list to be sorted.\n+\n+    Returns:\n+        List[int]: The sorted list.\n+\n+    Examples:\n+        >>> block_sort([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5])\n+        [1, 1, 2, 3, 3, 4, 5, 5, 5, 6, 9]\n+        >>> block_sort([5, 4, 3, 2, 1])\n+        [1, 2, 3, 4, 5]\n+        >>> block_sort([])\n+        []\n+    \"\"\"",
    "comment": "can you add edge cases and some larger input lists like this:",
    "line_number": 22,
    "enriched": "File: sorts/block_sort.py\nCode: @@ -0,0 +1,52 @@\n+def block_sort(lst: list) -> list:\n+    \"\"\"\n+    Sorts a list using the Block Sort algorithm.\n+\n+    The Block Sort algorithm works by dividing the input list into blocks of size\n+    sqrt(n), sorting each block, and then merging the sorted blocks into a single\n+    sorted list.\n+\n+    Args:\n+        lst (List[int]): The unsorted list to be sorted.\n+\n+    Returns:\n+        List[int]: The sorted list.\n+\n+    Examples:\n+        >>> block_sort([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5])\n+        [1, 1, 2, 3, 3, 4, 5, 5, 5, 6, 9]\n+        >>> block_sort([5, 4, 3, 2, 1])\n+        [1, 2, 3, 4, 5]\n+        >>> block_sort([])\n+        []\n+    \"\"\"\nComment: Can you add edge cases and some larger input lists like this:\r\n\r\n```suggestion\r\n    \"\"\"\r\n    >>> block_sort([1, 1, 1, 1, 1])\r\n        [1, 1, 1, 1, 1]\r\n        >>> block_sort([7, 6, 5, 4, 3, 2, 1])\r\n        [1, 2, 3, 4, 5, 6, 7]\r\n        >>> block_sort([10, 9, 8, 7, 6, 5, 4, 3, 2, 1])\r\n        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n        >>> block_sort([1])\r\n        [1]\r\n        >>> block_sort([2, 1])\r\n        [1, 2]\r\n        >>> block_sort([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\r\n        [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\r\n        >>> block_sort([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n        >>> block_sort([1000, 100, 10, 1, 10000, 100000, 1000000])\r\n        [1, 10, 100, 1000, 10000, 100000, 1000000]\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "sorts/block_sort.py",
    "pr_number": 10734,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1369615107,
    "comment_created_at": "2023-10-24T04:20:12Z"
  },
  {
    "code": "@@ -0,0 +1,44 @@\n+\"\"\"\n+Implements the Scaled Exponential Linear Unit or SELU function.\n+The function takes a vector of K real numbers and two real numbers\n+alpha(default = 1.6732) & lambda (default = 1.0507) as input and\n+then applies the SELU function to each element of the vector.\n+SELU is a self-normalizing activation function. It is a variant\n+of the ELU. The main advantage of SELU is that we can be sure\n+that the output will always be standardized due to its\n+self-normalizing behavior. That means there is no need to\n+include Batch-Normalization layers.\n+References :\n+https://iq.opengenus.org/scaled-exponential-linear-unit/\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def scaled_exponential_linear_unit(\n+    vector: np.ndarray, alpha: float = 1.6732, _lambda: float = 1.0507",
    "comment": "is there a reason why _lambda has an underscore? is the user not meant to change this coefficient?",
    "line_number": 19,
    "enriched": "File: neural_network/activation_functions/scaled_exponential_linear_unit.py\nCode: @@ -0,0 +1,44 @@\n+\"\"\"\n+Implements the Scaled Exponential Linear Unit or SELU function.\n+The function takes a vector of K real numbers and two real numbers\n+alpha(default = 1.6732) & lambda (default = 1.0507) as input and\n+then applies the SELU function to each element of the vector.\n+SELU is a self-normalizing activation function. It is a variant\n+of the ELU. The main advantage of SELU is that we can be sure\n+that the output will always be standardized due to its\n+self-normalizing behavior. That means there is no need to\n+include Batch-Normalization layers.\n+References :\n+https://iq.opengenus.org/scaled-exponential-linear-unit/\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def scaled_exponential_linear_unit(\n+    vector: np.ndarray, alpha: float = 1.6732, _lambda: float = 1.0507\nComment: Is there a reason why `_lambda` has an underscore? Is the user not meant to change this coefficient?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "neural_network/activation_functions/scaled_exponential_linear_unit.py",
    "pr_number": 9027,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1316723239,
    "comment_created_at": "2023-09-06T04:52:48Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+\"\"\"\n+Project Euler Problem 79: https://projecteuler.net/problem=79\n+\n+Passcode derivation\n+\n+A common security method used for online banking is to ask the user for three\n+random characters from a passcode. For example, if the passcode was 531278,\n+they may ask for the 2nd, 3rd, and 5th characters; the expected reply would\n+be: 317.\n+\n+The text file, keylog.txt, contains fifty successful login attempts.\n+\n+Given that the three characters are always asked for in order, analyse the file\n+so as to determine the shortest possible secret passcode of unknown length.\n+...\n+\n+\"\"\"\n+import itertools\n+import os\n+\n+\n+def solution() -> int:\n+    \"\"\"\n+    Returns the shortest possible secret passcode of unknown length.\n+\n+    >>> solution()\n+    73162890\n+    \"\"\"\n+    with open(os.path.dirname(__file__) + \"/p079_keylog.txt\") as file:\n+        logins = [tuple(line.strip()) for line in file]\n+\n+    unique_chars = {char for login in logins for char in login}\n+\n+    for permutation in itertools.permutations(unique_chars):\n+        satisfied = True\n+        for login in logins:\n+            if not (\n+                permutation.index(login[0])\n+                < permutation.index(login[1])\n+                < permutation.index(login[2])\n+            ):\n+                satisfied = False\n+                break\n+\n+        if satisfied:\n+            return int(\"\".join(permutation))\n+            break\n+\n+    return 0",
    "comment": "it's better to raise an exception. what do you think?",
    "line_number": 49,
    "enriched": "File: project_euler/problem_079/sol1.py\nCode: @@ -0,0 +1,53 @@\n+\"\"\"\n+Project Euler Problem 79: https://projecteuler.net/problem=79\n+\n+Passcode derivation\n+\n+A common security method used for online banking is to ask the user for three\n+random characters from a passcode. For example, if the passcode was 531278,\n+they may ask for the 2nd, 3rd, and 5th characters; the expected reply would\n+be: 317.\n+\n+The text file, keylog.txt, contains fifty successful login attempts.\n+\n+Given that the three characters are always asked for in order, analyse the file\n+so as to determine the shortest possible secret passcode of unknown length.\n+...\n+\n+\"\"\"\n+import itertools\n+import os\n+\n+\n+def solution() -> int:\n+    \"\"\"\n+    Returns the shortest possible secret passcode of unknown length.\n+\n+    >>> solution()\n+    73162890\n+    \"\"\"\n+    with open(os.path.dirname(__file__) + \"/p079_keylog.txt\") as file:\n+        logins = [tuple(line.strip()) for line in file]\n+\n+    unique_chars = {char for login in logins for char in login}\n+\n+    for permutation in itertools.permutations(unique_chars):\n+        satisfied = True\n+        for login in logins:\n+            if not (\n+                permutation.index(login[0])\n+                < permutation.index(login[1])\n+                < permutation.index(login[2])\n+            ):\n+                satisfied = False\n+                break\n+\n+        if satisfied:\n+            return int(\"\".join(permutation))\n+            break\n+\n+    return 0\nComment: It's better to raise an exception. What do you think?\r\n\r\n```suggestion\r\n    raise Exception(\"Unable to find the secret passcode\")\r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "project_euler/problem_079/sol1.py",
    "pr_number": 8607,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1155239295,
    "comment_created_at": "2023-04-02T04:53:12Z"
  },
  {
    "code": "@@ -83,6 +83,29 @@ def fib_iterative(n: int) -> list[int]:\n     return fib\n \n \n+def fib_recursive_term(i: int) -> int:\n+    \"\"\"\n+    Calculates the i-th (0-indexed) Fibonacci number using recursion\n+    >>> fib_recursive_term(0)\n+    0\n+    >>> fib_recursive_term(1)\n+    1\n+    >>> fib_recursive_term(5)\n+    5\n+    >>> fib_recursive_term(10)\n+    55\n+    >>> fib_recursive_term(-1)\n+    Traceback (most recent call last):\n+        ...\n+    Exception: n is negative\n+    \"\"\"",
    "comment": "can we:\r\n1. removed the large block of empty lines in the middle of the fib_recursive function.\r\n2. ensured the docstring for the fib_recursive function is correctly formatted and includes the expected outputs for the doctests.",
    "line_number": 101,
    "enriched": "File: maths/fibonacci.py\nCode: @@ -83,6 +83,29 @@ def fib_iterative(n: int) -> list[int]:\n     return fib\n \n \n+def fib_recursive_term(i: int) -> int:\n+    \"\"\"\n+    Calculates the i-th (0-indexed) Fibonacci number using recursion\n+    >>> fib_recursive_term(0)\n+    0\n+    >>> fib_recursive_term(1)\n+    1\n+    >>> fib_recursive_term(5)\n+    5\n+    >>> fib_recursive_term(10)\n+    55\n+    >>> fib_recursive_term(-1)\n+    Traceback (most recent call last):\n+        ...\n+    Exception: n is negative\n+    \"\"\"\nComment: Can we:\r\n1. Removed the large block of empty lines in the middle of the fib_recursive function.\r\n2. Ensured the docstring for the fib_recursive function is correctly formatted and includes the expected outputs for the doctests.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "maths/fibonacci.py",
    "pr_number": 11301,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1600843236,
    "comment_created_at": "2024-05-15T01:46:44Z"
  },
  {
    "code": "@@ -0,0 +1,80 @@\n+# Author:- Arnab Nath\n+# Python program to find an element x\n+# in a sorted array using Exponential Search\n+\n+# A recursive binary search function returns\n+# location of x in given array arr[l..r] is\n+# present, otherwise -1\n+arr = [4, 7, 10, 34, 44, 70]\n+n = len(arr)\n+x = 44\n+if arr[0] == x:\n+    print(\"0\")\n+i = 1\n+# Finding range for binarySearch\n+while i < n and arr[i] <= x:\n+    i = i * 2\n+min1 = min(i, len(arr))\n+\n+\n+def binarysearch(arr: list, le: int, r: int, x: int) -> int:",
    "comment": "please provide descriptive name for the parameter: r\n\nplease provide descriptive name for the parameter: x",
    "line_number": 20,
    "enriched": "File: searches/exponentialsearch.py\nCode: @@ -0,0 +1,80 @@\n+# Author:- Arnab Nath\n+# Python program to find an element x\n+# in a sorted array using Exponential Search\n+\n+# A recursive binary search function returns\n+# location of x in given array arr[l..r] is\n+# present, otherwise -1\n+arr = [4, 7, 10, 34, 44, 70]\n+n = len(arr)\n+x = 44\n+if arr[0] == x:\n+    print(\"0\")\n+i = 1\n+# Finding range for binarySearch\n+while i < n and arr[i] <= x:\n+    i = i * 2\n+min1 = min(i, len(arr))\n+\n+\n+def binarysearch(arr: list, le: int, r: int, x: int) -> int:\nComment: Please provide descriptive name for the parameter: `r`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "searches/exponentialsearch.py",
    "pr_number": 7315,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996434089,
    "comment_created_at": "2022-10-16T12:06:26Z"
  },
  {
    "code": "@@ -134,25 +134,27 @@ def max_parts(self, num_cuts: float) -> float:\n \n         >>> circle = Circle(5)\n         >>> circle.max_parts(0)\n-        1.0\n+        1",
    "comment": "why do you wanna add a \".0\" is the rest of test cases ???",
    "line_number": 137,
    "enriched": "File: geometry/geometry.py\nCode: @@ -134,25 +134,27 @@ def max_parts(self, num_cuts: float) -> float:\n \n         >>> circle = Circle(5)\n         >>> circle.max_parts(0)\n-        1.0\n+        1\nComment: why do you wanna add a \".0\" is the rest of test cases ???",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "geometry/geometry.py",
    "pr_number": 11271,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1518459173,
    "comment_created_at": "2024-03-09T02:22:19Z"
  },
  {
    "code": "@@ -0,0 +1,150 @@\n+import numpy as np\r\n+\r\n+class ART1:\r\n+    \"\"\"\r\n+    Adaptive Resonance Theory 1 (ART1) model for binary data clustering.\r\n+\r\n+    The ART1 algorithm is a type of neural network used for unsupervised \r\n+    learning and clustering of binary input data. It continuously learns \r\n+    to categorize inputs based on similarity while preserving previously \r\n+    learned categories. The vigilance parameter controls the degree of \r\n+    similarity required to assign an input to an existing category,\r\n+    allowing for flexible and adaptive clustering.\r\n+\r\n+    Attributes:\r\n+        num_features (int): Number of features in the input data.\r\n+        vigilance (float): Threshold for similarity that determines whether \r\n+        an input matches an existing cluster.\r\n+        weights (list): List of cluster weights representing the learned categories.\r\n+    \"\"\"\r\n+    \r\n+    def __init__(self, num_features: int, vigilance: float = 0.7) -> None:\r\n+        \"\"\"\r\n+        Initialize the ART1 model with the given number of features and vigilance parameter.\r\n+\r\n+        Args:\r\n+            num_features (int): Number of features in the input data.\r\n+            vigilance (float): Threshold for similarity (default is 0.7).\r\n+            \r\n+        Examples:\r\n+            >>> model = ART1(num_features=4, vigilance=0.5)\r\n+            >>> model.num_features\r\n+            4\r\n+            >>> model.vigilance\r\n+            0.5\r\n+        \"\"\"\r\n+        self.vigilance = vigilance  # Controls cluster strictness\r\n+        self.num_features = num_features\r\n+        self.weights = []           # List of cluster weights\r\n+        \r\n+    def train(self, data: np.ndarray) -> None:\r\n+        \"\"\"\r\n+        Train the ART1 model on the provided data.\r\n+\r\n+        Args:\r\n+            data (np.ndarray): A 2D array of binary input data (num_samples x num_features).\r\n+\r\n+        Examples:\r\n+            >>> model = ART1(num_features=4, vigilance=0.5)\r\n+            >>> data = np.array([[1, 1, 0, 0], [1, 1, 1, 0]])\r\n+            >>> model.train(data)\r\n+            >>> len(model.weights)\r\n+            2\r\n+        \"\"\"\r\n+        for x in data:\r\n+            match = False\r\n+            for i, w in enumerate(self.weights):\r\n+                if self._similarity(w, x) >= self.vigilance:\r\n+                    self.weights[i] = self._learn(w, x)\r\n+                    match = True\r\n+                    break\r\n+            if not match:\r\n+                self.weights.append(x.copy())  # Add a new cluster\r\n+\r\n+    def _similarity(self, w: np.ndarray, x: np.ndarray) -> float:\r",
    "comment": "please provide descriptive name for the parameter: w\n\nplease provide descriptive name for the parameter: x",
    "line_number": 64,
    "enriched": "File: neural_network/adaptive_resonance_theory.py\nCode: @@ -0,0 +1,150 @@\n+import numpy as np\r\n+\r\n+class ART1:\r\n+    \"\"\"\r\n+    Adaptive Resonance Theory 1 (ART1) model for binary data clustering.\r\n+\r\n+    The ART1 algorithm is a type of neural network used for unsupervised \r\n+    learning and clustering of binary input data. It continuously learns \r\n+    to categorize inputs based on similarity while preserving previously \r\n+    learned categories. The vigilance parameter controls the degree of \r\n+    similarity required to assign an input to an existing category,\r\n+    allowing for flexible and adaptive clustering.\r\n+\r\n+    Attributes:\r\n+        num_features (int): Number of features in the input data.\r\n+        vigilance (float): Threshold for similarity that determines whether \r\n+        an input matches an existing cluster.\r\n+        weights (list): List of cluster weights representing the learned categories.\r\n+    \"\"\"\r\n+    \r\n+    def __init__(self, num_features: int, vigilance: float = 0.7) -> None:\r\n+        \"\"\"\r\n+        Initialize the ART1 model with the given number of features and vigilance parameter.\r\n+\r\n+        Args:\r\n+            num_features (int): Number of features in the input data.\r\n+            vigilance (float): Threshold for similarity (default is 0.7).\r\n+            \r\n+        Examples:\r\n+            >>> model = ART1(num_features=4, vigilance=0.5)\r\n+            >>> model.num_features\r\n+            4\r\n+            >>> model.vigilance\r\n+            0.5\r\n+        \"\"\"\r\n+        self.vigilance = vigilance  # Controls cluster strictness\r\n+        self.num_features = num_features\r\n+        self.weights = []           # List of cluster weights\r\n+        \r\n+    def train(self, data: np.ndarray) -> None:\r\n+        \"\"\"\r\n+        Train the ART1 model on the provided data.\r\n+\r\n+        Args:\r\n+            data (np.ndarray): A 2D array of binary input data (num_samples x num_features).\r\n+\r\n+        Examples:\r\n+            >>> model = ART1(num_features=4, vigilance=0.5)\r\n+            >>> data = np.array([[1, 1, 0, 0], [1, 1, 1, 0]])\r\n+            >>> model.train(data)\r\n+            >>> len(model.weights)\r\n+            2\r\n+        \"\"\"\r\n+        for x in data:\r\n+            match = False\r\n+            for i, w in enumerate(self.weights):\r\n+                if self._similarity(w, x) >= self.vigilance:\r\n+                    self.weights[i] = self._learn(w, x)\r\n+                    match = True\r\n+                    break\r\n+            if not match:\r\n+                self.weights.append(x.copy())  # Add a new cluster\r\n+\r\n+    def _similarity(self, w: np.ndarray, x: np.ndarray) -> float:\r\nComment: Please provide descriptive name for the parameter: `w`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "neural_network/adaptive_resonance_theory.py",
    "pr_number": 12324,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1821851117,
    "comment_created_at": "2024-10-30T04:25:06Z"
  },
  {
    "code": "@@ -1,17 +1,28 @@\n #!/usr/bin/env python3\n \n+from __future__ import annotations",
    "comment": "this repo runs code on the current python (3.11 this month, 3.12 next month) so this import is not required.",
    "line_number": 3,
    "enriched": "File: scripts/build_directory_md.py\nCode: @@ -1,17 +1,28 @@\n #!/usr/bin/env python3\n \n+from __future__ import annotations\nComment: This repo runs code on the current Python (3.11 this month, 3.12 next month) so this import is not required.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "scripts/build_directory_md.py",
    "pr_number": 7592,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1316746627,
    "comment_created_at": "2023-09-06T05:27:52Z"
  },
  {
    "code": "@@ -4,34 +4,41 @@\n # quickly find a good approximation for the root of a real-valued function\n from __future__ import annotations\n \n-from decimal import Decimal\n-from math import *  # noqa: F403\n+from sympy import diff, symbols, sympify\n \n-from sympy import diff\n \n-\n-def newton_raphson(\n-    func: str, a: float | Decimal, precision: float = 10**-10\n-) -> float:\n+def newton_raphson(func: str, a: float, precision: float = 10**-10) -> float:",
    "comment": "can we come up with a more self-documenting name than a?",
    "line_number": 10,
    "enriched": "File: arithmetic_analysis/newton_raphson.py\nCode: @@ -4,34 +4,41 @@\n # quickly find a good approximation for the root of a real-valued function\n from __future__ import annotations\n \n-from decimal import Decimal\n-from math import *  # noqa: F403\n+from sympy import diff, symbols, sympify\n \n-from sympy import diff\n \n-\n-def newton_raphson(\n-    func: str, a: float | Decimal, precision: float = 10**-10\n-) -> float:\n+def newton_raphson(func: str, a: float, precision: float = 10**-10) -> float:\nComment: Can we come up with a more self-documenting name than `a`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "arithmetic_analysis/newton_raphson.py",
    "pr_number": 8869,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1264623592,
    "comment_created_at": "2023-07-16T07:11:14Z"
  },
  {
    "code": "@@ -68,6 +70,32 @@ def calculate_prime_numbers(max_number: int) -> list[int]:\n     return [2] + [2 * i + 1 for i in range(1, max_number // 2) if is_prime[i]]\n \n \n+def np_calculate_prime_numbers(max_number: int) -> list[int]:",
    "comment": "is it possible to make this as a helper function in a common module as i see it being used in your other pr as well or maybe add it in maths/ and re-use it here or (assuming that there's already similar function in maths/) re-use an existing implementation?",
    "line_number": 73,
    "enriched": "File: project_euler/problem_187/sol1.py\nCode: @@ -68,6 +70,32 @@ def calculate_prime_numbers(max_number: int) -> list[int]:\n     return [2] + [2 * i + 1 for i in range(1, max_number // 2) if is_prime[i]]\n \n \n+def np_calculate_prime_numbers(max_number: int) -> list[int]:\nComment: Is it possible to make this as a helper function in a common module as I see it being used in your other PR as well or maybe add it in `maths/` and re-use it here or (assuming that there's already similar function in `maths/`) re-use an existing implementation?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "project_euler/problem_187/sol1.py",
    "pr_number": 10580,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375369671,
    "comment_created_at": "2023-10-29T05:06:52Z"
  },
  {
    "code": "@@ -0,0 +1,111 @@\n+\"\"\"\n+Python implementation of the Primal-Dual Interior-Point Method for solving linear programs with\n+- `>=`, `<=`, and `=` constraints and\n+- each variable `x1, x2, ... >= 0`.\n+\n+Resources:\n+https://en.wikipedia.org/wiki/Interior-point_method\n+\"\"\"\n+\n+import numpy as np\n+\n+class InteriorPointMethod:\n+    \"\"\"\n+    Operate on linear programming problems using the Primal-Dual Interior-Point Method.\n+\n+    Attributes:\n+        c (np.ndarray): Coefficient matrix for the objective function.\n+        a (np.ndarray): Constraint matrix.\n+        b (np.ndarray): Constraint bounds.\n+        tol (float): Tolerance for stopping criterion.\n+        max_iter (int): Maximum number of iterations.\n+    \"\"\"\n+\n+    def __init__(self, c: np.ndarray, a: np.ndarray, b: np.ndarray, tol: float = 1e-8, max_iter: int = 100) -> None:",
    "comment": "please provide descriptive name for the parameter: c\n\nplease provide descriptive name for the parameter: a\n\nplease provide descriptive name for the parameter: b",
    "line_number": 24,
    "enriched": "File: linear_programming/interior_point_method.py\nCode: @@ -0,0 +1,111 @@\n+\"\"\"\n+Python implementation of the Primal-Dual Interior-Point Method for solving linear programs with\n+- `>=`, `<=`, and `=` constraints and\n+- each variable `x1, x2, ... >= 0`.\n+\n+Resources:\n+https://en.wikipedia.org/wiki/Interior-point_method\n+\"\"\"\n+\n+import numpy as np\n+\n+class InteriorPointMethod:\n+    \"\"\"\n+    Operate on linear programming problems using the Primal-Dual Interior-Point Method.\n+\n+    Attributes:\n+        c (np.ndarray): Coefficient matrix for the objective function.\n+        a (np.ndarray): Constraint matrix.\n+        b (np.ndarray): Constraint bounds.\n+        tol (float): Tolerance for stopping criterion.\n+        max_iter (int): Maximum number of iterations.\n+    \"\"\"\n+\n+    def __init__(self, c: np.ndarray, a: np.ndarray, b: np.ndarray, tol: float = 1e-8, max_iter: int = 100) -> None:\nComment: Please provide descriptive name for the parameter: `c`\n\nPlease provide descriptive name for the parameter: `a`\n\nPlease provide descriptive name for the parameter: `b`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "linear_programming/interior_point_method.py",
    "pr_number": 11497,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1694037338,
    "comment_created_at": "2024-07-28T00:38:49Z"
  },
  {
    "code": "@@ -0,0 +1,81 @@\n+from math import fabs\n+\n+\n+def split_list(timings: list) -> tuple:\n+\n+    \"\"\"\n+\n+    this is a case of the partition problem.\n+    it accepts a multiset ( list ) of positive integers,\n+    distributes them, and returns a tuple, containing two lists,\n+    with minimal difference between their sums\n+\n+    >>> split_list([27, 21, 92, 87, 1, 32])\n+    ([27, 21, 87], [92, 1, 32], 10)\n+    >>> split_list([52, 385, 9956, 25, 2367, 1111, 17, 925])\n+    ([9956], [52, 385, 25, 2367, 1111, 17, 925], 5074)\n+    >>> split_list([12, 10, 11, 9])\n+    ([10, 11], [12, 9], 0)\n+    >>> split_list([-1551, 2712, 2325, 2623])\n+    ([1551, 2712], [2325, 2623], 685)\n+    >>> split_list([\"12.5\", \"10\", \"11\", \"9\"])\n+    ([10, 11], [12.5, 9], 0.5)\n+    >>> split_list([\"twelve\", \"ten\", \"eleven\", \"nine\"])\n+    ([0], [0, 0, 0], 0)\n+\n+    \"\"\"\n+\n+    result = None\n+\n+    def split_workload(arr: list) -> tuple:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file scheduling/split_workload.py, please provide doctest for the function split_workload",
    "line_number": 30,
    "enriched": "File: scheduling/split_workload.py\nCode: @@ -0,0 +1,81 @@\n+from math import fabs\n+\n+\n+def split_list(timings: list) -> tuple:\n+\n+    \"\"\"\n+\n+    this is a case of the partition problem.\n+    it accepts a multiset ( list ) of positive integers,\n+    distributes them, and returns a tuple, containing two lists,\n+    with minimal difference between their sums\n+\n+    >>> split_list([27, 21, 92, 87, 1, 32])\n+    ([27, 21, 87], [92, 1, 32], 10)\n+    >>> split_list([52, 385, 9956, 25, 2367, 1111, 17, 925])\n+    ([9956], [52, 385, 25, 2367, 1111, 17, 925], 5074)\n+    >>> split_list([12, 10, 11, 9])\n+    ([10, 11], [12, 9], 0)\n+    >>> split_list([-1551, 2712, 2325, 2623])\n+    ([1551, 2712], [2325, 2623], 685)\n+    >>> split_list([\"12.5\", \"10\", \"11\", \"9\"])\n+    ([10, 11], [12.5, 9], 0.5)\n+    >>> split_list([\"twelve\", \"ten\", \"eleven\", \"nine\"])\n+    ([0], [0, 0, 0], 0)\n+\n+    \"\"\"\n+\n+    result = None\n+\n+    def split_workload(arr: list) -> tuple:\nComment: As there is no test file in this pull request nor any test function or class in the file `scheduling/split_workload.py`, please provide doctest for the function `split_workload`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "scheduling/split_workload.py",
    "pr_number": 8868,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1264399702,
    "comment_created_at": "2023-07-15T12:22:19Z"
  },
  {
    "code": "@@ -0,0 +1,36 @@\n+# Auteur : Bosolindo Edhiengene Roger\r\n+# email : rogerbosolinndo34@gmail.com\r\n+\r\n+\r\n+def calc(operations: dict) -> float:\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file other/time_algo_exec.py, please provide doctest for the function calc",
    "line_number": 5,
    "enriched": "File: other/time_algo_exec.py\nCode: @@ -0,0 +1,36 @@\n+# Auteur : Bosolindo Edhiengene Roger\r\n+# email : rogerbosolinndo34@gmail.com\r\n+\r\n+\r\n+def calc(operations: dict) -> float:\r\nComment: As there is no test file in this pull request nor any test function or class in the file `other/time_algo_exec.py`, please provide doctest for the function `calc`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "other/time_algo_exec.py",
    "pr_number": 12761,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2100372493,
    "comment_created_at": "2025-05-21T13:58:04Z"
  },
  {
    "code": "@@ -0,0 +1,242 @@\n+# \"\"\"\n+#   Connected-Component Labeling\n+#\n+#   Source:\n+#   https://en.wikipedia.org/wiki/Connected-component_labeling\n+#\n+#   Basically, there's two versions of CCL two-pass algorithm:\n+#   4-connectivity and 8-connectivity.\n+#   4-connectivity means that only horizontal and vertical neighbours\n+#   are considered, while 8-connectivity means that diagonal neighbours\n+#   are also considered.\n+# \"\"\"\n+\n+\n+def connected_component_labeling_8(matrix: list) -> list:\n+    \"\"\"\n+    Connected Component Labeling algorithm (8-connectivity version)\n+    Takes a matrix of binary values and returns a matrix of labels,\n+    where each connected component is assigned a unique label.\n+\n+    args:\n+        matrix: 2D list of 0s and 1s\n+\n+    returns:\n+        2D list of labels\n+\n+    >>> connected_component_labeling_8([[1, 0, 0, 1],\n+    ...                                 [1, 0, 0, 1],\n+    ...                                 [0, 0, 1, 0],\n+    ...                                 [0, 1, 1, 0]])\n+    [[1, 0, 0, 2], [1, 0, 0, 2], [0, 0, 2, 0], [0, 2, 2, 0]]\n+    >>> connected_component_labeling_8([[0, 1, 0, 1],\n+    ...                                 [1, 1, 0, 0],\n+    ...                                 [0, 0, 1, 1],\n+    ...                                 [1, 0, 1, 0]])\n+    [[0, 1, 0, 2], [1, 1, 0, 0], [0, 0, 1, 1], [3, 0, 1, 0]]\n+    >>> connected_component_labeling_4([])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be non-empty\n+    >>> connected_component_labeling_4([[1, 1, 1], [1, 0], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be rectangular\n+    >>> connected_component_labeling_4([[1, 0, 0], [1, 0, 2], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must contain only 0s and 1s\n+    \"\"\"\n+\n+    if len(matrix) == 0 or len(matrix[0]) == 0:\n+        raise ValueError(\"Matrix must be non-empty\")\n+    if not all(len(row) == len(matrix[0]) for row in matrix):\n+        raise ValueError(\"Matrix must be rectangular\")\n+    if not all(all(x in (0, 1) for x in row) for row in matrix):\n+        raise ValueError(\"Matrix must contain only 0s and 1s\")\n+\n+    row_len = len(matrix[0])\n+\n+    # add padding\n+    padded_matrix = (\n+        [[0] * (row_len + 2)]\n+        + [[0] + row + [0] for row in matrix]\n+        + [[0] * (row_len + 2)]\n+    )\n+\n+    # after the first traverse, we might label some elements incorrectly.\n+    # when we spot incorrect labeling, we add neighbours label as a key\n+    # and the element label as a value\n+    mapping: dict = {}\n+\n+    next_label = 1\n+\n+    # first traverse\n+    for i in range(1, len(padded_matrix) - 1):\n+        for j in range(1, row_len + 1):\n+\n+            if padded_matrix[i][j] == 1:\n+\n+                neighbours = [\n+                    padded_matrix[i - 1][j - 1],\n+                    padded_matrix[i - 1][j],\n+                    padded_matrix[i - 1][j + 1],\n+                    padded_matrix[i][j - 1],\n+                ]\n+\n+                if any(neighbours):\n+\n+                    min_label = min([v for v in neighbours if v > 0])\n+                    padded_matrix[i][j] = min_label\n+\n+                    for label in neighbours:\n+                        # if any of the neighbors appears to have label \\\n+                        # greater than current element - add it's \\\n+                        # label to mapping\n+                        if label > min_label:\n+                            if label not in mapping:\n+                                mapping[label] = set()\n+                            mapping[label].add(min_label)\n+\n+                else:\n+                    padded_matrix[i][j] = next_label\n+                    next_label += 1\n+\n+    # \"unpad\" the matrix\n+    matrix = [row[1:-1] for row in padded_matrix[1:-1]]\n+\n+    # restructure the mapping, so that:\n+    # 3: (2, 1)         3: (2, 1)\n+    # 6: (3)        ->  6: (3, 1)\n+    #                   2: (1,)\n+    for key in sorted(mapping.keys()):\n+        adj_labels = mapping[key]\n+        min_label = min(adj_labels)\n+        for adj_label in adj_labels:\n+            if adj_label not in mapping:\n+                mapping[adj_label] = set()\n+            mapping[adj_label].add(min_label)\n+\n+    # apply the mapping\n+    for i in range(len(matrix)):\n+        for j in range(row_len):\n+            if matrix[i][j] in mapping:\n+                matrix[i][j] = min(mapping[matrix[i][j]])\n+\n+    return matrix\n+\n+\n+def connected_component_labeling_4(matrix: list) -> list:\n+    \"\"\"\n+    Connected Component Labeling algorithm (4-connectivity version)\n+    Takes a matrix of binary values and returns a matrix of labels,\n+    where each connected component is assigned a unique label.\n+\n+\n+    args:\n+        matrix: 2D list of 0s and 1s\n+\n+    returns:\n+        2D list of labels\n+\n+    >>> connected_component_labeling_4([[1, 0, 0, 1],\n+    ...                                 [1, 0, 0, 1],\n+    ...                                 [0, 0, 1, 0],\n+    ...                                 [0, 1, 1, 0]])\n+    [[1, 0, 0, 2], [1, 0, 0, 2], [0, 0, 3, 0], [0, 3, 3, 0]]\n+    >>> connected_component_labeling_4([[0, 1, 0, 1],\n+    ...                                 [1, 1, 0, 0],\n+    ...                                 [0, 0, 1, 1],\n+    ...                                 [1, 0, 1, 0]])\n+    [[0, 1, 0, 2], [1, 1, 0, 0], [0, 0, 4, 4], [5, 0, 4, 0]]\n+    >>> connected_component_labeling_4([])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be non-empty\n+    >>> connected_component_labeling_4([[1, 1, 1], [1, 0], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be rectangular\n+    >>> connected_component_labeling_4([[1, 0, 0], [1, 0, 2], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must contain only 0s and 1s\n+    \"\"\"\n+\n+    if len(matrix) == 0 or len(matrix[0]) == 0:\n+        raise ValueError(\"Matrix must be non-empty\")\n+    if not all(len(row) == len(matrix[0]) for row in matrix):\n+        raise ValueError(\"Matrix must be rectangular\")\n+    if not all(all(x in (0, 1) for x in row) for row in matrix):\n+        raise ValueError(\"Matrix must contain only 0s and 1s\")\n+\n+    row_len = len(matrix[0])\n+\n+    # add padding\n+    padded_matrix = (\n+        [[0] * (row_len + 2)]\n+        + [[0] + row + [0] for row in matrix]\n+        + [[0] * (row_len + 2)]\n+    )\n+\n+    # after the first traverse, we might label some elements incorrectly.\n+    # when we spot incorrect labeling, we add neighbours label as a key\n+    # and the element label as a value\n+    mapping: dict = {}\n+\n+    next_label = 1\n+\n+    # first traverse\n+    for i in range(1, len(padded_matrix) - 1):\n+        for j in range(1, row_len + 1):\n+\n+            if padded_matrix[i][j] == 1:\n+\n+                neighbours = [padded_matrix[i - 1][j], padded_matrix[i][j - 1]]\n+\n+                if any(neighbours):\n+\n+                    min_label = min([v for v in neighbours if v > 0])\n+                    padded_matrix[i][j] = min_label\n+\n+                    for label in neighbours:\n+                        # if any of the neighbors appears to have label \\\n+                        # greater than current element - add it's \\\n+                        # label to mapping\n+                        if label > min_label:\n+                            if label not in mapping:\n+                                mapping[label] = set()\n+                            mapping[label].add(min_label)\n+\n+                else:\n+                    padded_matrix[i][j] = next_label\n+                    next_label += 1\n+\n+    # \"unpad\" the matrix\n+    matrix = [row[1:-1] for row in padded_matrix[1:-1]]\n+\n+    # restructure the mapping, so that:\n+    # 3: (2, 1)         3: (2, 1)\n+    # 6: (3)        ->  6: (3, 1)\n+    #                   2: (1,)\n+    for key in sorted(mapping.keys()):\n+        adj_labels = mapping[key]\n+        min_label = min(adj_labels)\n+        for adj_label in adj_labels:\n+            if adj_label not in mapping:\n+                mapping[adj_label] = set()\n+            mapping[adj_label].add(min_label)\n+\n+    # apply the mapping\n+    for i in range(len(matrix)):\n+        for j in range(row_len):\n+            if matrix[i][j] in mapping:\n+                matrix[i][j] = min(mapping[matrix[i][j]])\n+\n+    return matrix\n+\n+\n+if __name__ == \"__main__\":\n+    import doctest\n+\n+    doctest.testmod()",
    "comment": "as far as i'm concerned, github automatically removes one space in the browser from the end of the file.\r\n\r\nif the space was not actually at the end of the file, github would have been marked it as shown below:\r\n![image](https://user-images.githubusercontent.com/32400447/195177146-df76edf2-d22e-4834-b74e-58fe9e6a7946.png)\r\n\r\nthank you :)",
    "line_number": 242,
    "enriched": "File: computer_vision/connected_component_labeling.py\nCode: @@ -0,0 +1,242 @@\n+# \"\"\"\n+#   Connected-Component Labeling\n+#\n+#   Source:\n+#   https://en.wikipedia.org/wiki/Connected-component_labeling\n+#\n+#   Basically, there's two versions of CCL two-pass algorithm:\n+#   4-connectivity and 8-connectivity.\n+#   4-connectivity means that only horizontal and vertical neighbours\n+#   are considered, while 8-connectivity means that diagonal neighbours\n+#   are also considered.\n+# \"\"\"\n+\n+\n+def connected_component_labeling_8(matrix: list) -> list:\n+    \"\"\"\n+    Connected Component Labeling algorithm (8-connectivity version)\n+    Takes a matrix of binary values and returns a matrix of labels,\n+    where each connected component is assigned a unique label.\n+\n+    args:\n+        matrix: 2D list of 0s and 1s\n+\n+    returns:\n+        2D list of labels\n+\n+    >>> connected_component_labeling_8([[1, 0, 0, 1],\n+    ...                                 [1, 0, 0, 1],\n+    ...                                 [0, 0, 1, 0],\n+    ...                                 [0, 1, 1, 0]])\n+    [[1, 0, 0, 2], [1, 0, 0, 2], [0, 0, 2, 0], [0, 2, 2, 0]]\n+    >>> connected_component_labeling_8([[0, 1, 0, 1],\n+    ...                                 [1, 1, 0, 0],\n+    ...                                 [0, 0, 1, 1],\n+    ...                                 [1, 0, 1, 0]])\n+    [[0, 1, 0, 2], [1, 1, 0, 0], [0, 0, 1, 1], [3, 0, 1, 0]]\n+    >>> connected_component_labeling_4([])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be non-empty\n+    >>> connected_component_labeling_4([[1, 1, 1], [1, 0], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be rectangular\n+    >>> connected_component_labeling_4([[1, 0, 0], [1, 0, 2], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must contain only 0s and 1s\n+    \"\"\"\n+\n+    if len(matrix) == 0 or len(matrix[0]) == 0:\n+        raise ValueError(\"Matrix must be non-empty\")\n+    if not all(len(row) == len(matrix[0]) for row in matrix):\n+        raise ValueError(\"Matrix must be rectangular\")\n+    if not all(all(x in (0, 1) for x in row) for row in matrix):\n+        raise ValueError(\"Matrix must contain only 0s and 1s\")\n+\n+    row_len = len(matrix[0])\n+\n+    # add padding\n+    padded_matrix = (\n+        [[0] * (row_len + 2)]\n+        + [[0] + row + [0] for row in matrix]\n+        + [[0] * (row_len + 2)]\n+    )\n+\n+    # after the first traverse, we might label some elements incorrectly.\n+    # when we spot incorrect labeling, we add neighbours label as a key\n+    # and the element label as a value\n+    mapping: dict = {}\n+\n+    next_label = 1\n+\n+    # first traverse\n+    for i in range(1, len(padded_matrix) - 1):\n+        for j in range(1, row_len + 1):\n+\n+            if padded_matrix[i][j] == 1:\n+\n+                neighbours = [\n+                    padded_matrix[i - 1][j - 1],\n+                    padded_matrix[i - 1][j],\n+                    padded_matrix[i - 1][j + 1],\n+                    padded_matrix[i][j - 1],\n+                ]\n+\n+                if any(neighbours):\n+\n+                    min_label = min([v for v in neighbours if v > 0])\n+                    padded_matrix[i][j] = min_label\n+\n+                    for label in neighbours:\n+                        # if any of the neighbors appears to have label \\\n+                        # greater than current element - add it's \\\n+                        # label to mapping\n+                        if label > min_label:\n+                            if label not in mapping:\n+                                mapping[label] = set()\n+                            mapping[label].add(min_label)\n+\n+                else:\n+                    padded_matrix[i][j] = next_label\n+                    next_label += 1\n+\n+    # \"unpad\" the matrix\n+    matrix = [row[1:-1] for row in padded_matrix[1:-1]]\n+\n+    # restructure the mapping, so that:\n+    # 3: (2, 1)         3: (2, 1)\n+    # 6: (3)        ->  6: (3, 1)\n+    #                   2: (1,)\n+    for key in sorted(mapping.keys()):\n+        adj_labels = mapping[key]\n+        min_label = min(adj_labels)\n+        for adj_label in adj_labels:\n+            if adj_label not in mapping:\n+                mapping[adj_label] = set()\n+            mapping[adj_label].add(min_label)\n+\n+    # apply the mapping\n+    for i in range(len(matrix)):\n+        for j in range(row_len):\n+            if matrix[i][j] in mapping:\n+                matrix[i][j] = min(mapping[matrix[i][j]])\n+\n+    return matrix\n+\n+\n+def connected_component_labeling_4(matrix: list) -> list:\n+    \"\"\"\n+    Connected Component Labeling algorithm (4-connectivity version)\n+    Takes a matrix of binary values and returns a matrix of labels,\n+    where each connected component is assigned a unique label.\n+\n+\n+    args:\n+        matrix: 2D list of 0s and 1s\n+\n+    returns:\n+        2D list of labels\n+\n+    >>> connected_component_labeling_4([[1, 0, 0, 1],\n+    ...                                 [1, 0, 0, 1],\n+    ...                                 [0, 0, 1, 0],\n+    ...                                 [0, 1, 1, 0]])\n+    [[1, 0, 0, 2], [1, 0, 0, 2], [0, 0, 3, 0], [0, 3, 3, 0]]\n+    >>> connected_component_labeling_4([[0, 1, 0, 1],\n+    ...                                 [1, 1, 0, 0],\n+    ...                                 [0, 0, 1, 1],\n+    ...                                 [1, 0, 1, 0]])\n+    [[0, 1, 0, 2], [1, 1, 0, 0], [0, 0, 4, 4], [5, 0, 4, 0]]\n+    >>> connected_component_labeling_4([])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be non-empty\n+    >>> connected_component_labeling_4([[1, 1, 1], [1, 0], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be rectangular\n+    >>> connected_component_labeling_4([[1, 0, 0], [1, 0, 2], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must contain only 0s and 1s\n+    \"\"\"\n+\n+    if len(matrix) == 0 or len(matrix[0]) == 0:\n+        raise ValueError(\"Matrix must be non-empty\")\n+    if not all(len(row) == len(matrix[0]) for row in matrix):\n+        raise ValueError(\"Matrix must be rectangular\")\n+    if not all(all(x in (0, 1) for x in row) for row in matrix):\n+        raise ValueError(\"Matrix must contain only 0s and 1s\")\n+\n+    row_len = len(matrix[0])\n+\n+    # add padding\n+    padded_matrix = (\n+        [[0] * (row_len + 2)]\n+        + [[0] + row + [0] for row in matrix]\n+        + [[0] * (row_len + 2)]\n+    )\n+\n+    # after the first traverse, we might label some elements incorrectly.\n+    # when we spot incorrect labeling, we add neighbours label as a key\n+    # and the element label as a value\n+    mapping: dict = {}\n+\n+    next_label = 1\n+\n+    # first traverse\n+    for i in range(1, len(padded_matrix) - 1):\n+        for j in range(1, row_len + 1):\n+\n+            if padded_matrix[i][j] == 1:\n+\n+                neighbours = [padded_matrix[i - 1][j], padded_matrix[i][j - 1]]\n+\n+                if any(neighbours):\n+\n+                    min_label = min([v for v in neighbours if v > 0])\n+                    padded_matrix[i][j] = min_label\n+\n+                    for label in neighbours:\n+                        # if any of the neighbors appears to have label \\\n+                        # greater than current element - add it's \\\n+                        # label to mapping\n+                        if label > min_label:\n+                            if label not in mapping:\n+                                mapping[label] = set()\n+                            mapping[label].add(min_label)\n+\n+                else:\n+                    padded_matrix[i][j] = next_label\n+                    next_label += 1\n+\n+    # \"unpad\" the matrix\n+    matrix = [row[1:-1] for row in padded_matrix[1:-1]]\n+\n+    # restructure the mapping, so that:\n+    # 3: (2, 1)         3: (2, 1)\n+    # 6: (3)        ->  6: (3, 1)\n+    #                   2: (1,)\n+    for key in sorted(mapping.keys()):\n+        adj_labels = mapping[key]\n+        min_label = min(adj_labels)\n+        for adj_label in adj_labels:\n+            if adj_label not in mapping:\n+                mapping[adj_label] = set()\n+            mapping[adj_label].add(min_label)\n+\n+    # apply the mapping\n+    for i in range(len(matrix)):\n+        for j in range(row_len):\n+            if matrix[i][j] in mapping:\n+                matrix[i][j] = min(mapping[matrix[i][j]])\n+\n+    return matrix\n+\n+\n+if __name__ == \"__main__\":\n+    import doctest\n+\n+    doctest.testmod()\nComment: As far as I'm concerned, github automatically removes one space in the browser from the end of the file.\r\n\r\nIf the space was not actually at the end of the file, github would have been marked it as shown below:\r\n![image](https://user-images.githubusercontent.com/32400447/195177146-df76edf2-d22e-4834-b74e-58fe9e6a7946.png)\r\n\r\nThank you :)\r\n",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "computer_vision/connected_component_labeling.py",
    "pr_number": 6960,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 992686904,
    "comment_created_at": "2022-10-11T19:06:11Z"
  },
  {
    "code": "@@ -8,7 +8,7 @@\n \n \n class FilterType(Protocol):\n-    def process(self, sample: float) -> float:\n+    def process(self, sample: float) -> float:  # noqa: ARG002",
    "comment": "what would happen if we delete the unused variables?",
    "line_number": 11,
    "enriched": "File: audio_filters/show_response.py\nCode: @@ -8,7 +8,7 @@\n \n \n class FilterType(Protocol):\n-    def process(self, sample: float) -> float:\n+    def process(self, sample: float) -> float:  # noqa: ARG002\nComment: What would happen if we delete the unused variables?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "audio_filters/show_response.py",
    "pr_number": 11382,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1586233967,
    "comment_created_at": "2024-05-01T12:06:10Z"
  },
  {
    "code": "@@ -27,7 +27,7 @@ def mf_knapsack(i, wt, val, j):\n \n \n def knapsack(w, wt, val, n):\n-    dp = [[0 for i in range(w + 1)] for j in range(n + 1)]\n+    dp = [[0 for _ in range(w + 1)] for _ in range(n + 1)]",
    "comment": "is some tool recommending that we do this?!?  if not then let's not do it as this **_reuse_** is unsettling.\r\n\r\nin general, i would rather that we do not pre-create a matrix unless it is vital.  it seems un-pythonic to do so.",
    "line_number": 30,
    "enriched": "File: dynamic_programming/knapsack.py\nCode: @@ -27,7 +27,7 @@ def mf_knapsack(i, wt, val, j):\n \n \n def knapsack(w, wt, val, n):\n-    dp = [[0 for i in range(w + 1)] for j in range(n + 1)]\n+    dp = [[0 for _ in range(w + 1)] for _ in range(n + 1)]\nComment: Is some tool recommending that we do this?!?  If not then let's not do it as this **_reuse_** is unsettling.\r\n```suggestion\r\n    dp = [[0] * (w + 1) for _ in range(n + 1)]\r\n```\r\nIn general, I would rather that we do not pre-create a matrix unless it is vital.  It seems un-Pythonic to do so.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "dynamic_programming/knapsack.py",
    "pr_number": 7271,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 999531579,
    "comment_created_at": "2022-10-19T14:23:59Z"
  },
  {
    "code": "@@ -659,7 +659,10 @@ def kullback_leibler_divergence(y_true: np.ndarray, y_pred: np.ndarray) -> float\n     if len(y_true) != len(y_pred):\n         raise ValueError(\"Input arrays must have the same length.\")\n \n-    kl_loss = y_true * np.log(y_true / y_pred)\n+    kl_loss = 0\n+    if y_true != 0:",
    "comment": "shouldn't you check the length of the y_true array greater than 0?",
    "line_number": 663,
    "enriched": "File: machine_learning/loss_functions.py\nCode: @@ -659,7 +659,10 @@ def kullback_leibler_divergence(y_true: np.ndarray, y_pred: np.ndarray) -> float\n     if len(y_true) != len(y_pred):\n         raise ValueError(\"Input arrays must have the same length.\")\n \n-    kl_loss = y_true * np.log(y_true / y_pred)\n+    kl_loss = 0\n+    if y_true != 0:\nComment: Shouldn't you check the length of the `y_true` array greater than 0?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "machine_learning/loss_functions.py",
    "pr_number": 12249,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1818136518,
    "comment_created_at": "2024-10-27T16:31:50Z"
  },
  {
    "code": "@@ -1,8 +1,8 @@\n alphabets = [chr(i) for i in range(32, 126)]\n-gear_one = [i for i in range(len(alphabets))]\n-gear_two = [i for i in range(len(alphabets))]\n-gear_three = [i for i in range(len(alphabets))]\n-reflector = [i for i in reversed(range(len(alphabets)))]\n+gear_one = range(len(alphabets))\n+gear_two = range(len(alphabets))\n+gear_three = range(len(alphabets))\n+reflector = reversed(range(len(alphabets)))",
    "comment": "the old code is constructing a list, but the updated code is giving us a range object. is this ok?",
    "line_number": 5,
    "enriched": "File: hashes/enigma_machine.py\nCode: @@ -1,8 +1,8 @@\n alphabets = [chr(i) for i in range(32, 126)]\n-gear_one = [i for i in range(len(alphabets))]\n-gear_two = [i for i in range(len(alphabets))]\n-gear_three = [i for i in range(len(alphabets))]\n-reflector = [i for i in reversed(range(len(alphabets)))]\n+gear_one = range(len(alphabets))\n+gear_two = range(len(alphabets))\n+gear_three = range(len(alphabets))\n+reflector = reversed(range(len(alphabets)))\nComment: The old code is constructing a list, but the updated code is giving us a `range` object. Is this ok?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "hashes/enigma_machine.py",
    "pr_number": 7235,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996319996,
    "comment_created_at": "2022-10-15T15:26:28Z"
  },
  {
    "code": "@@ -0,0 +1,327 @@\n+\"\"\"\n+Python implementation of the simplex algorithm for solving linear programs in\n+tabular form with\n+- `>=`, `<=`, and `=` constraints and\n+- each variable `x1, x2, ...>= 0`.\n+\n+See https://gist.github.com/imengus/f9619a568f7da5bc74eaf20169a24d98 for how\n+to convert linear programs to simplex tableaus, and the steps taken in the\n+simplex algorithm.\n+\n+Resources:\n+https://en.wikipedia.org/wiki/Linear_programming\n+https://en.wikipedia.org/wiki/Simplex_algorithm\n+\n+https://towardsdatascience.com/ \\\n+    a-beginners-guide-to-linear-programming-and-the-simplex-algorithm \\\n+    -87db017e92b4",
    "comment": "i can't click this link, either use a shortener or put it all on one line",
    "line_number": 17,
    "enriched": "File: linear_programming/simplex.py\nCode: @@ -0,0 +1,327 @@\n+\"\"\"\n+Python implementation of the simplex algorithm for solving linear programs in\n+tabular form with\n+- `>=`, `<=`, and `=` constraints and\n+- each variable `x1, x2, ...>= 0`.\n+\n+See https://gist.github.com/imengus/f9619a568f7da5bc74eaf20169a24d98 for how\n+to convert linear programs to simplex tableaus, and the steps taken in the\n+simplex algorithm.\n+\n+Resources:\n+https://en.wikipedia.org/wiki/Linear_programming\n+https://en.wikipedia.org/wiki/Simplex_algorithm\n+\n+https://towardsdatascience.com/ \\\n+    a-beginners-guide-to-linear-programming-and-the-simplex-algorithm \\\n+    -87db017e92b4\nComment: I can't click this link, either use a shortener or put it all on one line",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "linear_programming/simplex.py",
    "pr_number": 8825,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1232802680,
    "comment_created_at": "2023-06-16T21:33:53Z"
  },
  {
    "code": "@@ -0,0 +1,18 @@\n+# 5 Fibonacci series\n+n = int(input(\"Number of terms in Fibonacci series : \"))\n+a, b = 0, 1\n+count = 0\n+# check if the number of terms is valid\n+if n <= 0:\n+    print(\"Please enter a positive integer\")\n+elif n == 1:\n+    print(\"Fibonacci series upto\", n, \"terms is :\", a)\n+# generate fibonacci sequence\n+else:\n+    print(\"Fibonacci series upto \", n, \" terms is :\")\n+    while count < n:\n+        print(a, end=\" \")\n+        c = a + b\n+        a = b\n+        b = c\n+        count += 1",
    "comment": "did you do pre-commit run --all-files",
    "line_number": 18,
    "enriched": "File: fib.py\nCode: @@ -0,0 +1,18 @@\n+# 5 Fibonacci series\n+n = int(input(\"Number of terms in Fibonacci series : \"))\n+a, b = 0, 1\n+count = 0\n+# check if the number of terms is valid\n+if n <= 0:\n+    print(\"Please enter a positive integer\")\n+elif n == 1:\n+    print(\"Fibonacci series upto\", n, \"terms is :\", a)\n+# generate fibonacci sequence\n+else:\n+    print(\"Fibonacci series upto \", n, \" terms is :\")\n+    while count < n:\n+        print(a, end=\" \")\n+        c = a + b\n+        a = b\n+        b = c\n+        count += 1\nComment: did you do `pre-commit run --all-files`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "fib.py",
    "pr_number": 10979,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1372585394,
    "comment_created_at": "2023-10-26T04:59:50Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+\"\"\"\r\n+Program to find max length of valid paranthesis\r\n+author : aakash_2001\r\n+\"\"\"\r\n+def find_max_len(string) -> int:\r",
    "comment": "please provide type hint for the parameter: string",
    "line_number": 5,
    "enriched": "File: strings/valid_paranthesis.py\nCode: @@ -0,0 +1,56 @@\n+\"\"\"\r\n+Program to find max length of valid paranthesis\r\n+author : aakash_2001\r\n+\"\"\"\r\n+def find_max_len(string) -> int:\r\nComment: Please provide type hint for the parameter: `string`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "strings/valid_paranthesis.py",
    "pr_number": 7716,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1006611867,
    "comment_created_at": "2022-10-27T09:13:51Z"
  },
  {
    "code": "@@ -6,19 +6,61 @@\n https://en.wikipedia.org/wiki/Morse_code\n \"\"\"\n \n-# fmt: off\n MORSE_CODE_DICT = {\n-    \"A\": \".-\", \"B\": \"-...\", \"C\": \"-.-.\", \"D\": \"-..\", \"E\": \".\", \"F\": \"..-.\", \"G\": \"--.\",\n-    \"H\": \"....\", \"I\": \"..\", \"J\": \".---\", \"K\": \"-.-\", \"L\": \".-..\", \"M\": \"--\", \"N\": \"-.\",\n-    \"O\": \"---\", \"P\": \".--.\", \"Q\": \"--.-\", \"R\": \".-.\", \"S\": \"...\", \"T\": \"-\", \"U\": \"..-\",\n-    \"V\": \"...-\", \"W\": \".--\", \"X\": \"-..-\", \"Y\": \"-.--\", \"Z\": \"--..\", \"1\": \".----\",\n-    \"2\": \"..---\", \"3\": \"...--\", \"4\": \"....-\", \"5\": \".....\", \"6\": \"-....\", \"7\": \"--...\",\n-    \"8\": \"---..\", \"9\": \"----.\", \"0\": \"-----\", \"&\": \".-...\", \"@\": \".--.-.\",\n-    \":\": \"---...\", \",\": \"--..--\", \".\": \".-.-.-\", \"'\": \".----.\", '\"': \".-..-.\",\n-    \"?\": \"..--..\", \"/\": \"-..-.\", \"=\": \"-...-\", \"+\": \".-.-.\", \"-\": \"-....-\",\n-    \"(\": \"-.--.\", \")\": \"-.--.-\", \"!\": \"-.-.--\", \" \": \"/\"\n-}  # Exclamation mark is not in ITU-R recommendation\n-# fmt: on",
    "comment": "the current code uses # fmt: off and #fmt: on properly and i see no reason to mess with that.",
    "line_number": 21,
    "enriched": "File: ciphers/morse_code.py\nCode: @@ -6,19 +6,61 @@\n https://en.wikipedia.org/wiki/Morse_code\n \"\"\"\n \n-# fmt: off\n MORSE_CODE_DICT = {\n-    \"A\": \".-\", \"B\": \"-...\", \"C\": \"-.-.\", \"D\": \"-..\", \"E\": \".\", \"F\": \"..-.\", \"G\": \"--.\",\n-    \"H\": \"....\", \"I\": \"..\", \"J\": \".---\", \"K\": \"-.-\", \"L\": \".-..\", \"M\": \"--\", \"N\": \"-.\",\n-    \"O\": \"---\", \"P\": \".--.\", \"Q\": \"--.-\", \"R\": \".-.\", \"S\": \"...\", \"T\": \"-\", \"U\": \"..-\",\n-    \"V\": \"...-\", \"W\": \".--\", \"X\": \"-..-\", \"Y\": \"-.--\", \"Z\": \"--..\", \"1\": \".----\",\n-    \"2\": \"..---\", \"3\": \"...--\", \"4\": \"....-\", \"5\": \".....\", \"6\": \"-....\", \"7\": \"--...\",\n-    \"8\": \"---..\", \"9\": \"----.\", \"0\": \"-----\", \"&\": \".-...\", \"@\": \".--.-.\",\n-    \":\": \"---...\", \",\": \"--..--\", \".\": \".-.-.-\", \"'\": \".----.\", '\"': \".-..-.\",\n-    \"?\": \"..--..\", \"/\": \"-..-.\", \"=\": \"-...-\", \"+\": \".-.-.\", \"-\": \"-....-\",\n-    \"(\": \"-.--.\", \")\": \"-.--.-\", \"!\": \"-.-.--\", \" \": \"/\"\n-}  # Exclamation mark is not in ITU-R recommendation\n-# fmt: on\nComment: The current code uses `# fmt: off` and `#fmt: on` properly and I see no reason to mess with that.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "ciphers/morse_code.py",
    "pr_number": 8649,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1169789774,
    "comment_created_at": "2023-04-18T10:01:03Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+# Einstein's Mass-Energy Equivalence\n+# This code calculates the energy equivalent of a given mass using Einstein's mass-energy equivalence equation, E=mc\u00b2.\n+\n+# Source: https://en.wikipedia.org/wiki/Mass%E2%80%93energy_equivalence\n+\n+def mass_energy_equivalence(mass: float, c: float = 299792458) -> float:",
    "comment": "please provide descriptive name for the parameter: c",
    "line_number": 6,
    "enriched": "File: physics/einstein_mass_energy_equivalence.py\nCode: @@ -0,0 +1,35 @@\n+# Einstein's Mass-Energy Equivalence\n+# This code calculates the energy equivalent of a given mass using Einstein's mass-energy equivalence equation, E=mc\u00b2.\n+\n+# Source: https://en.wikipedia.org/wiki/Mass%E2%80%93energy_equivalence\n+\n+def mass_energy_equivalence(mass: float, c: float = 299792458) -> float:\nComment: Please provide descriptive name for the parameter: `c`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "physics/einstein_mass_energy_equivalence.py",
    "pr_number": 9224,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342097353,
    "comment_created_at": "2023-10-01T08:17:20Z"
  },
  {
    "code": "@@ -1,44 +1,25 @@\n-# Print all subset combinations of n element in given set of r element.\n-\n-\n-def combination_util(arr, n, r, index, data, i):\n-    \"\"\"\n-    Current combination is ready to be printed, print it\n-    arr[]  ---> Input Array\n-    data[] ---> Temporary array to store current combination\n-    start & end ---> Staring and Ending indexes in arr[]\n-    index  ---> Current index in data[]\n-    r ---> Size of a combination to be printed\n-    \"\"\"\n-    if index == r:\n-        for j in range(r):\n-            print(data[j], end=\" \")\n-        print(\" \")\n-        return\n-    #  When no more elements are there to put in data[]\n-    if i >= n:\n-        return\n-    # current is included, put next at next location\n-    data[index] = arr[i]\n-    combination_util(arr, n, r, index + 1, data, i + 1)\n-    # current is excluded, replace it with\n-    # next (Note that i+1 is passed, but\n-    # index is not changed)\n-    combination_util(arr, n, r, index, data, i + 1)\n-    # The main function that prints all combinations\n-    # of size r in arr[] of size n. This function\n-    # mainly uses combinationUtil()\n-\n-\n-def print_combination(arr, n, r):\n-    # A temporary array to store all combination one by one\n-    data = [0] * r\n-    # Print all combination using temporary array 'data[]'\n-    combination_util(arr, n, r, 0, data, 0)\n+# return all subset combinations of n element in given set of r element.\n+\n+\n+def subset_combinations_dp(elements, n):",
    "comment": "no type hints, no docstring, no doctests, n is not a self-documenting variable name.\r\n\r\nwhy ask me to review this pull request when we have 200+ open pull requests that are more in alignment with contributing.md?",
    "line_number": 4,
    "enriched": "File: dynamic_programming/subset_generation.py\nCode: @@ -1,44 +1,25 @@\n-# Print all subset combinations of n element in given set of r element.\n-\n-\n-def combination_util(arr, n, r, index, data, i):\n-    \"\"\"\n-    Current combination is ready to be printed, print it\n-    arr[]  ---> Input Array\n-    data[] ---> Temporary array to store current combination\n-    start & end ---> Staring and Ending indexes in arr[]\n-    index  ---> Current index in data[]\n-    r ---> Size of a combination to be printed\n-    \"\"\"\n-    if index == r:\n-        for j in range(r):\n-            print(data[j], end=\" \")\n-        print(\" \")\n-        return\n-    #  When no more elements are there to put in data[]\n-    if i >= n:\n-        return\n-    # current is included, put next at next location\n-    data[index] = arr[i]\n-    combination_util(arr, n, r, index + 1, data, i + 1)\n-    # current is excluded, replace it with\n-    # next (Note that i+1 is passed, but\n-    # index is not changed)\n-    combination_util(arr, n, r, index, data, i + 1)\n-    # The main function that prints all combinations\n-    # of size r in arr[] of size n. This function\n-    # mainly uses combinationUtil()\n-\n-\n-def print_combination(arr, n, r):\n-    # A temporary array to store all combination one by one\n-    data = [0] * r\n-    # Print all combination using temporary array 'data[]'\n-    combination_util(arr, n, r, 0, data, 0)\n+# return all subset combinations of n element in given set of r element.\n+\n+\n+def subset_combinations_dp(elements, n):\nComment: No type hints, No docstring, no doctests, `n` is not a self-documenting variable name.\r\n\r\nWhy ask me to review this pull request when we have 200+ open pull requests that are more in alignment with CONTRIBUTING.md?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "dynamic_programming/subset_generation.py",
    "pr_number": 10191,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359900443,
    "comment_created_at": "2023-10-15T15:40:01Z"
  },
  {
    "code": "@@ -35,17 +33,17 @@ def join(separator: str, separated: list[str]) -> str:\n     Additional test case with a different separator:\n     >>> join(\"-\", [\"apple\", \"banana\", \"cherry\"])\n     'apple-banana-cherry'\n+    >>> join(\",\", [\"\", \"\", \"\"])\n+    ',,,'  # This test will now pass correctly\n+\n     \"\"\"\n \n-    joined = \"\"\n-    for word_or_phrase in separated:\n-        if not isinstance(word_or_phrase, str):\n-            raise Exception(\"join() accepts only strings\")\n-        joined += word_or_phrase + separator\n+    if not all(isinstance(word_or_phrase, str) for word_or_phrase in separated):\n+        raise Exception(\"join() accepts only strings\")\n+\n+    joined = separator.join(separated)",
    "comment": "question about this join though. isn't the point of this repository to show possible implementations of python functions? because if that's the case, i was wondering why the join function now internally uses str.join?",
    "line_number": 44,
    "enriched": "File: strings/join.py\nCode: @@ -35,17 +33,17 @@ def join(separator: str, separated: list[str]) -> str:\n     Additional test case with a different separator:\n     >>> join(\"-\", [\"apple\", \"banana\", \"cherry\"])\n     'apple-banana-cherry'\n+    >>> join(\",\", [\"\", \"\", \"\"])\n+    ',,,'  # This test will now pass correctly\n+\n     \"\"\"\n \n-    joined = \"\"\n-    for word_or_phrase in separated:\n-        if not isinstance(word_or_phrase, str):\n-            raise Exception(\"join() accepts only strings\")\n-        joined += word_or_phrase + separator\n+    if not all(isinstance(word_or_phrase, str) for word_or_phrase in separated):\n+        raise Exception(\"join() accepts only strings\")\n+\n+    joined = separator.join(separated)\nComment: Question about this join though. Isn't the point of this repository to show possible implementations of python functions? Because if that's the case, I was wondering why the join function now internally uses str.join?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "strings/join.py",
    "pr_number": 12438,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1887684538,
    "comment_created_at": "2024-12-16T22:58:37Z"
  },
  {
    "code": "@@ -35,8 +35,8 @@ def binary_and(a: int, b: int) -> str:\n     if a < 0 or b < 0:\n         raise ValueError(\"the value of both inputs must be positive\")\n \n-    a_binary = str(bin(a))[2:]  # remove the leading \"0b\"",
    "comment": "why we need to more the comments?? \r\nin the given line of code, the comment is explaining what the line does, which is to remove the leading \"0b\" from the binary representation of the integer a. comments are useful for making the code more understandable to other developers (or to your future self) who might read the code later.",
    "line_number": 38,
    "enriched": "File: bit_manipulation/binary_and_operator.py\nCode: @@ -35,8 +35,8 @@ def binary_and(a: int, b: int) -> str:\n     if a < 0 or b < 0:\n         raise ValueError(\"the value of both inputs must be positive\")\n \n-    a_binary = str(bin(a))[2:]  # remove the leading \"0b\"\nComment: Why we need to more the comments?? \r\nIn the given line of code, the comment is explaining what the line does, which is to remove the leading \"0b\" from the binary representation of the integer a. Comments are useful for making the code more understandable to other developers (or to your future self) who might read the code later.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "bit_manipulation/binary_and_operator.py",
    "pr_number": 11307,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1503561897,
    "comment_created_at": "2024-02-27T02:41:37Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+\"\"\"\n+Title : calculating the time period of a simple pendulum\n+\n+A simple pendulum can be defined as a device where its point mass\n+is attached to a light inextensible string and suspended from a fixed\n+support. Equilibrium position is subjected to a restoring force when a\n+pendulum is displaced sideways from its resting due to its gravity\n+that will accelerate it back toward the equilibrium position. A simple\n+pendulum consists of a small metal ball (called bob) suspended by a\n+long thread from a rigid support by a massless and inextensible\n+string, such that the bob is free to swing back and forth. When the\n+bob from its mean position is dragged to one side and then released,\n+the pendulum is set to motion and the bob moves oppositely on either\n+side of its mean position. And when the pendulum bob is displaced it\n+oscillates on a plane about the vertical line through the support.\n+\n+The Period (T) of a simple pendulum is the time taken for the bob to complete one\n+oscillation about the mean position.\n+\n+Period (T) of a simple pendulum is T=2\u03c0\u221aL/g.\n+\n+where :\n+T = time period of the pendulum (in seconds)\n+L = length of the massless and inextensible string (in meters)\n+g = acceleration due to gravity (value taken : 9.8 m/s^2)\n+\u03c0 = mathematical constant (value taken : 3.14159)\n+\n+Reference : https://unacademy.com/content/nda/study-material/physics/what-is-the-time-period/#:~:text=The%20formula%20for%20determining%20the,full%20back%20and%20forth%20swing.\n+\"\"\"\n+\n+\n+def simple_pendulum_time_period(l: float) -> float:",
    "comment": "please provide descriptive name for the parameter: l",
    "line_number": 32,
    "enriched": "File: physics/simple_pendulum_time_period.py\nCode: @@ -0,0 +1,58 @@\n+\"\"\"\n+Title : calculating the time period of a simple pendulum\n+\n+A simple pendulum can be defined as a device where its point mass\n+is attached to a light inextensible string and suspended from a fixed\n+support. Equilibrium position is subjected to a restoring force when a\n+pendulum is displaced sideways from its resting due to its gravity\n+that will accelerate it back toward the equilibrium position. A simple\n+pendulum consists of a small metal ball (called bob) suspended by a\n+long thread from a rigid support by a massless and inextensible\n+string, such that the bob is free to swing back and forth. When the\n+bob from its mean position is dragged to one side and then released,\n+the pendulum is set to motion and the bob moves oppositely on either\n+side of its mean position. And when the pendulum bob is displaced it\n+oscillates on a plane about the vertical line through the support.\n+\n+The Period (T) of a simple pendulum is the time taken for the bob to complete one\n+oscillation about the mean position.\n+\n+Period (T) of a simple pendulum is T=2\u03c0\u221aL/g.\n+\n+where :\n+T = time period of the pendulum (in seconds)\n+L = length of the massless and inextensible string (in meters)\n+g = acceleration due to gravity (value taken : 9.8 m/s^2)\n+\u03c0 = mathematical constant (value taken : 3.14159)\n+\n+Reference : https://unacademy.com/content/nda/study-material/physics/what-is-the-time-period/#:~:text=The%20formula%20for%20determining%20the,full%20back%20and%20forth%20swing.\n+\"\"\"\n+\n+\n+def simple_pendulum_time_period(l: float) -> float:\nComment: Please provide descriptive name for the parameter: `l`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "physics/simple_pendulum_time_period.py",
    "pr_number": 9734,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346278609,
    "comment_created_at": "2023-10-04T18:13:03Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+\"\"\"\n+Implements the Exponential Linear Unit or ELU function.\n+\n+The function takes a vector of K real numbers and a real number alpha as\n+input and then applies the ELU function to each element of the vector.\n+\n+Script inspired from its corresponding Wikipedia article\n+https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def elu_activation(vector: np.array, alpha: float) -> np.array:",
    "comment": "mypy is not happy with the type hint:\r\nnone\r\nfunction \"numpy.core.multiarray.array\" is not valid as a type  [valid-type]mypy(error)\r\n\r\ni guess you need to change to np.ndarray and give it a type parameter.",
    "line_number": 14,
    "enriched": "File: maths/elu_activation.py\nCode: @@ -0,0 +1,39 @@\n+\"\"\"\n+Implements the Exponential Linear Unit or ELU function.\n+\n+The function takes a vector of K real numbers and a real number alpha as\n+input and then applies the ELU function to each element of the vector.\n+\n+Script inspired from its corresponding Wikipedia article\n+https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def elu_activation(vector: np.array, alpha: float) -> np.array:\nComment: Mypy is not happy with the type hint:\r\n```none\r\nFunction \"numpy.core.multiarray.array\" is not valid as a type  [valid-type]mypy(error)\r\n```\r\nI guess you need to change to `np.ndarray` and give it a type parameter.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "maths/elu_activation.py",
    "pr_number": 8694,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1179788828,
    "comment_created_at": "2023-04-27T23:14:17Z"
  },
  {
    "code": "@@ -13,26 +22,19 @@\n \n     url = f\"https://www.google.com/search?q={query}&num=100\"\n \n-    res = requests.get(\n+    res = httpx.get(\n         url,\n         headers={\"User-Agent\": str(UserAgent().random)},\n         timeout=10,\n     )\n+    print(BeautifulSoup(res.text, \"html.parser\"))",
    "comment": "extra print. maybe this was added for debug purpose?",
    "line_number": 30,
    "enriched": "File: web_programming/open_google_results.py\nCode: @@ -13,26 +22,19 @@\n \n     url = f\"https://www.google.com/search?q={query}&num=100\"\n \n-    res = requests.get(\n+    res = httpx.get(\n         url,\n         headers={\"User-Agent\": str(UserAgent().random)},\n         timeout=10,\n     )\n+    print(BeautifulSoup(res.text, \"html.parser\"))\nComment: Extra `print`. Maybe this was added for debug purpose?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "web_programming/open_google_results.py",
    "pr_number": 12744,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2087663345,
    "comment_created_at": "2025-05-13T21:29:44Z"
  },
  {
    "code": "@@ -128,7 +128,11 @@ addopts = [\n ]\n \n [tool.coverage.report]\n-omit = [\".env/*\"]\n+omit = [\n+  \".env/*\",\n+  \"project_euler/*\",\n+  \"scripts/*\"",
    "comment": "let's leave scripts in.  i dislike not testing things.\r\n\r\n\r\ni think the coverage report is way too long and users need to scroll across hundreds of lines (that no one reads) to find their failing pytests.\r\n\r\nwould it be possible to put coverage in a separate github actions step so its output is not tagged directly on the end of pytest output?",
    "line_number": 134,
    "enriched": "File: pyproject.toml\nCode: @@ -128,7 +128,11 @@ addopts = [\n ]\n \n [tool.coverage.report]\n-omit = [\".env/*\"]\n+omit = [\n+  \".env/*\",\n+  \"project_euler/*\",\n+  \"scripts/*\"\nComment: Let's leave scripts in.  I dislike not testing things.\r\n```suggestion\r\n```\r\n\r\nI think the coverage report is way too long and users need to scroll across hundreds of lines (that no one reads) to find their failing pytests.\r\n\r\nWould it be possible to put coverage in a separate GitHub Actions step so its output is not tagged directly on the end of pytest output?",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "pyproject.toml",
    "pr_number": 10469,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359638328,
    "comment_created_at": "2023-10-14T20:30:13Z"
  },
  {
    "code": "@@ -1,38 +1,69 @@\n \"\"\"\n-Reference: https://www.investopedia.com/terms/p/presentvalue.asp\n+Reference: https://www.auditexcel.co.za/blog/discounting-cash-flows-with-multiple-discount-rates/\n \n An algorithm that calculates the present value of a stream of yearly cash flows given...\n-1. The discount rate (as a decimal, not a percent)\n+1. An array containing tuples of discount rates and their associated duration.\n+\n+For example [(0.05, 2), (0.06, 3)] would mean that a discount rate of 5% is applied to\n+the first 2 cash flows, while a discount rate of 6% is applied to the next 3 cash flows.\n+\n+If the discount rate is fixed for the entire duration, then the user may provide just\n+the discount rate(as a float).\n+\n 2. An array of cash flows, with the index of the cash flow being the associated year\n \n Note: This algorithm assumes that cash flows are paid at the end of the specified year\n \"\"\"\n \n \n-def present_value(discount_rate: float, cash_flows: list[float]) -> float:\n+def present_value(discount_rates: list[tuple], cash_flows: list[float]) -> float:\n     \"\"\"\n     >>> present_value(0.13, [10, 20.70, -293, 297])",
    "comment": "i am confused as to why line 21 delivers 4.69 in the old code and 4.15 in the new.\r\nwill it do the same if 0.13 is changed to [0.13] on line 21?\r\ni would like us to keep the old tests (with modifications if required) to prove that the old and new are equivelent.",
    "line_number": 21,
    "enriched": "File: financial/present_value.py\nCode: @@ -1,38 +1,69 @@\n \"\"\"\n-Reference: https://www.investopedia.com/terms/p/presentvalue.asp\n+Reference: https://www.auditexcel.co.za/blog/discounting-cash-flows-with-multiple-discount-rates/\n \n An algorithm that calculates the present value of a stream of yearly cash flows given...\n-1. The discount rate (as a decimal, not a percent)\n+1. An array containing tuples of discount rates and their associated duration.\n+\n+For example [(0.05, 2), (0.06, 3)] would mean that a discount rate of 5% is applied to\n+the first 2 cash flows, while a discount rate of 6% is applied to the next 3 cash flows.\n+\n+If the discount rate is fixed for the entire duration, then the user may provide just\n+the discount rate(as a float).\n+\n 2. An array of cash flows, with the index of the cash flow being the associated year\n \n Note: This algorithm assumes that cash flows are paid at the end of the specified year\n \"\"\"\n \n \n-def present_value(discount_rate: float, cash_flows: list[float]) -> float:\n+def present_value(discount_rates: list[tuple], cash_flows: list[float]) -> float:\n     \"\"\"\n     >>> present_value(0.13, [10, 20.70, -293, 297])\nComment: I am confused as to why line 21 delivers `4.69` in the old code and `4.15` in the new.\r\nWill it do the same if `0.13` is changed to `[0.13]` on line 21?\r\nI would like us to keep the old tests (with modifications if required) to prove that the old and new are equivelent.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "financial/present_value.py",
    "pr_number": 8834,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1241931724,
    "comment_created_at": "2023-06-26T09:54:46Z"
  },
  {
    "code": "@@ -29,12 +29,20 @@ def is_prime(number: int) -> bool:\n     True\n     >>> is_prime(67483)\n     False\n+    >>> is_prime(16.1)\n+    Traceback (most recent call last):\n+        ...\n+    AssertionError: is_prime() only accepts positive integers\n+    >>> is_prime(-4)\n+    Traceback (most recent call last):\n+        ...\n+    AssertionError: is_prime() only accepts positive integers\n     \"\"\"\n \n     # precondition\n     assert isinstance(number, int) and (\n         number >= 0\n-    ), \"'number' must been an int and positive\"\n+    ), \"is_prime() only accepts positive integers\"",
    "comment": "could you change this assert to a valueerror instead?",
    "line_number": 45,
    "enriched": "File: maths/prime_check.py\nCode: @@ -29,12 +29,20 @@ def is_prime(number: int) -> bool:\n     True\n     >>> is_prime(67483)\n     False\n+    >>> is_prime(16.1)\n+    Traceback (most recent call last):\n+        ...\n+    AssertionError: is_prime() only accepts positive integers\n+    >>> is_prime(-4)\n+    Traceback (most recent call last):\n+        ...\n+    AssertionError: is_prime() only accepts positive integers\n     \"\"\"\n \n     # precondition\n     assert isinstance(number, int) and (\n         number >= 0\n-    ), \"'number' must been an int and positive\"\n+    ), \"is_prime() only accepts positive integers\"\nComment: Could you change this `assert` to a `ValueError` instead?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "maths/prime_check.py",
    "pr_number": 10930,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1372281983,
    "comment_created_at": "2023-10-25T20:18:37Z"
  },
  {
    "code": "@@ -0,0 +1,34 @@\n+def get_highest_set_bit_position(number: int) -> int:\n+    \"\"\"\n+    Returns position of the highest set bit of a number.\n+    Ref - https://graphics.stanford.edu/~seander/bithacks.html#IntegerLogObvious\n+    >>> get_highest_set_bit_position(25)\n+    5\n+    >>> get_highest_set_bit_position(37)\n+    6\n+    >>> get_highest_set_bit_position(1)\n+    1\n+    >>> get_highest_set_bit_position(4)\n+    3\n+    >>> get_highest_set_bit_position(0.8)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value must be an 'int' type\n+    \"\"\"\n+    if not isinstance(number, int):\n+        raise TypeError(\"Input value must be an 'int' type\")\n+\n+    position = 0\n+    while number:\n+        position += 1\n+        number >>= 1\n+        if number == 0:\n+            break",
    "comment": "what happens if we remove these two lines?",
    "line_number": 26,
    "enriched": "File: bit_manipulation/highest_set_bit.py\nCode: @@ -0,0 +1,34 @@\n+def get_highest_set_bit_position(number: int) -> int:\n+    \"\"\"\n+    Returns position of the highest set bit of a number.\n+    Ref - https://graphics.stanford.edu/~seander/bithacks.html#IntegerLogObvious\n+    >>> get_highest_set_bit_position(25)\n+    5\n+    >>> get_highest_set_bit_position(37)\n+    6\n+    >>> get_highest_set_bit_position(1)\n+    1\n+    >>> get_highest_set_bit_position(4)\n+    3\n+    >>> get_highest_set_bit_position(0.8)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value must be an 'int' type\n+    \"\"\"\n+    if not isinstance(number, int):\n+        raise TypeError(\"Input value must be an 'int' type\")\n+\n+    position = 0\n+    while number:\n+        position += 1\n+        number >>= 1\n+        if number == 0:\n+            break\nComment: What happens if we remove these two lines?\r\n```suggestion\r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "bit_manipulation/highest_set_bit.py",
    "pr_number": 7586,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004979052,
    "comment_created_at": "2022-10-25T21:26:27Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+def is_happy_number(number: int) -> bool:\r\n+    \"\"\"\r\n+    Check if a number is a happy number.\r\n+    https://en.wikipedia.org/wiki/Happy_number\r\n+     A happy number is defined by the following process:\r\n+    1. Starting with any positive integer, replace the number by\r\n+       the sum of the squares of its digits.\r\n+    2. Repeat the process until the number equals 1 (happy) or\r\n+       it loops endlessly in a cycle (not happy).\r\n+\r\n+    Args:\r\n+        n (int): The number to check for happiness.\r\n+\r\n+    Returns:\r\n+        bool: True if the number is a happy number, False otherwise.\r\n+\r\n+    Examples:\r\n+    >>> is_happy_number(19)\r\n+    True\r\n+    >>> is_happy_number(4)\r\n+    False\r\n+    >>> is_happy_number(23)\r\n+    True\r",
    "comment": "please add tests the break things.",
    "line_number": 23,
    "enriched": "File: maths/special_numbers/happy_number.py\nCode: @@ -0,0 +1,39 @@\n+def is_happy_number(number: int) -> bool:\r\n+    \"\"\"\r\n+    Check if a number is a happy number.\r\n+    https://en.wikipedia.org/wiki/Happy_number\r\n+     A happy number is defined by the following process:\r\n+    1. Starting with any positive integer, replace the number by\r\n+       the sum of the squares of its digits.\r\n+    2. Repeat the process until the number equals 1 (happy) or\r\n+       it loops endlessly in a cycle (not happy).\r\n+\r\n+    Args:\r\n+        n (int): The number to check for happiness.\r\n+\r\n+    Returns:\r\n+        bool: True if the number is a happy number, False otherwise.\r\n+\r\n+    Examples:\r\n+    >>> is_happy_number(19)\r\n+    True\r\n+    >>> is_happy_number(4)\r\n+    False\r\n+    >>> is_happy_number(23)\r\n+    True\r\nComment: Please add tests the break things.\r\n```suggestion\r\n    True\r\n    >>> is_happy_number(0)\r\n    ?\r\n    >>> is_happy_number(-19)\r\n    ?\r\n    >>> is_happy_number(19.1)\r\n    ?\r\n    >>> is_happy_number(\"Happy\")\r\n    ?\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/special_numbers/happy_number.py",
    "pr_number": 10864,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375818540,
    "comment_created_at": "2023-10-30T08:13:23Z"
  },
  {
    "code": "@@ -29,12 +28,14 @@ def __init__(self, key: int = 0):\n         # private field\n         self.__key = key\n \n-    def encrypt(self, content: str, key: int) -> list[str]:\n+    def encrypt(self, content: str, key: int, type_: int) -> list[str] | str:",
    "comment": "why don't you overload and typehint type_ to either literal 0 or literal 1",
    "line_number": 31,
    "enriched": "File: ciphers/xor_cipher.py\nCode: @@ -29,12 +28,14 @@ def __init__(self, key: int = 0):\n         # private field\n         self.__key = key\n \n-    def encrypt(self, content: str, key: int) -> list[str]:\n+    def encrypt(self, content: str, key: int, type_: int) -> list[str] | str:\nComment: Why don't you overload and typehint `type_` to either literal `0` or literal `1`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "ciphers/xor_cipher.py",
    "pr_number": 7112,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 995861312,
    "comment_created_at": "2022-10-14T15:02:46Z"
  },
  {
    "code": "@@ -112,17 +112,19 @@ def strassen(matrix1: list, matrix2: list) -> list:\n     [[139, 163], [121, 134], [100, 121]]\n     \"\"\"\n     if matrix_dimensions(matrix1)[1] != matrix_dimensions(matrix2)[0]:\n-        raise Exception(\n-            \"Unable to multiply these matrices, please check the dimensions. \\n\"\n-            f\"Matrix A:{matrix1} \\nMatrix B:{matrix2}\"\n+        msg = (\n+            \"Unable to multiply these matrices, please check the dimensions.\\n\"\n+            f\"Matrix A: {matrix1}\\n\"\n+            f\"Matrix B: {matrix2}\"\n         )\n+        raise Exception(msg)",
    "comment": "what is the point of this change? i preferred it without this change",
    "line_number": 120,
    "enriched": "File: divide_and_conquer/strassen_matrix_multiplication.py\nCode: @@ -112,17 +112,19 @@ def strassen(matrix1: list, matrix2: list) -> list:\n     [[139, 163], [121, 134], [100, 121]]\n     \"\"\"\n     if matrix_dimensions(matrix1)[1] != matrix_dimensions(matrix2)[0]:\n-        raise Exception(\n-            \"Unable to multiply these matrices, please check the dimensions. \\n\"\n-            f\"Matrix A:{matrix1} \\nMatrix B:{matrix2}\"\n+        msg = (\n+            \"Unable to multiply these matrices, please check the dimensions.\\n\"\n+            f\"Matrix A: {matrix1}\\n\"\n+            f\"Matrix B: {matrix2}\"\n         )\n+        raise Exception(msg)\nComment: What is the point of this change? I preferred it without this change ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "divide_and_conquer/strassen_matrix_multiplication.py",
    "pr_number": 8784,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1209382911,
    "comment_created_at": "2023-05-29T14:48:47Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+\"\"\"\n+# Git Push to Slack Notifier\n+\n+This Python script allows you to send a message to a Slack channel when a Git push event occurs in your repository. You can use this script to receive notifications about code changes directly in your Slack workspace.\n+\n+## Prerequisites\n+\n+Before using this script, make sure you have the following:\n+\n+- Python 3.x installed on your system.\n+- The `requests` library installed. You can install it using `pip`:\n+\n+   ```bash\n+   pip install requests\n+\n+\"\"\"\n+\n+import requests\n+import json\n+\n+# Your Slack webhook URL\n+slack_webhook_url = \"YOUR_SLACK_WEBHOOK_URL\"\n+\n+# Define the message to send to Slack\n+push_message = {\n+    \"text\": \"A Git push event has occurred!\",\n+    \"attachments\": [\n+        {\n+            \"color\": \"#36a64f\",  # Slack attachment color\n+            \"title\": \"Git Push Event\",\n+            \"fields\": [\n+                {\"title\": \"Repository\", \"value\": \"Your Repository Name\", \"short\": True},\n+                {\"title\": \"Branch\", \"value\": \"Branch Name\", \"short\": True},\n+                {\"title\": \"Committer\", \"value\": \"Committer Name\", \"short\": True},\n+            ],\n+        }\n+    ],\n+}\n+\n+# Send the message to Slack\n+response = requests.post(\n+    slack_webhook_url,\n+    data=json.dumps(push_message),",
    "comment": "i thought that requests.post() would automatically do the json.dumps() piece for you.\r\nhttps://requests.readthedocs.io/en/latest/api/#requests.post",
    "line_number": 43,
    "enriched": "File: web_programming/git_to_slack.py\nCode: @@ -0,0 +1,53 @@\n+\"\"\"\n+# Git Push to Slack Notifier\n+\n+This Python script allows you to send a message to a Slack channel when a Git push event occurs in your repository. You can use this script to receive notifications about code changes directly in your Slack workspace.\n+\n+## Prerequisites\n+\n+Before using this script, make sure you have the following:\n+\n+- Python 3.x installed on your system.\n+- The `requests` library installed. You can install it using `pip`:\n+\n+   ```bash\n+   pip install requests\n+\n+\"\"\"\n+\n+import requests\n+import json\n+\n+# Your Slack webhook URL\n+slack_webhook_url = \"YOUR_SLACK_WEBHOOK_URL\"\n+\n+# Define the message to send to Slack\n+push_message = {\n+    \"text\": \"A Git push event has occurred!\",\n+    \"attachments\": [\n+        {\n+            \"color\": \"#36a64f\",  # Slack attachment color\n+            \"title\": \"Git Push Event\",\n+            \"fields\": [\n+                {\"title\": \"Repository\", \"value\": \"Your Repository Name\", \"short\": True},\n+                {\"title\": \"Branch\", \"value\": \"Branch Name\", \"short\": True},\n+                {\"title\": \"Committer\", \"value\": \"Committer Name\", \"short\": True},\n+            ],\n+        }\n+    ],\n+}\n+\n+# Send the message to Slack\n+response = requests.post(\n+    slack_webhook_url,\n+    data=json.dumps(push_message),\nComment: I thought that `requests.post()` would automatically do the `json.dumps()` piece for you.\r\nhttps://requests.readthedocs.io/en/latest/api/#requests.post",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "web_programming/git_to_slack.py",
    "pr_number": 11087,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375709135,
    "comment_created_at": "2023-10-30T06:07:10Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+\"\"\"\r\n+In geometry, Stewart's theorem yields a relation between the\r\n+lengths of the sides and the length of a cevian in a triangle.\r\n+Its name is in honour of the Scottish mathematician Matthew\r\n+Stewart, who published the theorem in 1746.[1]\r\n+\r\n+Source: https://en.wikipedia.org/wiki/Stewart%27s_theorem\r\n+\"\"\"\r\n+\r\n+\r\n+def stewart(a: float, b: float, c: float, n: float, m: float) -> float:\r",
    "comment": "please provide descriptive name for the parameter: a\n\nplease provide descriptive name for the parameter: b\n\nplease provide descriptive name for the parameter: c\n\nplease provide descriptive name for the parameter: n\n\nplease provide descriptive name for the parameter: m",
    "line_number": 11,
    "enriched": "File: maths/stewart.py\nCode: @@ -0,0 +1,42 @@\n+\"\"\"\r\n+In geometry, Stewart's theorem yields a relation between the\r\n+lengths of the sides and the length of a cevian in a triangle.\r\n+Its name is in honour of the Scottish mathematician Matthew\r\n+Stewart, who published the theorem in 1746.[1]\r\n+\r\n+Source: https://en.wikipedia.org/wiki/Stewart%27s_theorem\r\n+\"\"\"\r\n+\r\n+\r\n+def stewart(a: float, b: float, c: float, n: float, m: float) -> float:\r\nComment: Please provide descriptive name for the parameter: `a`\n\nPlease provide descriptive name for the parameter: `b`\n\nPlease provide descriptive name for the parameter: `c`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `m`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/stewart.py",
    "pr_number": 9887,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1348162734,
    "comment_created_at": "2023-10-06T01:49:29Z"
  },
  {
    "code": "@@ -11,75 +11,54 @@\n from collections import defaultdict\n \n \n-class AssignmentUsingBitmask:\n-    def __init__(self, task_performed, total):\n-        self.total_tasks = total  # total no of tasks (N)\n-\n-        # DP table will have a dimension of (2^M)*N\n-        # initially all values are set to -1\n+class TaskAssignment:\n+    def __init__(self, task_performed, total_tasks):",
    "comment": "no type hint, no doctests?",
    "line_number": 15,
    "enriched": "File: dynamic_programming/bitmask.py\nCode: @@ -11,75 +11,54 @@\n from collections import defaultdict\n \n \n-class AssignmentUsingBitmask:\n-    def __init__(self, task_performed, total):\n-        self.total_tasks = total  # total no of tasks (N)\n-\n-        # DP table will have a dimension of (2^M)*N\n-        # initially all values are set to -1\n+class TaskAssignment:\n+    def __init__(self, task_performed, total_tasks):\nComment: No type hint, no doctests?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "dynamic_programming/bitmask.py",
    "pr_number": 9699,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1345963404,
    "comment_created_at": "2023-10-04T14:58:18Z"
  },
  {
    "code": "@@ -0,0 +1,100 @@\n+\"\"\"\n+Brent's Method for root finding.\n+\n+This function implements Brent's Method, an efficient algorithm for finding the\n+root of a function. It combines the bisection method, the secant method, and\n+inverse quadratic interpolation.\n+\n+Reference:\n+- https://en.wikipedia.org/wiki/Brent%27s_method\n+- https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brentq.html\n+\n+\n+>>> def f(x): return x**3 - x - 2\n+>>> round(brent_method(f, 1, 2), 6)\n+1.52138\n+>>> brent_method(f, 1, 1.5)  # No sign change, should raise an error\n+Traceback (most recent call last):\n+    ...\n+ValueError: f(a) and f(b) must have different signs\n+\"\"\"\n+\n+from collections.abc import Callable\n+\n+\n+def brent_method(\n+    f: Callable[[float], float],",
    "comment": "please provide descriptive name for the parameter: f",
    "line_number": 26,
    "enriched": "File: maths/numerical_analysis/brent_method.py\nCode: @@ -0,0 +1,100 @@\n+\"\"\"\n+Brent's Method for root finding.\n+\n+This function implements Brent's Method, an efficient algorithm for finding the\n+root of a function. It combines the bisection method, the secant method, and\n+inverse quadratic interpolation.\n+\n+Reference:\n+- https://en.wikipedia.org/wiki/Brent%27s_method\n+- https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brentq.html\n+\n+\n+>>> def f(x): return x**3 - x - 2\n+>>> round(brent_method(f, 1, 2), 6)\n+1.52138\n+>>> brent_method(f, 1, 1.5)  # No sign change, should raise an error\n+Traceback (most recent call last):\n+    ...\n+ValueError: f(a) and f(b) must have different signs\n+\"\"\"\n+\n+from collections.abc import Callable\n+\n+\n+def brent_method(\n+    f: Callable[[float], float],\nComment: Please provide descriptive name for the parameter: `f`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/numerical_analysis/brent_method.py",
    "pr_number": 13089,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2397200227,
    "comment_created_at": "2025-10-02T06:25:49Z"
  },
  {
    "code": "@@ -0,0 +1,33 @@\n+\"\"\"\n+Example to show how this method works\n+\n+>>> swap_case (\"Hello\")\n+hELLO\n+\n+>>> swap_case (\"water\")\n+WATER\n+\"\"\"\n+\n+\n+def swap_case(input_string) -> str:",
    "comment": "please provide type hint for the parameter: input_string",
    "line_number": 12,
    "enriched": "File: strings/swapcase.py\nCode: @@ -0,0 +1,33 @@\n+\"\"\"\n+Example to show how this method works\n+\n+>>> swap_case (\"Hello\")\n+hELLO\n+\n+>>> swap_case (\"water\")\n+WATER\n+\"\"\"\n+\n+\n+def swap_case(input_string) -> str:\nComment: Please provide type hint for the parameter: `input_string`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "strings/swapcase.py",
    "pr_number": 11745,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1787663750,
    "comment_created_at": "2024-10-04T12:45:46Z"
  },
  {
    "code": "@@ -0,0 +1,100 @@\n+\"\"\"\n+Range Maximum Query Problem:\n+\n+    Given an array of integers and q queries,\n+    For each query find the maximum value in the range [l, r).\n+\n+    The array is 0-indexed and the queries are 0-indexed.\n+\n+    More details:\n+        https://www.geeksforgeeks.org/range-maximum-query-using-sparse-table/\n+\"\"\"\n+\n+\n+class RangeMaximumQuery:\n+    def __init__(self, array: list) -> None:\n+        \"\"\"\n+        Initialize RangeMaximumQuery with given array.\n+\n+        Parameters:\n+            array: list[int]\n+\n+        Example:\n+        >>> rmq = RangeMaximumQuery([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n+        \"\"\"\n+        self.array = array\n+        self.size = len(array)",
    "comment": "please define a [__len__() function](https://docs.python.org/3/reference/datamodel.html#object.__len__) instead of using a size variable.",
    "line_number": 26,
    "enriched": "File: dynamic_programming/range_maximum_query.py\nCode: @@ -0,0 +1,100 @@\n+\"\"\"\n+Range Maximum Query Problem:\n+\n+    Given an array of integers and q queries,\n+    For each query find the maximum value in the range [l, r).\n+\n+    The array is 0-indexed and the queries are 0-indexed.\n+\n+    More details:\n+        https://www.geeksforgeeks.org/range-maximum-query-using-sparse-table/\n+\"\"\"\n+\n+\n+class RangeMaximumQuery:\n+    def __init__(self, array: list) -> None:\n+        \"\"\"\n+        Initialize RangeMaximumQuery with given array.\n+\n+        Parameters:\n+            array: list[int]\n+\n+        Example:\n+        >>> rmq = RangeMaximumQuery([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n+        \"\"\"\n+        self.array = array\n+        self.size = len(array)\nComment: Please define a [`__len__()` function](https://docs.python.org/3/reference/datamodel.html#object.__len__) instead of using a `size` variable.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "dynamic_programming/range_maximum_query.py",
    "pr_number": 8056,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1057915910,
    "comment_created_at": "2022-12-27T21:07:57Z"
  },
  {
    "code": "@@ -0,0 +1,63 @@\n+\"\"\"\n+Title : computing the Reynolds number to find\n+        out the type of flow (laminar or turbulent)\n+\n+Reynolds number is a dimensionless quantity that is used to determine\n+the type of flow pattern as laminar or turbulent while flowing through a\n+pipe. Reynolds number is defined by the ratio of inertial forces to that of\n+viscous forces.\n+\n+R = Inertial Forces / Viscous Forces\n+R = (\u03c1 * V * D)/\u03bc\n+\n+where :\n+\u03c1 = Density of fluid (in Kg/m^3)\n+D = Diameter of pipe through which fluid flows (in m)\n+V = Velocity of flow of the fluid (in m/s)\n+\u03bc = Viscosity of the fluid (in Ns/m^2)\n+\n+If the Reynolds number calculated is high (greater than 2000), then the\n+flow through the pipe is said to be turbulent. If Reynolds number is low\n+(less than 2000), the flow is said to be laminar. Numerically, these are\n+acceptable values, although in general the laminar and turbulent flows\n+are classified according to a range. Laminar flow falls below Reynolds\n+number of 1100 and turbulent falls in a range greater than 2200.\n+Laminar flow is the type of flow in which the fluid travels smoothly in\n+regular paths. Conversely, turbulent flow isn't smooth and follows an\n+irregular math with lots of mixing.",
    "comment": "typo",
    "line_number": 27,
    "enriched": "File: physics/reynolds_number.py\nCode: @@ -0,0 +1,63 @@\n+\"\"\"\n+Title : computing the Reynolds number to find\n+        out the type of flow (laminar or turbulent)\n+\n+Reynolds number is a dimensionless quantity that is used to determine\n+the type of flow pattern as laminar or turbulent while flowing through a\n+pipe. Reynolds number is defined by the ratio of inertial forces to that of\n+viscous forces.\n+\n+R = Inertial Forces / Viscous Forces\n+R = (\u03c1 * V * D)/\u03bc\n+\n+where :\n+\u03c1 = Density of fluid (in Kg/m^3)\n+D = Diameter of pipe through which fluid flows (in m)\n+V = Velocity of flow of the fluid (in m/s)\n+\u03bc = Viscosity of the fluid (in Ns/m^2)\n+\n+If the Reynolds number calculated is high (greater than 2000), then the\n+flow through the pipe is said to be turbulent. If Reynolds number is low\n+(less than 2000), the flow is said to be laminar. Numerically, these are\n+acceptable values, although in general the laminar and turbulent flows\n+are classified according to a range. Laminar flow falls below Reynolds\n+number of 1100 and turbulent falls in a range greater than 2200.\n+Laminar flow is the type of flow in which the fluid travels smoothly in\n+regular paths. Conversely, turbulent flow isn't smooth and follows an\n+irregular math with lots of mixing.\nComment: ```suggestion\r\nirregular path with lots of mixing.\r\n```\r\nTypo",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "physics/reynolds_number.py",
    "pr_number": 9913,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1351503035,
    "comment_created_at": "2023-10-10T05:15:55Z"
  },
  {
    "code": "@@ -0,0 +1,73 @@\n+# https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm\n+\n+\n+def knuth_morris_pratt(text: str, pattern: str) -> list[int]:",
    "comment": "there are a couple edgecases you could add support for:\r\n- an empty pattern\r\n- a pattern longer than the text",
    "line_number": 4,
    "enriched": "File: searches/knuth_morris_pratt_algorithm.py\nCode: @@ -0,0 +1,73 @@\n+# https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm\n+\n+\n+def knuth_morris_pratt(text: str, pattern: str) -> list[int]:\nComment: There are a couple edgecases you could add support for:\r\n- an empty pattern\r\n- a pattern longer than the text",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "searches/knuth_morris_pratt_algorithm.py",
    "pr_number": 9366,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1792539943,
    "comment_created_at": "2024-10-08T21:35:24Z"
  },
  {
    "code": "@@ -0,0 +1,38 @@\n+\"\"\"\n+Squareplus Activation Function\n+\n+Use Case: Squareplus designed to enhance positive values and suppress negative values.\n+For more detailed information, you can refer to the following link:\n+https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Squareplus\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def squareplus(vector: np.ndarray, beta: float) -> np.ndarray:\n+    \"\"\"\n+    Implements the SquarePlus activation function.\n+\n+    Parameters:\n+        vector (np.ndarray): The input array for the SquarePlus activation.\n+        beta (float): size of the curved region\n+\n+    Returns:\n+        np.ndarray: The input array after applying the SquarePlus activation.\n+\n+    Formula: f(x) = ( x + (x^2 + b)**0.5 ) / 2",
    "comment": "small fix in the formula to make it more readable",
    "line_number": 23,
    "enriched": "File: neural_network/activation_functions/squareplus.py\nCode: @@ -0,0 +1,38 @@\n+\"\"\"\n+Squareplus Activation Function\n+\n+Use Case: Squareplus designed to enhance positive values and suppress negative values.\n+For more detailed information, you can refer to the following link:\n+https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Squareplus\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def squareplus(vector: np.ndarray, beta: float) -> np.ndarray:\n+    \"\"\"\n+    Implements the SquarePlus activation function.\n+\n+    Parameters:\n+        vector (np.ndarray): The input array for the SquarePlus activation.\n+        beta (float): size of the curved region\n+\n+    Returns:\n+        np.ndarray: The input array after applying the SquarePlus activation.\n+\n+    Formula: f(x) = ( x + (x^2 + b)**0.5 ) / 2\nComment: ```suggestion\r\n    Formula: f(x) = ( x + sqrt(x^2 + b) ) / 2\r\n```\r\nSmall fix in the formula to make it more readable",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "neural_network/activation_functions/squareplus.py",
    "pr_number": 9977,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349739937,
    "comment_created_at": "2023-10-08T16:05:54Z"
  },
  {
    "code": "@@ -28,6 +28,59 @@ def find_min(nums: list[int | float]) -> int | float:\n     return min_num\n \n \n+# Divide and Conquer algorithm\n+def find_min_recursive(nums: list[int | float], left: int, right: int) -> int | float:\n+    \"\"\"\n+    find min value in list\n+    :param nums: contains elements\n+    :param left: index of first element\n+    :param right: index of last element\n+    :return: min in nums\n+\n+    >>> for nums in ([3, 2, 1], [-3, -2, -1], [3, -3, 0], [3.0, 3.1, 2.9]):\n+    ...     find_min_recursive(nums, 0, len(nums) - 1) == min(nums)\n+    True\n+    True\n+    True\n+    True\n+    >>> nums = [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\n+    >>> find_min_recursive(nums, 0, len(nums) - 1) == min(nums)\n+    True\n+    >>> find_min_recursive([], 0, 0)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: find_min_recursive() arg is an empty sequence\n+    >>> find_min_recursive(nums, 0, len(nums)) == min(nums)\n+    Traceback (most recent call last):\n+        ...\n+    IndexError: list index out of range\n+    >>> find_min_recursive(nums, -len(nums), -1) == min(nums)\n+    True\n+    >>> find_min_recursive(nums, -len(nums) - 1, -1) == min(nums)\n+    Traceback (most recent call last):\n+        ...\n+    IndexError: list index out of range\n+    \"\"\"\n+    if len(nums) == 0:\n+        raise ValueError(\"find_min_recursive() arg is an empty sequence\")\n+    if (\n+        left >= len(nums)\n+        or left < -len(nums)\n+        or right >= len(nums)\n+        or right < -len(nums)\n+    ):\n+        raise IndexError(\"list index out of range\")\n+    if left == right:\n+        return nums[left]\n+    mid = (left + right) >> 1  # the middle",
    "comment": "if the goal of this line is to take the int average of left and right, then i think this is clearer",
    "line_number": 75,
    "enriched": "File: maths/find_min.py\nCode: @@ -28,6 +28,59 @@ def find_min(nums: list[int | float]) -> int | float:\n     return min_num\n \n \n+# Divide and Conquer algorithm\n+def find_min_recursive(nums: list[int | float], left: int, right: int) -> int | float:\n+    \"\"\"\n+    find min value in list\n+    :param nums: contains elements\n+    :param left: index of first element\n+    :param right: index of last element\n+    :return: min in nums\n+\n+    >>> for nums in ([3, 2, 1], [-3, -2, -1], [3, -3, 0], [3.0, 3.1, 2.9]):\n+    ...     find_min_recursive(nums, 0, len(nums) - 1) == min(nums)\n+    True\n+    True\n+    True\n+    True\n+    >>> nums = [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\n+    >>> find_min_recursive(nums, 0, len(nums) - 1) == min(nums)\n+    True\n+    >>> find_min_recursive([], 0, 0)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: find_min_recursive() arg is an empty sequence\n+    >>> find_min_recursive(nums, 0, len(nums)) == min(nums)\n+    Traceback (most recent call last):\n+        ...\n+    IndexError: list index out of range\n+    >>> find_min_recursive(nums, -len(nums), -1) == min(nums)\n+    True\n+    >>> find_min_recursive(nums, -len(nums) - 1, -1) == min(nums)\n+    Traceback (most recent call last):\n+        ...\n+    IndexError: list index out of range\n+    \"\"\"\n+    if len(nums) == 0:\n+        raise ValueError(\"find_min_recursive() arg is an empty sequence\")\n+    if (\n+        left >= len(nums)\n+        or left < -len(nums)\n+        or right >= len(nums)\n+        or right < -len(nums)\n+    ):\n+        raise IndexError(\"list index out of range\")\n+    if left == right:\n+        return nums[left]\n+    mid = (left + right) >> 1  # the middle\nComment: ```suggestion\r\n    mid = (left + right) // 2  # the middle\r\n```\r\nIf the goal of this line is to take the int average of `left` and `right`, then I think this is clearer",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "maths/find_min.py",
    "pr_number": 8103,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1097736658,
    "comment_created_at": "2023-02-06T17:56:41Z"
  },
  {
    "code": "@@ -6,7 +6,7 @@\n from __future__ import annotations\n \n import numpy as np\n-import numpy.typing as NDArray\n+import numpy.typing as NDArray  # noqa: N812",
    "comment": "and then on lines 14 and 15 change ndarray --> arraylike",
    "line_number": 9,
    "enriched": "File: arithmetic_analysis/lu_decomposition.py\nCode: @@ -6,7 +6,7 @@\n from __future__ import annotations\n \n import numpy as np\n-import numpy.typing as NDArray\n+import numpy.typing as NDArray  # noqa: N812\nComment: ```suggestion\r\nfrom numpy.typing import ArrayLike\r\n```\r\nAnd then on lines 14 and 15 change `NDArray` --> `ArrayLike`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "arithmetic_analysis/lu_decomposition.py",
    "pr_number": 7062,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 993779382,
    "comment_created_at": "2022-10-12T18:29:47Z"
  },
  {
    "code": "@@ -0,0 +1,25 @@\n+# First, make sure you have Matplotlib installed. You can install it using pip if you haven't already",
    "comment": "make sure pass through ruff",
    "line_number": 1,
    "enriched": "File: physics/bernouli_principle.py\nCode: @@ -0,0 +1,25 @@\n+# First, make sure you have Matplotlib installed. You can install it using pip if you haven't already\nComment: make sure pass through ruff",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "physics/bernouli_principle.py",
    "pr_number": 10309,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1356543451,
    "comment_created_at": "2023-10-12T09:23:03Z"
  },
  {
    "code": "@@ -0,0 +1,47 @@\n+\"\"\"\r\n+A NAND Gate is a logic gate in boolean algebra which results to 0 (False) if both the\r\n+inputs are 1, and 1 (True) otherwise. It's similar to adding\r\n+a NOT gate along with an AND gate.\r\n+Following is the truth table of a NAND Gate:\r\n+    ------------------------------\r\n+    | Input 1 | Input 2 | Output |\r\n+    ------------------------------\r\n+    |    0    |    0    |    1   |\r\n+    |    0    |    1    |    1   |\r\n+    |    1    |    0    |    1   |\r\n+    |    1    |    1    |    0   |\r\n+    ------------------------------\r\n+Refer - https://www.geeksforgeeks.org/logic-gates-in-python/\r\n+\"\"\"\r\n+\r\n+\r\n+def nand_gate(input_1: int, input_2: int) -> int:\r\n+    \"\"\"\r\n+    Calculate AND of the input values\r",
    "comment": "and -> nand",
    "line_number": 20,
    "enriched": "File: boolean_algebra/nand_gate.py\nCode: @@ -0,0 +1,47 @@\n+\"\"\"\r\n+A NAND Gate is a logic gate in boolean algebra which results to 0 (False) if both the\r\n+inputs are 1, and 1 (True) otherwise. It's similar to adding\r\n+a NOT gate along with an AND gate.\r\n+Following is the truth table of a NAND Gate:\r\n+    ------------------------------\r\n+    | Input 1 | Input 2 | Output |\r\n+    ------------------------------\r\n+    |    0    |    0    |    1   |\r\n+    |    0    |    1    |    1   |\r\n+    |    1    |    0    |    1   |\r\n+    |    1    |    1    |    0   |\r\n+    ------------------------------\r\n+Refer - https://www.geeksforgeeks.org/logic-gates-in-python/\r\n+\"\"\"\r\n+\r\n+\r\n+def nand_gate(input_1: int, input_2: int) -> int:\r\n+    \"\"\"\r\n+    Calculate AND of the input values\r\nComment: AND -> NAND",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "boolean_algebra/nand_gate.py",
    "pr_number": 7596,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1003372515,
    "comment_created_at": "2022-10-24T14:17:56Z"
  },
  {
    "code": "@@ -61,23 +61,35 @@ def get_failure_array(pattern: str) -> list[int]:\n     pattern = \"abc1abc12\"\n     text1 = \"alskfjaldsabc1abc1abc12k23adsfabcabc\"\n     text2 = \"alskfjaldsk23adsfabcabc\"\n-    assert kmp(pattern, text1) and not kmp(pattern, text2)\n+    assert knuth_morris_pratt(text1, pattern) and knuth_morris_pratt(text2, pattern)\n \n     # Test 2)\n     pattern = \"ABABX\"\n     text = \"ABABZABABYABABX\"\n-    assert kmp(pattern, text)\n+    assert knuth_morris_pratt(text, pattern)\n \n     # Test 3)\n     pattern = \"AAAB\"\n     text = \"ABAAAAAB\"\n-    assert kmp(pattern, text)\n+    assert knuth_morris_pratt(text, pattern)\n \n     # Test 4)\n     pattern = \"abcdabcy\"\n     text = \"abcxabcdabxabcdabcdabcy\"\n-    assert kmp(pattern, text)\n-\n-    # Test 5)\n+    assert knuth_morris_pratt(text, pattern)\n+\n+    # Test 5) -> Doctests\n+    kmp = \"knuth_morris_pratt\"\n+    assert knuth_morris_pratt(kmp, \"kn\") == kmp.find(\"kn\")\n+    assert knuth_morris_pratt(kmp, \"h_m\") == kmp.find(\"h_m\")\n+    assert knuth_morris_pratt(kmp, \"rr\") == kmp.find(\"rr\")\n+    assert knuth_morris_pratt(kmp, \"tt\") == kmp.find(\"tt\")\n+    assert knuth_morris_pratt(kmp, \"not there\") == kmp.find(\"not there\")",
    "comment": "remove the duplicates.",
    "line_number": 87,
    "enriched": "File: strings/knuth_morris_pratt.py\nCode: @@ -61,23 +61,35 @@ def get_failure_array(pattern: str) -> list[int]:\n     pattern = \"abc1abc12\"\n     text1 = \"alskfjaldsabc1abc1abc12k23adsfabcabc\"\n     text2 = \"alskfjaldsk23adsfabcabc\"\n-    assert kmp(pattern, text1) and not kmp(pattern, text2)\n+    assert knuth_morris_pratt(text1, pattern) and knuth_morris_pratt(text2, pattern)\n \n     # Test 2)\n     pattern = \"ABABX\"\n     text = \"ABABZABABYABABX\"\n-    assert kmp(pattern, text)\n+    assert knuth_morris_pratt(text, pattern)\n \n     # Test 3)\n     pattern = \"AAAB\"\n     text = \"ABAAAAAB\"\n-    assert kmp(pattern, text)\n+    assert knuth_morris_pratt(text, pattern)\n \n     # Test 4)\n     pattern = \"abcdabcy\"\n     text = \"abcxabcdabxabcdabcdabcy\"\n-    assert kmp(pattern, text)\n-\n-    # Test 5)\n+    assert knuth_morris_pratt(text, pattern)\n+\n+    # Test 5) -> Doctests\n+    kmp = \"knuth_morris_pratt\"\n+    assert knuth_morris_pratt(kmp, \"kn\") == kmp.find(\"kn\")\n+    assert knuth_morris_pratt(kmp, \"h_m\") == kmp.find(\"h_m\")\n+    assert knuth_morris_pratt(kmp, \"rr\") == kmp.find(\"rr\")\n+    assert knuth_morris_pratt(kmp, \"tt\") == kmp.find(\"tt\")\n+    assert knuth_morris_pratt(kmp, \"not there\") == kmp.find(\"not there\")\nComment: Remove the duplicates.\r\n```suggestion\r\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "strings/knuth_morris_pratt.py",
    "pr_number": 9083,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1340512077,
    "comment_created_at": "2023-09-28T18:10:26Z"
  },
  {
    "code": "@@ -0,0 +1,131 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval: int | str | float = None) -> None:\n+        \"\"\"\n+        An empty left and right branches will be created for every value inserted,\n+        to perform better in recursive methods\n+        \"\"\"\n+        self.value = initval\n+        if self.value:\n+            self.left = Tree()\n+            self.right = Tree()\n+        else:\n+            self.left = None\n+            self.right = None\n+        return\n+\n+    # Empty nodes are None valued\n+    def isempty(self) -> bool:\n+        \"\"\"\n+        Returns true if tree is empty else False\n+        >>> isempty([])\n+        True\n+        >>> isempty([1,2,3])\n+        False\n+        \"\"\"\n+        return self.value == None\n+\n+    def isleaf(self) -> bool:\n+        \"\"\"\n+        Returns true if leaf is a node\n+        Suppose in [1,2,3,4,5,6,7,8,9,10]\n+        4 had empty left and right branches.\n+        >>> isleaf(4)\n+        True\n+        \"\"\"\n+        return self.value != None and self.left.isempty() and self.right.isempty()\n+\n+    # Inorder Traversal\n+    def inorder(self) -> list:\n+        \"\"\"\n+        Returns a list sorted in increasing order from the Binary Search Tree.\n+        >>> tree_sort([23235,82107,35775,91961,4323,40556,76603,64302,27316,74372])\n+        [4323, 23235, 27316, 35775, 40556, 64302, 74372, 76603, 82107, 91961]\n+        >>> tree_sort([50,52,54,74,93,100,114,124,130,143])\n+        [50, 52, 54, 74, 93, 100, 114, 124, 130, 143]\n+        \"\"\"\n+        # Corner Case\n+        if self.isempty():\n+            return []\n+        else:\n+            return self.left.inorder() + [self.value] + self.right.inorder()\n+\n+    # Display Tree\n+    def __str__(self) -> str:\n+        \"\"\"\n+        Prints the sorted tree\n+        Suppose tree is t.inorder() = [1,2,3]\n+        >>> print(t)\n+        [1,2,3]\n+        \"\"\"\n+        return str(self.inorder())\n+\n+    # Insert new element\n+    def insert(self, data: int | str | float) -> None:\n+        \"\"\"\n+        Inserts the value data to the Binary Search Tree\n+        Let\n+        t = Tree()\n+        >>> t.insert(1)\n+        [1]\n+        >>> t.insert(2)\n+        [1,2]\n+        >>> t.insert(3)\n+        [1,2,3]\n+        \"\"\"\n+        # Create new tree\n+        if self.isempty():\n+            self.value = data\n+            self.left = Tree()\n+            self.right = Tree()\n+\n+        # Value already exists\n+        if self.value == data:\n+            return\n+\n+        if data < self.value:\n+            self.left.insert(data)\n+            return\n+        if data > self.value:\n+            self.right.insert(data)\n+            return\n+\n+\n+def tree_sort(list_data: list) -> list:\n+    \"\"\"\n+    Returns a list array sorted in increasing order from the Binary Search Tree.\n+    >>> tree_sort([23235,82107,35775,91961,4323,40556,76603,64302,27316,74372])\n+    [4323, 23235, 27316, 35775, 40556, 64302, 74372, 76603, 82107, 91961]\n+    >>> tree_sort([50,52,54,74,93,100,114,124,130,143])\n+    [50, 52, 54, 74, 93, 100, 114, 124, 130, 143]\n+    \"\"\"\n+    if len(list_data) == 0 or len(list_data) == 1:\n+        return list_data\n+    tree = Tree()\n+    for i in list_data:\n+        tree.insert(i)\n+    return tree.inorder()\n+\n+\n+if __name__ == \"__main__\":",
    "comment": "can you run the doctest's in here please",
    "line_number": 128,
    "enriched": "File: sorts/tree_sort_2.py\nCode: @@ -0,0 +1,131 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval: int | str | float = None) -> None:\n+        \"\"\"\n+        An empty left and right branches will be created for every value inserted,\n+        to perform better in recursive methods\n+        \"\"\"\n+        self.value = initval\n+        if self.value:\n+            self.left = Tree()\n+            self.right = Tree()\n+        else:\n+            self.left = None\n+            self.right = None\n+        return\n+\n+    # Empty nodes are None valued\n+    def isempty(self) -> bool:\n+        \"\"\"\n+        Returns true if tree is empty else False\n+        >>> isempty([])\n+        True\n+        >>> isempty([1,2,3])\n+        False\n+        \"\"\"\n+        return self.value == None\n+\n+    def isleaf(self) -> bool:\n+        \"\"\"\n+        Returns true if leaf is a node\n+        Suppose in [1,2,3,4,5,6,7,8,9,10]\n+        4 had empty left and right branches.\n+        >>> isleaf(4)\n+        True\n+        \"\"\"\n+        return self.value != None and self.left.isempty() and self.right.isempty()\n+\n+    # Inorder Traversal\n+    def inorder(self) -> list:\n+        \"\"\"\n+        Returns a list sorted in increasing order from the Binary Search Tree.\n+        >>> tree_sort([23235,82107,35775,91961,4323,40556,76603,64302,27316,74372])\n+        [4323, 23235, 27316, 35775, 40556, 64302, 74372, 76603, 82107, 91961]\n+        >>> tree_sort([50,52,54,74,93,100,114,124,130,143])\n+        [50, 52, 54, 74, 93, 100, 114, 124, 130, 143]\n+        \"\"\"\n+        # Corner Case\n+        if self.isempty():\n+            return []\n+        else:\n+            return self.left.inorder() + [self.value] + self.right.inorder()\n+\n+    # Display Tree\n+    def __str__(self) -> str:\n+        \"\"\"\n+        Prints the sorted tree\n+        Suppose tree is t.inorder() = [1,2,3]\n+        >>> print(t)\n+        [1,2,3]\n+        \"\"\"\n+        return str(self.inorder())\n+\n+    # Insert new element\n+    def insert(self, data: int | str | float) -> None:\n+        \"\"\"\n+        Inserts the value data to the Binary Search Tree\n+        Let\n+        t = Tree()\n+        >>> t.insert(1)\n+        [1]\n+        >>> t.insert(2)\n+        [1,2]\n+        >>> t.insert(3)\n+        [1,2,3]\n+        \"\"\"\n+        # Create new tree\n+        if self.isempty():\n+            self.value = data\n+            self.left = Tree()\n+            self.right = Tree()\n+\n+        # Value already exists\n+        if self.value == data:\n+            return\n+\n+        if data < self.value:\n+            self.left.insert(data)\n+            return\n+        if data > self.value:\n+            self.right.insert(data)\n+            return\n+\n+\n+def tree_sort(list_data: list) -> list:\n+    \"\"\"\n+    Returns a list array sorted in increasing order from the Binary Search Tree.\n+    >>> tree_sort([23235,82107,35775,91961,4323,40556,76603,64302,27316,74372])\n+    [4323, 23235, 27316, 35775, 40556, 64302, 74372, 76603, 82107, 91961]\n+    >>> tree_sort([50,52,54,74,93,100,114,124,130,143])\n+    [50, 52, 54, 74, 93, 100, 114, 124, 130, 143]\n+    \"\"\"\n+    if len(list_data) == 0 or len(list_data) == 1:\n+        return list_data\n+    tree = Tree()\n+    for i in list_data:\n+        tree.insert(i)\n+    return tree.inorder()\n+\n+\n+if __name__ == \"__main__\":\nComment: Can you run the doctest's in here please\r\n```suggestion\r\nif __name__ == \"__main__\":\r\n    import doctest\r\n    \r\n    doctest.testmod()\r\n    \r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "sorts/tree_sort_2.py",
    "pr_number": 7462,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000831332,
    "comment_created_at": "2022-10-20T15:59:37Z"
  },
  {
    "code": "@@ -10,13 +10,12 @@\n \n import math\n \n-import qiskit\n-from qiskit import Aer, ClassicalRegister, QuantumCircuit, QuantumRegister, execute\n+import qiskit as q",
    "comment": "using just q reduces readability for new users of qiskit and does not really save much space.",
    "line_number": 13,
    "enriched": "File: quantum/q_full_adder.py\nCode: @@ -10,13 +10,12 @@\n \n import math\n \n-import qiskit\n-from qiskit import Aer, ClassicalRegister, QuantumCircuit, QuantumRegister, execute\n+import qiskit as q\nComment: Using just `q` reduces readability for new users of qiskit and does not really save much space.\r\n```suggestion\r\nimport qiskit\r\n```",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "quantum/q_full_adder.py",
    "pr_number": 7417,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 998936286,
    "comment_created_at": "2022-10-19T04:44:28Z"
  },
  {
    "code": "@@ -0,0 +1,49 @@\n+A greedy algorithm is an algorithmic paradigm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. In other words, it makes the best choice at each step without considering the long-term consequences. Greedy algorithms are often used for optimization problems, where the goal is to find the best solution among a set of possible solutions.",
    "comment": "an error occurred while parsing the file: greedy_methods/greedy.py\npython\ntraceback (most recent call last):\n  file \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 146, in parse\n    reports = lint_file(\n              ^^^^^^^^^^\nlibcst._exceptions.parsersyntaxerror: syntax error @ 1:1.\ntokenizer error: unterminated string literal\n\na greedy algorithm is an algorithmic paradigm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. in other words, it makes the best choice at each step without considering the long-term consequences. greedy algorithms are often used for optimization problems, where the goal is to find the best solution among a set of possible solutions.\n^",
    "line_number": 1,
    "enriched": "File: greedy_methods/greedy.py\nCode: @@ -0,0 +1,49 @@\n+A greedy algorithm is an algorithmic paradigm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. In other words, it makes the best choice at each step without considering the long-term consequences. Greedy algorithms are often used for optimization problems, where the goal is to find the best solution among a set of possible solutions.\nComment: An error occurred while parsing the file: `greedy_methods/greedy.py`\n```python\nTraceback (most recent call last):\n  File \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 146, in parse\n    reports = lint_file(\n              ^^^^^^^^^^\nlibcst._exceptions.ParserSyntaxError: Syntax Error @ 1:1.\ntokenizer error: unterminated string literal\n\nA greedy algorithm is an algorithmic paradigm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. In other words, it makes the best choice at each step without considering the long-term consequences. Greedy algorithms are often used for optimization problems, where the goal is to find the best solution among a set of possible solutions.\n^\n\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "greedy_methods/greedy.py",
    "pr_number": 11103,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1376564783,
    "comment_created_at": "2023-10-30T17:16:15Z"
  },
  {
    "code": "@@ -33,10 +33,10 @@ def retroactive_resolution(\n \n     x: NDArray[float64] = np.zeros((rows, 1), dtype=float)\n     for row in reversed(range(rows)):\n-        total = 0\n-        for col in range(row + 1, columns):\n-            total += coefficients[row, col] * x[col]\n-\n+        total = np.dot(\n+            [coefficients[row, col] for col in range(row + 1, columns)],\n+            [x[col] for col in range(row + 1, columns)],\n+        )",
    "comment": "i believe you can simplify this even further using numpy array slicing:",
    "line_number": 39,
    "enriched": "File: arithmetic_analysis/gaussian_elimination.py\nCode: @@ -33,10 +33,10 @@ def retroactive_resolution(\n \n     x: NDArray[float64] = np.zeros((rows, 1), dtype=float)\n     for row in reversed(range(rows)):\n-        total = 0\n-        for col in range(row + 1, columns):\n-            total += coefficients[row, col] * x[col]\n-\n+        total = np.dot(\n+            [coefficients[row, col] for col in range(row + 1, columns)],\n+            [x[col] for col in range(row + 1, columns)],\n+        )\nComment: I believe you can simplify this even further using numpy array slicing:\r\n```suggestion\r\n        total = np.dot(coefficients[row, row + 1 :], x[row + 1 :])\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "arithmetic_analysis/gaussian_elimination.py",
    "pr_number": 8987,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1298398126,
    "comment_created_at": "2023-08-18T12:32:28Z"
  },
  {
    "code": "@@ -1,3 +1,6 @@\n+from math import sqrt\n+",
    "comment": "import doctest and call the testmod()",
    "line_number": 2,
    "enriched": "File: maths/factors.py\nCode: @@ -1,3 +1,6 @@\n+from math import sqrt\n+\nComment: import doctest and call the `testmod()`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/factors.py",
    "pr_number": 7429,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 999567845,
    "comment_created_at": "2022-10-19T14:45:44Z"
  },
  {
    "code": "@@ -3,36 +3,40 @@\n \n def climb_stairs(number_of_steps: int) -> int:\n     \"\"\"\n-    LeetCdoe No.70: Climbing Stairs\n-    Distinct ways to climb a number_of_steps staircase where each time you can either\n-    climb 1 or 2 steps.\n+    Calculate the number of distinct ways to climb a staircase with a given number of steps.\n+\n+    This problem is from LeetCode No. 70: Climbing Stairs.\n \n     Args:\n-        number_of_steps: number of steps on the staircase\n+        number_of_steps (int): The number of steps on the staircase.\n \n     Returns:\n-        Distinct ways to climb a number_of_steps staircase\n+        int: The number of distinct ways to climb the staircase.\n \n     Raises:\n-        AssertionError: number_of_steps not positive integer\n-\n-    >>> climb_stairs(3)\n-    3\n-    >>> climb_stairs(1)\n-    1\n-    >>> climb_stairs(-7)  # doctest: +ELLIPSIS\n-    Traceback (most recent call last):\n-        ...\n-    AssertionError: number_of_steps needs to be positive integer, your input -7\n+        AssertionError: If number_of_steps is not a positive integer.\n+\n+    Example:\n+        >>> climb_stairs(3)\n+        3\n+        >>> climb_stairs(1)\n+        1\n+        >>> climb_stairs(-7)  # doctest: +ELLIPSIS\n+        Traceback (most recent call last):\n+            ...\n+        AssertionError: number_of_steps needs to be a positive integer, your input: -7\n     \"\"\"\n     assert (\n         isinstance(number_of_steps, int) and number_of_steps > 0\n-    ), f\"number_of_steps needs to be positive integer, your input {number_of_steps}\"\n+    ), f\"number_of_steps needs to be a positive integer, your input: {number_of_steps}\"",
    "comment": "please change this assert to a valueerror",
    "line_number": 31,
    "enriched": "File: dynamic_programming/climbing_stairs.py\nCode: @@ -3,36 +3,40 @@\n \n def climb_stairs(number_of_steps: int) -> int:\n     \"\"\"\n-    LeetCdoe No.70: Climbing Stairs\n-    Distinct ways to climb a number_of_steps staircase where each time you can either\n-    climb 1 or 2 steps.\n+    Calculate the number of distinct ways to climb a staircase with a given number of steps.\n+\n+    This problem is from LeetCode No. 70: Climbing Stairs.\n \n     Args:\n-        number_of_steps: number of steps on the staircase\n+        number_of_steps (int): The number of steps on the staircase.\n \n     Returns:\n-        Distinct ways to climb a number_of_steps staircase\n+        int: The number of distinct ways to climb the staircase.\n \n     Raises:\n-        AssertionError: number_of_steps not positive integer\n-\n-    >>> climb_stairs(3)\n-    3\n-    >>> climb_stairs(1)\n-    1\n-    >>> climb_stairs(-7)  # doctest: +ELLIPSIS\n-    Traceback (most recent call last):\n-        ...\n-    AssertionError: number_of_steps needs to be positive integer, your input -7\n+        AssertionError: If number_of_steps is not a positive integer.\n+\n+    Example:\n+        >>> climb_stairs(3)\n+        3\n+        >>> climb_stairs(1)\n+        1\n+        >>> climb_stairs(-7)  # doctest: +ELLIPSIS\n+        Traceback (most recent call last):\n+            ...\n+        AssertionError: number_of_steps needs to be a positive integer, your input: -7\n     \"\"\"\n     assert (\n         isinstance(number_of_steps, int) and number_of_steps > 0\n-    ), f\"number_of_steps needs to be positive integer, your input {number_of_steps}\"\n+    ), f\"number_of_steps needs to be a positive integer, your input: {number_of_steps}\"\nComment: Please change this assert to a ValueError",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/climbing_stairs.py",
    "pr_number": 10685,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1365580805,
    "comment_created_at": "2023-10-19T13:50:36Z"
  },
  {
    "code": "@@ -1,8 +1,28 @@\n-# https://en.wikipedia.org/wiki/Trifid_cipher\r\n+\"\"\"\r\n+The trifid cipher uses a table to fractionate each plaintext letter into a\r\n+trigram,mixes the constituents of the trigrams, and then applies the table\r\n+in reverse to turn these mixed trigrams into ciphertext letters.\r\n+https://en.wikipedia.org/wiki/Trifid_cipher\r\n+\"\"\"\r\n+\r\n from __future__ import annotations\r\n \r\n \r",
    "comment": "let's define character_to_number just once outside the functions.\r\n\r\nnumber_to_character can be created from character_to_number in one line of code.\r\n\r\nhttps://github.com/thealgorithms/python/blob/579937613a6dc7e099b710e3d57767a2fab115ad/ciphers/morse_code.py#l9-l22",
    "line_number": 10,
    "enriched": "File: ciphers/trifid_cipher.py\nCode: @@ -1,8 +1,28 @@\n-# https://en.wikipedia.org/wiki/Trifid_cipher\r\n+\"\"\"\r\n+The trifid cipher uses a table to fractionate each plaintext letter into a\r\n+trigram,mixes the constituents of the trigrams, and then applies the table\r\n+in reverse to turn these mixed trigrams into ciphertext letters.\r\n+https://en.wikipedia.org/wiki/Trifid_cipher\r\n+\"\"\"\r\n+\r\n from __future__ import annotations\r\n \r\n \r\nComment: Let's define `character_to_number` just once outside the functions.\r\n\r\n`number_to_character` can be created from `character_to_number` in one line of code.\r\n\r\nhttps://github.com/TheAlgorithms/Python/blob/579937613a6dc7e099b710e3d57767a2fab115ad/ciphers/morse_code.py#L9-L22",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "ciphers/trifid_cipher.py",
    "pr_number": 10716,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1366863821,
    "comment_created_at": "2023-10-20T11:43:32Z"
  },
  {
    "code": "@@ -0,0 +1,520 @@\n+\"\"\"",
    "comment": "please rename the file path to machine_learning/random_forest/random_forest.py",
    "line_number": 1,
    "enriched": "File: machine_learning/Random Forest/RandomForest.py\nCode: @@ -0,0 +1,520 @@\n+\"\"\"\nComment: Please rename the file path to `machine_learning/random_forest/random_forest.py`",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "machine_learning/Random Forest/RandomForest.py",
    "pr_number": 11018,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375030099,
    "comment_created_at": "2023-10-27T20:47:40Z"
  },
  {
    "code": "@@ -208,27 +207,33 @@ def make_highshelf(\n \n     >>> filter = make_highshelf(1000, 48000, 6)\n     >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE\n-    [2.2229172136088806, -3.9587208137297303, 1.7841414181566304, 4.295432981120543,\n-     -7.922740859457287, 3.6756456963725253]\n+    [2.2229172136088806, -3.616080374557819, 1.4122939941901636, 0.5690651073937642,\n+     -1.3335115824061685, 0.7666141956641752]\n     \"\"\"\n     w0 = tau * frequency / samplerate\n     _sin = sin(w0)\n     _cos = cos(w0)\n     alpha = _sin / (2 * q_factor)\n     big_a = 10 ** (gain_db / 40)\n-    pmc = (big_a + 1) - (big_a - 1) * _cos\n-    ppmc = (big_a + 1) + (big_a - 1) * _cos\n-    mpc = (big_a - 1) - (big_a + 1) * _cos\n-    pmpc = (big_a - 1) + (big_a + 1) * _cos\n+    pmc = (big_a + 1) + (big_a - 1) * _cos\n+    ppmc = (big_a + 1) - (big_a - 1) * _cos\n+    mpc = (big_a - 1) + (big_a + 1) * _cos\n+    pmpc = (big_a - 1) - (big_a + 1) * _cos\n     aa2 = 2 * sqrt(big_a) * alpha\n \n-    b0 = big_a * (ppmc + aa2)\n-    b1 = -2 * big_a * pmpc\n-    b2 = big_a * (ppmc - aa2)\n-    a0 = pmc + aa2\n-    a1 = 2 * mpc\n-    a2 = pmc - aa2\n+    b0 = big_a * (pmc - aa2)\n+    b1 = -2 * big_a * mpc\n+    b2 = big_a * (pmc + aa2)\n+    a0 = ppmc - aa2\n+    a1 = 2 * pmpc\n+    a2 = ppmc + aa2\n \n     filt = IIRFilter(2)\n     filt.set_coefficients([a0, a1, a2], [b0, b1, b2])\n     return filt\n+\n+\n+if __name__ == \"__main__\":\n+    import doctest\n+\n+    doctest.testmod()",
    "comment": "from math import cos, sin, sqrt, tau\r\nfrom audio_filters.iir_filter import iirfilter\r\n\r\n\"\"\"\r\ncreate 2nd-order iir filters with butterworth design.\r\ncode based on https://webaudio.github.io/audio-eq-cookbook/audio-eq-cookbook.html\r\nalternatively, you can use scipy.signal.butter, which should yield the same results.\r\n\"\"\"\r\n\r\ndef make_highshelf(frequency, samplerate, gain_db, q_factor=0.707):\r\n    \"\"\"\r\n    creates a high-shelf filter based on the specified parameters.\r\n\r\n    args:\r\n        frequency (float): center frequency of the filter.\r\n        samplerate (float): sampling rate.\r\n        gain_db (float): gain in decibels.\r\n        q_factor (float): quality factor.\r\n\r\n    returns:\r\n        iirfilter: configured high-shelf filter.\r\n\r\n    examples:\r\n        >>> filter = make_highshelf(1000, 48000, 6)\r\n        >>> filter.a_coeffs  # doctest: +normalize_whitespace\r\n        [2.2229172136088806, -3.9587208137297303, 1.7841414181566304]\r\n        >>> filter.b_coeffs  # doctest: +normalize_whitespace\r\n        [4.295432981120543, -7.922740859457287, 3.6756456963725253]\r\n    \"\"\"\r\n    w0 = tau * frequency / samplerate\r\n    _sin = sin(w0)\r\n    _cos = cos(w0)\r\n    alpha = _sin / (2 * q_factor)\r\n    big_a = 10 ** (gain_db / 40)\r\n\r\n    pmc = (big_a + 1) + (big_a - 1) * _cos\r\n    ppmc = (big_a + 1) - (big_a - 1) * _cos\r\n    mpc = (big_a - 1) + (big_a + 1) * _cos\r\n    pmpc = (big_a - 1) - (big_a + 1) * _cos\r\n    aa2 = 2 * sqrt(big_a) * alpha\r\n\r\n    b0 = big_a * (ppmc + aa2)\r\n    b1 = -2 * big_a * pmpc\r\n    b2 = big_a * (ppmc - aa2)\r\n    a0 = pmc + aa2\r\n    a1 = -2 * mpc\r\n    a2 = pmc - aa2\r\n\r\n    filt = iirfilter(2)\r\n    filt.set_coefficients([a0, a1, a2], [b0, b1, b2])\r\n    return filt\r\n\r\nif __name__ == \"__main__\":\r\n    import doctest\r\n    doctest.testmod()",
    "line_number": 239,
    "enriched": "File: audio_filters/butterworth_filter.py\nCode: @@ -208,27 +207,33 @@ def make_highshelf(\n \n     >>> filter = make_highshelf(1000, 48000, 6)\n     >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE\n-    [2.2229172136088806, -3.9587208137297303, 1.7841414181566304, 4.295432981120543,\n-     -7.922740859457287, 3.6756456963725253]\n+    [2.2229172136088806, -3.616080374557819, 1.4122939941901636, 0.5690651073937642,\n+     -1.3335115824061685, 0.7666141956641752]\n     \"\"\"\n     w0 = tau * frequency / samplerate\n     _sin = sin(w0)\n     _cos = cos(w0)\n     alpha = _sin / (2 * q_factor)\n     big_a = 10 ** (gain_db / 40)\n-    pmc = (big_a + 1) - (big_a - 1) * _cos\n-    ppmc = (big_a + 1) + (big_a - 1) * _cos\n-    mpc = (big_a - 1) - (big_a + 1) * _cos\n-    pmpc = (big_a - 1) + (big_a + 1) * _cos\n+    pmc = (big_a + 1) + (big_a - 1) * _cos\n+    ppmc = (big_a + 1) - (big_a - 1) * _cos\n+    mpc = (big_a - 1) + (big_a + 1) * _cos\n+    pmpc = (big_a - 1) - (big_a + 1) * _cos\n     aa2 = 2 * sqrt(big_a) * alpha\n \n-    b0 = big_a * (ppmc + aa2)\n-    b1 = -2 * big_a * pmpc\n-    b2 = big_a * (ppmc - aa2)\n-    a0 = pmc + aa2\n-    a1 = 2 * mpc\n-    a2 = pmc - aa2\n+    b0 = big_a * (pmc - aa2)\n+    b1 = -2 * big_a * mpc\n+    b2 = big_a * (pmc + aa2)\n+    a0 = ppmc - aa2\n+    a1 = 2 * pmpc\n+    a2 = ppmc + aa2\n \n     filt = IIRFilter(2)\n     filt.set_coefficients([a0, a1, a2], [b0, b1, b2])\n     return filt\n+\n+\n+if __name__ == \"__main__\":\n+    import doctest\n+\n+    doctest.testmod()\nComment: ```\r\nfrom math import cos, sin, sqrt, tau\r\nfrom audio_filters.iir_filter import IIRFilter\r\n\r\n\"\"\"\r\nCreate 2nd-order IIR filters with Butterworth design.\r\nCode based on https://webaudio.github.io/Audio-EQ-Cookbook/audio-eq-cookbook.html\r\nAlternatively, you can use scipy.signal.butter, which should yield the same results.\r\n\"\"\"\r\n\r\ndef make_highshelf(frequency, samplerate, gain_db, q_factor=0.707):\r\n    \"\"\"\r\n    Creates a high-shelf filter based on the specified parameters.\r\n\r\n    Args:\r\n        frequency (float): Center frequency of the filter.\r\n        samplerate (float): Sampling rate.\r\n        gain_db (float): Gain in decibels.\r\n        q_factor (float): Quality factor.\r\n\r\n    Returns:\r\n        IIRFilter: Configured high-shelf filter.\r\n\r\n    Examples:\r\n        >>> filter = make_highshelf(1000, 48000, 6)\r\n        >>> filter.a_coeffs  # doctest: +NORMALIZE_WHITESPACE\r\n        [2.2229172136088806, -3.9587208137297303, 1.7841414181566304]\r\n        >>> filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE\r\n        [4.295432981120543, -7.922740859457287, 3.6756456963725253]\r\n    \"\"\"\r\n    w0 = tau * frequency / samplerate\r\n    _sin = sin(w0)\r\n    _cos = cos(w0)\r\n    alpha = _sin / (2 * q_factor)\r\n    big_a = 10 ** (gain_db / 40)\r\n\r\n    pmc = (big_a + 1) + (big_a - 1) * _cos\r\n    ppmc = (big_a + 1) - (big_a - 1) * _cos\r\n    mpc = (big_a - 1) + (big_a + 1) * _cos\r\n    pmpc = (big_a - 1) - (big_a + 1) * _cos\r\n    aa2 = 2 * sqrt(big_a) * alpha\r\n\r\n    b0 = big_a * (ppmc + aa2)\r\n    b1 = -2 * big_a * pmpc\r\n    b2 = big_a * (ppmc - aa2)\r\n    a0 = pmc + aa2\r\n    a1 = -2 * mpc\r\n    a2 = pmc - aa2\r\n\r\n    filt = IIRFilter(2)\r\n    filt.set_coefficients([a0, a1, a2], [b0, b1, b2])\r\n    return filt\r\n\r\nif __name__ == \"__main__\":\r\n    import doctest\r\n    doctest.testmod()\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "audio_filters/butterworth_filter.py",
    "pr_number": 12348,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1830371455,
    "comment_created_at": "2024-11-06T04:23:55Z"
  },
  {
    "code": "@@ -136,22 +153,21 @@ def get_successors(self, parent: Node) -> list[Node]:\n             pos_x = parent.pos_x + action[1]",
    "comment": "you could get the successors using a list comprehension:\r\npy\r\nsuccessors = [\r\n    node(\r\n        (pos_x := parent.pos_x + action[1]),\r\n        (pos_y := parent.pos_y + action[0]),\r\n        self.target.pos_x,\r\n        self.target.pos_y,\r\n        parent.g_cost + 1,\r\n        parent,\r\n    )\r\n    for action in delta\r\n    if (\r\n        0 <= pos_x < len(self.grid[0])\r\n        and 0 <= pos_y < len(self.grid)\r\n        and self.grid[pos_y][pos_x] == 0\r\n    )\r\n]\r\n\r\n\r\nbut ultimately i could go either way since this isn't necessarily more readable than what you already have.",
    "line_number": 153,
    "enriched": "File: graphs/greedy_best_first.py\nCode: @@ -136,22 +153,21 @@ def get_successors(self, parent: Node) -> list[Node]:\n             pos_x = parent.pos_x + action[1]\nComment: You could get the successors using a list comprehension:\r\n```py\r\nsuccessors = [\r\n    Node(\r\n        (pos_x := parent.pos_x + action[1]),\r\n        (pos_y := parent.pos_y + action[0]),\r\n        self.target.pos_x,\r\n        self.target.pos_y,\r\n        parent.g_cost + 1,\r\n        parent,\r\n    )\r\n    for action in delta\r\n    if (\r\n        0 <= pos_x < len(self.grid[0])\r\n        and 0 <= pos_y < len(self.grid)\r\n        and self.grid[pos_y][pos_x] == 0\r\n    )\r\n]\r\n```\r\n\r\nbut ultimately I could go either way since this isn't necessarily more readable than what you already have.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "graphs/greedy_best_first.py",
    "pr_number": 8775,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1278600899,
    "comment_created_at": "2023-07-30T18:25:12Z"
  },
  {
    "code": "@@ -96,7 +96,7 @@ def test_nearest_neighbour(\n \n \n def test_local_binary_pattern():\n-    file_path = \"digital_image_processing/image_data/lena.jpg\"\n+    file_path = \"digital_image_processing/image_data/lena_small.jpg\"",
    "comment": "let's use https://docs.github.com/en/actions/learn-github-actions/variables#default-environment-variables",
    "line_number": 99,
    "enriched": "File: digital_image_processing/test_digital_image_processing.py\nCode: @@ -96,7 +96,7 @@ def test_nearest_neighbour(\n \n \n def test_local_binary_pattern():\n-    file_path = \"digital_image_processing/image_data/lena.jpg\"\n+    file_path = \"digital_image_processing/image_data/lena_small.jpg\"\nComment: Let's use https://docs.github.com/en/actions/learn-github-actions/variables#default-environment-variables\r\n```suggestion\r\n    from os import getenv  # Speed up our Continuous Integration tests\r\n    file_name = \"lena_small.jpg\" if getenv(\"CI\") else \"lena.jpg\"\r\n    file_path = f\"digital_image_processing/image_data/{file_name}.jpg\"\r\n```",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "digital_image_processing/test_digital_image_processing.py",
    "pr_number": 10161,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1350214823,
    "comment_created_at": "2023-10-09T12:02:42Z"
  },
  {
    "code": "@@ -12,6 +12,19 @@\n def method_2(boundary, steps):\n     # \"Simpson Rule\"\n     # int(f) = delta_x/2 * (b-a)/3*(f1 + 4f2 + 2f_3 + ... + fn)\n+    \"\"\"\n+    Calculate the definite integral of a function using Simpson's Rule.\n+\n+     :param boundary: A list containing the lower and upper bounds of integration.\n+     :param steps: The number of steps or resolution for the integration.\n+     :return: The approximate integral value.\n+\n+     >>> round(method_2([0, 1], 10), 10)\n+     0.3333333333\n+\n+     >>> round(method_2([0, 2], 10), 10)\n+     2.6666666667\n+    \"\"\"",
    "comment": "please add a few more tests for unexpected inputs...",
    "line_number": 27,
    "enriched": "File: maths/simpson_rule.py\nCode: @@ -12,6 +12,19 @@\n def method_2(boundary, steps):\n     # \"Simpson Rule\"\n     # int(f) = delta_x/2 * (b-a)/3*(f1 + 4f2 + 2f_3 + ... + fn)\n+    \"\"\"\n+    Calculate the definite integral of a function using Simpson's Rule.\n+\n+     :param boundary: A list containing the lower and upper bounds of integration.\n+     :param steps: The number of steps or resolution for the integration.\n+     :return: The approximate integral value.\n+\n+     >>> round(method_2([0, 1], 10), 10)\n+     0.3333333333\n+\n+     >>> round(method_2([0, 2], 10), 10)\n+     2.6666666667\n+    \"\"\"\nComment: Please add a few more tests for unexpected inputs...\r\n```suggestion\r\n    >>> round(method_2([0, 2], 0), 10)\r\n    >>> round(method_2((0, 2), -10), 10)\r\n    >>> round(method_2([0, 2, 4], 10), 10)\r\n    >>> round(method_2([2, 0], 10), 10)\r\n    >>> round(method_2([-2, -1], 10), 10)\r\n    \"\"\"\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/simpson_rule.py",
    "pr_number": 10269,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1354420273,
    "comment_created_at": "2023-10-11T08:37:43Z"
  },
  {
    "code": "@@ -0,0 +1,38 @@\n+import re\n+\n+\n+def srilanka_phone_validator(phone: str) -> bool:\n+    \"\"\"\n+    Determine whether the string is a valid sri lankan phone number or not\n+    :param phone:\n+    :return: Boolean",
    "comment": "this tells us nothing that we don't already know",
    "line_number": 8,
    "enriched": "File: strings/srilankan_phone_validator.py\nCode: @@ -0,0 +1,38 @@\n+import re\n+\n+\n+def srilanka_phone_validator(phone: str) -> bool:\n+    \"\"\"\n+    Determine whether the string is a valid sri lankan phone number or not\n+    :param phone:\n+    :return: Boolean\nComment: ```suggestion\n\n```\nThis tells us nothing that we don't already know",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "strings/srilankan_phone_validator.py",
    "pr_number": 7658,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1005413254,
    "comment_created_at": "2022-10-26T09:01:32Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+\"\"\"\n+Author  : Naman Sharma\n+Date    : October 2, 2023\n+\n+Task:\n+Find largest power of 2 less than or equal to a given number\n+\n+Implementation notes: Use bit manipulation.\n+We start from 1 & left shift the set bit to check if (res<<1)<=number.\n+Each left bit shift represents a pow of 2.\n+\n+For example:\n+number: 15\n+res:    1   0b1\n+        2   0b10\n+        4   0b100\n+        8   0b1000\n+        16  0b10000 (Exit)\n+\"\"\"\n+\n+",
    "comment": "i dont think we need so long one instead we can use \r\ndef find(a):\r\n    if a<=0:\r\n        return 0;\r\n    for i in range(a+1):\r\n        if (2**i <= a):\r\n            c=i;\r\n        else :\r\n            return c;\r\nb=int(input())\r\nprint(find(b))",
    "line_number": 21,
    "enriched": "File: bit_manipulation/largest_pow_of_two_less_than_or_equal_to_a_number.py\nCode: @@ -0,0 +1,60 @@\n+\"\"\"\n+Author  : Naman Sharma\n+Date    : October 2, 2023\n+\n+Task:\n+Find largest power of 2 less than or equal to a given number\n+\n+Implementation notes: Use bit manipulation.\n+We start from 1 & left shift the set bit to check if (res<<1)<=number.\n+Each left bit shift represents a pow of 2.\n+\n+For example:\n+number: 15\n+res:    1   0b1\n+        2   0b10\n+        4   0b100\n+        8   0b1000\n+        16  0b10000 (Exit)\n+\"\"\"\n+\n+\nComment: i dont think we need so long one instead we can use \r\ndef find(a):\r\n    if a<=0:\r\n        return 0;\r\n    for i in range(a+1):\r\n        if (2**i <= a):\r\n            c=i;\r\n        else :\r\n            return c;\r\nb=int(input())\r\nprint(find(b))\r\n",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "bit_manipulation/largest_pow_of_two_less_than_or_equal_to_a_number.py",
    "pr_number": 9374,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1345305670,
    "comment_created_at": "2023-10-04T07:13:28Z"
  },
  {
    "code": "@@ -19,116 +17,94 @@\n from digital_image_processing.filters import sobel_filter as sob\n from digital_image_processing.resize import resize as rs\n \n-img = imread(r\"digital_image_processing/image_data/lena_small.jpg\")\n-gray = cvtColor(img, COLOR_BGR2GRAY)\n+# Sample image path for testing\n+IMG_PATH = r\"C:/Users/USER/Python/digital_image_processing/image_data/lena_small.jpg\"",
    "comment": "our github actions-based tests run on ubuntu linux so they are not going to find files on a hard-coded windows path.  faster but failing the tests is not going to be accepted.",
    "line_number": 21,
    "enriched": "File: digital_image_processing/test_digital_image_processing.py\nCode: @@ -19,116 +17,94 @@\n from digital_image_processing.filters import sobel_filter as sob\n from digital_image_processing.resize import resize as rs\n \n-img = imread(r\"digital_image_processing/image_data/lena_small.jpg\")\n-gray = cvtColor(img, COLOR_BGR2GRAY)\n+# Sample image path for testing\n+IMG_PATH = r\"C:/Users/USER/Python/digital_image_processing/image_data/lena_small.jpg\"\nComment: Our GitHub Actions-based tests run on Ubuntu Linux so they are not going to find files on a hard-coded Windows path.  Faster but failing the tests is not going to be accepted.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "digital_image_processing/test_digital_image_processing.py",
    "pr_number": 11482,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1693982566,
    "comment_created_at": "2024-07-27T16:16:27Z"
  },
  {
    "code": "@@ -1,35 +1,38 @@\n+from collections.abc import Callable\n+\n+\n class Heap:\n     \"\"\"\n     A generic Heap class, can be used as min or max by passing the key function\n     accordingly.\n     \"\"\"\n \n-    def __init__(self, key=None):\n+    def __init__(self, key: Callable | None = None) -> None:\n         # Stores actual heap items.\n-        self.arr = list()\n+        self.arr: list = list()\n         # Stores indexes of each item for supporting updates and deletion.\n-        self.pos_map = {}\n+        self.pos_map: dict = {}",
    "comment": "these are really not needed.  both python and mypy are able to figure this out without hinting.",
    "line_number": 14,
    "enriched": "File: data_structures/heap/heap_generic.py\nCode: @@ -1,35 +1,38 @@\n+from collections.abc import Callable\n+\n+\n class Heap:\n     \"\"\"\n     A generic Heap class, can be used as min or max by passing the key function\n     accordingly.\n     \"\"\"\n \n-    def __init__(self, key=None):\n+    def __init__(self, key: Callable | None = None) -> None:\n         # Stores actual heap items.\n-        self.arr = list()\n+        self.arr: list = list()\n         # Stores indexes of each item for supporting updates and deletion.\n-        self.pos_map = {}\n+        self.pos_map: dict = {}\nComment: These are really not needed.  Both Python and mypy are able to figure this out without hinting.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "data_structures/heap/heap_generic.py",
    "pr_number": 7044,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 993402128,
    "comment_created_at": "2022-10-12T12:37:00Z"
  },
  {
    "code": "@@ -96,7 +96,7 @@ We want your work to be readable by others; therefore, we encourage you to note\n \n   ```bash\n   python3 -m pip install ruff  # only required the first time\n-  ruff .\n+  ruff check .",
    "comment": "looks like we don't need to specify the current directory anymore, since ruff will run in the current directory by default: https://docs.astral.sh/ruff/tutorial/#getting-started",
    "line_number": 99,
    "enriched": "File: CONTRIBUTING.md\nCode: @@ -96,7 +96,7 @@ We want your work to be readable by others; therefore, we encourage you to note\n \n   ```bash\n   python3 -m pip install ruff  # only required the first time\n-  ruff .\n+  ruff check .\nComment: ```suggestion\r\n  ruff check\r\n```\r\nLooks like we don't need to specify the current directory anymore, since ruff will run in the current directory by default: https://docs.astral.sh/ruff/tutorial/#getting-started",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "CONTRIBUTING.md",
    "pr_number": 11772,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1788648593,
    "comment_created_at": "2024-10-05T17:17:36Z"
  },
  {
    "code": "@@ -0,0 +1,63 @@\n+\"\"\"\n+Description : The law states that the magnitude of the electrostatic force of attraction or repulsion between two point charges is directly proportional",
    "comment": "use ruff . then ruff . --fix to fix ruff and pre-commit issues",
    "line_number": 2,
    "enriched": "File: physics/coulomb_law.py\nCode: @@ -0,0 +1,63 @@\n+\"\"\"\n+Description : The law states that the magnitude of the electrostatic force of attraction or repulsion between two point charges is directly proportional\nComment: use `ruff .` then `ruff . --fix` to fix ruff and pre-commit issues",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "physics/coulomb_law.py",
    "pr_number": 8714,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1186863643,
    "comment_created_at": "2023-05-07T14:30:18Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+from __future__ import annotations",
    "comment": "the code in this repository is targeted towards python 3.10+",
    "line_number": 1,
    "enriched": "File: dynamic_programming/Minimum_cost_path.py\nCode: @@ -0,0 +1,35 @@\n+from __future__ import annotations\nComment: ```suggestion\r\n```\r\nThe code in this repository is targeted towards python 3.10+",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "dynamic_programming/Minimum_cost_path.py",
    "pr_number": 7410,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 998609830,
    "comment_created_at": "2022-10-18T19:06:25Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+\"\"\"\n+== Juggler Sequence ==\n+Juggler sequence start with any positive integer n. The next term is\n+obtained as follows:\n+    If n term is even, the next term is floor value of square root of n .\n+    If n is odd, the next term is floor value of 3 time the square root of n.\n+\n+https://en.wikipedia.org/wiki/Juggler_sequence\n+\"\"\"\n+\n+# Author : Akshay Dubey (https://github.com/itsAkshayDubey)\n+import math\n+\n+\n+def juggler_sequence(number: int) -> list[int]:\n+    \"\"\"\n+    # doctest: +NORMALIZE_WHITESPACE\n+    >>> juggler_sequence(0)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value of [number=0] must be greater than 0\n+    >>> juggler_sequence(1)\n+    [1]\n+    >>> juggler_sequence(2)\n+    [2, 1]\n+    >>> juggler_sequence(3)\n+    [3, 5, 11, 36, 6, 2, 1]\n+    >>> juggler_sequence(5)\n+    [5, 11, 36, 6, 2, 1]\n+    >>> juggler_sequence(6.0)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value of [number=6.0] must be an integer\n+    >>> juggler_sequence(-1)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value of [number=-1] must be positive\n+    \"\"\"\n+    if not isinstance(number, int):\n+        raise TypeError(f\"Input value of [number={number}] must be an integer\")\n+    if number < 0:\n+        raise TypeError(f\"Input value of [number={number}] must be positive\")\n+    if number == 0:\n+        raise TypeError(f\"Input value of [number={number}] must be greater than 0\")\n+    sequence = [number]\n+    while number != 1:\n+        if number % 2 == 0:\n+            temp = int(math.floor(math.sqrt(number)))\n+        else:\n+            temp = int(\n+                math.floor(math.sqrt(number) * math.sqrt(number) * math.sqrt(number))\n+            )\n+        number = temp",
    "comment": "temp is not needed...",
    "line_number": 53,
    "enriched": "File: maths/juggler_sequence.py\nCode: @@ -0,0 +1,61 @@\n+\"\"\"\n+== Juggler Sequence ==\n+Juggler sequence start with any positive integer n. The next term is\n+obtained as follows:\n+    If n term is even, the next term is floor value of square root of n .\n+    If n is odd, the next term is floor value of 3 time the square root of n.\n+\n+https://en.wikipedia.org/wiki/Juggler_sequence\n+\"\"\"\n+\n+# Author : Akshay Dubey (https://github.com/itsAkshayDubey)\n+import math\n+\n+\n+def juggler_sequence(number: int) -> list[int]:\n+    \"\"\"\n+    # doctest: +NORMALIZE_WHITESPACE\n+    >>> juggler_sequence(0)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value of [number=0] must be greater than 0\n+    >>> juggler_sequence(1)\n+    [1]\n+    >>> juggler_sequence(2)\n+    [2, 1]\n+    >>> juggler_sequence(3)\n+    [3, 5, 11, 36, 6, 2, 1]\n+    >>> juggler_sequence(5)\n+    [5, 11, 36, 6, 2, 1]\n+    >>> juggler_sequence(6.0)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value of [number=6.0] must be an integer\n+    >>> juggler_sequence(-1)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value of [number=-1] must be positive\n+    \"\"\"\n+    if not isinstance(number, int):\n+        raise TypeError(f\"Input value of [number={number}] must be an integer\")\n+    if number < 0:\n+        raise TypeError(f\"Input value of [number={number}] must be positive\")\n+    if number == 0:\n+        raise TypeError(f\"Input value of [number={number}] must be greater than 0\")\n+    sequence = [number]\n+    while number != 1:\n+        if number % 2 == 0:\n+            temp = int(math.floor(math.sqrt(number)))\n+        else:\n+            temp = int(\n+                math.floor(math.sqrt(number) * math.sqrt(number) * math.sqrt(number))\n+            )\n+        number = temp\nComment: `temp` is not needed...\r\n```suggestion\r\n            number = int(math.floor(math.sqrt(number)))\r\n        else:\r\n            number = int(\r\n                math.floor(math.sqrt(number) * math.sqrt(number) * math.sqrt(number))\r\n            )\r\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "maths/juggler_sequence.py",
    "pr_number": 7985,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1022809797,
    "comment_created_at": "2022-11-15T13:49:44Z"
  },
  {
    "code": "@@ -82,3 +82,4 @@ def triangle(\n \n     vertices = [(-175, -125), (0, 175), (175, -125)]  # vertices of triangle\n     triangle(vertices[0], vertices[1], vertices[2], int(sys.argv[1]))\n+    turtle.Screen().exitonclick()",
    "comment": "make sure the file ends in an empty line",
    "line_number": 85,
    "enriched": "File: fractals/sierpinski_triangle.py\nCode: @@ -82,3 +82,4 @@ def triangle(\n \n     vertices = [(-175, -125), (0, 175), (175, -125)]  # vertices of triangle\n     triangle(vertices[0], vertices[1], vertices[2], int(sys.argv[1]))\n+    turtle.Screen().exitonclick()\nComment: ```suggestion\r\n    turtle.Screen().exitonclick()\r\n\r\n```\r\nMake sure the file ends in an empty line",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "fractals/sierpinski_triangle.py",
    "pr_number": 8625,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1280060200,
    "comment_created_at": "2023-08-01T02:55:03Z"
  },
  {
    "code": "@@ -25,6 +25,41 @@ def add(self, item: Any) -> None:\n         self.head = Node(item, self.head)\n         self.size += 1\n \n+    def add_at_position(self, item: Any, position: int) -> bool:\n+        \"\"\"\n+        Adds a new node with the given item at the specified position in the linked list\n+\n+        Args:\n+            item (Any): The item to be added to the linked list.\n+            position (int): The position at which the item should be inserted.\n+\n+        Returns:\n+            bool: True if the insertion was successful, False otherwise.\n+\n+        >>> linked_list = LinkedList()\n+        >>> linked_list.add(1)\n+        >>> linked_list.add(2)\n+        >>> linked_list.add(3)\n+        >>> linked_list.add_at_position(10, 1)\n+        True\n+",
    "comment": "here there should be a test displaying linked_list",
    "line_number": 45,
    "enriched": "File: data_structures/linked_list/__init__.py\nCode: @@ -25,6 +25,41 @@ def add(self, item: Any) -> None:\n         self.head = Node(item, self.head)\n         self.size += 1\n \n+    def add_at_position(self, item: Any, position: int) -> bool:\n+        \"\"\"\n+        Adds a new node with the given item at the specified position in the linked list\n+\n+        Args:\n+            item (Any): The item to be added to the linked list.\n+            position (int): The position at which the item should be inserted.\n+\n+        Returns:\n+            bool: True if the insertion was successful, False otherwise.\n+\n+        >>> linked_list = LinkedList()\n+        >>> linked_list.add(1)\n+        >>> linked_list.add(2)\n+        >>> linked_list.add(3)\n+        >>> linked_list.add_at_position(10, 1)\n+        True\n+\nComment: Here there should be a test displaying `linked_list`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/linked_list/__init__.py",
    "pr_number": 9020,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1310825027,
    "comment_created_at": "2023-08-30T21:06:22Z"
  },
  {
    "code": "@@ -0,0 +1,62 @@\n+from __future__ import annotations\n+\n+\n+def largest_divisible_subset(array: list[int]) -> list[int]:\n+    \"\"\"\n+    Algorithm to find the biggest subset\n+    in the given array such that for any\n+    2 elements x and y in the subset,\n+    either x divides y or y divides x\n+    >>> largest_divisible_subset([1,16,7,8,4])\n+    [16, 8, 4, 1]\n+    >>> largest_divisible_subset([1,2,3])\n+    [2, 1]\n+    >>> largest_divisible_subset([1, 2, 4, 8])\n+    [8, 4, 2, 1]",
    "comment": "let's add some test cases.",
    "line_number": 15,
    "enriched": "File: dynamic_programming/largest_divisible_subset.py\nCode: @@ -0,0 +1,62 @@\n+from __future__ import annotations\n+\n+\n+def largest_divisible_subset(array: list[int]) -> list[int]:\n+    \"\"\"\n+    Algorithm to find the biggest subset\n+    in the given array such that for any\n+    2 elements x and y in the subset,\n+    either x divides y or y divides x\n+    >>> largest_divisible_subset([1,16,7,8,4])\n+    [16, 8, 4, 1]\n+    >>> largest_divisible_subset([1,2,3])\n+    [2, 1]\n+    >>> largest_divisible_subset([1, 2, 4, 8])\n+    [8, 4, 2, 1]\nComment: Let's add some test cases.\r\n```suggestion\r\n    >>> largest_divisible_subset([1, 16, 7, 8, 4])\r\n    [16, 8, 4, 1]\r\n    >>> largest_divisible_subset([1, 2, 3])\r\n    [2, 1]\r\n    >>> largest_divisible_subset([-1, -2, -3])\r\n    [-3]\r\n    >>> largest_divisible_subset([1, 2, 4, 8])\r\n    [8, 4, 2, 1]\r\n    >>> largest_divisible_subset((1, 2, 4, 8))\r\n    [8, 4, 2, 1]\r\n    >>> largest_divisible_subset([1, 1, 1])\r\n    [1, 1, 1]\r\n    >>> largest_divisible_subset([0, 0, 0])\r\n    [0, 0, 0]\r\n    >>> largest_divisible_subset([-1, -1, -1])\r\n    [-1, -1, -1]\r\n    >>> largest_divisible_subset([])\r\n    []\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/largest_divisible_subset.py",
    "pr_number": 9825,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347549941,
    "comment_created_at": "2023-10-05T14:48:48Z"
  },
  {
    "code": "@@ -0,0 +1,98 @@\n+import random\n+def GenerateMineSweeperMap(n, k):\n+    arr = [[0 for row in range(n)] for column in range(n)]\n+    for num in range(k):\n+        x = random.randint(0,n-1)\n+        y = random.randint(0,n-1)\n+        arr[y][x] = 'X'\n+        if (x >=0 and x <= n-2) and (y >= 0 and y <= n-1):\n+            if arr[y][x+1] != 'X':\n+                arr[y][x+1] += 1 # center right\n+        if (x >=1 and x <= n-1) and (y >= 0 and y <= n-1):\n+            if arr[y][x-1] != 'X':\n+                arr[y][x-1] += 1 # center left\n+        if (x >= 1 and x <= n-1) and (y >= 1 and y <= n-1):\n+            if arr[y-1][x-1] != 'X':\n+                arr[y-1][x-1] += 1 # top left\n+ \n+        if (x >= 0 and x <= n-2) and (y >= 1 and y <= n-1):\n+            if arr[y-1][x+1] != 'X':\n+                arr[y-1][x+1] += 1 # top right\n+        if (x >= 0 and x <= n-1) and (y >= 1 and y <= n-1):\n+            if arr[y-1][x] != 'X':\n+                arr[y-1][x] += 1 # top center\n+ \n+        if (x >=0 and x <= n-2) and (y >= 0 and y <= n-2):\n+            if arr[y+1][x+1] != 'X':\n+                arr[y+1][x+1] += 1 # bottom right\n+        if (x >= 1 and x <= n-1) and (y >= 0 and y <= n-2):\n+            if arr[y+1][x-1] != 'X':\n+                arr[y+1][x-1] += 1 # bottom left\n+        if (x >= 0 and x <= n-1) and (y >= 0 and y <= n-2):\n+            if arr[y+1][x] != 'X':\n+                arr[y+1][x] += 1 # bottom center\n+    return arr\n+def GeneratePlayerMap(n):\n+    arr = [['-' for row in range(n)] for column in range(n)]\n+    return arr\n+def DisplayMap(map):\n+    for row in map:\n+        print(\" \".join(str(cell) for cell in row))\n+        print(\"\")\n+def CheckWon(map):\n+    for row in map:\n+        for cell in row:\n+            if cell == '-':\n+                return False\n+    return True\n+def CheckContinueGame(score):\n+    print(\"Your score: \", score)\n+    isContinue = input(\"Do you want to try again? (y/n) :\")\n+    if isContinue == 'n':\n+        return False\n+    return True\n+def Game():\n+    GameStatus = True\n+    while GameStatus:\n+        difficulty = input(\"Select your difficulty (b, i, h):\")\n+        if difficulty.lower() == 'b':\n+            n = 5\n+            k = 3\n+        elif difficulty.lower() == 'i':\n+            n = 6\n+            k = 8\n+        else:\n+            n = 8\n+            k = 20\n+ \n+        minesweeper_map = GenerateMineSweeperMap(n, k)\n+        player_map = GeneratePlayerMap(n)\n+        score = 0\n+        while True:\n+            if CheckWon(player_map) == False:\n+                print(\"Enter your cell you want to open :\")\n+                x = input(\"X (1 to 5) :\")\n+                y = input(\"Y (1 to 5) :\")\n+                x = int(x) \u2014 1 # 0 based indexing",
    "comment": "an error occured while parsing the file: backtracking/minesweeper_manil-keo.py\npython\ntraceback (most recent call last):\n  file \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 145, in parse\n    reports = lint_file(\nlibcst._exceptions.parsersyntaxerror: syntax error @ 76:28.\n'\u2014' is not a valid token.\n\n                x = int(x) \u2014 1 # 0 based indexing\n                           ^",
    "line_number": 76,
    "enriched": "File: backtracking/minesweeper_ManiL-Keo.py\nCode: @@ -0,0 +1,98 @@\n+import random\n+def GenerateMineSweeperMap(n, k):\n+    arr = [[0 for row in range(n)] for column in range(n)]\n+    for num in range(k):\n+        x = random.randint(0,n-1)\n+        y = random.randint(0,n-1)\n+        arr[y][x] = 'X'\n+        if (x >=0 and x <= n-2) and (y >= 0 and y <= n-1):\n+            if arr[y][x+1] != 'X':\n+                arr[y][x+1] += 1 # center right\n+        if (x >=1 and x <= n-1) and (y >= 0 and y <= n-1):\n+            if arr[y][x-1] != 'X':\n+                arr[y][x-1] += 1 # center left\n+        if (x >= 1 and x <= n-1) and (y >= 1 and y <= n-1):\n+            if arr[y-1][x-1] != 'X':\n+                arr[y-1][x-1] += 1 # top left\n+ \n+        if (x >= 0 and x <= n-2) and (y >= 1 and y <= n-1):\n+            if arr[y-1][x+1] != 'X':\n+                arr[y-1][x+1] += 1 # top right\n+        if (x >= 0 and x <= n-1) and (y >= 1 and y <= n-1):\n+            if arr[y-1][x] != 'X':\n+                arr[y-1][x] += 1 # top center\n+ \n+        if (x >=0 and x <= n-2) and (y >= 0 and y <= n-2):\n+            if arr[y+1][x+1] != 'X':\n+                arr[y+1][x+1] += 1 # bottom right\n+        if (x >= 1 and x <= n-1) and (y >= 0 and y <= n-2):\n+            if arr[y+1][x-1] != 'X':\n+                arr[y+1][x-1] += 1 # bottom left\n+        if (x >= 0 and x <= n-1) and (y >= 0 and y <= n-2):\n+            if arr[y+1][x] != 'X':\n+                arr[y+1][x] += 1 # bottom center\n+    return arr\n+def GeneratePlayerMap(n):\n+    arr = [['-' for row in range(n)] for column in range(n)]\n+    return arr\n+def DisplayMap(map):\n+    for row in map:\n+        print(\" \".join(str(cell) for cell in row))\n+        print(\"\")\n+def CheckWon(map):\n+    for row in map:\n+        for cell in row:\n+            if cell == '-':\n+                return False\n+    return True\n+def CheckContinueGame(score):\n+    print(\"Your score: \", score)\n+    isContinue = input(\"Do you want to try again? (y/n) :\")\n+    if isContinue == 'n':\n+        return False\n+    return True\n+def Game():\n+    GameStatus = True\n+    while GameStatus:\n+        difficulty = input(\"Select your difficulty (b, i, h):\")\n+        if difficulty.lower() == 'b':\n+            n = 5\n+            k = 3\n+        elif difficulty.lower() == 'i':\n+            n = 6\n+            k = 8\n+        else:\n+            n = 8\n+            k = 20\n+ \n+        minesweeper_map = GenerateMineSweeperMap(n, k)\n+        player_map = GeneratePlayerMap(n)\n+        score = 0\n+        while True:\n+            if CheckWon(player_map) == False:\n+                print(\"Enter your cell you want to open :\")\n+                x = input(\"X (1 to 5) :\")\n+                y = input(\"Y (1 to 5) :\")\n+                x = int(x) \u2014 1 # 0 based indexing\nComment: An error occured while parsing the file: `backtracking/minesweeper_ManiL-Keo.py`\n```python\nTraceback (most recent call last):\n  File \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 145, in parse\n    reports = lint_file(\nlibcst._exceptions.ParserSyntaxError: Syntax Error @ 76:28.\n'\u2014' is not a valid token.\n\n                x = int(x) \u2014 1 # 0 based indexing\n                           ^\n\n```",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "backtracking/minesweeper_ManiL-Keo.py",
    "pr_number": 7120,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994647725,
    "comment_created_at": "2022-10-13T13:29:35Z"
  },
  {
    "code": "@@ -62,6 +63,45 @@ def random_characters(chars_incl, i):\n     pass  # Put your code here...\n \n \n+# This Will Check Whether A Given Password Is Strong Or Not\n+# It Follows The Rule that Length Of Password Should Be At Least 8 Characters\n+# And At Least 1 Lower, 1 Upper, 1 Number And 1 Special Character\n+def strong_password_detector(password: str, min_length: int = 8) -> str:\n+    \"\"\"\n+    >>> strong_password_detector('Hwea7$2!')\n+    'This is a strong Password'\n+\n+    >>> strong_password_detector('Sh0r1')\n+    'Your Password must be at least 8 characters long'\n+\n+    >>> strong_password_detector('Hello123')\n+    'Password should contain UPPERCASE, lowercase, numbers, special characters'\n+\n+    >>> strong_password_detector('Hello1238udfhiaf038fajdvjjf!jaiuFhkqi1')\n+    'This is a strong Password'\n+\n+    >>> strong_password_detector(0)\n+    'Your Password must be at least 8 characters long'\n+    \"\"\"\n+\n+    if len(str(password)) < 8:",
    "comment": "also, short passwords and weak passwords should raise valueerrors.",
    "line_number": 87,
    "enriched": "File: other/password.py\nCode: @@ -62,6 +63,45 @@ def random_characters(chars_incl, i):\n     pass  # Put your code here...\n \n \n+# This Will Check Whether A Given Password Is Strong Or Not\n+# It Follows The Rule that Length Of Password Should Be At Least 8 Characters\n+# And At Least 1 Lower, 1 Upper, 1 Number And 1 Special Character\n+def strong_password_detector(password: str, min_length: int = 8) -> str:\n+    \"\"\"\n+    >>> strong_password_detector('Hwea7$2!')\n+    'This is a strong Password'\n+\n+    >>> strong_password_detector('Sh0r1')\n+    'Your Password must be at least 8 characters long'\n+\n+    >>> strong_password_detector('Hello123')\n+    'Password should contain UPPERCASE, lowercase, numbers, special characters'\n+\n+    >>> strong_password_detector('Hello1238udfhiaf038fajdvjjf!jaiuFhkqi1')\n+    'This is a strong Password'\n+\n+    >>> strong_password_detector(0)\n+    'Your Password must be at least 8 characters long'\n+    \"\"\"\n+\n+    if len(str(password)) < 8:\nComment: ```suggestion\r\n    if len(password) < min_length:\r\n```\r\n Also, short passwords and weak passwords should raise ValueErrors.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "other/password.py",
    "pr_number": 7939,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1011688023,
    "comment_created_at": "2022-11-02T12:31:17Z"
  },
  {
    "code": "@@ -11,6 +11,39 @@ def perfect_cube(n: int) -> bool:\n     return (val * val * val) == n\n \n \n+def perfect_cube_binary_search(n: int) -> bool:\n+    \"\"\"\n+    Check if a number is a perfect cube or not using binary search.\n+    Time complexity : O(Log(n))\n+    Space complexity: O(1)\n+\n+    >>> perfect_cube_binary_search(27)\n+    True\n+    >>> perfect_cube_binary_search(64)\n+    True\n+    >>> perfect_cube_binary_search(4)\n+    False\n+    >>> perfect_cube_binary_search(\"a\")\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: '<=' not supported between instances of 'int' and 'str'",
    "comment": "please add your own typeerror check for invalid inputs like this rather than relying on the built-in error message for <=",
    "line_number": 29,
    "enriched": "File: maths/perfect_cube.py\nCode: @@ -11,6 +11,39 @@ def perfect_cube(n: int) -> bool:\n     return (val * val * val) == n\n \n \n+def perfect_cube_binary_search(n: int) -> bool:\n+    \"\"\"\n+    Check if a number is a perfect cube or not using binary search.\n+    Time complexity : O(Log(n))\n+    Space complexity: O(1)\n+\n+    >>> perfect_cube_binary_search(27)\n+    True\n+    >>> perfect_cube_binary_search(64)\n+    True\n+    >>> perfect_cube_binary_search(4)\n+    False\n+    >>> perfect_cube_binary_search(\"a\")\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: '<=' not supported between instances of 'int' and 'str'\nComment: Please add your own TypeError check for invalid inputs like this rather than relying on the built-in error message for `<=`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/perfect_cube.py",
    "pr_number": 10477,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1361630517,
    "comment_created_at": "2023-10-17T07:30:24Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+def strong_password_detection(password):\n+    AllowedSymbols = [\"#\", \"@\", \"$\", \"_\", \"*\", \"-\"]",
    "comment": "we require type hints and doctests...  please add tests to cover all the cases below.",
    "line_number": 2,
    "enriched": "File: strong_password_detection.py\nCode: @@ -0,0 +1,42 @@\n+def strong_password_detection(password):\n+    AllowedSymbols = [\"#\", \"@\", \"$\", \"_\", \"*\", \"-\"]\nComment: We require type hints and doctests...  Please add tests to cover all the cases below.\r\n```suggestion\r\ndef strong_password_detection(password: str) -> bool:\r\n    \"\"\"\r\n    >>> strong_password_detection(\"str0ng@Password\")\r\n    True\r\n    >>> strong_password_detection(\"str0ngPassword\")\r\n    False\r\n    \"\"\"\r\n    allowed_symbols = {c for c in \"#@$_*-\"}\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "strong_password_detection.py",
    "pr_number": 10885,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1369908079,
    "comment_created_at": "2023-10-24T09:56:38Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+def first_missing_positive(nums: list[int]) -> int:\n+    \"\"\"\n+    Given an unsorted integer array nums, returns the smallest missing positive integer.\n+\n+    The algorithm must run in O(n) time and use O(1) auxiliary space.\n+\n+    Hints:\n+    - Try to place each number in its correct position, so that nums[i] equals i + 1.\n+    - Iterate through the array to find the first position where nums[i] != i + 1,\n+      indicating the missing positive integer.\n+\n+    :param nums: The input unsorted integer array.\n+    :return: The smallest missing positive integer.\n+\n+    Examples:\n+    >>> first_missing_positive([1, 2, 0])",
    "comment": "please add doctests for\r\n\r\n>>> first_missing_positive([])\r\n>>> first_missing_positive([0])\r\n>>> first_missing_positive([1])\r\n>>> first_missing_positive([1.1, 2.2, 3.3])\r\n>>> first_missing_positive([-1, -2, -3])\r\n>>> first_missing_positive(\"abc\")",
    "line_number": 16,
    "enriched": "File: data_structures/arrays/first_missing_positive.py\nCode: @@ -0,0 +1,60 @@\n+def first_missing_positive(nums: list[int]) -> int:\n+    \"\"\"\n+    Given an unsorted integer array nums, returns the smallest missing positive integer.\n+\n+    The algorithm must run in O(n) time and use O(1) auxiliary space.\n+\n+    Hints:\n+    - Try to place each number in its correct position, so that nums[i] equals i + 1.\n+    - Iterate through the array to find the first position where nums[i] != i + 1,\n+      indicating the missing positive integer.\n+\n+    :param nums: The input unsorted integer array.\n+    :return: The smallest missing positive integer.\n+\n+    Examples:\n+    >>> first_missing_positive([1, 2, 0])\nComment: Please add doctests for\r\n```\r\n>>> first_missing_positive([])\r\n>>> first_missing_positive([0])\r\n>>> first_missing_positive([1])\r\n>>> first_missing_positive([1.1, 2.2, 3.3])\r\n>>> first_missing_positive([-1, -2, -3])\r\n>>> first_missing_positive(\"ABC\")\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/arrays/first_missing_positive.py",
    "pr_number": 11186,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1409791999,
    "comment_created_at": "2023-11-29T19:48:31Z"
  },
  {
    "code": "@@ -0,0 +1,85 @@\n+from collections.abc import Callable\n+\n+import numpy as np\n+\n+\n+def weierstrass_method(\n+    polynomial: Callable[[np.ndarray], np.ndarray],\n+    degree: int,\n+    roots: np.ndarray | None = None,\n+    max_iter: int = 100,\n+) -> np.ndarray:\n+    \"\"\"\n+    Approximates all complex roots of a polynomial using the\n+    Weierstrass (Durand-Kerner) method.\n+    Args:\n+        polynomial: A function that takes a NumPy array of complex numbers and returns\n+                    the polynomial values at those points.\n+        degree: Degree of the polynomial (number of roots to find). Must be \u2265 1.\n+        roots:  Optional initial guess as a NumPy array of complex numbers.\n+                Must have length equal to 'degree'.\n+                If None, perturbed complex roots of unity are used.\n+        max_iter: Number of iterations to perform (default: 100).\n+\n+    Returns:\n+        np.ndarray: Array of approximated complex roots.\n+\n+    Raises:\n+        ValueError: If degree < 1, or if initial roots length doesn't match the degree.\n+\n+    Note:\n+        - Root updates are clipped to prevent numerical overflow.\n+\n+    Example:\n+        >>> import numpy as np\n+        >>> def f(x): return x**2 - 1\n+        >>> roots = weierstrass_method(f, 2)\n+        >>> np.allclose(np.sort(roots), np.sort(np.array([1, -1])))\n+        True\n+",
    "comment": "lets add some more tests:",
    "line_number": 51,
    "enriched": "File: maths/numerical_analysis/weierstrass_method.py\nCode: @@ -0,0 +1,85 @@\n+from collections.abc import Callable\n+\n+import numpy as np\n+\n+\n+def weierstrass_method(\n+    polynomial: Callable[[np.ndarray], np.ndarray],\n+    degree: int,\n+    roots: np.ndarray | None = None,\n+    max_iter: int = 100,\n+) -> np.ndarray:\n+    \"\"\"\n+    Approximates all complex roots of a polynomial using the\n+    Weierstrass (Durand-Kerner) method.\n+    Args:\n+        polynomial: A function that takes a NumPy array of complex numbers and returns\n+                    the polynomial values at those points.\n+        degree: Degree of the polynomial (number of roots to find). Must be \u2265 1.\n+        roots:  Optional initial guess as a NumPy array of complex numbers.\n+                Must have length equal to 'degree'.\n+                If None, perturbed complex roots of unity are used.\n+        max_iter: Number of iterations to perform (default: 100).\n+\n+    Returns:\n+        np.ndarray: Array of approximated complex roots.\n+\n+    Raises:\n+        ValueError: If degree < 1, or if initial roots length doesn't match the degree.\n+\n+    Note:\n+        - Root updates are clipped to prevent numerical overflow.\n+\n+    Example:\n+        >>> import numpy as np\n+        >>> def f(x): return x**2 - 1\n+        >>> roots = weierstrass_method(f, 2)\n+        >>> np.allclose(np.sort(roots), np.sort(np.array([1, -1])))\n+        True\n+\nComment: Lets add some more tests:\r\n```suggestion\r\n        >>> def f(x): return x**3 - 6*x**2 + 11*x - 6\r\n        >>> roots = weierstrass_method(f, 3)\r\n        >>> np.allclose(np.sort(roots), np.array([1, 2, 3]))\r\n        True\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/numerical_analysis/weierstrass_method.py",
    "pr_number": 12877,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2307894524,
    "comment_created_at": "2025-08-28T16:10:29Z"
  },
  {
    "code": "@@ -0,0 +1,41 @@\n+def minkowski_distance(\n+    point_a: list[float],\n+    point_b: list[float],\n+    order: int,\n+) -> float:\n+    \"\"\"\n+    This function calculates the Minkowski distance for a given order between\n+    two n-dimensional points represented as lists. For the case of order = 1,\n+    the Minkowski distance degenerates to the Manhattan distance. For\n+    order = 2, the usual Euclidean distance is obtained.\n+\n+    https://en.wikipedia.org/wiki/Minkowski_distance\n+\n+    >>> minkowski_distance([1.0, 1.0], [2.0, 2.0], 1)\n+    2.0\n+    >>> minkowski_distance([1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], 2)\n+    8.0\n+    >>> minkowski_distance([1.0], [2.0], -1)\n+    Traceback (most recent call last):\n+        ...\n+    Exception: The order must be greater than or equal to 1.\n+    >>> minkowski_distance([1.0], [1.0, 2.0], 1)\n+    Traceback (most recent call last):\n+        ...\n+    Exception: Both points must have the same dimension.\n+    \"\"\"\n+    if order < 1:\n+        raise Exception(\"The order must be greater than or equal to 1.\")",
    "comment": "please make the error of a more specific type",
    "line_number": 28,
    "enriched": "File: maths/minkowski_distance.py\nCode: @@ -0,0 +1,41 @@\n+def minkowski_distance(\n+    point_a: list[float],\n+    point_b: list[float],\n+    order: int,\n+) -> float:\n+    \"\"\"\n+    This function calculates the Minkowski distance for a given order between\n+    two n-dimensional points represented as lists. For the case of order = 1,\n+    the Minkowski distance degenerates to the Manhattan distance. For\n+    order = 2, the usual Euclidean distance is obtained.\n+\n+    https://en.wikipedia.org/wiki/Minkowski_distance\n+\n+    >>> minkowski_distance([1.0, 1.0], [2.0, 2.0], 1)\n+    2.0\n+    >>> minkowski_distance([1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], 2)\n+    8.0\n+    >>> minkowski_distance([1.0], [2.0], -1)\n+    Traceback (most recent call last):\n+        ...\n+    Exception: The order must be greater than or equal to 1.\n+    >>> minkowski_distance([1.0], [1.0, 2.0], 1)\n+    Traceback (most recent call last):\n+        ...\n+    Exception: Both points must have the same dimension.\n+    \"\"\"\n+    if order < 1:\n+        raise Exception(\"The order must be greater than or equal to 1.\")\nComment: ```suggestion\r\n        raise ValueError(\"The order must be greater than or equal to 1.\")\r\n```\r\nPlease make the error of a more specific type",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "maths/minkowski_distance.py",
    "pr_number": 10143,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349789820,
    "comment_created_at": "2023-10-08T22:48:59Z"
  },
  {
    "code": "@@ -0,0 +1,43 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 1, 2022\n+\n+Task:\n+Given a positive int number. Return True if this number is power of 2\n+and False otherwise.\n+\n+Implementation notes: Use bit manipulation.\n+For example if the number is the power of two it's bits representation:\n+n     = 0..100..00\n+n - 1 = 0..011..11\n+\n+n & (n - 1) - no intersections = 0\n+\n+\"\"\"\n+\n+\n+def is_power_of_two(number: int) -> bool:\n+    \"\"\"\n+    >>> is_power_of_two(2)",
    "comment": "please add tests for 0, -2, -8, 8.0, 8.1",
    "line_number": 21,
    "enriched": "File: bit_manipulation/is_power_of_two.py\nCode: @@ -0,0 +1,43 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 1, 2022\n+\n+Task:\n+Given a positive int number. Return True if this number is power of 2\n+and False otherwise.\n+\n+Implementation notes: Use bit manipulation.\n+For example if the number is the power of two it's bits representation:\n+n     = 0..100..00\n+n - 1 = 0..011..11\n+\n+n & (n - 1) - no intersections = 0\n+\n+\"\"\"\n+\n+\n+def is_power_of_two(number: int) -> bool:\n+    \"\"\"\n+    >>> is_power_of_two(2)\nComment: Please add tests for 0, -2, -8, 8.0, 8.1",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "bit_manipulation/is_power_of_two.py",
    "pr_number": 7936,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1010555451,
    "comment_created_at": "2022-11-01T15:18:15Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+def nth_sgonal_num(n: int, s: int) -> int:\n+    \"\"\"\n+    Returns the `n`th `s`-gonal number. It is assumed that `n` >= 0 and `s` >= 3\n+    (see for reference https://en.wikipedia.org/wiki/Polygonal_number).\n+\n+    >>> nth_sgonal_num(0, 3)\n+    0\n+    >>> nth_sgonal_num(3, 3)\n+    6\n+    >>> nth_sgonal_num(5, 4)\n+    25\n+    >>> nth_sgonal_num(2, 5)\n+    5\n+    \"\"\"\n+    return ((s - 2) * (n**2) - (s - 4) * n) // 2",
    "comment": "handle special cases: the code currently returns 0 for n = 0, but it doesn't handle the case when n > 0 and s < 3.\r\n\r\n         if n < 0 or sides < 3:\r\n                raise valueerror(\"invalid input: n must be >= 0 and sides must be >= 3.\")",
    "line_number": 15,
    "enriched": "File: maths/nth_sgonal_num.py\nCode: @@ -0,0 +1,21 @@\n+def nth_sgonal_num(n: int, s: int) -> int:\n+    \"\"\"\n+    Returns the `n`th `s`-gonal number. It is assumed that `n` >= 0 and `s` >= 3\n+    (see for reference https://en.wikipedia.org/wiki/Polygonal_number).\n+\n+    >>> nth_sgonal_num(0, 3)\n+    0\n+    >>> nth_sgonal_num(3, 3)\n+    6\n+    >>> nth_sgonal_num(5, 4)\n+    25\n+    >>> nth_sgonal_num(2, 5)\n+    5\n+    \"\"\"\n+    return ((s - 2) * (n**2) - (s - 4) * n) // 2\nComment: Handle special cases: The code currently returns 0 for n = 0, but it doesn't handle the case when n > 0 and s < 3.\r\n\r\n         if n < 0 or sides < 3:\r\n                raise ValueError(\"Invalid input: n must be >= 0 and sides must be >= 3.\")\r\n",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/nth_sgonal_num.py",
    "pr_number": 8750,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1199548146,
    "comment_created_at": "2023-05-20T03:37:14Z"
  },
  {
    "code": "@@ -39,23 +40,30 @@ def binary_insertion_sort(collection: list) -> list:\n \n     n = len(collection)\n     for i in range(1, n):\n-        val = collection[i]\n+        value_to_insert = collection[i]\n         low = 0\n         high = i - 1\n \n         while low <= high:\n             mid = (low + high) // 2\n-            if val < collection[mid]:\n+            if value_to_insert < collection[mid]:\n                 high = mid - 1\n             else:\n                 low = mid + 1\n+\n         for j in range(i, low, -1):\n             collection[j] = collection[j - 1]\n-        collection[low] = val\n+\n+        collection[low] = value_to_insert\n+\n     return collection\n \n \n-if __name__ == \"__main__\":\n-    user_input = input(\"Enter numbers separated by a comma:\\n\").strip()\n-    unsorted = [int(item) for item in user_input.split(\",\")]\n-    print(binary_insertion_sort(unsorted))\n+if __name__ == \"__main\":\n+    try:\n+        user_input = input(\"Enter numbers separated by a comma:\\n\").strip()\n+        unsorted = [int(item) for item in user_input.split(\",\")]\n+        sorted_list = binary_insertion_sort(unsorted)\n+        print(sorted_list)\n+    except ValueError:\n+        print(\"Invalid input. Please enter valid integers separated by commas.\")",
    "comment": "https://peps.python.org/pep-0008/#programming-recommendations\r\n> limit the try clause to the absolute minimum amount of code necessary.",
    "line_number": 69,
    "enriched": "File: sorts/binary_insertion_sort.py\nCode: @@ -39,23 +40,30 @@ def binary_insertion_sort(collection: list) -> list:\n \n     n = len(collection)\n     for i in range(1, n):\n-        val = collection[i]\n+        value_to_insert = collection[i]\n         low = 0\n         high = i - 1\n \n         while low <= high:\n             mid = (low + high) // 2\n-            if val < collection[mid]:\n+            if value_to_insert < collection[mid]:\n                 high = mid - 1\n             else:\n                 low = mid + 1\n+\n         for j in range(i, low, -1):\n             collection[j] = collection[j - 1]\n-        collection[low] = val\n+\n+        collection[low] = value_to_insert\n+\n     return collection\n \n \n-if __name__ == \"__main__\":\n-    user_input = input(\"Enter numbers separated by a comma:\\n\").strip()\n-    unsorted = [int(item) for item in user_input.split(\",\")]\n-    print(binary_insertion_sort(unsorted))\n+if __name__ == \"__main\":\n+    try:\n+        user_input = input(\"Enter numbers separated by a comma:\\n\").strip()\n+        unsorted = [int(item) for item in user_input.split(\",\")]\n+        sorted_list = binary_insertion_sort(unsorted)\n+        print(sorted_list)\n+    except ValueError:\n+        print(\"Invalid input. Please enter valid integers separated by commas.\")\nComment: https://peps.python.org/pep-0008/#programming-recommendations\r\n> Limit the try clause to the absolute minimum amount of code necessary.\r\n```suggestion\r\n    user_input = input(\"Enter numbers separated by a comma:\\n\").strip()\r\n    try:\r\n        unsorted = [int(item) for item in user_input.split(\",\")]\r\n    except ValueError:\r\n        print(\"Invalid input. Please enter valid integers separated by commas.\")\r\n        raise\r\n    print(f\"{binary_insertion_sort(unsorted) = }\")\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "sorts/binary_insertion_sort.py",
    "pr_number": 10918,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1370849357,
    "comment_created_at": "2023-10-24T21:45:29Z"
  },
  {
    "code": "@@ -0,0 +1,83 @@\n+\"\"\"\n+https://atharayil.medium.com/median-of-two-sorted-arrays-day-36-python-fcbd2dbbb668\n+\"\"\"\n+\n+\n+def find_median_sorted_arrays(nums1: list[int], nums2: list[int]) -> float:\n+    \"\"\"\n+    Finds the median of two sorted arrays.\n+\n+    :param nums1: The first sorted array.\n+    :param nums2: The second sorted array.\n+    :return: The median of the combined sorted arrays.\n+    :raises ValueError: If both input arrays are empty.\n+\n+    >>> find_median_sorted_arrays([1, 3], [2])",
    "comment": "test please for negative numbers, floating point numbers, letters, two empty arrays, arrays that are not sorted, etc?",
    "line_number": 15,
    "enriched": "File: data_structures/arrays/find_median_sorted_arrays.py\nCode: @@ -0,0 +1,83 @@\n+\"\"\"\n+https://atharayil.medium.com/median-of-two-sorted-arrays-day-36-python-fcbd2dbbb668\n+\"\"\"\n+\n+\n+def find_median_sorted_arrays(nums1: list[int], nums2: list[int]) -> float:\n+    \"\"\"\n+    Finds the median of two sorted arrays.\n+\n+    :param nums1: The first sorted array.\n+    :param nums2: The second sorted array.\n+    :return: The median of the combined sorted arrays.\n+    :raises ValueError: If both input arrays are empty.\n+\n+    >>> find_median_sorted_arrays([1, 3], [2])\nComment: Test please for negative numbers, floating point numbers, letters, two empty arrays, arrays that are not sorted, etc?",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/arrays/find_median_sorted_arrays.py",
    "pr_number": 11182,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1409832063,
    "comment_created_at": "2023-11-29T20:30:44Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 1, 2022\n+\n+Task:\n+Given a list of days when you need to travel. Each day is int from 1 to 365.\n+You are able to use tickets for 1 day, 7 days and 30 days.\n+Each ticket has a cost.\n+\n+Find the minimum cost you need to travel every day in the given list of days.\n+\n+Implementation notes:\n+implementation Dynamic Programming up bottom approach.\n+\n+The implementation was tested on the\n+leetcode: https://leetcode.com/problems/minimum-cost-for-tickets/\n+\"\"\"\n+\n+from typing import List\n+\n+\"\"\"\n+Minimum Cost For Tickets\n+Dynamic Programming: up -> down.\n+\"\"\"\n+\n+\n+def mincost_tickets(days: list[int], costs: list[int]) -> int:\n+    \"\"\"\n+    >>> mincost_tickets([1,4,6,7,8,20], [2,7,15])\n+    11\n+    >>> mincost_tickets([1,2,3,4,5,6,7,8,9,10,30,31], [2,7,15])\n+    17\n+    >>> mincost_tickets([1,2,3,4,5,6,7,8,9,10,30,31], [2,90,150])\n+    24",
    "comment": "please add tests with zero, negative numbers, floating point numbers, and mixing empty lists and non-empty lists.",
    "line_number": 34,
    "enriched": "File: dynamic_programming/minimum_tickets_cost.py\nCode: @@ -0,0 +1,61 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 1, 2022\n+\n+Task:\n+Given a list of days when you need to travel. Each day is int from 1 to 365.\n+You are able to use tickets for 1 day, 7 days and 30 days.\n+Each ticket has a cost.\n+\n+Find the minimum cost you need to travel every day in the given list of days.\n+\n+Implementation notes:\n+implementation Dynamic Programming up bottom approach.\n+\n+The implementation was tested on the\n+leetcode: https://leetcode.com/problems/minimum-cost-for-tickets/\n+\"\"\"\n+\n+from typing import List\n+\n+\"\"\"\n+Minimum Cost For Tickets\n+Dynamic Programming: up -> down.\n+\"\"\"\n+\n+\n+def mincost_tickets(days: list[int], costs: list[int]) -> int:\n+    \"\"\"\n+    >>> mincost_tickets([1,4,6,7,8,20], [2,7,15])\n+    11\n+    >>> mincost_tickets([1,2,3,4,5,6,7,8,9,10,30,31], [2,7,15])\n+    17\n+    >>> mincost_tickets([1,2,3,4,5,6,7,8,9,10,30,31], [2,90,150])\n+    24\nComment: Please add tests with zero, negative numbers, floating point numbers, and mixing empty lists and non-empty lists.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/minimum_tickets_cost.py",
    "pr_number": 7934,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1010821032,
    "comment_created_at": "2022-11-01T19:28:12Z"
  },
  {
    "code": "@@ -0,0 +1,72 @@\n+\"\"\"\n+wissamfawaz12@gmail.com | github.com/wissamfawaz\n+This implementation demonstrates how to generate the\n+elements of a Pascal's triangle.\n+What is Pascal's triangle?\n+- Refer to (https://en.wikipedia.org/wiki/Pascal%27s_triangle)\n+for more info about this triangle.\n+\"\"\"\n+\n+\n+def generate_pascal_triangle(num_rows: int) -> None:\n+    \"\"\"\n+    Print Pascal's triangle for different number of rows\n+    >>> generate_pascal_triangle(1)\n+    [[1]]\n+    >>> generate_pascal_triangle(2)\n+    [[1], [1, 1]]\n+    >>> generate_pascal_triangle(3)\n+    [[1], [1, 1], [1, 2, 1]]\n+    >>> generate_pascal_triangle(4)\n+    [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1]]\n+    >>> generate_pascal_triangle(5)\n+    [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1], [1, 4, 6, 4, 1]]\n+    \"\"\"\n+    triangle: list[list[int]] = []\n+    for current_row_idx in range(num_rows):\n+        populate_current_row(triangle, current_row_idx)\n+    print(triangle)",
    "comment": "you should return the triangle, not print it",
    "line_number": 28,
    "enriched": "File: other/pascal_triangle.py\nCode: @@ -0,0 +1,72 @@\n+\"\"\"\n+wissamfawaz12@gmail.com | github.com/wissamfawaz\n+This implementation demonstrates how to generate the\n+elements of a Pascal's triangle.\n+What is Pascal's triangle?\n+- Refer to (https://en.wikipedia.org/wiki/Pascal%27s_triangle)\n+for more info about this triangle.\n+\"\"\"\n+\n+\n+def generate_pascal_triangle(num_rows: int) -> None:\n+    \"\"\"\n+    Print Pascal's triangle for different number of rows\n+    >>> generate_pascal_triangle(1)\n+    [[1]]\n+    >>> generate_pascal_triangle(2)\n+    [[1], [1, 1]]\n+    >>> generate_pascal_triangle(3)\n+    [[1], [1, 1], [1, 2, 1]]\n+    >>> generate_pascal_triangle(4)\n+    [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1]]\n+    >>> generate_pascal_triangle(5)\n+    [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1], [1, 4, 6, 4, 1]]\n+    \"\"\"\n+    triangle: list[list[int]] = []\n+    for current_row_idx in range(num_rows):\n+        populate_current_row(triangle, current_row_idx)\n+    print(triangle)\nComment: You should return the triangle, not print it",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "other/pascal_triangle.py",
    "pr_number": 7310,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996428833,
    "comment_created_at": "2022-10-16T11:20:08Z"
  },
  {
    "code": "@@ -1,11 +1,27 @@\n def perfect_cube(n: int) -> bool:\n     \"\"\"\n-    Check if a number is a perfect cube or not.\n+      Check if a number is a perfect cube or not.\n \n     >>> perfect_cube(27)\n-    True\n-    >>> perfect_cube(4)\n-    False\n+      True\n+      >>> perfect_cube(4)\n+      False\n+      >>> perfect_cube(8)\n+      False\n+      >>> perfect_cube(125)\n+      True\n+      >>> perfect_cube(64)\n+      True\n+      >>> perfect_cube(0)\n+      True\n+      >>> perfect_cube(1)\n+      True\n+      >>> perfect_cube(3.375)",
    "comment": "there are three tests that should raise valueerror",
    "line_number": 19,
    "enriched": "File: maths/perfect_cube.py\nCode: @@ -1,11 +1,27 @@\n def perfect_cube(n: int) -> bool:\n     \"\"\"\n-    Check if a number is a perfect cube or not.\n+      Check if a number is a perfect cube or not.\n \n     >>> perfect_cube(27)\n-    True\n-    >>> perfect_cube(4)\n-    False\n+      True\n+      >>> perfect_cube(4)\n+      False\n+      >>> perfect_cube(8)\n+      False\n+      >>> perfect_cube(125)\n+      True\n+      >>> perfect_cube(64)\n+      True\n+      >>> perfect_cube(0)\n+      True\n+      >>> perfect_cube(1)\n+      True\n+      >>> perfect_cube(3.375)\nComment: There are three tests that should raise ValueError\r\n",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/perfect_cube.py",
    "pr_number": 10801,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367881728,
    "comment_created_at": "2023-10-22T10:47:38Z"
  },
  {
    "code": "@@ -8,36 +8,54 @@\n import math\n import os\n \n-import cv2\n-import numpy as np\n-\n PIXEL_MAX = 255.0\n \n \n-def peak_signal_to_noise_ratio(original: float, contrast: float) -> float:\n-    mse = np.mean((original - contrast) ** 2)\n+def peak_signal_to_noise_ratio(original, contrast):",
    "comment": "please revert the changes to this line. removing the type hints is not allowed.  that would be going in the wrong direction since it would be blinding mypy which runs in our github actions.\r\n\r\nmodifying functions that have no tests is also not allowed.  please create a separate pull request that adds doctests.  once that pull request is merged then we can come back to this pull request.",
    "line_number": 14,
    "enriched": "File: compression/peak_signal_to_noise_ratio.py\nCode: @@ -8,36 +8,54 @@\n import math\n import os\n \n-import cv2\n-import numpy as np\n-\n PIXEL_MAX = 255.0\n \n \n-def peak_signal_to_noise_ratio(original: float, contrast: float) -> float:\n-    mse = np.mean((original - contrast) ** 2)\n+def peak_signal_to_noise_ratio(original, contrast):\nComment: Please revert the changes to this line. Removing the type hints is NOT allowed.  That would be going in the wrong direction since it would be blinding `mypy` which runs in our GitHub Actions.\r\n\r\nModifying functions that have no tests is also NOT allowed.  Please create a separate pull request that adds doctests.  Once that pull request is merged then we can come back to this pull request.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "compression/peak_signal_to_noise_ratio.py",
    "pr_number": 10744,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367662824,
    "comment_created_at": "2023-10-21T04:50:57Z"
  },
  {
    "code": "@@ -35,36 +50,45 @@ def minimax(\n     12\n     \"\"\"\n \n+    # Check for invalid inputs",
    "comment": "i feel that this comment isn't really necessary. the if-statement is very simple so it's already pretty clear what it does.",
    "line_number": 53,
    "enriched": "File: backtracking/minimax.py\nCode: @@ -35,36 +50,45 @@ def minimax(\n     12\n     \"\"\"\n \n+    # Check for invalid inputs\nComment: ```suggestion\r\n```\r\nI feel that this comment isn't really necessary. The if-statement is very simple so it's already pretty clear what it does.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "backtracking/minimax.py",
    "pr_number": 10838,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368675753,
    "comment_created_at": "2023-10-23T13:31:11Z"
  },
  {
    "code": "@@ -79,24 +79,26 @@ def jacobi_iteration_method(\n     rows2, cols2 = constant_matrix.shape\r\n \r\n     if rows1 != cols1:\r\n-        raise ValueError(\r\n-            f\"Coefficient matrix dimensions must be nxn but received {rows1}x{cols1}\"\r\n-        )\r\n+        msg = f\"Coefficient matrix dimensions must be nxn but received {rows1}x{cols1}\"\r\n+        raise ValueError(msg)\r\n \r\n     if cols2 != 1:\r\n-        raise ValueError(f\"Constant matrix must be nx1 but received {rows2}x{cols2}\")\r\n+        msg = f\"Constant matrix must be nx1 but received {rows2}x{cols2}\"\r\n+        raise ValueError(msg)\r\n \r\n     if rows1 != rows2:\r\n-        raise ValueError(\r\n-            f\"\"\"Coefficient and constant matrices dimensions must be nxn and nx1 but\r\n-            received {rows1}x{cols1} and {rows2}x{cols2}\"\"\"\r\n+        msg = (\r\n+            \"Coefficient and constant matrices dimensions must be nxn and nx1 but\\n\"\r\n+            f\"            received {rows1}x{cols1} and {rows2}x{cols2}\"\r",
    "comment": "ruff did format this correctly as earlier it was in triple quoted string, but this makes more sense.",
    "line_number": 92,
    "enriched": "File: arithmetic_analysis/jacobi_iteration_method.py\nCode: @@ -79,24 +79,26 @@ def jacobi_iteration_method(\n     rows2, cols2 = constant_matrix.shape\r\n \r\n     if rows1 != cols1:\r\n-        raise ValueError(\r\n-            f\"Coefficient matrix dimensions must be nxn but received {rows1}x{cols1}\"\r\n-        )\r\n+        msg = f\"Coefficient matrix dimensions must be nxn but received {rows1}x{cols1}\"\r\n+        raise ValueError(msg)\r\n \r\n     if cols2 != 1:\r\n-        raise ValueError(f\"Constant matrix must be nx1 but received {rows2}x{cols2}\")\r\n+        msg = f\"Constant matrix must be nx1 but received {rows2}x{cols2}\"\r\n+        raise ValueError(msg)\r\n \r\n     if rows1 != rows2:\r\n-        raise ValueError(\r\n-            f\"\"\"Coefficient and constant matrices dimensions must be nxn and nx1 but\r\n-            received {rows1}x{cols1} and {rows2}x{cols2}\"\"\"\r\n+        msg = (\r\n+            \"Coefficient and constant matrices dimensions must be nxn and nx1 but\\n\"\r\n+            f\"            received {rows1}x{cols1} and {rows2}x{cols2}\"\r\nComment: ```suggestion\r\n        msg = (\r\n            \"Coefficient and constant matrices dimensions must be nxn and nx1 but \"\r\n            f\"received {rows1}x{cols1} and {rows2}x{cols2}\"\r\n```\r\n\r\nRuff did format this correctly as earlier it was in triple quoted string, but this makes more sense.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "arithmetic_analysis/jacobi_iteration_method.py",
    "pr_number": 8767,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1206290976,
    "comment_created_at": "2023-05-26T06:27:31Z"
  },
  {
    "code": "@@ -65,18 +93,68 @@ def peek(self) -> T:\n         return self.stack[-1]\n \n     def is_empty(self) -> bool:\n-        \"\"\"Check if a stack is empty.\"\"\"\n+        \"\"\"\n+        Check if a stack is empty.\n+\n+        >>> S = Stack()\n+        >>> S.is_empty()\n+        True\n+\n+        >>> S = Stack()\n+        >>> S.push(10)\n+        >>> S.is_empty()\n+        False\n+        \"\"\"\n         return not bool(self.stack)\n \n     def is_full(self) -> bool:\n+        \"\"\"\n+        >>> S = Stack()\n+        >>> S.is_full()\n+        False\n+\n+        >>> S = Stack(1)\n+        >>> S.push(10)\n+        >>> S.is_full()\n+        True\n+        \"\"\"\n         return self.size() == self.limit\n \n     def size(self) -> int:\n-        \"\"\"Return the size of the stack.\"\"\"\n+        \"\"\"\n+        Return the size of the stack.\n+\n+        >>> S = Stack(3)\n+        >>> S.size()\n+        0\n+\n+        >>> S = Stack(3)\n+        >>> S.push(10)\n+        >>> S.size()\n+        1\n+\n+        >>> S = Stack(3)\n+        >>> S.push(10)\n+        >>> S.push(20)\n+        >>> S.size()\n+        2\n+        \"\"\"\n         return len(self.stack)\n \n     def __contains__(self, item: T) -> bool:\n-        \"\"\"Check if item is in stack\"\"\"\n+        \"\"\"\n+        Check if item is in stack\n+\n+        >>> S = Stack(3)\n+        >>> S.push(10)\n+        >>> S.__contains__(10)",
    "comment": "the point of __contains__ is to overload the built-in in operator",
    "line_number": 150,
    "enriched": "File: data_structures/stacks/stack.py\nCode: @@ -65,18 +93,68 @@ def peek(self) -> T:\n         return self.stack[-1]\n \n     def is_empty(self) -> bool:\n-        \"\"\"Check if a stack is empty.\"\"\"\n+        \"\"\"\n+        Check if a stack is empty.\n+\n+        >>> S = Stack()\n+        >>> S.is_empty()\n+        True\n+\n+        >>> S = Stack()\n+        >>> S.push(10)\n+        >>> S.is_empty()\n+        False\n+        \"\"\"\n         return not bool(self.stack)\n \n     def is_full(self) -> bool:\n+        \"\"\"\n+        >>> S = Stack()\n+        >>> S.is_full()\n+        False\n+\n+        >>> S = Stack(1)\n+        >>> S.push(10)\n+        >>> S.is_full()\n+        True\n+        \"\"\"\n         return self.size() == self.limit\n \n     def size(self) -> int:\n-        \"\"\"Return the size of the stack.\"\"\"\n+        \"\"\"\n+        Return the size of the stack.\n+\n+        >>> S = Stack(3)\n+        >>> S.size()\n+        0\n+\n+        >>> S = Stack(3)\n+        >>> S.push(10)\n+        >>> S.size()\n+        1\n+\n+        >>> S = Stack(3)\n+        >>> S.push(10)\n+        >>> S.push(20)\n+        >>> S.size()\n+        2\n+        \"\"\"\n         return len(self.stack)\n \n     def __contains__(self, item: T) -> bool:\n-        \"\"\"Check if item is in stack\"\"\"\n+        \"\"\"\n+        Check if item is in stack\n+\n+        >>> S = Stack(3)\n+        >>> S.push(10)\n+        >>> S.__contains__(10)\nComment: ```suggestion\r\n        >>> 10 in S\r\n```\r\nThe point of `__contains__` is to overload the built-in `in` operator",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "data_structures/stacks/stack.py",
    "pr_number": 11149,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1389865216,
    "comment_created_at": "2023-11-10T20:07:22Z"
  },
  {
    "code": "@@ -1,7 +1,7 @@\n \"\"\"\n YouTube Explanation: https://www.youtube.com/watch?v=f2xi3c1S95M\n \n-Given an integer n, return the minimum steps to 1\n+Integer n is given, return the minimum steps from n to 1",
    "comment": "the phrasing in the first part of the sentence was more natural",
    "line_number": 4,
    "enriched": "File: dynamic_programming/minimum_steps_to_one.py\nCode: @@ -1,7 +1,7 @@\n \"\"\"\n YouTube Explanation: https://www.youtube.com/watch?v=f2xi3c1S95M\n \n-Given an integer n, return the minimum steps to 1\n+Integer n is given, return the minimum steps from n to 1\nComment: ```suggestion\r\nGiven an integer n, return the minimum steps from n to 1\r\n```\r\nThe phrasing in the first part of the sentence was more natural",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "dynamic_programming/minimum_steps_to_one.py",
    "pr_number": 9841,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1351404736,
    "comment_created_at": "2023-10-10T04:29:51Z"
  },
  {
    "code": "@@ -0,0 +1,83 @@\n+\"\"\"\n+Convert speed units\n+\n+https://en.wikipedia.org/wiki/Kilometres_per_hour\n+https://en.wikipedia.org/wiki/Miles_per_hour\n+https://en.wikipedia.org/wiki/Knot_(unit)\n+https://en.wikipedia.org/wiki/Metre_per_second\n+\n+\"\"\"\n+\n+speed_chart: dict[str, float] = {\n+    \"km/h\": 1.0,\n+    \"m/s\": 3.6,\n+    \"mph\": 1.609344,\n+    \"knot\": 1.852,\n+}\n+\n+speed_chart_inverse: dict[str, float] = {\n+    \"km/h\": 1.0,\n+    \"m/s\": 0.277777778,\n+    \"mph\": 0.621371192,\n+    \"knot\": 0.539956803,\n+}\n+\n+\n+def convert_speed(speed: float, unit_from: str, unit_to: str) -> float:\n+    \"\"\"\n+    Convert speed from one unit to another unit using the speed_chart\n+\n+    \"km/h\": 1.0,\n+    \"m/s\": 3.6,\n+    \"mph\": 1.609344,\n+    \"knot\": 1.852,\n+\n+    >>> convert_speed(100, \"km/h\", \"m/s\")\n+    27.778\n+\n+    >>> convert_speed(100, \"km/h\", \"mph\")\n+    62.137\n+\n+    >>> convert_speed(100, \"km/h\", \"knot\")\n+    53.996\n+\n+    >>> convert_speed(100, \"m/s\", \"km/h\")\n+    360.0\n+\n+    >>> convert_speed(100, \"m/s\", \"mph\")\n+    223.694\n+\n+    >>> convert_speed(100, \"m/s\", \"knot\")\n+    194.384\n+\n+    >>> convert_speed(100, \"mph\", \"km/h\")\n+    160.934\n+\n+    >>> convert_speed(100, \"mph\", \"m/s\")\n+    44.704\n+\n+    >>> convert_speed(100, \"mph\", \"knot\")\n+    86.898\n+\n+    >>> convert_speed(100, \"knot\", \"km/h\")\n+    185.2\n+\n+    >>> convert_speed(100, \"knot\", \"m/s\")\n+    51.444\n+",
    "comment": "let's decrease the amount of scrolling required to read the implementation.  ;-)",
    "line_number": 67,
    "enriched": "File: conversions/speed_conversions.py\nCode: @@ -0,0 +1,83 @@\n+\"\"\"\n+Convert speed units\n+\n+https://en.wikipedia.org/wiki/Kilometres_per_hour\n+https://en.wikipedia.org/wiki/Miles_per_hour\n+https://en.wikipedia.org/wiki/Knot_(unit)\n+https://en.wikipedia.org/wiki/Metre_per_second\n+\n+\"\"\"\n+\n+speed_chart: dict[str, float] = {\n+    \"km/h\": 1.0,\n+    \"m/s\": 3.6,\n+    \"mph\": 1.609344,\n+    \"knot\": 1.852,\n+}\n+\n+speed_chart_inverse: dict[str, float] = {\n+    \"km/h\": 1.0,\n+    \"m/s\": 0.277777778,\n+    \"mph\": 0.621371192,\n+    \"knot\": 0.539956803,\n+}\n+\n+\n+def convert_speed(speed: float, unit_from: str, unit_to: str) -> float:\n+    \"\"\"\n+    Convert speed from one unit to another unit using the speed_chart\n+\n+    \"km/h\": 1.0,\n+    \"m/s\": 3.6,\n+    \"mph\": 1.609344,\n+    \"knot\": 1.852,\n+\n+    >>> convert_speed(100, \"km/h\", \"m/s\")\n+    27.778\n+\n+    >>> convert_speed(100, \"km/h\", \"mph\")\n+    62.137\n+\n+    >>> convert_speed(100, \"km/h\", \"knot\")\n+    53.996\n+\n+    >>> convert_speed(100, \"m/s\", \"km/h\")\n+    360.0\n+\n+    >>> convert_speed(100, \"m/s\", \"mph\")\n+    223.694\n+\n+    >>> convert_speed(100, \"m/s\", \"knot\")\n+    194.384\n+\n+    >>> convert_speed(100, \"mph\", \"km/h\")\n+    160.934\n+\n+    >>> convert_speed(100, \"mph\", \"m/s\")\n+    44.704\n+\n+    >>> convert_speed(100, \"mph\", \"knot\")\n+    86.898\n+\n+    >>> convert_speed(100, \"knot\", \"km/h\")\n+    185.2\n+\n+    >>> convert_speed(100, \"knot\", \"m/s\")\n+    51.444\n+\nComment: Let's decrease the amount of scrolling required to read the implementation.  ;-)\r\n```suggestion\r\n    >>> convert_speed(100, \"km/h\", \"mph\")\r\n    62.137\r\n    >>> convert_speed(100, \"km/h\", \"knot\")\r\n    53.996\r\n    >>> convert_speed(100, \"m/s\", \"km/h\")\r\n    360.0\r\n    >>> convert_speed(100, \"m/s\", \"mph\")\r\n    223.694\r\n    >>> convert_speed(100, \"m/s\", \"knot\")\r\n    194.384\r\n    >>> convert_speed(100, \"mph\", \"km/h\")\r\n    160.934\r\n    >>> convert_speed(100, \"mph\", \"m/s\")\r\n    44.704\r\n    >>> convert_speed(100, \"mph\", \"knot\")\r\n    86.898\r\n    >>> convert_speed(100, \"knot\", \"km/h\")\r\n    185.2\r\n    >>> convert_speed(100, \"knot\", \"m/s\")\r\n    51.444\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "conversions/speed_conversions.py",
    "pr_number": 7128,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 995422428,
    "comment_created_at": "2022-10-14T07:14:37Z"
  },
  {
    "code": "@@ -36,7 +36,7 @@ def download_image(url: str) -> str:\n     if not image_data:\n         return f\"Failed to download the image from {image_url}.\"\n \n-    file_name = f\"{datetime.now():%Y-%m-%d_%H:%M:%S}.jpg\"\n+    file_name = f\"{datetime.now(tz=UTC):%Y-%m-%d_%H:%M:%S}.jpg\"",
    "comment": "i would think the user would expect us to use their local timezone here.",
    "line_number": 39,
    "enriched": "File: web_programming/instagram_pic.py\nCode: @@ -36,7 +36,7 @@ def download_image(url: str) -> str:\n     if not image_data:\n         return f\"Failed to download the image from {image_url}.\"\n \n-    file_name = f\"{datetime.now():%Y-%m-%d_%H:%M:%S}.jpg\"\n+    file_name = f\"{datetime.now(tz=UTC):%Y-%m-%d_%H:%M:%S}.jpg\"\nComment: I would think the user would expect us to use their local timezone here.\r\n```suggestion\r\n    file_name = f\"{datetime.now(tz=None):%Y-%m-%d_%H:%M:%S}.jpg\"\r\n```",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "web_programming/instagram_pic.py",
    "pr_number": 11327,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1538179982,
    "comment_created_at": "2024-03-25T20:15:44Z"
  },
  {
    "code": "@@ -25,20 +27,26 @@ def main():\n \n     # Gaussian Naive Bayes\n     nb_model = GaussianNB()\n-    nb_model.fit(x_train, y_train)\n+    time.sleep(2.9)",
    "comment": "why are we waiting for 3 seconds?!?  comments needed to be added to explain such things.",
    "line_number": 30,
    "enriched": "File: machine_learning/gaussian_naive_bayes.py\nCode: @@ -25,20 +27,26 @@ def main():\n \n     # Gaussian Naive Bayes\n     nb_model = GaussianNB()\n-    nb_model.fit(x_train, y_train)\n+    time.sleep(2.9)\nComment: Why are we waiting for 3 seconds?!?  Comments needed to be added to explain such things.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "machine_learning/gaussian_naive_bayes.py",
    "pr_number": 7406,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 999327659,
    "comment_created_at": "2022-10-19T11:42:51Z"
  },
  {
    "code": "@@ -3,52 +3,34 @@\n import string\n \n \n-def atbash_slow(sequence: str) -> str:\n+def atbash(text: str) -> str:\n     \"\"\"\n-    >>> atbash_slow(\"ABCDEFG\")\n-    'ZYXWVUT'\n+    Encodes or decodes text using the Atbash cipher.\n \n-    >>> atbash_slow(\"aW;;123BX\")\n-    'zD;;123YC'\n-    \"\"\"\n-    output = \"\"\n-    for i in sequence:\n-        extract = ord(i)\n-        if 65 <= extract <= 90:\n-            output += chr(155 - extract)\n-        elif 97 <= extract <= 122:\n-            output += chr(219 - extract)\n-        else:\n-            output += i\n-    return output\n-\n-\n-def atbash(sequence: str) -> str:\n-    \"\"\"\n-    >>> atbash(\"ABCDEFG\")\n-    'ZYXWVUT'\n-\n-    >>> atbash(\"aW;;123BX\")\n-    'zD;;123YC'\n-    \"\"\"\n-    letters = string.ascii_letters\n-    letters_reversed = string.ascii_lowercase[::-1] + string.ascii_uppercase[::-1]\n-    return \"\".join(\n-        letters_reversed[letters.index(c)] if c in letters else c for c in sequence\n-    )\n+    The Atbash cipher substitutes each letter with its mirror in the alphabet:\n+    A -> Z, B -> Y, C -> X, ... Z -> A (case is preserved)\n+    Non-alphabetic characters are left unchanged.\n \n+    Args:\n+        text: The input string to encode/decode\n \n-def benchmark() -> None:\n-    \"\"\"Let's benchmark our functions side-by-side...\"\"\"\n-    from timeit import timeit\n+    Returns:\n+        The transformed string\n+    \"\"\"\n+    # Create translation tables for uppercase and lowercase\n+    lowercase_map = str.maketrans(string.ascii_lowercase, string.ascii_lowercase[::-1])\n+    uppercase_map = str.maketrans(string.ascii_uppercase, string.ascii_uppercase[::-1])\n \n-    print(\"Running performance benchmarks...\")\n-    setup = \"from string import printable ; from __main__ import atbash, atbash_slow\"\n-    print(f\"> atbash_slow(): {timeit('atbash_slow(printable)', setup=setup)} seconds\")\n-    print(f\">      atbash(): {timeit('atbash(printable)', setup=setup)} seconds\")\n+    # Apply both translation mappings\n+    return text.translate(lowercase_map).translate(uppercase_map)",
    "comment": "the goal of benchmark is to measure the speed of the algorithms.  replacing the algorithm defeats the purpose of this function.",
    "line_number": 25,
    "enriched": "File: ciphers/atbash.py\nCode: @@ -3,52 +3,34 @@\n import string\n \n \n-def atbash_slow(sequence: str) -> str:\n+def atbash(text: str) -> str:\n     \"\"\"\n-    >>> atbash_slow(\"ABCDEFG\")\n-    'ZYXWVUT'\n+    Encodes or decodes text using the Atbash cipher.\n \n-    >>> atbash_slow(\"aW;;123BX\")\n-    'zD;;123YC'\n-    \"\"\"\n-    output = \"\"\n-    for i in sequence:\n-        extract = ord(i)\n-        if 65 <= extract <= 90:\n-            output += chr(155 - extract)\n-        elif 97 <= extract <= 122:\n-            output += chr(219 - extract)\n-        else:\n-            output += i\n-    return output\n-\n-\n-def atbash(sequence: str) -> str:\n-    \"\"\"\n-    >>> atbash(\"ABCDEFG\")\n-    'ZYXWVUT'\n-\n-    >>> atbash(\"aW;;123BX\")\n-    'zD;;123YC'\n-    \"\"\"\n-    letters = string.ascii_letters\n-    letters_reversed = string.ascii_lowercase[::-1] + string.ascii_uppercase[::-1]\n-    return \"\".join(\n-        letters_reversed[letters.index(c)] if c in letters else c for c in sequence\n-    )\n+    The Atbash cipher substitutes each letter with its mirror in the alphabet:\n+    A -> Z, B -> Y, C -> X, ... Z -> A (case is preserved)\n+    Non-alphabetic characters are left unchanged.\n \n+    Args:\n+        text: The input string to encode/decode\n \n-def benchmark() -> None:\n-    \"\"\"Let's benchmark our functions side-by-side...\"\"\"\n-    from timeit import timeit\n+    Returns:\n+        The transformed string\n+    \"\"\"\n+    # Create translation tables for uppercase and lowercase\n+    lowercase_map = str.maketrans(string.ascii_lowercase, string.ascii_lowercase[::-1])\n+    uppercase_map = str.maketrans(string.ascii_uppercase, string.ascii_uppercase[::-1])\n \n-    print(\"Running performance benchmarks...\")\n-    setup = \"from string import printable ; from __main__ import atbash, atbash_slow\"\n-    print(f\"> atbash_slow(): {timeit('atbash_slow(printable)', setup=setup)} seconds\")\n-    print(f\">      atbash(): {timeit('atbash(printable)', setup=setup)} seconds\")\n+    # Apply both translation mappings\n+    return text.translate(lowercase_map).translate(uppercase_map)\nComment: The goal of benchmark is to measure the speed of the algorithms.  Replacing the algorithm defeats the purpose of this function.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "ciphers/atbash.py",
    "pr_number": 12811,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2186967507,
    "comment_created_at": "2025-07-05T08:37:22Z"
  },
  {
    "code": "@@ -58,9 +79,29 @@ def is_palindrome_slice(s: str) -> bool:\n     return s == s[::-1]\n \n \n+def benchmark_function(name: str) -> None:\n+    setup = f\"from __main__ import test_data, {name}\"\n+    number = 100000\n+    res = timeit(\n+        f\"all({name}(key) is value for key, value in test_data.items())\",\n+        setup=setup,\n+        number=number,\n+    )\n+    print(f\"{name:<35} finished {number} runs in {res:.5f} seconds\")\n+\n+\n if __name__ == \"__main__\":\n     for key, value in test_data.items():\n         assert is_palindrome(key) is is_palindrome_recursive(key)\n         assert is_palindrome(key) is is_palindrome_slice(key)\n         print(f\"{key:21} {value}\")\n     print(\"a man a plan a canal panama\")\n+\n+    benchmark_function(\"is_palindrome\")  # finished 100000 runs in 0.33785 seconds\n+    benchmark_function(\n+        \"is_palindrome_traversal\"\n+    )  # finished 100000 runs in 0.70002 seconds\n+    benchmark_function(\n+        \"is_palindrome_recursive\"\n+    )  # finished 100000 runs in 0.48514 seconds\n+    benchmark_function(\"is_palindrome_slice\")  # finished 100000 runs in 0.18703 seconds",
    "comment": "easier to read... sorted fastest to slowest.",
    "line_number": 107,
    "enriched": "File: strings/palindrome.py\nCode: @@ -58,9 +79,29 @@ def is_palindrome_slice(s: str) -> bool:\n     return s == s[::-1]\n \n \n+def benchmark_function(name: str) -> None:\n+    setup = f\"from __main__ import test_data, {name}\"\n+    number = 100000\n+    res = timeit(\n+        f\"all({name}(key) is value for key, value in test_data.items())\",\n+        setup=setup,\n+        number=number,\n+    )\n+    print(f\"{name:<35} finished {number} runs in {res:.5f} seconds\")\n+\n+\n if __name__ == \"__main__\":\n     for key, value in test_data.items():\n         assert is_palindrome(key) is is_palindrome_recursive(key)\n         assert is_palindrome(key) is is_palindrome_slice(key)\n         print(f\"{key:21} {value}\")\n     print(\"a man a plan a canal panama\")\n+\n+    benchmark_function(\"is_palindrome\")  # finished 100000 runs in 0.33785 seconds\n+    benchmark_function(\n+        \"is_palindrome_traversal\"\n+    )  # finished 100000 runs in 0.70002 seconds\n+    benchmark_function(\n+        \"is_palindrome_recursive\"\n+    )  # finished 100000 runs in 0.48514 seconds\n+    benchmark_function(\"is_palindrome_slice\")  # finished 100000 runs in 0.18703 seconds\nComment: Easier to read... Sorted fastest to slowest.\r\n```suggestion\r\n    # finished 100,000 runs in 0.18703 seconds\r\n    benchmark_function(\"is_palindrome_slice\")\r\n    # finished 100,000 runs in 0.33785 seconds\r\n    benchmark_function(\"is_palindrome\")\r\n    # finished 100,000 runs in 0.48514 seconds\r\n    benchmark_function(\"is_palindrome_recursive\")\r\n    # finished 100,000 runs in 0.70002 seconds\r\n    benchmark_function(\"is_palindrome_traversal\")\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "strings/palindrome.py",
    "pr_number": 8749,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1205194096,
    "comment_created_at": "2023-05-25T08:43:10Z"
  },
  {
    "code": "@@ -0,0 +1,57 @@\n+\"\"\"\n+A unit of time is any particular time interval, used as a standard\n+way of measuring or expressing duration.\n+The base unit of time in the International System of Units (SI),\n+and by extension most of the Western world, is the second,\n+defined as about 9 billion oscillations of the caesium atom.\n+\n+WIKI: https://en.wikipedia.org/wiki/Unit_of_time",
    "comment": "let's use our 88 characters per line...",
    "line_number": 8,
    "enriched": "File: conversions/time_conversions.py\nCode: @@ -0,0 +1,57 @@\n+\"\"\"\n+A unit of time is any particular time interval, used as a standard\n+way of measuring or expressing duration.\n+The base unit of time in the International System of Units (SI),\n+and by extension most of the Western world, is the second,\n+defined as about 9 billion oscillations of the caesium atom.\n+\n+WIKI: https://en.wikipedia.org/wiki/Unit_of_time\nComment: Let's use our 88 characters per line...\r\n```suggestion\r\nA unit of time is any particular time interval, used as a standard way of measuring or\r\nexpressing duration.  The base unit of time in the International System of Units (SI),\r\nand by extension most of the Western world, is the second, defined as about 9 billion\r\noscillations of the caesium atom.\r\n\r\nhttps://en.wikipedia.org/wiki/Unit_of_time\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "conversions/time_conversions.py",
    "pr_number": 10749,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367851198,
    "comment_created_at": "2023-10-22T07:13:06Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+\"\"\"\n+Knuth-Yao dynamic programming speedup,\n+also known as the Knuth-Yao optimization, is a technique that accelerates\n+the computation of certain dynamic programming algorithms.\n+It was introduced by Donald Knuth and Andrew Yao in their paper\n+\"Efficient Binary-Search Trees\" in 1976.\n+\n+This implementation the following recurrence relation,\n+\n+dp[i][j] = min(i < k < j){dp[i][k] + dp[k][j] + cost[i][j]},\n+\n+where opt[i][j-1] <= opt[i][j] <= opt[i+1][j] with opt[i][j] representing\n+the value of 'k' that minimizes the given expression.\n+\n+Equivalently, the cost function satisfies either of the following conditions.\n+\n+1) cost[b][c] <= cost[a][d]\n+2) cost[a][c]+cost[b][d] <= cost[a][d]+cost[b][c]\n+\n+Reference: https://cp-algorithms.com/dynamic_programming/knuth-optimization.html\n+\n+- time complexity: O(n^2)\n+- space complexity: O(n^2)\n+\n+>>> knuth_yao_speedup([[1,2,3,4],[3,4,5,1],[1,1,1,3],[2,2,2,2]])\n+15\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import sys\n+\n+\n+def knuth_yao_speedup(cost: list[list[int]]) -> int:\n+    n = len(cost)",
    "comment": "add below doctests or add yours if you have...\r\n\r\n        >>> knuth_yao_speedup([[1]])\r\n        0\r\n        >>> knuth_yao_speedup([[1, 2], [3, 4]])\r\n        5\r\n        >>> knuth_yao_speedup([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\r\n        15",
    "line_number": 35,
    "enriched": "File: dynamic_programming/knuth_yao_speedup.py\nCode: @@ -0,0 +1,61 @@\n+\"\"\"\n+Knuth-Yao dynamic programming speedup,\n+also known as the Knuth-Yao optimization, is a technique that accelerates\n+the computation of certain dynamic programming algorithms.\n+It was introduced by Donald Knuth and Andrew Yao in their paper\n+\"Efficient Binary-Search Trees\" in 1976.\n+\n+This implementation the following recurrence relation,\n+\n+dp[i][j] = min(i < k < j){dp[i][k] + dp[k][j] + cost[i][j]},\n+\n+where opt[i][j-1] <= opt[i][j] <= opt[i+1][j] with opt[i][j] representing\n+the value of 'k' that minimizes the given expression.\n+\n+Equivalently, the cost function satisfies either of the following conditions.\n+\n+1) cost[b][c] <= cost[a][d]\n+2) cost[a][c]+cost[b][d] <= cost[a][d]+cost[b][c]\n+\n+Reference: https://cp-algorithms.com/dynamic_programming/knuth-optimization.html\n+\n+- time complexity: O(n^2)\n+- space complexity: O(n^2)\n+\n+>>> knuth_yao_speedup([[1,2,3,4],[3,4,5,1],[1,1,1,3],[2,2,2,2]])\n+15\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import sys\n+\n+\n+def knuth_yao_speedup(cost: list[list[int]]) -> int:\n+    n = len(cost)\nComment: add below doctests or add yours if you have...\r\n\r\n        >>> knuth_yao_speedup([[1]])\r\n        0\r\n        >>> knuth_yao_speedup([[1, 2], [3, 4]])\r\n        5\r\n        >>> knuth_yao_speedup([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\r\n        15",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/knuth_yao_speedup.py",
    "pr_number": 9024,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1316860375,
    "comment_created_at": "2023-09-06T07:33:58Z"
  },
  {
    "code": "@@ -0,0 +1,120 @@\n+\"\"\"\n+https://en.wikipedia.org/wiki/Kruskal%27s_algorithm\n+\"\"\"\n+\n+import doctest",
    "comment": "please move this down into __main__ because we only need it there.",
    "line_number": 5,
    "enriched": "File: greedy_methods/kruskal.py\nCode: @@ -0,0 +1,120 @@\n+\"\"\"\n+https://en.wikipedia.org/wiki/Kruskal%27s_algorithm\n+\"\"\"\n+\n+import doctest\nComment: Please move this down into `__main__` because we only need it there.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "greedy_methods/kruskal.py",
    "pr_number": 11185,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1409816168,
    "comment_created_at": "2023-11-29T20:13:27Z"
  },
  {
    "code": "@@ -14,29 +17,32 @@ def double_sort(lst):\n     >>> double_sort([-3, 10, 16, -42, 29]) == sorted([-3, 10, 16, -42, 29])\r\n     True\r\n     \"\"\"\r\n-    no_of_elements = len(lst)\r\n+    no_of_elements = len(collection)\r\n     for _ in range(\r\n         int(((no_of_elements - 1) / 2) + 1)\r\n     ):  # we don't need to traverse to end of list as\r\n         for j in range(no_of_elements - 1):\r\n             if (\r\n-                lst[j + 1] < lst[j]\r\n+                collection[j + 1] < collection[j]\r\n             ):  # applying bubble sort algorithm from left to right (or forwards)\r\n-                temp = lst[j + 1]\r\n-                lst[j + 1] = lst[j]\r\n-                lst[j] = temp\r\n+                collection[j], collection[j + 1] = collection[j + 1], collection[j]\r\n             if (\r\n-                lst[no_of_elements - 1 - j] < lst[no_of_elements - 2 - j]\r\n+                collection[no_of_elements - 1 - j] < collection[no_of_elements - 2 - j]\r\n             ):  # applying bubble sort algorithm from right to left (or backwards)\r",
    "comment": "let's not wrap lines of code just to make room for a trailing comment.  please put the comment on a line of its own _before_ the if statement.  please repeat on any other comments in this file that force a line to be wrapped.",
    "line_number": 31,
    "enriched": "File: sorts/double_sort.py\nCode: @@ -14,29 +17,32 @@ def double_sort(lst):\n     >>> double_sort([-3, 10, 16, -42, 29]) == sorted([-3, 10, 16, -42, 29])\r\n     True\r\n     \"\"\"\r\n-    no_of_elements = len(lst)\r\n+    no_of_elements = len(collection)\r\n     for _ in range(\r\n         int(((no_of_elements - 1) / 2) + 1)\r\n     ):  # we don't need to traverse to end of list as\r\n         for j in range(no_of_elements - 1):\r\n             if (\r\n-                lst[j + 1] < lst[j]\r\n+                collection[j + 1] < collection[j]\r\n             ):  # applying bubble sort algorithm from left to right (or forwards)\r\n-                temp = lst[j + 1]\r\n-                lst[j + 1] = lst[j]\r\n-                lst[j] = temp\r\n+                collection[j], collection[j + 1] = collection[j + 1], collection[j]\r\n             if (\r\n-                lst[no_of_elements - 1 - j] < lst[no_of_elements - 2 - j]\r\n+                collection[no_of_elements - 1 - j] < collection[no_of_elements - 2 - j]\r\n             ):  # applying bubble sort algorithm from right to left (or backwards)\r\nComment: Let's not wrap lines of code just to make room for a trailing comment.  Please put the comment on a line of its own _before_ the `if` statement.  Please repeat on any other comments in this file that force a line to be wrapped.\r\n```suggestion\r\n            ):\r\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "sorts/double_sort.py",
    "pr_number": 10798,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368238206,
    "comment_created_at": "2023-10-23T07:35:28Z"
  },
  {
    "code": "@@ -47,6 +47,7 @@ repos:\n           - flake8-broken-line\n           - flake8-comprehensions\n           - pep8-naming\n+          - flake8-docstrings",
    "comment": "please keep lists of dependencies in alphabetical order to make it easy to spot missing dependencies and difficult to add duplicates.",
    "line_number": 50,
    "enriched": "File: .pre-commit-config.yaml\nCode: @@ -47,6 +47,7 @@ repos:\n           - flake8-broken-line\n           - flake8-comprehensions\n           - pep8-naming\n+          - flake8-docstrings\nComment: Please keep lists of dependencies in alphabetical order to make it easy to spot missing dependencies and difficult to add duplicates.\r\n```suggestion\r\n          - flake8-comprehensions\r\n          - flake8-docstrings\r\n          - pep8-naming\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": ".pre-commit-config.yaml",
    "pr_number": 7962,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1014622798,
    "comment_created_at": "2022-11-05T11:25:25Z"
  },
  {
    "code": "@@ -3,7 +3,7 @@\n name: directory_writer\n on: [push]\n jobs:\n-  build:\n+  directory_writer:",
    "comment": "it is too confusing to have two github actions jobs named build, so rename this job to document what it really does.",
    "line_number": 6,
    "enriched": "File: .github/workflows/directory_writer.yml\nCode: @@ -3,7 +3,7 @@\n name: directory_writer\n on: [push]\n jobs:\n-  build:\n+  directory_writer:\nComment: It is too confusing to have two GitHub Actions jobs named `build`, so rename this job to document what it really does.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": ".github/workflows/directory_writer.yml",
    "pr_number": 12772,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2112460559,
    "comment_created_at": "2025-05-28T17:57:21Z"
  },
  {
    "code": "@@ -0,0 +1,121 @@\n+'''\n+Wiki Explanation: https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding\n+'''\n+class  node :\n+    def __init__(self) -> None:\n+        self.sym=''",
    "comment": "class names should follow the [camelcase](https://en.wikipedia.org/wiki/camel_case) naming convention. please update the following name accordingly: node",
    "line_number": 6,
    "enriched": "File: compression/shannon_fano.py\nCode: @@ -0,0 +1,121 @@\n+'''\n+Wiki Explanation: https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding\n+'''\n+class  node :\n+    def __init__(self) -> None:\n+        self.sym=''\nComment: Class names should follow the [`CamelCase`](https://en.wikipedia.org/wiki/Camel_case) naming convention. Please update the following name accordingly: `node`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "compression/shannon_fano.py",
    "pr_number": 7524,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002538297,
    "comment_created_at": "2022-10-22T17:19:49Z"
  },
  {
    "code": "@@ -7,6 +7,9 @@\n \"\"\"\n \n \n+from typing import List",
    "comment": "you can just use list to keep consistency with the file\r\nthis repository also targets at python 3.10+",
    "line_number": 10,
    "enriched": "File: matrix/spiral_print.py\nCode: @@ -7,6 +7,9 @@\n \"\"\"\n \n \n+from typing import List\nComment: ```suggestion\r\n```\r\nYou can just use `list` to keep consistency with the file\r\nThis repository also targets at python 3.10+",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "matrix/spiral_print.py",
    "pr_number": 7674,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1005745870,
    "comment_created_at": "2022-10-26T14:17:34Z"
  },
  {
    "code": "@@ -12,6 +12,40 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n     def _collision_resolution(self, key, data=None):\n+        \"\"\"\n+        Quadratic probing is an open addressing scheme used for resolving\n+        collisions in hash table.\n+\n+        It works by taking the original hash index and adding successive\n+        values of an arbitrary quadratic polynomial until open slot is found.\n+\n+        Hash + 1\u00b2, Hash + 2\u00b2, Hash + 3\u00b2 .... Hash + n\u00b2\n+\n+        e.g:\n+        1. Create hash table with size 7\n+        >>> qp = QuadraticProbing(7)\n+        >>> qp.insert_data(90)\n+        >>> qp.insert_data(340)\n+        >>> qp.insert_data(24)\n+        >>> qp.insert_data(45)\n+        >>> qp.insert_data(99)\n+        >>> qp.insert_data(73)\n+        >>> qp.insert_data(7)\n+        >>> qp.keys()\n+        {11: 45, 14: 99, 7: 24, 0: 340, 5: 73, 6: 90, 8: 7}\n+\n+        2. Create hash table with size 8\n+        >>> qp = QuadraticProbing(8)\n+        >>> qp.insert_data(0)\n+        >>> qp.insert_data(999)\n+        >>> qp.insert_data(111)\n+        >>> qp.keys()\n+        {0: 0, 7: 999, 3: 111}\n+\n+        reference:\n+            - https://en.wikipedia.org/wiki/Quadratic_probing",
    "comment": "let's put the url above the tests so visitors can look at the tests and implementation without any visual clutter.",
    "line_number": 46,
    "enriched": "File: data_structures/hashing/quadratic_probing.py\nCode: @@ -12,6 +12,40 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n     def _collision_resolution(self, key, data=None):\n+        \"\"\"\n+        Quadratic probing is an open addressing scheme used for resolving\n+        collisions in hash table.\n+\n+        It works by taking the original hash index and adding successive\n+        values of an arbitrary quadratic polynomial until open slot is found.\n+\n+        Hash + 1\u00b2, Hash + 2\u00b2, Hash + 3\u00b2 .... Hash + n\u00b2\n+\n+        e.g:\n+        1. Create hash table with size 7\n+        >>> qp = QuadraticProbing(7)\n+        >>> qp.insert_data(90)\n+        >>> qp.insert_data(340)\n+        >>> qp.insert_data(24)\n+        >>> qp.insert_data(45)\n+        >>> qp.insert_data(99)\n+        >>> qp.insert_data(73)\n+        >>> qp.insert_data(7)\n+        >>> qp.keys()\n+        {11: 45, 14: 99, 7: 24, 0: 340, 5: 73, 6: 90, 8: 7}\n+\n+        2. Create hash table with size 8\n+        >>> qp = QuadraticProbing(8)\n+        >>> qp.insert_data(0)\n+        >>> qp.insert_data(999)\n+        >>> qp.insert_data(111)\n+        >>> qp.keys()\n+        {0: 0, 7: 999, 3: 111}\n+\n+        reference:\n+            - https://en.wikipedia.org/wiki/Quadratic_probing\nComment: Let's put the URL above the tests so visitors can look at the tests and implementation without any visual clutter.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/hashing/quadratic_probing.py",
    "pr_number": 10996,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1373104628,
    "comment_created_at": "2023-10-26T12:40:10Z"
  },
  {
    "code": "@@ -115,6 +115,20 @@ def jacobi_iteration_method(\n \r\n     strictly_diagonally_dominant(table)\r\n \r\n+    \"\"\"\r\n+    denom - a list of values along the diagonal\r\n+    val - values of the last column of the table array\r\n+\r\n+    masks - boolean mask of all strings without diagonal\r\n+    elements array coefficient_matrix\r\n+\r\n+    ttt - coefficient_matrix array values without diagonal elements\r\n+    ind - column indexes for each row without diagonal elements\r\n+    arr - list obtained by column indexes from the list init_val\r",
    "comment": "there shouldn't be such a long comment in the middle of the algorithm. instead of writing a block comment explaining what each variable does, why not just make the variable names clearer? you could also just add comments to individual lines.",
    "line_number": 127,
    "enriched": "File: arithmetic_analysis/jacobi_iteration_method.py\nCode: @@ -115,6 +115,20 @@ def jacobi_iteration_method(\n \r\n     strictly_diagonally_dominant(table)\r\n \r\n+    \"\"\"\r\n+    denom - a list of values along the diagonal\r\n+    val - values of the last column of the table array\r\n+\r\n+    masks - boolean mask of all strings without diagonal\r\n+    elements array coefficient_matrix\r\n+\r\n+    ttt - coefficient_matrix array values without diagonal elements\r\n+    ind - column indexes for each row without diagonal elements\r\n+    arr - list obtained by column indexes from the list init_val\r\nComment: There shouldn't be such a long comment in the middle of the algorithm. Instead of writing a block comment explaining what each variable does, why not just make the variable names clearer? You could also just add comments to individual lines.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "arithmetic_analysis/jacobi_iteration_method.py",
    "pr_number": 8938,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1292730790,
    "comment_created_at": "2023-08-13T09:18:29Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+class Solution:\n+    def find_median_sorted_arrays(self, nums1: list[int], nums2: list[int]) -> float:",
    "comment": "the class has a name that is not self documenting and self is never used so make it a function, not a class.",
    "line_number": 2,
    "enriched": "File: data_structures/arrays/median_two_array.py\nCode: @@ -0,0 +1,48 @@\n+class Solution:\n+    def find_median_sorted_arrays(self, nums1: list[int], nums2: list[int]) -> float:\nComment: The class has a name that is not self documenting and `self` is never used so make it a function, not a class.\r\n\r\n```suggestion\r\ndef find_median_sorted_arrays(nums1: list[int], nums2: list[int]) -> float:\r\n```",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "data_structures/arrays/median_two_array.py",
    "pr_number": 9386,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342331367,
    "comment_created_at": "2023-10-02T06:53:34Z"
  },
  {
    "code": "@@ -90,6 +90,9 @@ def quantum_fourier_transform(number_of_qubits: int = 3) -> qiskit.result.counts\n \n \n if __name__ == \"__main__\":\n+    import doctest",
    "comment": "we need to add some of the new tests here, rather than adding a import statement!!",
    "line_number": 93,
    "enriched": "File: quantum/q_fourier_transform.py\nCode: @@ -90,6 +90,9 @@ def quantum_fourier_transform(number_of_qubits: int = 3) -> qiskit.result.counts\n \n \n if __name__ == \"__main__\":\n+    import doctest\nComment: We need to add some of the new tests here, rather than adding a import statement!!",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "quantum/q_fourier_transform.py",
    "pr_number": 10931,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1371143718,
    "comment_created_at": "2023-10-25T04:36:50Z"
  },
  {
    "code": "@@ -0,0 +1,120 @@\n+import pandas as pd\n+import math\n+import matplotlib.pyplot as plt\n+from typing import Dict, List\n+class dbscan:",
    "comment": "class names should follow the [camelcase](https://en.wikipedia.org/wiki/camel_case) naming convention. please update the following name accordingly: dbscan",
    "line_number": 5,
    "enriched": "File: machine_learning/dbscan.py\nCode: @@ -0,0 +1,120 @@\n+import pandas as pd\n+import math\n+import matplotlib.pyplot as plt\n+from typing import Dict, List\n+class dbscan:\nComment: Class names should follow the [`CamelCase`](https://en.wikipedia.org/wiki/Camel_case) naming convention. Please update the following name accordingly: `dbscan`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "machine_learning/dbscan.py",
    "pr_number": 11632,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1783079031,
    "comment_created_at": "2024-10-01T15:26:48Z"
  },
  {
    "code": "@@ -65,7 +68,7 @@ def get_forbes_real_time_billionaires() -> list[dict[str, int | str]]:\n             \"Country\": person[\"countryOfCitizenship\"],\n             \"Gender\": person[\"gender\"],\n             \"Worth ($)\": f\"{person['finalWorth'] / 1000:.1f} Billion\",\n-            \"Age\": years_old(person[\"birthDate\"]),\n+            \"Age\": str(years_old(person[\"birthDate\"] / 1000)),",
    "comment": "if this / 1000 is required then please move it into the years_old() function and ensure that the tests pass.",
    "line_number": 71,
    "enriched": "File: web_programming/get_top_billionaires.py\nCode: @@ -65,7 +68,7 @@ def get_forbes_real_time_billionaires() -> list[dict[str, int | str]]:\n             \"Country\": person[\"countryOfCitizenship\"],\n             \"Gender\": person[\"gender\"],\n             \"Worth ($)\": f\"{person['finalWorth'] / 1000:.1f} Billion\",\n-            \"Age\": years_old(person[\"birthDate\"]),\n+            \"Age\": str(years_old(person[\"birthDate\"] / 1000)),\nComment: If this `/ 1000` is required then please move it into the `years_old()` function and ensure that the tests pass.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "web_programming/get_top_billionaires.py",
    "pr_number": 11466,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1641989870,
    "comment_created_at": "2024-06-16T21:53:26Z"
  },
  {
    "code": "@@ -0,0 +1,196 @@\n+class resizable_array:",
    "comment": "class names should follow the [camelcase](https://en.wikipedia.org/wiki/camel_case) naming convention. please update the following name accordingly: resizable_array",
    "line_number": 1,
    "enriched": "File: other/Resizable_Array.py\nCode: @@ -0,0 +1,196 @@\n+class resizable_array:\nComment: Class names should follow the [`CamelCase`](https://en.wikipedia.org/wiki/Camel_case) naming convention. Please update the following name accordingly: `resizable_array`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "other/Resizable_Array.py",
    "pr_number": 7016,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 992698688,
    "comment_created_at": "2022-10-11T19:21:29Z"
  },
  {
    "code": "@@ -183,6 +182,16 @@ def angle(self, other: Vector, deg: bool = False) -> float:\n         else:\n             return math.acos(num / den)\n \n+    def __eq__(self, vector: object) -> bool:",
    "comment": "could you move the method higher up in the file, before all the public methods? having this one dunder method within all the public methods, rather than having it alongside all the other dunder methods, makes it harder to find.",
    "line_number": 185,
    "enriched": "File: linear_algebra/src/lib.py\nCode: @@ -183,6 +182,16 @@ def angle(self, other: Vector, deg: bool = False) -> float:\n         else:\n             return math.acos(num / den)\n \n+    def __eq__(self, vector: object) -> bool:\nComment: Could you move the method higher up in the file, before all the public methods? Having this one dunder method within all the public methods, rather than having it alongside all the other dunder methods, makes it harder to find.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "linear_algebra/src/lib.py",
    "pr_number": 12448,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1898881020,
    "comment_created_at": "2024-12-28T09:35:22Z"
  },
  {
    "code": "@@ -1,47 +1,103 @@\n \"\"\"\n Numerical integration or quadrature for a smooth function f with known values at x_i\n \n-This method is the classical approach of suming 'Equally Spaced Abscissas'\n+This method is the classical approach of summing 'Equally Spaced Abscissas'\n \n-method 1:\n-\"extended trapezoidal rule\"\n+Method 1:\n+\"Extended Trapezoidal Rule\"\n \n \"\"\"\n \n \n def method_1(boundary, steps):",
    "comment": "let's rename the function to something more descriptive. i'm not sure why the code calls it \"method 1\" when there's only one method in the file.",
    "line_number": 12,
    "enriched": "File: maths/trapezoidal_rule.py\nCode: @@ -1,47 +1,103 @@\n \"\"\"\n Numerical integration or quadrature for a smooth function f with known values at x_i\n \n-This method is the classical approach of suming 'Equally Spaced Abscissas'\n+This method is the classical approach of summing 'Equally Spaced Abscissas'\n \n-method 1:\n-\"extended trapezoidal rule\"\n+Method 1:\n+\"Extended Trapezoidal Rule\"\n \n \"\"\"\n \n \n def method_1(boundary, steps):\nComment: ```suggestion\r\ndef trapezoidal_rule(boundary, steps):\r\n```\r\nLet's rename the function to something more descriptive. I'm not sure why the code calls it \"method 1\" when there's only one method in the file.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/trapezoidal_rule.py",
    "pr_number": 11640,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1787486089,
    "comment_created_at": "2024-10-04T10:05:30Z"
  },
  {
    "code": "@@ -19,50 +21,49 @@ def mixed_keyword(key: str = \"college\", pt: str = \"UNIVERSITY\") -> str:\n      'Y': 'T', 'Z': 'Y'}\n     'XKJGUFMJST'\n     \"\"\"\n-    key = key.upper()\n-    pt = pt.upper()\n-    temp = []\n-    for i in key:\n-        if i not in temp:\n-            temp.append(i)\n-    len_temp = len(temp)\n-    # print(temp)\n-    alpha = []\n-    modalpha = []\n-    for j in range(65, 91):\n-        t = chr(j)\n-        alpha.append(t)\n-        if t not in temp:\n-            temp.append(t)\n-    # print(temp)\n-    r = int(26 / 4)\n-    # print(r)\n+    keyword = keyword.upper()\n+    plaintext = plaintext.upper()\n+\n+    unique_chars = []\n+    for char in keyword:\n+        if char not in unique_chars:\n+            unique_chars.append(char)",
    "comment": "please make this a set.\r\n* https://docs.python.org/3/library/stdtypes.html#set",
    "line_number": 30,
    "enriched": "File: ciphers/mixed_keyword_cypher.py\nCode: @@ -19,50 +21,49 @@ def mixed_keyword(key: str = \"college\", pt: str = \"UNIVERSITY\") -> str:\n      'Y': 'T', 'Z': 'Y'}\n     'XKJGUFMJST'\n     \"\"\"\n-    key = key.upper()\n-    pt = pt.upper()\n-    temp = []\n-    for i in key:\n-        if i not in temp:\n-            temp.append(i)\n-    len_temp = len(temp)\n-    # print(temp)\n-    alpha = []\n-    modalpha = []\n-    for j in range(65, 91):\n-        t = chr(j)\n-        alpha.append(t)\n-        if t not in temp:\n-            temp.append(t)\n-    # print(temp)\n-    r = int(26 / 4)\n-    # print(r)\n+    keyword = keyword.upper()\n+    plaintext = plaintext.upper()\n+\n+    unique_chars = []\n+    for char in keyword:\n+        if char not in unique_chars:\n+            unique_chars.append(char)\nComment: Please make this a `set`.\r\n* https://docs.python.org/3/library/stdtypes.html#set",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "ciphers/mixed_keyword_cypher.py",
    "pr_number": 8626,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1161226961,
    "comment_created_at": "2023-04-09T06:38:38Z"
  },
  {
    "code": "@@ -13,3 +13,7 @@ def arc_length(angle: int, radius: int) -> float:\n \n if __name__ == \"__main__\":\n     print(arc_length(90, 10))\n+    \"\"\"\n+    >>> arc_length(90, 10)\n+    15.7079632679",
    "comment": "this should be placed with the rest of the doctests, not separately in the __main__ block",
    "line_number": 18,
    "enriched": "File: maths/arc_length.py\nCode: @@ -13,3 +13,7 @@ def arc_length(angle: int, radius: int) -> float:\n \n if __name__ == \"__main__\":\n     print(arc_length(90, 10))\n+    \"\"\"\n+    >>> arc_length(90, 10)\n+    15.7079632679\nComment: This should be placed with the rest of the doctests, not separately in the `__main__` block",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "maths/arc_length.py",
    "pr_number": 8964,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1297755329,
    "comment_created_at": "2023-08-17T21:10:09Z"
  },
  {
    "code": "@@ -0,0 +1,78 @@\n+# Python3 code to find minimum steps to reach\n+# to specific cell in minimum moves by Knight\n+\n+\n+class cell:",
    "comment": "class names should follow the [camelcase](https://en.wikipedia.org/wiki/camel_case) naming convention. please update the following name accordingly: cell",
    "line_number": 5,
    "enriched": "File: graphs/steps_by_knight.py\nCode: @@ -0,0 +1,78 @@\n+# Python3 code to find minimum steps to reach\n+# to specific cell in minimum moves by Knight\n+\n+\n+class cell:\nComment: Class names should follow the [`CamelCase`](https://en.wikipedia.org/wiki/Camel_case) naming convention. Please update the following name accordingly: `cell`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "graphs/steps_by_knight.py",
    "pr_number": 10340,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1357329780,
    "comment_created_at": "2023-10-12T19:49:23Z"
  },
  {
    "code": "@@ -0,0 +1,253 @@\n+\"\"\"\n+Title: Simplified Implementation of Artificial Neural Networks\n+\n+Description: This Python code offers a straightforward approach to utilizing Artificial Neural Networks for tasks involving both classification and regression. For more information on multiclass classification, you can refer to [this Wikipedia page](https://en.wikipedia.org/wiki/Multiclass_classification).\n+\n+\"\"\"\n+import tensorflow as tf\n+import pandas as pd\n+import numpy as np\n+import matplotlib.pyplot as plt\n+from tensorflow import keras\n+\n+class ANN_classifier:",
    "comment": "class names should follow the [camelcase](https://en.wikipedia.org/wiki/camel_case) naming convention. please update the following name accordingly: ann_classifier",
    "line_number": 13,
    "enriched": "File: neural_network/artificial_neural_networks.py\nCode: @@ -0,0 +1,253 @@\n+\"\"\"\n+Title: Simplified Implementation of Artificial Neural Networks\n+\n+Description: This Python code offers a straightforward approach to utilizing Artificial Neural Networks for tasks involving both classification and regression. For more information on multiclass classification, you can refer to [this Wikipedia page](https://en.wikipedia.org/wiki/Multiclass_classification).\n+\n+\"\"\"\n+import tensorflow as tf\n+import pandas as pd\n+import numpy as np\n+import matplotlib.pyplot as plt\n+from tensorflow import keras\n+\n+class ANN_classifier:\nComment: Class names should follow the [`CamelCase`](https://en.wikipedia.org/wiki/Camel_case) naming convention. Please update the following name accordingly: `ANN_classifier`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "neural_network/artificial_neural_networks.py",
    "pr_number": 9273,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342119890,
    "comment_created_at": "2023-10-01T11:18:47Z"
  },
  {
    "code": "@@ -60,3 +60,34 @@ def quick_select(items: list, index: int):\n     # must be in larger\n     else:\n         return quick_select(larger, index - (m + count))\n+\n+\n+def median(data: list):",
    "comment": "since this is an application of the quick_select algorithm, can you move it to its own file?",
    "line_number": 65,
    "enriched": "File: searches/quick_select.py\nCode: @@ -60,3 +60,34 @@ def quick_select(items: list, index: int):\n     # must be in larger\n     else:\n         return quick_select(larger, index - (m + count))\n+\n+\n+def median(data: list):\nComment: Since this is an application of the quick_select algorithm, can you move it to its own file?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "searches/quick_select.py",
    "pr_number": 12676,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2083925716,
    "comment_created_at": "2025-05-12T06:38:43Z"
  },
  {
    "code": "@@ -0,0 +1,269 @@\n+\"\"\"\n+Generative Adversarial Network\n+\n+Objective : To train a GAN model to generate handwritten digits that can be transferred to other domains.\n+\n+Resources GAN Theory :\n+    https://en.wikipedia.org/wiki/Generative_adversarial_network\n+Resources PyTorch: https://pytorch.org/\n+\n+Download dataset from :\n+PyTorch internal function\n+\n+1. Fetch the Dataset with PyTorch function.\n+2. Create Dataloader.\n+3. Create Discriminator and Generator.\n+4. Set the hyperparameters and models.\n+5. Set the loss functions.\n+6. Create the training loop.\n+7. Visualize the losses.\n+8. Visualize the result from GAN.\n+\n+\"\"\"\n+\n+import numpy as np\n+import torch\n+import matplotlib.pyplot as plt\n+from torchvision import datasets\n+import torchvision.transforms as transforms\n+\n+# number of subprocesses to use for data loading\n+num_workers = 0\n+# how many samples per batch to load\n+batch_size = 64\n+\n+# convert data to torch.FloatTensor\n+transform = transforms.ToTensor()\n+\n+# get the training datasets\n+train_data = datasets.MNIST(root='data', train=True,\n+                                   download=True, transform=transform)\n+\n+# prepare data loader\n+train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n+                                           num_workers=num_workers)\n+\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+# Creating Generator and Discriminator for GAN\n+\n+class discriminator(nn.Module):",
    "comment": "class names should follow the [camelcase](https://en.wikipedia.org/wiki/camel_case) naming convention. please update the following name accordingly: discriminator",
    "line_number": 51,
    "enriched": "File: computer_vision/Generative_Adversarial_Network_MNIST.py\nCode: @@ -0,0 +1,269 @@\n+\"\"\"\n+Generative Adversarial Network\n+\n+Objective : To train a GAN model to generate handwritten digits that can be transferred to other domains.\n+\n+Resources GAN Theory :\n+    https://en.wikipedia.org/wiki/Generative_adversarial_network\n+Resources PyTorch: https://pytorch.org/\n+\n+Download dataset from :\n+PyTorch internal function\n+\n+1. Fetch the Dataset with PyTorch function.\n+2. Create Dataloader.\n+3. Create Discriminator and Generator.\n+4. Set the hyperparameters and models.\n+5. Set the loss functions.\n+6. Create the training loop.\n+7. Visualize the losses.\n+8. Visualize the result from GAN.\n+\n+\"\"\"\n+\n+import numpy as np\n+import torch\n+import matplotlib.pyplot as plt\n+from torchvision import datasets\n+import torchvision.transforms as transforms\n+\n+# number of subprocesses to use for data loading\n+num_workers = 0\n+# how many samples per batch to load\n+batch_size = 64\n+\n+# convert data to torch.FloatTensor\n+transform = transforms.ToTensor()\n+\n+# get the training datasets\n+train_data = datasets.MNIST(root='data', train=True,\n+                                   download=True, transform=transform)\n+\n+# prepare data loader\n+train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n+                                           num_workers=num_workers)\n+\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+# Creating Generator and Discriminator for GAN\n+\n+class discriminator(nn.Module):\nComment: Class names should follow the [`CamelCase`](https://en.wikipedia.org/wiki/Camel_case) naming convention. Please update the following name accordingly: `discriminator`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "computer_vision/Generative_Adversarial_Network_MNIST.py",
    "pr_number": 11961,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1796179045,
    "comment_created_at": "2024-10-10T22:47:58Z"
  },
  {
    "code": "@@ -0,0 +1,75 @@\n+\"\"\"\r\n+Sum of all nodes in a binary tree.\r\n+\r\n+Python implementation:\r\n+    O(n) time complexity - Recurses through :meth:`depth_first_search`\r\n+                            with each element.\r\n+    O(n) space complexity - At any point in time maximum number of stack\r\n+                            frames that could be in memory is `n`\r\n+\"\"\"\r\n+\r\n+\r\n+from __future__ import annotations\r\n+\r\n+\r\n+class Node:\r\n+    \"\"\"\r\n+    A Node has value variable and pointers\r\n+    to Nodes to its left and right.\r",
    "comment": "use the 88 chars.  ;-)",
    "line_number": 18,
    "enriched": "File: data_structures/binary_tree/binary_tree_node_sum.py\nCode: @@ -0,0 +1,75 @@\n+\"\"\"\r\n+Sum of all nodes in a binary tree.\r\n+\r\n+Python implementation:\r\n+    O(n) time complexity - Recurses through :meth:`depth_first_search`\r\n+                            with each element.\r\n+    O(n) space complexity - At any point in time maximum number of stack\r\n+                            frames that could be in memory is `n`\r\n+\"\"\"\r\n+\r\n+\r\n+from __future__ import annotations\r\n+\r\n+\r\n+class Node:\r\n+    \"\"\"\r\n+    A Node has value variable and pointers\r\n+    to Nodes to its left and right.\r\nComment: Use the 88 chars.  ;-)\r\n```suggestion\r\n    A Node has a value variable and pointers to Nodes to its left and right.\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "data_structures/binary_tree/binary_tree_node_sum.py",
    "pr_number": 7162,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 995912890,
    "comment_created_at": "2022-10-14T15:59:13Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+# https://leetcode.com/problems/monotonic-array/\n+def is_monotonic(nums: list[int]) -> bool:\n+    \"\"\"\n+    Check if a list is monotonic.\n+\n+    >>> is_monotonic([1, 2, 2, 3])\n+    True\n+    >>> is_monotonic([6, 5, 4, 4])\n+    True\n+    >>> is_monotonic([1, 3, 2])\n+    False\n+    \"\"\"\n+    return all(nums[i] <= nums[i + 1] for i in range(len(nums) - 1)) or all(\n+        nums[i] >= nums[i + 1] for i in range(len(nums) - 1)\n+    )\n+\n+\n+# Test the function with your examples\n+print(is_monotonic([1, 2, 2, 3]))  # Output: True\n+print(is_monotonic([6, 5, 4, 4]))  # Output: True\n+print(is_monotonic([1, 3, 2]))  # Output: False",
    "comment": "please encapsulate all driver code within an if  __name__ == \"__main__\" block",
    "line_number": 21,
    "enriched": "File: data_structures/arrays/monotonic_array.py\nCode: @@ -0,0 +1,21 @@\n+# https://leetcode.com/problems/monotonic-array/\n+def is_monotonic(nums: list[int]) -> bool:\n+    \"\"\"\n+    Check if a list is monotonic.\n+\n+    >>> is_monotonic([1, 2, 2, 3])\n+    True\n+    >>> is_monotonic([6, 5, 4, 4])\n+    True\n+    >>> is_monotonic([1, 3, 2])\n+    False\n+    \"\"\"\n+    return all(nums[i] <= nums[i + 1] for i in range(len(nums) - 1)) or all(\n+        nums[i] >= nums[i + 1] for i in range(len(nums) - 1)\n+    )\n+\n+\n+# Test the function with your examples\n+print(is_monotonic([1, 2, 2, 3]))  # Output: True\n+print(is_monotonic([6, 5, 4, 4]))  # Output: True\n+print(is_monotonic([1, 3, 2]))  # Output: False\nComment: Please encapsulate all driver code within an `if  __name__ == \"__main__\"` block",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "data_structures/arrays/monotonic_array.py",
    "pr_number": 11025,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1374696497,
    "comment_created_at": "2023-10-27T14:54:08Z"
  },
  {
    "code": "@@ -0,0 +1,71 @@\n+def rgb_to_cmyk(r_input, g_input, b_input: int) -> tuple[int, int, int, int]:\r",
    "comment": "please provide type hint for the parameter: r_input\n\nplease provide type hint for the parameter: g_input",
    "line_number": 1,
    "enriched": "File: conversions/rgb_cmyk_conversion.py\nCode: @@ -0,0 +1,71 @@\n+def rgb_to_cmyk(r_input, g_input, b_input: int) -> tuple[int, int, int, int]:\r\nComment: Please provide type hint for the parameter: `r_input`\n\nPlease provide type hint for the parameter: `g_input`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "conversions/rgb_cmyk_conversion.py",
    "pr_number": 10741,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367581099,
    "comment_created_at": "2023-10-21T01:09:02Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+\"\"\"\n+This is pure Python implementation of DateTime\n+\n+For doctests run following command:\n+\n+python3 validate_datetime.py\n+\n+\"\"\"\n+\n+\n+import datetime\n+import doctest\n+import os\n+\n+\n+def parseOptions():\n+\n+    import optparse\n+\n+    parser = optparse.OptionParser(usage=\"-h\")\n+    parser.add_option(\"-d\", \"--difference\", type=\"int\")\n+    (options, args) = parser.parse_args()\n+    return options\n+\n+\n+now = datetime.datetime.now()\n+\n+\n+def subtime(a, b):\n+    \"\"\"(datetime,int) -> datetime\n+    Subtract b hours from a datetime.datetime and return the new datetime object\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),10)\n+    datetime.datetime(2013, 11, 11, 1, 0)\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),24)\n+    datetime.datetime(2013, 11, 10, 11, 0)\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),0)\n+    datetime.datetime(2013, 11, 11, 11, 0)\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),-5)\n+    datetime.datetime(2013, 11, 11, 16, 0)\n+\n+    \"\"\"\n+    subtract = datetime.timedelta(hours=b)\n+    difference = a - subtract\n+    return difference\n+\n+\n+if __name__ == \"__main__\":\n+    doctest.testmod()\n+\n+print",
    "comment": "this does nothing",
    "line_number": 54,
    "enriched": "File: scripts/validate_datetime.py\nCode: @@ -0,0 +1,61 @@\n+\"\"\"\n+This is pure Python implementation of DateTime\n+\n+For doctests run following command:\n+\n+python3 validate_datetime.py\n+\n+\"\"\"\n+\n+\n+import datetime\n+import doctest\n+import os\n+\n+\n+def parseOptions():\n+\n+    import optparse\n+\n+    parser = optparse.OptionParser(usage=\"-h\")\n+    parser.add_option(\"-d\", \"--difference\", type=\"int\")\n+    (options, args) = parser.parse_args()\n+    return options\n+\n+\n+now = datetime.datetime.now()\n+\n+\n+def subtime(a, b):\n+    \"\"\"(datetime,int) -> datetime\n+    Subtract b hours from a datetime.datetime and return the new datetime object\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),10)\n+    datetime.datetime(2013, 11, 11, 1, 0)\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),24)\n+    datetime.datetime(2013, 11, 10, 11, 0)\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),0)\n+    datetime.datetime(2013, 11, 11, 11, 0)\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),-5)\n+    datetime.datetime(2013, 11, 11, 16, 0)\n+\n+    \"\"\"\n+    subtract = datetime.timedelta(hours=b)\n+    difference = a - subtract\n+    return difference\n+\n+\n+if __name__ == \"__main__\":\n+    doctest.testmod()\n+\n+print\nComment: ```suggestion\r\n```\r\nThis does nothing",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "scripts/validate_datetime.py",
    "pr_number": 7230,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996326726,
    "comment_created_at": "2022-10-15T16:27:39Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+import numpy as np\n+\n+class GeneticAlgorithm:\n+    def __init__(self, func, bounds, pop_size=20, generations=100, mutation_rate=0.1, crossover_rate=0.8, selection_method='tournament'):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: func\n\nplease provide type hint for the parameter: bounds\n\nplease provide type hint for the parameter: pop_size\n\nplease provide type hint for the parameter: generations\n\nplease provide type hint for the parameter: mutation_rate\n\nplease provide type hint for the parameter: crossover_rate\n\nplease provide type hint for the parameter: selection_method",
    "line_number": 4,
    "enriched": "File: genetic_algorithm/ga_optimisation.py\nCode: @@ -0,0 +1,94 @@\n+import numpy as np\n+\n+class GeneticAlgorithm:\n+    def __init__(self, func, bounds, pop_size=20, generations=100, mutation_rate=0.1, crossover_rate=0.8, selection_method='tournament'):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `func`\n\nPlease provide type hint for the parameter: `bounds`\n\nPlease provide type hint for the parameter: `pop_size`\n\nPlease provide type hint for the parameter: `generations`\n\nPlease provide type hint for the parameter: `mutation_rate`\n\nPlease provide type hint for the parameter: `crossover_rate`\n\nPlease provide type hint for the parameter: `selection_method`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "genetic_algorithm/ga_optimisation.py",
    "pr_number": 12039,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1798744224,
    "comment_created_at": "2024-10-14T04:50:08Z"
  },
  {
    "code": "@@ -0,0 +1,49 @@\n+\"\"\"\n+Financial ratios are quantitative metrics used to analyze and assess the relationships\n+between different financial elements in a company's financial statements, providing\n+insights into its performance, profitability, liquidity, and overall financial health.\n+\n+Reference: https://en.wikipedia.org/wiki/Current_ratio\n+The Current Ratio is a liquidity ratio that measures whether a firm has enough\n+resources to meet its short-term obligations.\n+\n+Reference: https://en.wikipedia.org/wiki/Quick_ratio\n+The quick ratio, also known as the acid-test ratio, is a financial ratio that measures\n+a company's ability to cover its short-term liabilities with its most liquid assets,\n+excluding inventory. It is calculated by dividing the sum of cash, marketable\n+securities, and accounts receivable by the total current liabilities. The quick ratio\n+provides a more conservative assessment of a company's liquidity compared to the\n+current ratio, as it excludes inventory, which may not be as readily convertible\n+to cash in the short term. A higher quick ratio indicates a stronger\n+ability to meet short-term obligations.\n+\"\"\"\n+\n+\n+def current_ratio(current_assets: int, current_liabilities) -> float:",
    "comment": "please provide type hint for the parameter: current_liabilities",
    "line_number": 22,
    "enriched": "File: financial/ratios.py\nCode: @@ -0,0 +1,49 @@\n+\"\"\"\n+Financial ratios are quantitative metrics used to analyze and assess the relationships\n+between different financial elements in a company's financial statements, providing\n+insights into its performance, profitability, liquidity, and overall financial health.\n+\n+Reference: https://en.wikipedia.org/wiki/Current_ratio\n+The Current Ratio is a liquidity ratio that measures whether a firm has enough\n+resources to meet its short-term obligations.\n+\n+Reference: https://en.wikipedia.org/wiki/Quick_ratio\n+The quick ratio, also known as the acid-test ratio, is a financial ratio that measures\n+a company's ability to cover its short-term liabilities with its most liquid assets,\n+excluding inventory. It is calculated by dividing the sum of cash, marketable\n+securities, and accounts receivable by the total current liabilities. The quick ratio\n+provides a more conservative assessment of a company's liquidity compared to the\n+current ratio, as it excludes inventory, which may not be as readily convertible\n+to cash in the short term. A higher quick ratio indicates a stronger\n+ability to meet short-term obligations.\n+\"\"\"\n+\n+\n+def current_ratio(current_assets: int, current_liabilities) -> float:\nComment: Please provide type hint for the parameter: `current_liabilities`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "financial/ratios.py",
    "pr_number": 8977,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1296626546,
    "comment_created_at": "2023-08-17T03:13:24Z"
  },
  {
    "code": "@@ -0,0 +1,26 @@\n+import hashlib\n+\n+class PeyxwBlock:\n+    \n+    def __init__(self, previous_block_hash, transaction_list):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: previous_block_hash\n\nplease provide type hint for the parameter: transaction_list",
    "line_number": 5,
    "enriched": "File: blockchain/simple_blockchain_algorithm.py\nCode: @@ -0,0 +1,26 @@\n+import hashlib\n+\n+class PeyxwBlock:\n+    \n+    def __init__(self, previous_block_hash, transaction_list):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `previous_block_hash`\n\nPlease provide type hint for the parameter: `transaction_list`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "blockchain/simple_blockchain_algorithm.py",
    "pr_number": 8671,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1171457619,
    "comment_created_at": "2023-04-19T14:47:08Z"
  },
  {
    "code": "@@ -0,0 +1,98 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval=None) -> None:",
    "comment": "please provide type hint for the parameter: initval",
    "line_number": 21,
    "enriched": "File: sorts/tree_sort_2.py\nCode: @@ -0,0 +1,98 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval=None) -> None:\nComment: Please provide type hint for the parameter: `initval`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "sorts/tree_sort_2.py",
    "pr_number": 7459,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000783884,
    "comment_created_at": "2022-10-20T15:23:06Z"
  },
  {
    "code": "@@ -0,0 +1,29 @@\n+rect1 = {\n+    \"x\": 10,\n+    \"y\": 10,\n+    \"height\": 30,\n+    \"width\": 50\n+}\n+\n+rect2 = {\n+    \"x\": 20,\n+    \"y\": 30,\n+    \"height\": 40,\n+    \"width\": 30\n+}\n+\n+def check_collision(rect1, rect2) -> bool:",
    "comment": "please provide type hint for the parameter: rect1\n\nplease provide type hint for the parameter: rect2",
    "line_number": 15,
    "enriched": "File: maths/collision_between_rectangles.py\nCode: @@ -0,0 +1,29 @@\n+rect1 = {\n+    \"x\": 10,\n+    \"y\": 10,\n+    \"height\": 30,\n+    \"width\": 50\n+}\n+\n+rect2 = {\n+    \"x\": 20,\n+    \"y\": 30,\n+    \"height\": 40,\n+    \"width\": 30\n+}\n+\n+def check_collision(rect1, rect2) -> bool:\nComment: Please provide type hint for the parameter: `rect1`\n\nPlease provide type hint for the parameter: `rect2`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "maths/collision_between_rectangles.py",
    "pr_number": 7831,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008653854,
    "comment_created_at": "2022-10-29T07:34:22Z"
  },
  {
    "code": "@@ -11,29 +11,40 @@\n synchronization could be used.\n \"\"\"\n from multiprocessing import Lock, Pipe, Process\n+from traceback import format_exc\n \n # lock used to ensure that two processes do not access a pipe at the same time\n process_lock = Lock()\n \n \"\"\"\n The function run by the processes that sorts the list\n \n-position = the position in the list the process represents, used to know which\n-            neighbor we pass our value to\n-value = the initial value at list[position]\n+\"position\" ----> the position in the list the process represents, used to know which\n+neighbor we pass our value to\n+\"value\" ----> the initial value at list[position]\n LSend, RSend = the pipes we use to send to our left and right neighbors\n LRcv, RRcv = the pipes we use to receive from our left and right neighbors\n resultPipe = the pipe used to send results back to main\n+arr_len = length of the list to be sorted\n \"\"\"\n \n \n-def oe_process(position, value, l_send, r_send, lr_cv, rr_cv, result_pipe):\n+def oe_process(\n+    position,\n+    value,\n+    l_send,\n+    r_send,\n+    lr_cv,\n+    rr_cv,\n+    result_pipe,",
    "comment": "please add type hints.",
    "line_number": 39,
    "enriched": "File: sorts/odd_even_transposition_parallel.py\nCode: @@ -11,29 +11,40 @@\n synchronization could be used.\n \"\"\"\n from multiprocessing import Lock, Pipe, Process\n+from traceback import format_exc\n \n # lock used to ensure that two processes do not access a pipe at the same time\n process_lock = Lock()\n \n \"\"\"\n The function run by the processes that sorts the list\n \n-position = the position in the list the process represents, used to know which\n-            neighbor we pass our value to\n-value = the initial value at list[position]\n+\"position\" ----> the position in the list the process represents, used to know which\n+neighbor we pass our value to\n+\"value\" ----> the initial value at list[position]\n LSend, RSend = the pipes we use to send to our left and right neighbors\n LRcv, RRcv = the pipes we use to receive from our left and right neighbors\n resultPipe = the pipe used to send results back to main\n+arr_len = length of the list to be sorted\n \"\"\"\n \n \n-def oe_process(position, value, l_send, r_send, lr_cv, rr_cv, result_pipe):\n+def oe_process(\n+    position,\n+    value,\n+    l_send,\n+    r_send,\n+    lr_cv,\n+    rr_cv,\n+    result_pipe,\nComment: Please add type hints.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "sorts/odd_even_transposition_parallel.py",
    "pr_number": 10976,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375062087,
    "comment_created_at": "2023-10-27T21:34:36Z"
  },
  {
    "code": "@@ -0,0 +1,150 @@\n+from selenium import webdriver\n+from selenium.webdriver.common.by import By\n+\n+\n+class BankDetails:\n+    def __init__(self, ifsc_code) -> None:",
    "comment": "please provide type hint for the parameter: ifsc_code",
    "line_number": 6,
    "enriched": "File: web_programming/indian_bank_info.py\nCode: @@ -0,0 +1,150 @@\n+from selenium import webdriver\n+from selenium.webdriver.common.by import By\n+\n+\n+class BankDetails:\n+    def __init__(self, ifsc_code) -> None:\nComment: Please provide type hint for the parameter: `ifsc_code`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "web_programming/indian_bank_info.py",
    "pr_number": 10454,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359479543,
    "comment_created_at": "2023-10-14T16:43:38Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+\"\"\"\n+Title : Calculating the speed of sound\n+\n+Description :\n+    The speed of sound (c) is the speed that a sound wave travels\n+    per unit time (m/s). During propagation, the sound wave propagates\n+    through an elastic medium. Its SI unit is meter per second (m/s).\n+\n+    Only longitudinal waves can propagate in liquids and gas other then\n+    solid where they also travel in  transverse wave. The following Algo-\n+    rithem calculates the speed of sound in fluid depanding on the bulk\n+    module and the density of the fluid.\n+\n+    Equation for calculating speed od sound in fluid:\n+    c_fluid = (K_s*p)**0.5\n+\n+    c_fluid: speed of sound in fluid\n+    K_s: isentropic bulk modulus\n+    p: density of fluid\n+\n+\n+\n+Source : https://en.wikipedia.org/wiki/Speed_of_sound\n+\"\"\"\n+\n+\n+class SpeedOfSound:\n+    @staticmethod\n+    def fluid(fluid_density: float, bulk_modulus: float) -> float:",
    "comment": "we don't really use the class so it seems like unnecessary overhead.",
    "line_number": 29,
    "enriched": "File: physics/speed_of_sound.py\nCode: @@ -0,0 +1,56 @@\n+\"\"\"\n+Title : Calculating the speed of sound\n+\n+Description :\n+    The speed of sound (c) is the speed that a sound wave travels\n+    per unit time (m/s). During propagation, the sound wave propagates\n+    through an elastic medium. Its SI unit is meter per second (m/s).\n+\n+    Only longitudinal waves can propagate in liquids and gas other then\n+    solid where they also travel in  transverse wave. The following Algo-\n+    rithem calculates the speed of sound in fluid depanding on the bulk\n+    module and the density of the fluid.\n+\n+    Equation for calculating speed od sound in fluid:\n+    c_fluid = (K_s*p)**0.5\n+\n+    c_fluid: speed of sound in fluid\n+    K_s: isentropic bulk modulus\n+    p: density of fluid\n+\n+\n+\n+Source : https://en.wikipedia.org/wiki/Speed_of_sound\n+\"\"\"\n+\n+\n+class SpeedOfSound:\n+    @staticmethod\n+    def fluid(fluid_density: float, bulk_modulus: float) -> float:\nComment: We don't really use the class so it seems like unnecessary overhead.\r\n```suggestion\r\ndef speed_of_sound_in_a_fluid(density: float, bulk_modulus: float) -> float:\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "physics/speed_of_sound.py",
    "pr_number": 8803,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1222041184,
    "comment_created_at": "2023-06-07T19:01:28Z"
  },
  {
    "code": "@@ -0,0 +1,26 @@\n+# Python program to draw\n+# Rainbow Benzene\n+# using Turtle Programming\n+import turtle\n+\n+pen = turtle.Turtle()\n+pen.speed(0)\n+# it will make a 3D look like circle\n+pen.fillcolor(\"white\")\n+pen.begin_fill()\n+\n+# Radius of circle\n+pen.circle(100)\n+pen.end_fill()\n+pen.hideturtle()\n+\n+# for the colour of rainbow\n+colors = [\"red\", \"purple\", \"blue\", \"green\", \"orange\", \"yellow\"]\n+# to generate the benzene\n+t = turtle.Pen()\n+turtle.bgcolor(\"black\")",
    "comment": "i don't think you need this line...",
    "line_number": 21,
    "enriched": "File: graphics/rainbow_benzene.py\nCode: @@ -0,0 +1,26 @@\n+# Python program to draw\n+# Rainbow Benzene\n+# using Turtle Programming\n+import turtle\n+\n+pen = turtle.Turtle()\n+pen.speed(0)\n+# it will make a 3D look like circle\n+pen.fillcolor(\"white\")\n+pen.begin_fill()\n+\n+# Radius of circle\n+pen.circle(100)\n+pen.end_fill()\n+pen.hideturtle()\n+\n+# for the colour of rainbow\n+colors = [\"red\", \"purple\", \"blue\", \"green\", \"orange\", \"yellow\"]\n+# to generate the benzene\n+t = turtle.Pen()\n+turtle.bgcolor(\"black\")\nComment: I don't think you need this line...",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "graphics/rainbow_benzene.py",
    "pr_number": 10286,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1357672919,
    "comment_created_at": "2023-10-13T02:08:27Z"
  },
  {
    "code": "@@ -1,48 +1,148 @@\n-\"\"\"Binary Exponentiation.\"\"\"\n+\"\"\"\n+Binary Exponentiation\n \n-# Author : Junth Basnet\n-# Time Complexity : O(logn)\n+This is a method to find a^b in O(log b) time complexity and is one of the most commonly\n+used methods of exponentiation. The method is also useful for modular exponentiation,\n+when the solution to (a^b) % c is required.\n \n+To calculate a^b:\n+- If b is even, then a b = (a * a)^(b / 2)\n+- If b is odd, then a^b = a * a^(b - 1)\n+Repeat until b = 1 or b = 0\n \n-def binary_exponentiation(a: int, n: int) -> int:\n+For modular exponentiation, we use the fact that (a * b) % c = ((a % c) * (b % c)) % c\n+\"\"\"\n+\n+\n+def binary_exp_recursive(base: int, exponent: int) -> int:\n     \"\"\"\n-    Compute a number raised by some quantity",
    "comment": "why remove this?  exp is a bit cryptic in the function name and a line of documentation here is useful.",
    "line_number": 9,
    "enriched": "File: maths/binary_exponentiation.py\nCode: @@ -1,48 +1,148 @@\n-\"\"\"Binary Exponentiation.\"\"\"\n+\"\"\"\n+Binary Exponentiation\n \n-# Author : Junth Basnet\n-# Time Complexity : O(logn)\n+This is a method to find a^b in O(log b) time complexity and is one of the most commonly\n+used methods of exponentiation. The method is also useful for modular exponentiation,\n+when the solution to (a^b) % c is required.\n \n+To calculate a^b:\n+- If b is even, then a b = (a * a)^(b / 2)\n+- If b is odd, then a^b = a * a^(b - 1)\n+Repeat until b = 1 or b = 0\n \n-def binary_exponentiation(a: int, n: int) -> int:\n+For modular exponentiation, we use the fact that (a * b) % c = ((a % c) * (b % c)) % c\n+\"\"\"\n+\n+\n+def binary_exp_recursive(base: int, exponent: int) -> int:\n     \"\"\"\n-    Compute a number raised by some quantity\nComment: Why remove this?  `exp` is a bit cryptic in the function name and a line of documentation here is useful.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "maths/binary_exponentiation.py",
    "pr_number": 10742,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367665971,
    "comment_created_at": "2023-10-21T05:17:19Z"
  },
  {
    "code": "@@ -4,7 +4,22 @@\n \r\n \r\n def sieve(n: int) -> list[int]:\r\n-    \"\"\"Segmented Sieve.\"\"\"\r\n+    \"\"\"\r\n+    Segmented Sieve.\r\n+\r\n+    Examples:\r\n+    >>> sieve(2**3)\r\n+    [2, 3, 5, 7]\r\n+\r\n+    >>> sieve(3**3)\r\n+    [2, 3, 5, 7, 11, 13, 17, 19, 23]\r\n+\r\n+    >>> sieve()\r\n+    Traceback (most recent call last):\r\n+        ...\r\n+    TypeError: sieve() missing 1 required positional argument: 'n'\r\n+\r",
    "comment": "i think it's fine to assume that the function will be called on the right number of inputs, so i don't think this test is necessary",
    "line_number": 21,
    "enriched": "File: maths/segmented_sieve.py\nCode: @@ -4,7 +4,22 @@\n \r\n \r\n def sieve(n: int) -> list[int]:\r\n-    \"\"\"Segmented Sieve.\"\"\"\r\n+    \"\"\"\r\n+    Segmented Sieve.\r\n+\r\n+    Examples:\r\n+    >>> sieve(2**3)\r\n+    [2, 3, 5, 7]\r\n+\r\n+    >>> sieve(3**3)\r\n+    [2, 3, 5, 7, 11, 13, 17, 19, 23]\r\n+\r\n+    >>> sieve()\r\n+    Traceback (most recent call last):\r\n+        ...\r\n+    TypeError: sieve() missing 1 required positional argument: 'n'\r\n+\r\nComment: ```suggestion\r\n```\r\nI think it's fine to assume that the function will be called on the right number of inputs, so I don't think this test is necessary",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/segmented_sieve.py",
    "pr_number": 9945,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349307798,
    "comment_created_at": "2023-10-06T20:45:47Z"
  },
  {
    "code": "@@ -4,7 +4,17 @@\n class Onepad:\n     @staticmethod\n     def encrypt(text: str) -> tuple[list[int], list[int]]:\n-        \"\"\"Function to encrypt text using pseudo-random numbers\"\"\"\n+        \"\"\"\n+        Function to encrypt text using pseudo-random numbers\n+        >>> Onepad().encrypt(\"\")\n+        ([], [])\n+        >>> random.seed(1)\n+        >>> Onepad().encrypt(\" \")\n+        ([6969], [69])\n+        >>> random.seed(1)\n+        >>> Onepad().encrypt(\"Hello\")\n+        ([9729, 114756, 4653, 31309, 10492], [69, 292, 33, 131, 61])",
    "comment": "let's make it raise some exceptions:",
    "line_number": 16,
    "enriched": "File: ciphers/onepad_cipher.py\nCode: @@ -4,7 +4,17 @@\n class Onepad:\n     @staticmethod\n     def encrypt(text: str) -> tuple[list[int], list[int]]:\n-        \"\"\"Function to encrypt text using pseudo-random numbers\"\"\"\n+        \"\"\"\n+        Function to encrypt text using pseudo-random numbers\n+        >>> Onepad().encrypt(\"\")\n+        ([], [])\n+        >>> random.seed(1)\n+        >>> Onepad().encrypt(\" \")\n+        ([6969], [69])\n+        >>> random.seed(1)\n+        >>> Onepad().encrypt(\"Hello\")\n+        ([9729, 114756, 4653, 31309, 10492], [69, 292, 33, 131, 61])\nComment: Let's make it raise some Exceptions:\r\n```suggestion\r\n        ([9729, 114756, 4653, 31309, 10492], [69, 292, 33, 131, 61])\r\n        >>> Onepad().encrypt(1)\r\n        >>> Onepad().encrypt(1.1)\r\n        >>> Onepad().encrypt([])\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "ciphers/onepad_cipher.py",
    "pr_number": 10740,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367667224,
    "comment_created_at": "2023-10-21T05:28:57Z"
  },
  {
    "code": "@@ -21,6 +21,29 @@ def __init__(\n         self._keys: dict = {}\n \n     def keys(self):",
    "comment": "i don't love this name .keys() because {0: 5, 4: 4, 3: 3, 2: 2, 1: 1}.keys() returns dict_keys([0, 4, 3, 2, 1]) which is a list, not a dict.  but let's leave those considerations for a separate pull request.\r\n* [https://docs.python.org/3/library/stdtypes.html#object.__dict__](https://docs.python.org/3/library/stdtypes.html#object.__dict__)",
    "line_number": 23,
    "enriched": "File: data_structures/hashing/hash_table.py\nCode: @@ -21,6 +21,29 @@ def __init__(\n         self._keys: dict = {}\n \n     def keys(self):\nComment: I don't love this name `.keys()` because `{0: 5, 4: 4, 3: 3, 2: 2, 1: 1}.keys()` returns `dict_keys([0, 4, 3, 2, 1])` which is a `list`, not a `dict`.  But let's leave those considerations for a separate pull request.\r\n* [`https://docs.python.org/3/library/stdtypes.html#object.__dict__`](https://docs.python.org/3/library/stdtypes.html#object.__dict__)",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "data_structures/hashing/hash_table.py",
    "pr_number": 10984,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1373057344,
    "comment_created_at": "2023-10-26T12:12:12Z"
  },
  {
    "code": "@@ -1,134 +1,36 @@\n-\"\"\"\n-https://en.wikipedia.org/wiki/Image_texture\n-https://en.wikipedia.org/wiki/Co-occurrence_matrix#Application_to_image_analysis\n-\"\"\"\n-",
    "comment": "why remove the references?",
    "line_number": 5,
    "enriched": "File: computer_vision/haralick_descriptors.py\nCode: @@ -1,134 +1,36 @@\n-\"\"\"\n-https://en.wikipedia.org/wiki/Image_texture\n-https://en.wikipedia.org/wiki/Co-occurrence_matrix#Application_to_image_analysis\n-\"\"\"\n-\nComment: Why remove the references?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "computer_vision/haralick_descriptors.py",
    "pr_number": 11597,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1782115189,
    "comment_created_at": "2024-10-01T05:05:04Z"
  },
  {
    "code": "@@ -1,19 +1,22 @@\n-def reverse_letters(input_str: str) -> str:\n+def reverse_letters(sentence: str, length: int) -> str:",
    "comment": "let's make the length an optional parameter so that we can still preserve the same functionality as the original reverse_letters\r\n\r\nplease also add a test case for length = 0",
    "line_number": 1,
    "enriched": "File: strings/reverse_letters.py\nCode: @@ -1,19 +1,22 @@\n-def reverse_letters(input_str: str) -> str:\n+def reverse_letters(sentence: str, length: int) -> str:\nComment: ```suggestion\r\ndef reverse_letters(sentence: str, length: int = 0) -> str:\r\n```\r\nLet's make the length an optional parameter so that we can still preserve the same functionality as the original `reverse_letters`\r\n\r\nPlease also add a test case for `length = 0`",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "strings/reverse_letters.py",
    "pr_number": 10107,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349754894,
    "comment_created_at": "2023-10-08T18:08:17Z"
  },
  {
    "code": "@@ -28,15 +33,35 @@ def explicit_euler(\n     >>> y = explicit_euler(f, y0, 0.0, 0.01, 5)\n     >>> y[-1]\n     144.77277243257308\n+    >>> # the exact solution is [math.sin(x),math.cos(x)\n+    >>> def f(x,y):\n+    ...     return np.array([y[1],-y[0]])\n+    >>> y0 = np.array([0,1])\n+    >>> y = explicit_euler(f, y0, 0.0, 0.01, 7)\n+    >>> y[0,-1]\n+    0.6802048957744236\n+    >>> y[1,-1]\n+    0.7809133930833114\n     \"\"\"\n     n = int(np.ceil((x_end - x0) / step_size))\n-    y = np.zeros((n + 1,))\n-    y[0] = y0\n-    x = x0\n \n-    for k in range(n):\n-        y[k + 1] = y[k] + step_size * ode_func(x, y[k])\n-        x += step_size\n+    if type(y0) == np.ndarray or type(y0) == list:",
    "comment": "could be written as if isinstance(y0, (np.ndarray, list)): or if isinstance(y0, np.ndarray | list): in newer version(i think 3.10)",
    "line_number": 48,
    "enriched": "File: maths/euler_method.py\nCode: @@ -28,15 +33,35 @@ def explicit_euler(\n     >>> y = explicit_euler(f, y0, 0.0, 0.01, 5)\n     >>> y[-1]\n     144.77277243257308\n+    >>> # the exact solution is [math.sin(x),math.cos(x)\n+    >>> def f(x,y):\n+    ...     return np.array([y[1],-y[0]])\n+    >>> y0 = np.array([0,1])\n+    >>> y = explicit_euler(f, y0, 0.0, 0.01, 7)\n+    >>> y[0,-1]\n+    0.6802048957744236\n+    >>> y[1,-1]\n+    0.7809133930833114\n     \"\"\"\n     n = int(np.ceil((x_end - x0) / step_size))\n-    y = np.zeros((n + 1,))\n-    y[0] = y0\n-    x = x0\n \n-    for k in range(n):\n-        y[k + 1] = y[k] + step_size * ode_func(x, y[k])\n-        x += step_size\n+    if type(y0) == np.ndarray or type(y0) == list:\nComment: Could be written as `if isinstance(y0, (np.ndarray, list)):` or `if isinstance(y0, np.ndarray | list):` in newer version(I think 3.10)",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "maths/euler_method.py",
    "pr_number": 8783,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1208653532,
    "comment_created_at": "2023-05-28T20:59:53Z"
  },
  {
    "code": "@@ -0,0 +1,18 @@\n+# Created by NiyazUrazaev on 03/05/23\n+\n+import requests\n+\n+\n+def send_message_to_telegram(token: str, chat_id: str, message_text: str):\n+    url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n+    data = {\"chat_id\": chat_id, \"text\": message_text}\n+    response = requests.post(url, data=data)\n+    if not response.ok:\n+        print(\"Failed to send a message in Telegram:\", response.text)",
    "comment": "please raise an exception instead.  see contributing.md because algorithmic functions should not print().",
    "line_number": 11,
    "enriched": "File: web_programming/telegram_message.py\nCode: @@ -0,0 +1,18 @@\n+# Created by NiyazUrazaev on 03/05/23\n+\n+import requests\n+\n+\n+def send_message_to_telegram(token: str, chat_id: str, message_text: str):\n+    url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n+    data = {\"chat_id\": chat_id, \"text\": message_text}\n+    response = requests.post(url, data=data)\n+    if not response.ok:\n+        print(\"Failed to send a message in Telegram:\", response.text)\nComment: Please raise an Exception instead.  See CONTRIBUTING.md because algorithmic functions should not print().",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "web_programming/telegram_message.py",
    "pr_number": 8708,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1184160492,
    "comment_created_at": "2023-05-03T19:25:52Z"
  },
  {
    "code": "@@ -27,6 +27,12 @@ def points_to_polynomial(coordinates: list[list[int]]) -> str:\n     f(x)=x^2*-1.0+x^1*-0.0+x^0*-2.0\n     >>> print(points_to_polynomial([[1, 5], [2, 2], [3, 9]]))\n     f(x)=x^2*5.0+x^1*-18.0+x^0*18.0\n+    >>> print(points_to_polynomial([[1, 1], [1, 2], [1, 3]]))\n+    x=1\n+    >>> print(points_to_polynomial([[1, 1], [2, 2], [2, 2]]))",
    "comment": "please try removing the print()...",
    "line_number": 32,
    "enriched": "File: linear_algebra/src/polynom_for_points.py\nCode: @@ -27,6 +27,12 @@ def points_to_polynomial(coordinates: list[list[int]]) -> str:\n     f(x)=x^2*-1.0+x^1*-0.0+x^0*-2.0\n     >>> print(points_to_polynomial([[1, 5], [2, 2], [3, 9]]))\n     f(x)=x^2*5.0+x^1*-18.0+x^0*18.0\n+    >>> print(points_to_polynomial([[1, 1], [1, 2], [1, 3]]))\n+    x=1\n+    >>> print(points_to_polynomial([[1, 1], [2, 2], [2, 2]]))\nComment: Please try removing the `print()`...\r\n```suggestion\r\n    >>> points_to_polynomial([[1, 1], [1, 2], [1, 3]])\r\n    x=1\r\n    >>> points_to_polynomial([[1, 1], [2, 2], [2, 2]])\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "linear_algebra/src/polynom_for_points.py",
    "pr_number": 11811,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1788937849,
    "comment_created_at": "2024-10-06T07:15:55Z"
  },
  {
    "code": "@@ -0,0 +1,50 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Installation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interv_start: The start of your internal scale.\n+            - interv_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Examples:\n+    interv_start:0\n+    interv_end:100\n+    number:50\n+    output: 50.0\n+    interv_start:6\n+    interv_end:12\n+    number:9\n+    output:50.0\n+\n+    Link: https://en.wikipedia.org/wiki/Conversion_of_units\n+\"\"\"\n+\n+\n+def absolute_conversion(interv_start: float, interv_end: float, number: float) -> str:\n+    \"\"\"\n+    >>> absolute_conversion(0, 10, 4)\n+    '40.0'\n+    >>> absolute_conversion(120, 140, 125)\n+    '25.0'\n+    >>> absolute_conversion(0,20,21)\n+    '105.0'\n+    \"\"\"\n+    if interv_start > interv_end:\n+        message = \"Invalid arguments, start is higher than the end.\"\n+        return message",
    "comment": "you should raise an error instead of returning a message",
    "line_number": 37,
    "enriched": "File: conversions/absolute_conversion.py\nCode: @@ -0,0 +1,50 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Installation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interv_start: The start of your internal scale.\n+            - interv_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Examples:\n+    interv_start:0\n+    interv_end:100\n+    number:50\n+    output: 50.0\n+    interv_start:6\n+    interv_end:12\n+    number:9\n+    output:50.0\n+\n+    Link: https://en.wikipedia.org/wiki/Conversion_of_units\n+\"\"\"\n+\n+\n+def absolute_conversion(interv_start: float, interv_end: float, number: float) -> str:\n+    \"\"\"\n+    >>> absolute_conversion(0, 10, 4)\n+    '40.0'\n+    >>> absolute_conversion(120, 140, 125)\n+    '25.0'\n+    >>> absolute_conversion(0,20,21)\n+    '105.0'\n+    \"\"\"\n+    if interv_start > interv_end:\n+        message = \"Invalid arguments, start is higher than the end.\"\n+        return message\nComment: You should raise an error instead of returning a message",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "conversions/absolute_conversion.py",
    "pr_number": 7266,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996365966,
    "comment_created_at": "2022-10-15T23:32:32Z"
  },
  {
    "code": "@@ -1,5 +1,4 @@\n-# Setup for pytest\n [pytest]\n markers =\n     mat_ops: mark a test as utilizing matrix operations.\n-addopts = --durations=10\n+addopts = --durations=10 --doctest-modules --doctest-continue-on-failure --showlocals",
    "comment": "let's leave out continue on failure.",
    "line_number": 4,
    "enriched": "File: pytest.ini\nCode: @@ -1,5 +1,4 @@\n-# Setup for pytest\n [pytest]\n markers =\n     mat_ops: mark a test as utilizing matrix operations.\n-addopts = --durations=10\n+addopts = --durations=10 --doctest-modules --doctest-continue-on-failure --showlocals\nComment: Let's leave out continue on failure.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pytest.ini",
    "pr_number": 7840,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008679651,
    "comment_created_at": "2022-10-29T10:49:10Z"
  },
  {
    "code": "@@ -0,0 +1,50 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Installation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interv_start: The start of your internal scale.\n+            - interv_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Examples:\n+    interv_start:0\n+    interv_end:100\n+    number:50\n+    output: 50.0\n+    interv_start:6\n+    interv_end:12\n+    number:9\n+    output:50.0\n+\n+    Link: https://en.wikipedia.org/wiki/Conversion_of_units\n+\"\"\"\n+\n+\n+def absolute_conversion(interv_start: float, interv_end: float, number: float) -> str:\n+    \"\"\"\n+    >>> absolute_conversion(0, 10, 4)\n+    40.0\n+    >>> absolute_conversion(120, 140, 125)\n+    25.0\n+    >>> absolute_conversion(0,20,21)\n+    105.0\n+    \"\"\"\n+    if interv_start > interv_end:\n+        message = \"Invalid arguments, start is higher than the end.\"\n+        return message",
    "comment": "instead of returning message, why not raise an error?",
    "line_number": 37,
    "enriched": "File: conversions/absolute_conversion.py\nCode: @@ -0,0 +1,50 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Installation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interv_start: The start of your internal scale.\n+            - interv_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Examples:\n+    interv_start:0\n+    interv_end:100\n+    number:50\n+    output: 50.0\n+    interv_start:6\n+    interv_end:12\n+    number:9\n+    output:50.0\n+\n+    Link: https://en.wikipedia.org/wiki/Conversion_of_units\n+\"\"\"\n+\n+\n+def absolute_conversion(interv_start: float, interv_end: float, number: float) -> str:\n+    \"\"\"\n+    >>> absolute_conversion(0, 10, 4)\n+    40.0\n+    >>> absolute_conversion(120, 140, 125)\n+    25.0\n+    >>> absolute_conversion(0,20,21)\n+    105.0\n+    \"\"\"\n+    if interv_start > interv_end:\n+        message = \"Invalid arguments, start is higher than the end.\"\n+        return message\nComment: Instead of returning `message`, why not raise an error?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "conversions/absolute_conversion.py",
    "pr_number": 7263,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996349733,
    "comment_created_at": "2022-10-15T20:11:05Z"
  },
  {
    "code": "@@ -161,14 +174,10 @@ def zigzag(root: Node | None) -> Sequence[Node | None] | list[Any]:\n \r\n \r\n def main() -> None:  # Main function for testing.\r\n-    \"\"\"\r\n-    Create binary tree.\r\n-    \"\"\"\r\n+    # Create binary tree.\r\n     root = make_tree()\r\n-    \"\"\"\r\n-    All Traversals of the binary are as follows:\r\n-    \"\"\"\r\n \r\n+    # All Traversals of the binary are as follows:\r\n     print(f\"In-order Traversal: {inorder(root)}\")\r",
    "comment": "add a print statement for the new traversal",
    "line_number": 181,
    "enriched": "File: data_structures/binary_tree/binary_tree_traversals.py\nCode: @@ -161,14 +174,10 @@ def zigzag(root: Node | None) -> Sequence[Node | None] | list[Any]:\n \r\n \r\n def main() -> None:  # Main function for testing.\r\n-    \"\"\"\r\n-    Create binary tree.\r\n-    \"\"\"\r\n+    # Create binary tree.\r\n     root = make_tree()\r\n-    \"\"\"\r\n-    All Traversals of the binary are as follows:\r\n-    \"\"\"\r\n \r\n+    # All Traversals of the binary are as follows:\r\n     print(f\"In-order Traversal: {inorder(root)}\")\r\nComment: ```suggestion\r\n    print(f\"In-order Traversal: {inorder(root)}\")\r\n    print(f\"Reverse In-order Traversal: {reverse_inorder(root)}\")\r\n```\r\n\r\nAdd a print statement for the new traversal",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "data_structures/binary_tree/binary_tree_traversals.py",
    "pr_number": 8726,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1194769865,
    "comment_created_at": "2023-05-16T08:00:09Z"
  },
  {
    "code": "@@ -10,38 +10,24 @@\n python3 binary_search.py\n \"\"\"\n from __future__ import annotations\n+from typing import List",
    "comment": "this is not needed in modern python.",
    "line_number": 13,
    "enriched": "File: searches/binary_search.py\nCode: @@ -10,38 +10,24 @@\n python3 binary_search.py\n \"\"\"\n from __future__ import annotations\n+from typing import List\nComment: This is not needed in modern Python.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "searches/binary_search.py",
    "pr_number": 10870,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1369850334,
    "comment_created_at": "2023-10-24T09:09:11Z"
  },
  {
    "code": "@@ -28,7 +28,6 @@\n Reference:\r\n     https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/smo-book.pdf\r\n     https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf\r\n-    https://web.cs.iastate.edu/~honavar/smo-svm.pdf\r",
    "comment": "this link does not exist; hence i removed it.",
    "line_number": 31,
    "enriched": "File: machine_learning/sequential_minimum_optimization.py\nCode: @@ -28,7 +28,6 @@\n Reference:\r\n     https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/smo-book.pdf\r\n     https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf\r\n-    https://web.cs.iastate.edu/~honavar/smo-svm.pdf\r\nComment: This link does not exist; hence I removed it.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "machine_learning/sequential_minimum_optimization.py",
    "pr_number": 7319,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996443832,
    "comment_created_at": "2022-10-16T13:38:33Z"
  },
  {
    "code": "@@ -230,6 +235,86 @@ def fib_binet(n: int) -> list[int]:\n     return [round(phi**i / sqrt_5) for i in range(n + 1)]\n \n \n+def matrix_pow_np(m: ndarray, power: int) -> ndarray:\n+    \"\"\"\n+    Raises a matrix to the power of 'power' using binary exponentiation.\n+\n+    Args:\n+        m: Matrix as a numpy array.\n+        power: The power to which the matrix is to be raised.\n+\n+    Returns:\n+        The matrix raised to the power.\n+\n+    Raises:\n+        ValueError: If power is negative.\n+\n+    >>> m = np.array([[1, 1], [1, 0]], dtype=int)\n+    >>> matrix_pow_np(m, 0)  # Identity matrix when raised to the power of 0\n+    array([[1, 0],\n+           [0, 1]])\n+\n+    >>> matrix_pow_np(m, 1)  # Same matrix when raised to the power of 1\n+    array([[1, 1],\n+           [1, 0]])\n+\n+    >>> matrix_pow_np(m, 5)\n+    array([[8, 5],\n+           [5, 3]])\n+\n+    >>> matrix_pow_np(m, -1)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: power is negative",
    "comment": "your code never actually checks whether the power is negative. i think this is why your tests are never finishing: this test is just running in an infinite loop with power = -1, -2, -3, ...",
    "line_number": 268,
    "enriched": "File: maths/fibonacci.py\nCode: @@ -230,6 +235,86 @@ def fib_binet(n: int) -> list[int]:\n     return [round(phi**i / sqrt_5) for i in range(n + 1)]\n \n \n+def matrix_pow_np(m: ndarray, power: int) -> ndarray:\n+    \"\"\"\n+    Raises a matrix to the power of 'power' using binary exponentiation.\n+\n+    Args:\n+        m: Matrix as a numpy array.\n+        power: The power to which the matrix is to be raised.\n+\n+    Returns:\n+        The matrix raised to the power.\n+\n+    Raises:\n+        ValueError: If power is negative.\n+\n+    >>> m = np.array([[1, 1], [1, 0]], dtype=int)\n+    >>> matrix_pow_np(m, 0)  # Identity matrix when raised to the power of 0\n+    array([[1, 0],\n+           [0, 1]])\n+\n+    >>> matrix_pow_np(m, 1)  # Same matrix when raised to the power of 1\n+    array([[1, 1],\n+           [1, 0]])\n+\n+    >>> matrix_pow_np(m, 5)\n+    array([[8, 5],\n+           [5, 3]])\n+\n+    >>> matrix_pow_np(m, -1)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: power is negative\nComment: Your code never actually checks whether the power is negative. I think this is why your tests are never finishing: this test is just running in an infinite loop with power = -1, -2, -3, ...",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/fibonacci.py",
    "pr_number": 11747,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1787909056,
    "comment_created_at": "2024-10-04T15:45:55Z"
  },
  {
    "code": "@@ -0,0 +1,90 @@\n+\"\"\"\n+Find the minimum number of multiplications needed to multiply chain of matrices.\n+Reference: https://www.geeksforgeeks.org/matrix-chain-multiplication-dp-8/\n+\n+The algorithm has interesting real-world applications. Example:\n+1. Image transformations in Computer Graphics as images are composed of matrix.\n+2. Solve complex polynomial equations in the field of algebra using least\n+processing power.\n+3. Calculate overall impact of macroeconomic decisions as economic\n+equations involve number of variables.\n+4. Self-driving car navigation can be made more accurate as matrix multiplication\n+can accurately determine position and orientation of obstacles in short time.\n+\n+Python doctests can be run with the following command:\n+python -m doctest -v matrix_chain_multiply.py\n+\n+Given a sequence arr[] that represents chain of 2D matrices such that\n+the dimension of ith matrix is arr[i-1]*arr[i].\n+So suppose arr = [40, 20, 30, 10, 30] means we have 4 matrices of\n+dimensions 40*20, 20*30, 30*10 and 10*30.\n+\n+matrix_chain_multiply() returns an integer denoting\n+minimum number of multiplications to multiply the chain.\n+\n+We do not need to perform actual multiplication here.\n+We only need to decide the order in which to perform the multiplication.\n+\n+Hints:\n+1. Number of multiplications (ie cost) to multiply 2 matrices\n+of size m*p and p*n is m*p*n.\n+2. Cost of matrix multiplication is neither associative ie (M1*M2)*M3 != M1*(M2*M3)",
    "comment": "this is just incorrect. if the individual multiplications are well-defined, then matrix multiplication _is_ associative.",
    "line_number": 31,
    "enriched": "File: dynamic_programming/matrix_chain_multiplication.py\nCode: @@ -0,0 +1,90 @@\n+\"\"\"\n+Find the minimum number of multiplications needed to multiply chain of matrices.\n+Reference: https://www.geeksforgeeks.org/matrix-chain-multiplication-dp-8/\n+\n+The algorithm has interesting real-world applications. Example:\n+1. Image transformations in Computer Graphics as images are composed of matrix.\n+2. Solve complex polynomial equations in the field of algebra using least\n+processing power.\n+3. Calculate overall impact of macroeconomic decisions as economic\n+equations involve number of variables.\n+4. Self-driving car navigation can be made more accurate as matrix multiplication\n+can accurately determine position and orientation of obstacles in short time.\n+\n+Python doctests can be run with the following command:\n+python -m doctest -v matrix_chain_multiply.py\n+\n+Given a sequence arr[] that represents chain of 2D matrices such that\n+the dimension of ith matrix is arr[i-1]*arr[i].\n+So suppose arr = [40, 20, 30, 10, 30] means we have 4 matrices of\n+dimensions 40*20, 20*30, 30*10 and 10*30.\n+\n+matrix_chain_multiply() returns an integer denoting\n+minimum number of multiplications to multiply the chain.\n+\n+We do not need to perform actual multiplication here.\n+We only need to decide the order in which to perform the multiplication.\n+\n+Hints:\n+1. Number of multiplications (ie cost) to multiply 2 matrices\n+of size m*p and p*n is m*p*n.\n+2. Cost of matrix multiplication is neither associative ie (M1*M2)*M3 != M1*(M2*M3)\nComment: This is just incorrect. If the individual multiplications are well-defined, then matrix multiplication _is_ associative.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "dynamic_programming/matrix_chain_multiplication.py",
    "pr_number": 10562,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359965102,
    "comment_created_at": "2023-10-15T22:48:59Z"
  },
  {
    "code": "@@ -35,6 +35,33 @@ def __hash_double_function(self, key, data, increment):\n         return (increment * self.__hash_function_2(key, data)) % self.size_table\n \n     def _collision_resolution(self, key, data=None):\n+        \"\"\"\n+        Examples:",
    "comment": "cool",
    "line_number": 39,
    "enriched": "File: data_structures/hashing/double_hash.py\nCode: @@ -35,6 +35,33 @@ def __hash_double_function(self, key, data, increment):\n         return (increment * self.__hash_function_2(key, data)) % self.size_table\n \n     def _collision_resolution(self, key, data=None):\n+        \"\"\"\n+        Examples:\nComment: Cool",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "data_structures/hashing/double_hash.py",
    "pr_number": 11020,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1374281770,
    "comment_created_at": "2023-10-27T09:00:46Z"
  },
  {
    "code": "@@ -25,6 +25,18 @@ def get_english_count(message: str) -> float:\n \n \n def remove_non_letters(message: str) -> str:",
    "comment": "nice\u2705",
    "line_number": 27,
    "enriched": "File: strings/detecting_english_programmatically.py\nCode: @@ -25,6 +25,18 @@ def get_english_count(message: str) -> float:\n \n \n def remove_non_letters(message: str) -> str:\nComment: Nice\u2705",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "strings/detecting_english_programmatically.py",
    "pr_number": 11135,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1380414335,
    "comment_created_at": "2023-11-02T16:21:23Z"
  },
  {
    "code": "@@ -28,6 +28,12 @@ def to_simple_case(str_: str) -> str:\n     \"\"\"\n     >>> to_simple_case(\"one two 31235three4four\")\n     'OneTwo31235three4four'\n+    >>> to_simple_case(\"This should be combined\")\n+    'ThisShouldBeCombined'\n+    >>> to_simple_case(\"The first letters are capitalized, then string is merged\")\n+    'TheFirstLettersAreCapitalizedThenStringIsMerged'\n+    >>> to_simple_case(\"special characters :, ', %, ^, $, are ignored\")",
    "comment": "nice",
    "line_number": 35,
    "enriched": "File: strings/string_switch_case.py\nCode: @@ -28,6 +28,12 @@ def to_simple_case(str_: str) -> str:\n     \"\"\"\n     >>> to_simple_case(\"one two 31235three4four\")\n     'OneTwo31235three4four'\n+    >>> to_simple_case(\"This should be combined\")\n+    'ThisShouldBeCombined'\n+    >>> to_simple_case(\"The first letters are capitalized, then string is merged\")\n+    'TheFirstLettersAreCapitalizedThenStringIsMerged'\n+    >>> to_simple_case(\"special characters :, ', %, ^, $, are ignored\")\nComment:  Nice ",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "strings/string_switch_case.py",
    "pr_number": 11136,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1381581590,
    "comment_created_at": "2023-11-03T12:09:05Z"
  },
  {
    "code": "@@ -17,7 +17,7 @@ def diophantine(a: int, b: int, c: int) -> tuple[float, float]:\n     >>> diophantine(391,299,-69)\n     (9.0, -12.0)\n \n-    But above equation has one more solution i.e., x = -4, y = 5.\n+    But above equation has one more solution i.e. x = -4, y = 5.",
    "comment": "the comma is not strictly incorrect. \"i.e.\" is usually followed by a comma in american usage:\r\n\r\n[merriam-webster dictionary](https://www.merriam-webster.com/grammar/ie-vs-eg-abbreviation-meaning-usage-difference):\r\n> while i.e. is often set off by brackets or parentheses, it can also sometimes follow a comma or em dash. it is usually followed by a comma.\r\n\r\n[wiktionary](https://en.wiktionary.org/wiki/i.e.)\r\n> american english prefers a comma after i.e., but british english usually does not use a comma there and often does not use dots either.",
    "line_number": 20,
    "enriched": "File: blockchain/diophantine_equation.py\nCode: @@ -17,7 +17,7 @@ def diophantine(a: int, b: int, c: int) -> tuple[float, float]:\n     >>> diophantine(391,299,-69)\n     (9.0, -12.0)\n \n-    But above equation has one more solution i.e., x = -4, y = 5.\n+    But above equation has one more solution i.e. x = -4, y = 5.\nComment: The comma is not strictly incorrect. \"i.e.\" is usually followed by a comma in American usage:\r\n\r\n[Merriam-Webster Dictionary](https://www.merriam-webster.com/grammar/ie-vs-eg-abbreviation-meaning-usage-difference):\r\n> While i.e. is often set off by brackets or parentheses, it can also sometimes follow a comma or em dash. It is usually followed by a comma.\r\n\r\n[Wiktionary](https://en.wiktionary.org/wiki/i.e.)\r\n> American English prefers a comma after i.e., but British English usually does not use a comma there and often does not use dots either.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "blockchain/diophantine_equation.py",
    "pr_number": 10814,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367939290,
    "comment_created_at": "2023-10-22T17:10:25Z"
  },
  {
    "code": "@@ -1,76 +1,85 @@\n-#!/usr/bin/python\n-\n-\"\"\"Author Anurag Kumar | anuragkumarak95@gmail.com | git/anuragkumarak95\n-\n-Simple example of Fractal generation using recursive function.\n-\n-What is Sierpinski Triangle?\n->>The Sierpinski triangle (also with the original orthography Sierpinski), also called\n-the Sierpinski gasket or the Sierpinski Sieve, is a fractal and attractive fixed set\n-with the overall shape of an equilateral triangle, subdivided recursively into smaller\n-equilateral triangles. Originally constructed as a curve, this is one of the basic\n-examples of self-similar sets, i.e., it is a mathematically generated pattern that can\n-be reproducible at any magnification or reduction. It is named after the Polish\n-mathematician Wac\u0142aw Sierpinski, but appeared as a decorative pattern many centuries\n-prior to the work of Sierpinski.\n+\"\"\"\n+Author Anurag Kumar | anuragkumarak95@gmail.com | git/anuragkumarak95\n \n-Requirements(pip):\n-  - turtle\n+Simple example of fractal generation using recursion.\n \n-Python:\n-  - 2.6\n+What is the Sierpi\u0144ski Triangle?\n+    The Sierpi\u0144ski triangle (sometimes spelled Sierpinski), also called the\n+Sierpi\u0144ski gasket or Sierpi\u0144ski sieve, is a fractal attractive fixed set with\n+the overall shape of an equilateral triangle, subdivided recursively into\n+smaller equilateral triangles. Originally constructed as a curve, this is one of\n+the basic examples of self-similar sets\u2014that is, it is a mathematically\n+generated pattern that is reproducible at any magnification or reduction. It is\n+named after the Polish mathematician Wac\u0142aw Sierpi\u0144ski, but appeared as a\n+decorative pattern many centuries before the work of Sierpi\u0144ski.\n \n-Usage:\n-  - $python sierpinski_triangle.py <int:depth_for_fractal>\n+Requirements (pip): turtle",
    "comment": "turtle builtin, not installed via pip... https://docs.python.org/3/library/turtle.html",
    "line_number": 16,
    "enriched": "File: fractals/sierpinski_triangle.py\nCode: @@ -1,76 +1,85 @@\n-#!/usr/bin/python\n-\n-\"\"\"Author Anurag Kumar | anuragkumarak95@gmail.com | git/anuragkumarak95\n-\n-Simple example of Fractal generation using recursive function.\n-\n-What is Sierpinski Triangle?\n->>The Sierpinski triangle (also with the original orthography Sierpinski), also called\n-the Sierpinski gasket or the Sierpinski Sieve, is a fractal and attractive fixed set\n-with the overall shape of an equilateral triangle, subdivided recursively into smaller\n-equilateral triangles. Originally constructed as a curve, this is one of the basic\n-examples of self-similar sets, i.e., it is a mathematically generated pattern that can\n-be reproducible at any magnification or reduction. It is named after the Polish\n-mathematician Wac\u0142aw Sierpinski, but appeared as a decorative pattern many centuries\n-prior to the work of Sierpinski.\n+\"\"\"\n+Author Anurag Kumar | anuragkumarak95@gmail.com | git/anuragkumarak95\n \n-Requirements(pip):\n-  - turtle\n+Simple example of fractal generation using recursion.\n \n-Python:\n-  - 2.6\n+What is the Sierpi\u0144ski Triangle?\n+    The Sierpi\u0144ski triangle (sometimes spelled Sierpinski), also called the\n+Sierpi\u0144ski gasket or Sierpi\u0144ski sieve, is a fractal attractive fixed set with\n+the overall shape of an equilateral triangle, subdivided recursively into\n+smaller equilateral triangles. Originally constructed as a curve, this is one of\n+the basic examples of self-similar sets\u2014that is, it is a mathematically\n+generated pattern that is reproducible at any magnification or reduction. It is\n+named after the Polish mathematician Wac\u0142aw Sierpi\u0144ski, but appeared as a\n+decorative pattern many centuries before the work of Sierpi\u0144ski.\n \n-Usage:\n-  - $python sierpinski_triangle.py <int:depth_for_fractal>\n+Requirements (pip): turtle\nComment: `turtle` builtin, not installed via pip... https://docs.python.org/3/library/turtle.html\r\n```suggestion\r\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "fractals/sierpinski_triangle.py",
    "pr_number": 8068,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1060023639,
    "comment_created_at": "2023-01-02T13:09:46Z"
  },
  {
    "code": "@@ -0,0 +1,134 @@\n+\"\"\"\n+VGG16 Model Implementation\n+\n+Paper: https://arxiv.org/abs/1409.1556\n+\"\"\"\n+\n+import torch\n+from torch import nn\n+\n+\n+class VGG16(nn.Module):\n+    def __init__(self, num_classes: int = 10) -> None:\n+        super().__init__()\n+\n+        # -------------------------- Feature Extraction Layers --------------------- #\n+        # The feature extraction layers consist of a series of convolutional and\n+        # max-pooling layers that learn hierarchical representations of input images.\n+        # The architecture follows a pattern of multiple convolutional layers with\n+        # ReLU activations, followed by max-pooling layers.\n+        # The number of convolutional layers per block increases as the network\n+        # goes deeper, and the number of output channels doubles after each\n+        # max-pooling layer.\n+        # -------------------------------------------------------------------------- #\n+\n+        self.features = nn.Sequential(\n+            # Layer 1\n+            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 2\n+            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 3\n+            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 4\n+            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 5\n+            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 6\n+            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 7\n+            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 8\n+            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 9\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 10\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 11\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 12\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 13\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+        )\n+\n+        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n+\n+        # -------------------------- Fully Connected Layers -------------------------- #\n+        # These layers receive the high-level features learned by the feature extraction\n+        # layers and use them to perform the final classification task.\n+        # The fully connected layers consist of three linear layers with ReLU\n+        # activations and dropout for regularization. The final layer has a\n+        # number of output units equal to the number of classes in the\n+        # classification problem.\n+        # ---------------------------------------------------------------------------- #\n+\n+        self.classifier = nn.Sequential(\n+            # Layer 14\n+            nn.Linear(512 * 7 * 7, 4096),\n+            nn.ReLU(inplace=True),\n+            nn.Dropout(),\n+            # Layer 15\n+            nn.Linear(4096, 4096),\n+            nn.ReLU(inplace=True),\n+            nn.Dropout(),\n+            # Layer 16\n+            nn.Linear(4096, num_classes),\n+        )\n+\n+    def forward(self, image: torch.tensor) -> torch.tensor:\n+        image = self.features(image)\n+        image = self.avgpool(image)\n+        image = torch.flatten(image, 1)\n+        image = self.classifier(image)\n+        return image\n+\n+\n+def test_model(image_tensor: torch.tensor) -> bool:\n+    \"\"\"\n+    Test the model for a batch of 64 images\n+\n+    Args:\n+        image_tensor (torch.tensor): Batch of 64 Images for the model.\n+\n+    Returns:\n+        bool: True if the model works, False otherwise\n+\n+    >>> test_model(torch.rand(64, 3, 224, 224))\n+    True\n+    \"\"\"\n+\n+    try:\n+        model = VGG16()\n+        output = model(image_tensor)\n+\n+    except Exception as e:",
    "comment": "https://realpython.com/the-most-diabolical-python-antipattern/",
    "line_number": 122,
    "enriched": "File: computer_vision/vgg_pytorch.py\nCode: @@ -0,0 +1,134 @@\n+\"\"\"\n+VGG16 Model Implementation\n+\n+Paper: https://arxiv.org/abs/1409.1556\n+\"\"\"\n+\n+import torch\n+from torch import nn\n+\n+\n+class VGG16(nn.Module):\n+    def __init__(self, num_classes: int = 10) -> None:\n+        super().__init__()\n+\n+        # -------------------------- Feature Extraction Layers --------------------- #\n+        # The feature extraction layers consist of a series of convolutional and\n+        # max-pooling layers that learn hierarchical representations of input images.\n+        # The architecture follows a pattern of multiple convolutional layers with\n+        # ReLU activations, followed by max-pooling layers.\n+        # The number of convolutional layers per block increases as the network\n+        # goes deeper, and the number of output channels doubles after each\n+        # max-pooling layer.\n+        # -------------------------------------------------------------------------- #\n+\n+        self.features = nn.Sequential(\n+            # Layer 1\n+            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 2\n+            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 3\n+            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 4\n+            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 5\n+            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 6\n+            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 7\n+            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 8\n+            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 9\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 10\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 11\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 12\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 13\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+        )\n+\n+        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n+\n+        # -------------------------- Fully Connected Layers -------------------------- #\n+        # These layers receive the high-level features learned by the feature extraction\n+        # layers and use them to perform the final classification task.\n+        # The fully connected layers consist of three linear layers with ReLU\n+        # activations and dropout for regularization. The final layer has a\n+        # number of output units equal to the number of classes in the\n+        # classification problem.\n+        # ---------------------------------------------------------------------------- #\n+\n+        self.classifier = nn.Sequential(\n+            # Layer 14\n+            nn.Linear(512 * 7 * 7, 4096),\n+            nn.ReLU(inplace=True),\n+            nn.Dropout(),\n+            # Layer 15\n+            nn.Linear(4096, 4096),\n+            nn.ReLU(inplace=True),\n+            nn.Dropout(),\n+            # Layer 16\n+            nn.Linear(4096, num_classes),\n+        )\n+\n+    def forward(self, image: torch.tensor) -> torch.tensor:\n+        image = self.features(image)\n+        image = self.avgpool(image)\n+        image = torch.flatten(image, 1)\n+        image = self.classifier(image)\n+        return image\n+\n+\n+def test_model(image_tensor: torch.tensor) -> bool:\n+    \"\"\"\n+    Test the model for a batch of 64 images\n+\n+    Args:\n+        image_tensor (torch.tensor): Batch of 64 Images for the model.\n+\n+    Returns:\n+        bool: True if the model works, False otherwise\n+\n+    >>> test_model(torch.rand(64, 3, 224, 224))\n+    True\n+    \"\"\"\n+\n+    try:\n+        model = VGG16()\n+        output = model(image_tensor)\n+\n+    except Exception as e:\nComment: https://realpython.com/the-most-diabolical-python-antipattern/",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "computer_vision/vgg_pytorch.py",
    "pr_number": 8620,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1160931758,
    "comment_created_at": "2023-04-07T19:58:35Z"
  },
  {
    "code": "@@ -0,0 +1,22 @@\n+# A Dynamic Programming based Python Program for 0-1 Knapsack problem",
    "comment": "add a link please",
    "line_number": 1,
    "enriched": "File: knapsack/assignment_problem.py\nCode: @@ -0,0 +1,22 @@\n+# A Dynamic Programming based Python Program for 0-1 Knapsack problem\nComment: Add a link please",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "knapsack/assignment_problem.py",
    "pr_number": 7414,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 998842080,
    "comment_created_at": "2022-10-19T00:58:24Z"
  },
  {
    "code": "@@ -1,7 +1,7 @@\n \"\"\"\r\n Find the area of various geometric shapes\r\n \"\"\"\r",
    "comment": "add a credible source as a reference.",
    "line_number": 3,
    "enriched": "File: maths/area.py\nCode: @@ -1,7 +1,7 @@\n \"\"\"\r\n Find the area of various geometric shapes\r\n \"\"\"\r\nComment: Add a credible source as a reference.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/area.py",
    "pr_number": 7438,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000115134,
    "comment_created_at": "2022-10-20T03:47:19Z"
  },
  {
    "code": "@@ -0,0 +1,86 @@\n+import bs4\n+import requests\n+from fake_useragent import UserAgent\n+\n+MOVIES_URL = \"https://www.coolgenerator.com/random-movie-generator\"\n+\n+\n+def get_imdb_link(movie_title: str) -> str:\n+    \"\"\"\n+        Return the IMDB url of the movie title\n+    Args:\n+        movie_title (str): Movie title\n+\n+    Returns:\n+        str: Imdb Url of the movie\n+    \"\"\"\n+    url = f\"https://www.imdb.com/find?q={movie_title.replace(' ', '+')}\"\n+    try:\n+        for _ in range(2):\n+            response = requests.get(url, headers={\"User-Agent\": UserAgent().firefox})\n+            if response.ok:\n+                break\n+        soup = bs4.BeautifulSoup(response.content, \"html.parser\")\n+        imdb_link = soup.find(\"a\", {\"class\": \"ipc-metadata-list-summary-item__t\"})\n+        url_imdb = \"https://www.imdb.com\" + imdb_link.attrs[\"href\"]\n+\n+    except Exception:",
    "comment": "https://realpython.com/the-most-diabolical-python-antipattern",
    "line_number": 27,
    "enriched": "File: web_programming/random_movie.py\nCode: @@ -0,0 +1,86 @@\n+import bs4\n+import requests\n+from fake_useragent import UserAgent\n+\n+MOVIES_URL = \"https://www.coolgenerator.com/random-movie-generator\"\n+\n+\n+def get_imdb_link(movie_title: str) -> str:\n+    \"\"\"\n+        Return the IMDB url of the movie title\n+    Args:\n+        movie_title (str): Movie title\n+\n+    Returns:\n+        str: Imdb Url of the movie\n+    \"\"\"\n+    url = f\"https://www.imdb.com/find?q={movie_title.replace(' ', '+')}\"\n+    try:\n+        for _ in range(2):\n+            response = requests.get(url, headers={\"User-Agent\": UserAgent().firefox})\n+            if response.ok:\n+                break\n+        soup = bs4.BeautifulSoup(response.content, \"html.parser\")\n+        imdb_link = soup.find(\"a\", {\"class\": \"ipc-metadata-list-summary-item__t\"})\n+        url_imdb = \"https://www.imdb.com\" + imdb_link.attrs[\"href\"]\n+\n+    except Exception:\nComment: https://realpython.com/the-most-diabolical-python-antipattern\r\n",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "web_programming/random_movie.py",
    "pr_number": 8145,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1111120834,
    "comment_created_at": "2023-02-18T22:43:25Z"
  },
  {
    "code": "@@ -24,6 +50,35 @@ def b_expo(a: int, b: int) -> int:\n \n \n def b_expo_mod(a: int, b: int, c: int) -> int:\n+    \"\"\"\n+    Calculate the result of (a * b) % c using binary exponentiation and modular arithmetic.",
    "comment": "please fix the ruff error on this line. the line is too long (88 chars max)",
    "line_number": 54,
    "enriched": "File: maths/binary_multiplication.py\nCode: @@ -24,6 +50,35 @@ def b_expo(a: int, b: int) -> int:\n \n \n def b_expo_mod(a: int, b: int, c: int) -> int:\n+    \"\"\"\n+    Calculate the result of (a * b) % c using binary exponentiation and modular arithmetic.\nComment: Please fix the ruff error on this line. The line is too long (88 chars max)",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "maths/binary_multiplication.py",
    "pr_number": 9513,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1343148374,
    "comment_created_at": "2023-10-02T20:46:38Z"
  },
  {
    "code": "@@ -1332,7 +1332,10 @@ def decorator(f):\n \n     @staticmethod\n     def _get_exc_class_and_code(exc_class_or_code):\n-        \"\"\"Ensure that we register only exceptions as handler keys\"\"\"\n+        \"\"\"Ensure that we register only exceptions as handler keys\n+        \n+        :param exc_class_or_code: the class for the or exception code as integer",
    "comment": "typo?",
    "line_number": 1337,
    "enriched": "File: src/flask/app.py\nCode: @@ -1332,7 +1332,10 @@ def decorator(f):\n \n     @staticmethod\n     def _get_exc_class_and_code(exc_class_or_code):\n-        \"\"\"Ensure that we register only exceptions as handler keys\"\"\"\n+        \"\"\"Ensure that we register only exceptions as handler keys\n+        \n+        :param exc_class_or_code: the class for the or exception code as integer\nComment: Typo?",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "src/flask/app.py",
    "pr_number": 3253,
    "repo": "flask",
    "owner": "pallets",
    "comment_id": 291809634,
    "comment_created_at": "2019-06-08T15:10:29Z"
  },
  {
    "code": "@@ -170,10 +170,12 @@ def _get_clickable(clickdata, form):\n     \"\"\"\n     clickables = [\n         el for el in form.xpath(\n-            'descendant::*[(self::input or self::button)'\n-            ' and re:test(@type, \"^submit$\", \"i\")]'\n-            '|descendant::button[not(@type)]',\n-            namespaces={\"re\": \"http://exslt.org/regular-expressions\"})",
    "comment": "is it me or these lines are changed from 4 spaces indentation to 5 spaces?",
    "line_number": 176,
    "enriched": "File: scrapy/http/request/form.py\nCode: @@ -170,10 +170,12 @@ def _get_clickable(clickdata, form):\n     \"\"\"\n     clickables = [\n         el for el in form.xpath(\n-            'descendant::*[(self::input or self::button)'\n-            ' and re:test(@type, \"^submit$\", \"i\")]'\n-            '|descendant::button[not(@type)]',\n-            namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\nComment: is it me or these lines are changed from 4 spaces indentation to 5 spaces?",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "scrapy/http/request/form.py",
    "pr_number": 3153,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 174508190,
    "comment_created_at": "2018-03-14T15:47:28Z"
  },
  {
    "code": "@@ -159,6 +159,24 @@ following methods:\n         :param spider: the spider to whom the start requests belong\n         :type spider: :class:`~scrapy.spiders.Spider` object\n \n+Accessing settings and other crawler attributes\n+-----------------------------------------------\n+\n+In order to access settings and other crawler attributes you have to use \n+``from_crawler`` factory method::\n+\n+  class CustomMiddleware(object):\n+\n+  def __init__(self, middleware_arg):\n+      self.middleware_arg = middleware_arg\n+      self.log('Middleware parameter: {}'.format(middleware_arg))\n+\n+  @classmethod\n+  def from_crawler(cls, crawler):\n+      settings = crawler.settings\n+      middleware_arg = settings.get('MIDDLEWARE_ARG')\n+    return cls(middleware_arg)\n+",
    "comment": "indentation does not look correct",
    "line_number": 179,
    "enriched": "File: docs/topics/spider-middleware.rst\nCode: @@ -159,6 +159,24 @@ following methods:\n         :param spider: the spider to whom the start requests belong\n         :type spider: :class:`~scrapy.spiders.Spider` object\n \n+Accessing settings and other crawler attributes\n+-----------------------------------------------\n+\n+In order to access settings and other crawler attributes you have to use \n+``from_crawler`` factory method::\n+\n+  class CustomMiddleware(object):\n+\n+  def __init__(self, middleware_arg):\n+      self.middleware_arg = middleware_arg\n+      self.log('Middleware parameter: {}'.format(middleware_arg))\n+\n+  @classmethod\n+  def from_crawler(cls, crawler):\n+      settings = crawler.settings\n+      middleware_arg = settings.get('MIDDLEWARE_ARG')\n+    return cls(middleware_arg)\n+\nComment: indentation does not look correct\n",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/topics/spider-middleware.rst",
    "pr_number": 2336,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 84040905,
    "comment_created_at": "2016-10-19T10:32:04Z"
  },
  {
    "code": "@@ -49,8 +49,14 @@ def robot_parser(self, request, spider):\n             )\n             dfd = self.crawler.engine.download(robotsreq, spider)\n             dfd.addCallback(self._parse_robots)\n+            dfd.addErrback(self._logerror, robotsreq, spider)\n         return self._parsers[netloc]\n \n+    def _logerror(self, failure, request, spider):\n+        if failure.type is not IgnoreRequest:\n+            log.msg(format=\"Error downloading %%(request)s: %s\" % failure.value,",
    "comment": "double percentage symbol is a typo @torymur ?",
    "line_number": 57,
    "enriched": "File: scrapy/contrib/downloadermiddleware/robotstxt.py\nCode: @@ -49,8 +49,14 @@ def robot_parser(self, request, spider):\n             )\n             dfd = self.crawler.engine.download(robotsreq, spider)\n             dfd.addCallback(self._parse_robots)\n+            dfd.addErrback(self._logerror, robotsreq, spider)\n         return self._parsers[netloc]\n \n+    def _logerror(self, failure, request, spider):\n+        if failure.type is not IgnoreRequest:\n+            log.msg(format=\"Error downloading %%(request)s: %s\" % failure.value,\nComment: double percentage symbol is a typo @torymur ?\n",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "scrapy/contrib/downloadermiddleware/robotstxt.py",
    "pr_number": 1131,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 28391950,
    "comment_created_at": "2015-04-15T03:59:05Z"
  },
  {
    "code": "@@ -875,11 +875,12 @@ def test_form_encoded_post_query_multivalued_element(self, httpbin):\n         assert prep.body == 'test=foo&test=baz'\n \n     def test_different_encodings_dont_break_post(self, httpbin):\n-        r = requests.post(httpbin('post'),\n-            data={'stuff': json.dumps({'a': 123})},\n-            params={'blah': 'asdf1234'},\n-            files={'file': ('test_requests.py', open(__file__, 'rb'))})\n-        assert r.status_code == 200\n+        with open(__file__, 'rb') as f:\n+            r = requests.post(httpbin('post'),\n+                data={'stuff': json.dumps({'a': 123})},\n+                params={'blah': 'asdf1234'},\n+                files={'file': ('test_requests.py', f)})\n+            assert r.status_code == 200",
    "comment": "let\u2019s pull the assert out to the same indentation level as with.",
    "line_number": 883,
    "enriched": "File: tests/test_requests.py\nCode: @@ -875,11 +875,12 @@ def test_form_encoded_post_query_multivalued_element(self, httpbin):\n         assert prep.body == 'test=foo&test=baz'\n \n     def test_different_encodings_dont_break_post(self, httpbin):\n-        r = requests.post(httpbin('post'),\n-            data={'stuff': json.dumps({'a': 123})},\n-            params={'blah': 'asdf1234'},\n-            files={'file': ('test_requests.py', open(__file__, 'rb'))})\n-        assert r.status_code == 200\n+        with open(__file__, 'rb') as f:\n+            r = requests.post(httpbin('post'),\n+                data={'stuff': json.dumps({'a': 123})},\n+                params={'blah': 'asdf1234'},\n+                files={'file': ('test_requests.py', f)})\n+            assert r.status_code == 200\nComment: Let\u2019s pull the `assert` out to the same indentation level as `with`.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "tests/test_requests.py",
    "pr_number": 4766,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 209787773,
    "comment_created_at": "2018-08-13T23:14:55Z"
  },
  {
    "code": "@@ -1330,11 +1332,29 @@ string_replace_strided_loop(\n             goto fail;\n         }\n         else if (i1_isnull || i2_isnull || i3_isnull) {\n-            if (!has_string_na) {\n-                npy_gil_error(PyExc_ValueError,\n-                              \"Null values are not supported as replacement arguments \"\n-                              \"for replace\");\n-                goto fail;\n+            if (has_null && !has_string_na) {\n+                if (i2_isnull || i3_isnull) {\n+                    npy_gil_error(PyExc_ValueError,\n+                                  \"Null values are not supported as search \"\n+                                  \"patterns or replacement strings for \"\n+                                  \"replace\");\n+                    goto fail;\n+                }\n+                else if (i1_isnull) {\n+                    if (has_nan_na) {\n+                        if (NpyString_pack_null(oallocator, ops) < 0) {\n+                            npy_gil_error(PyExc_MemoryError,\n+                                          \"Failed to deallocate string in replace\");\n+                            goto fail;\n+                        }\n+                        goto next_step;\n+                    }\n+                    else {\n+                    npy_gil_error(PyExc_ValueError,",
    "comment": "indentation off",
    "line_number": 1353,
    "enriched": "File: numpy/_core/src/umath/stringdtype_ufuncs.cpp\nCode: @@ -1330,11 +1332,29 @@ string_replace_strided_loop(\n             goto fail;\n         }\n         else if (i1_isnull || i2_isnull || i3_isnull) {\n-            if (!has_string_na) {\n-                npy_gil_error(PyExc_ValueError,\n-                              \"Null values are not supported as replacement arguments \"\n-                              \"for replace\");\n-                goto fail;\n+            if (has_null && !has_string_na) {\n+                if (i2_isnull || i3_isnull) {\n+                    npy_gil_error(PyExc_ValueError,\n+                                  \"Null values are not supported as search \"\n+                                  \"patterns or replacement strings for \"\n+                                  \"replace\");\n+                    goto fail;\n+                }\n+                else if (i1_isnull) {\n+                    if (has_nan_na) {\n+                        if (NpyString_pack_null(oallocator, ops) < 0) {\n+                            npy_gil_error(PyExc_MemoryError,\n+                                          \"Failed to deallocate string in replace\");\n+                            goto fail;\n+                        }\n+                        goto next_step;\n+                    }\n+                    else {\n+                    npy_gil_error(PyExc_ValueError,\nComment: Indentation off",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "numpy/_core/src/umath/stringdtype_ufuncs.cpp",
    "pr_number": 26355,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 1582157278,
    "comment_created_at": "2024-04-28T13:17:52Z"
  },
  {
    "code": "@@ -1165,6 +1165,10 @@ User-defined data types\n     *totype*. Any old casting function is over-written. A ``0`` is\n     returned on success or a ``-1`` on failure.\n \n+    .. c:type:: PyArray_VectorUnaryFunc",
    "comment": "is the indentation correct?",
    "line_number": 1168,
    "enriched": "File: doc/source/reference/c-api/array.rst\nCode: @@ -1165,6 +1165,10 @@ User-defined data types\n     *totype*. Any old casting function is over-written. A ``0`` is\n     returned on success or a ``-1`` on failure.\n \n+    .. c:type:: PyArray_VectorUnaryFunc\nComment: Is the indentation correct?",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "doc/source/reference/c-api/array.rst",
    "pr_number": 25851,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 1493870088,
    "comment_created_at": "2024-02-18T23:02:03Z"
  },
  {
    "code": "@@ -2092,4 +2092,4 @@ def test_even_thresholds_correctness_2(self, metric_cls):\n \n \n if __name__ == '__main__':\n-  tf.test.main()\n+    tf.test.main()",
    "comment": "corrected indentation from two spaces to ideal 4 spaces.",
    "line_number": 2095,
    "enriched": "File: keras/metrics/confusion_matrix_test.py\nCode: @@ -2092,4 +2092,4 @@ def test_even_thresholds_correctness_2(self, metric_cls):\n \n \n if __name__ == '__main__':\n-  tf.test.main()\n+    tf.test.main()\nComment: Corrected Indentation from two spaces to ideal 4 spaces.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "keras/metrics/confusion_matrix_test.py",
    "pr_number": 16695,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 900691008,
    "comment_created_at": "2022-06-18T03:30:13Z"
  },
  {
    "code": "@@ -522,6 +510,15 @@ def create(cls, *children, **props) -> Component:\n             height=props.pop(\"height\", \"100%\"),\n         )\n \n+    def get_event_triggers(self) -> dict[str, Union[Var, Any]]:\n+        \"\"\"Get the event triggers that pass the component's value to the handler.\n+        Returns:\n+            A dict mapping the event trigger to the var that is passed to the handler.\n+        \"\"\"\n+        return {\n+            EventTriggers.ON_ANIMATION_START: lambda: [],\n+            EventTriggers.ON_ANIMATION_START: lambda: [],",
    "comment": "typo with copy/paste? should be end i guess",
    "line_number": 520,
    "enriched": "File: reflex/components/recharts/charts.py\nCode: @@ -522,6 +510,15 @@ def create(cls, *children, **props) -> Component:\n             height=props.pop(\"height\", \"100%\"),\n         )\n \n+    def get_event_triggers(self) -> dict[str, Union[Var, Any]]:\n+        \"\"\"Get the event triggers that pass the component's value to the handler.\n+        Returns:\n+            A dict mapping the event trigger to the var that is passed to the handler.\n+        \"\"\"\n+        return {\n+            EventTriggers.ON_ANIMATION_START: lambda: [],\n+            EventTriggers.ON_ANIMATION_START: lambda: [],\nComment: typo with copy/paste? should be END I guess",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "reflex/components/recharts/charts.py",
    "pr_number": 3430,
    "repo": "reflex",
    "owner": "reflex-dev",
    "comment_id": 1625156717,
    "comment_created_at": "2024-06-03T23:25:37Z"
  },
  {
    "code": "@@ -1,64 +1,183 @@\n+import importlib\n from functools import wraps\n+from typing import Protocol, runtime_checkable\n \n+import numpy as np\n from scipy.sparse import issparse\n \n from .._config import get_config\n-from . import check_pandas_support\n from ._available_if import available_if\n-from .validation import _is_pandas_df\n \n \n-def _wrap_in_pandas_container(\n-    data_to_wrap,\n-    *,\n-    columns,\n-    index=None,\n-):\n-    \"\"\"Create a Pandas DataFrame.\n+def check_library_installed(library):\n+    \"\"\"Check library is installed\"\"\"\n+    try:\n+        return importlib.import_module(library)\n+    except ImportError as e:\n+        raise ImportError(\n+            f\"Setting output container to '{library}' requires {library} to be\"\n+            \" installed\"\n+        ) from e\n \n-    If `data_to_wrap` is a DataFrame, then the `columns` and `index` will be changed\n-    inplace. If `data_to_wrap` is a ndarray, then a new DataFrame is created with\n-    `columns` and `index`.\n \n-    Parameters\n-    ----------\n-    data_to_wrap : {ndarray, dataframe}\n-        Data to be wrapped as pandas dataframe.\n+def get_columns(columns):\n+    if callable(columns):\n+        try:\n+            return columns()\n+        except Exception:\n+            return None\n+    return columns\n \n-    columns : callable, ndarray, or None\n-        The column names or a callable that returns the column names. The\n-        callable is useful if the column names require some computation.\n-        If `columns` is a callable that raises an error, `columns` will have\n-        the same semantics as `None`. If `None` and `data_to_wrap` is already a\n-        dataframe, then the column names are not changed. If `None` and\n-        `data_to_wrap` is **not** a dataframe, then columns are\n-        `range(n_features)`.\n \n-    index : array-like, default=None\n-        Index for data. `index` is ignored if `data_to_wrap` is already a DataFrame.\n+@runtime_checkable\n+class ContainerAdapaterProtocol(Protocol):",
    "comment": "same typo in multiple places\r\n",
    "line_number": 33,
    "enriched": "File: sklearn/utils/_set_output.py\nCode: @@ -1,64 +1,183 @@\n+import importlib\n from functools import wraps\n+from typing import Protocol, runtime_checkable\n \n+import numpy as np\n from scipy.sparse import issparse\n \n from .._config import get_config\n-from . import check_pandas_support\n from ._available_if import available_if\n-from .validation import _is_pandas_df\n \n \n-def _wrap_in_pandas_container(\n-    data_to_wrap,\n-    *,\n-    columns,\n-    index=None,\n-):\n-    \"\"\"Create a Pandas DataFrame.\n+def check_library_installed(library):\n+    \"\"\"Check library is installed\"\"\"\n+    try:\n+        return importlib.import_module(library)\n+    except ImportError as e:\n+        raise ImportError(\n+            f\"Setting output container to '{library}' requires {library} to be\"\n+            \" installed\"\n+        ) from e\n \n-    If `data_to_wrap` is a DataFrame, then the `columns` and `index` will be changed\n-    inplace. If `data_to_wrap` is a ndarray, then a new DataFrame is created with\n-    `columns` and `index`.\n \n-    Parameters\n-    ----------\n-    data_to_wrap : {ndarray, dataframe}\n-        Data to be wrapped as pandas dataframe.\n+def get_columns(columns):\n+    if callable(columns):\n+        try:\n+            return columns()\n+        except Exception:\n+            return None\n+    return columns\n \n-    columns : callable, ndarray, or None\n-        The column names or a callable that returns the column names. The\n-        callable is useful if the column names require some computation.\n-        If `columns` is a callable that raises an error, `columns` will have\n-        the same semantics as `None`. If `None` and `data_to_wrap` is already a\n-        dataframe, then the column names are not changed. If `None` and\n-        `data_to_wrap` is **not** a dataframe, then columns are\n-        `range(n_features)`.\n \n-    index : array-like, default=None\n-        Index for data. `index` is ignored if `data_to_wrap` is already a DataFrame.\n+@runtime_checkable\n+class ContainerAdapaterProtocol(Protocol):\nComment: Same typo in multiple places\r\n```suggestion\r\nclass ContainerAdapterProtocol(Protocol):\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "sklearn/utils/_set_output.py",
    "pr_number": 27315,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1376456083,
    "comment_created_at": "2023-10-30T15:57:22Z"
  },
  {
    "code": "@@ -313,7 +315,7 @@ def no_available_apps(self):\n             \"Please define available_apps in TransactionTestCase and its subclasses.\"\n         )\n \n-    TransactionTestCase.available_apps = property(no_available_apps)\n+    TransactionTestCase.available_apps = classproperty(no_available_apps)",
    "comment": "suggest we rename the argument of no_available_apps from self to cls.",
    "line_number": 317,
    "enriched": "File: tests/runtests.py\nCode: @@ -313,7 +315,7 @@ def no_available_apps(self):\n             \"Please define available_apps in TransactionTestCase and its subclasses.\"\n         )\n \n-    TransactionTestCase.available_apps = property(no_available_apps)\n+    TransactionTestCase.available_apps = classproperty(no_available_apps)\nComment: Suggest we rename the argument of `no_available_apps` from `self` to `cls`.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "tests/runtests.py",
    "pr_number": 18628,
    "repo": "django",
    "owner": "django",
    "comment_id": 1863556042,
    "comment_created_at": "2024-11-29T13:51:28Z"
  },
  {
    "code": "@@ -1169,7 +1169,7 @@ def now(parser, token):\n     return NowNode(format_string, asvar)\n \n \n-@register.simple_tag(takes_context=True)\n+@register.simple_tag(name=\"querystring\", takes_context=True)\n def query_string(context, query_dict=None, **kwargs):",
    "comment": "since the tests were renamed to use querystring, is there a reason to not rename the function as well?",
    "line_number": 1173,
    "enriched": "File: django/template/defaulttags.py\nCode: @@ -1169,7 +1169,7 @@ def now(parser, token):\n     return NowNode(format_string, asvar)\n \n \n-@register.simple_tag(takes_context=True)\n+@register.simple_tag(name=\"querystring\", takes_context=True)\n def query_string(context, query_dict=None, **kwargs):\nComment: Since the tests were renamed to use `querystring`, is there a reason to not rename the function as well?",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "django/template/defaulttags.py",
    "pr_number": 18323,
    "repo": "django",
    "owner": "django",
    "comment_id": 1677857276,
    "comment_created_at": "2024-07-15T13:44:05Z"
  },
  {
    "code": "@@ -126,11 +126,31 @@ You could also write a decorator to encapsulate this logic::\n Just be aware that this logic will disable the signals whenever fixtures are\n deserialized, not just during ``loaddata``.\n \n-Note that the order in which fixture files are processed is undefined. However,\n-all fixture data is installed as a single transaction, so data in\n-one fixture can reference data in another fixture. If the database backend\n-supports row-level constraints, these constraints will be checked at the\n-end of the transaction.\n+Fixtures can be loaded in a specified order. For example:\n+\n+.. code-block:: shell\n+\n+    manage.py loaddata foo bar baz\n+\n+or (in a test case)\n+\n+.. code-block:: shell\n+\n+    Fixtures = ['foo', 'bar', 'baz']",
    "comment": "if i were writing this, i might use more real-sounding names, like people, places, events - i find that helps me imagine an example better.",
    "line_number": 139,
    "enriched": "File: docs/topics/db/fixtures.txt\nCode: @@ -126,11 +126,31 @@ You could also write a decorator to encapsulate this logic::\n Just be aware that this logic will disable the signals whenever fixtures are\n deserialized, not just during ``loaddata``.\n \n-Note that the order in which fixture files are processed is undefined. However,\n-all fixture data is installed as a single transaction, so data in\n-one fixture can reference data in another fixture. If the database backend\n-supports row-level constraints, these constraints will be checked at the\n-end of the transaction.\n+Fixtures can be loaded in a specified order. For example:\n+\n+.. code-block:: shell\n+\n+    manage.py loaddata foo bar baz\n+\n+or (in a test case)\n+\n+.. code-block:: shell\n+\n+    Fixtures = ['foo', 'bar', 'baz']\nComment: If I were writing this, I might use more real-sounding names, like `people`, `places`, `events` - I find that helps me imagine an example better.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/topics/db/fixtures.txt",
    "pr_number": 17384,
    "repo": "django",
    "owner": "django",
    "comment_id": 1366078761,
    "comment_created_at": "2023-10-19T20:30:23Z"
  },
  {
    "code": "@@ -297,10 +297,15 @@ def lookup_field(name, obj, model_admin=None):\n             attr = getattr(model_admin, name)\n             value = attr(obj)\n         else:\n-            attr = getattr(obj, name)\n+            sentinel = object()\n+            attr = getattr(obj, name, sentinel)\n             if callable(attr):\n                 value = attr()\n             else:\n+                if attr is sentinel:\n+                    attr = obj\n+                    for part in name.split(LOOKUP_SEP):\n+                        attr = getattr(attr, part)",
    "comment": "there is a potentially un-handled attributeerror in this line, for example when attr is a model field with null=true.\r\n\r\nan test that triggers this could be:\r\n\r\ndiff\r\ndiff --git a/tests/admin_changelist/tests.py b/tests/admin_changelist/tests.py\r\nindex 0db8cda4a0..9da3b4d8a2 100644\r\n--- a/tests/admin_changelist/tests.py\r\n+++ b/tests/admin_changelist/tests.py\r\n@@ -1629,6 +1629,21 @@ class changelisttests(testcase):\r\n         self.assertcontains(response, parent.name)\r\n         self.assertcontains(response, child.name)\r\n \r\n+    def test_list_display_foreign_field_null(self):\r\n+        parent = parent.objects.create(name=\"i am your father\")\r\n+        child = child.objects.create(name=\"i am your child\", parent=parent)\r\n+        grandchild.objects.create(name=\"i am parentless\", parent=none)\r\n+        request = self.factory.get(\"/grandchild/\")\r\n+        request.user = self.superuser\r\n+\r\n+        class grandchildadmin(admin.modeladmin):\r\n+            list_display = [\"parent__name\", \"parent__parent__name\"]\r\n+\r\n+        m = grandchildadmin(grandchild, custom_site)\r\n+        response = m.changelist_view(request)\r\n+        self.assertcontains(response, parent.name)\r\n+        self.assertcontains(response, child.name)\r\n+\r\n \r\n class getadminlogtests(testcase):\r\n     def test_custom_user_pk_not_named_id(self):\r\n\r\n\r\nerror is:\r\n\r\nattributeerror: 'nonetype' object has no attribute 'name'\r\n",
    "line_number": 308,
    "enriched": "File: django/contrib/admin/utils.py\nCode: @@ -297,10 +297,15 @@ def lookup_field(name, obj, model_admin=None):\n             attr = getattr(model_admin, name)\n             value = attr(obj)\n         else:\n-            attr = getattr(obj, name)\n+            sentinel = object()\n+            attr = getattr(obj, name, sentinel)\n             if callable(attr):\n                 value = attr()\n             else:\n+                if attr is sentinel:\n+                    attr = obj\n+                    for part in name.split(LOOKUP_SEP):\n+                        attr = getattr(attr, part)\nComment: There is a potentially un-handled `AttributeError` in this line, for example when `attr` is a model field with `null=True`.\r\n\r\nAn test that triggers this could be:\r\n\r\n```diff\r\ndiff --git a/tests/admin_changelist/tests.py b/tests/admin_changelist/tests.py\r\nindex 0db8cda4a0..9da3b4d8a2 100644\r\n--- a/tests/admin_changelist/tests.py\r\n+++ b/tests/admin_changelist/tests.py\r\n@@ -1629,6 +1629,21 @@ class ChangeListTests(TestCase):\r\n         self.assertContains(response, parent.name)\r\n         self.assertContains(response, child.name)\r\n \r\n+    def test_list_display_foreign_field_null(self):\r\n+        parent = Parent.objects.create(name=\"I am your father\")\r\n+        child = Child.objects.create(name=\"I am your child\", parent=parent)\r\n+        GrandChild.objects.create(name=\"I am parentless\", parent=None)\r\n+        request = self.factory.get(\"/grandchild/\")\r\n+        request.user = self.superuser\r\n+\r\n+        class GrandChildAdmin(admin.ModelAdmin):\r\n+            list_display = [\"parent__name\", \"parent__parent__name\"]\r\n+\r\n+        m = GrandChildAdmin(GrandChild, custom_site)\r\n+        response = m.changelist_view(request)\r\n+        self.assertContains(response, parent.name)\r\n+        self.assertContains(response, child.name)\r\n+\r\n \r\n class GetAdminLogTests(TestCase):\r\n     def test_custom_user_pk_not_named_id(self):\r\n```\r\n\r\nError is:\r\n```\r\nAttributeError: 'NoneType' object has no attribute 'name'\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "django/contrib/admin/utils.py",
    "pr_number": 17357,
    "repo": "django",
    "owner": "django",
    "comment_id": 1358421019,
    "comment_created_at": "2023-10-13T15:33:38Z"
  },
  {
    "code": "@@ -1554,6 +1554,19 @@ def test_validate_multiline_headers(self):\n                 \"Subject\\nMultiline\", \"Content\", \"from@example.com\", [\"to@example.com\"]\n             )\n \n+    def test_mutate_after_send(self) -> None:",
    "comment": "please remove type annotations. we don't currently use them in django.",
    "line_number": 1557,
    "enriched": "File: tests/mail/tests.py\nCode: @@ -1554,6 +1554,19 @@ def test_validate_multiline_headers(self):\n                 \"Subject\\nMultiline\", \"Content\", \"from@example.com\", [\"to@example.com\"]\n             )\n \n+    def test_mutate_after_send(self) -> None:\nComment: Please remove type annotations. We don't currently use them in Django.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/mail/tests.py",
    "pr_number": 17377,
    "repo": "django",
    "owner": "django",
    "comment_id": 1365092791,
    "comment_created_at": "2023-10-19T08:13:29Z"
  },
  {
    "code": "@@ -3755,7 +3755,7 @@ def feed_entry(name): reason = self._get_text(pemr, 'reason') or get_first(playability_statuses, 'reason') subreason = clean_html(self._get_text(pemr, 'subreason') or '') if subreason: - if subreason == 'The uploader has not made this video available in your country.': + if subreason == 'The uploader has not made this video available in your country':",
    "comment": "nice catch",
    "line_number": 3758,
    "enriched": "File: yt_dlp/extractor/youtube/_video.py\nCode: @@ -3755,7 +3755,7 @@ def feed_entry(name):\n             reason = self._get_text(pemr, 'reason') or get_first(playability_statuses, 'reason')\n             subreason = clean_html(self._get_text(pemr, 'subreason') or '')\n             if subreason:\n-                if subreason == 'The uploader has not made this video available in your country.':\n+                if subreason == 'The uploader has not made this video available in your country':\nComment: Nice catch",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "yt_dlp/extractor/youtube/_video.py",
    "pr_number": 13217,
    "repo": "yt-dlp",
    "owner": "yt-dlp",
    "comment_id": 2094817074,
    "comment_created_at": "2025-05-19T04:31:58Z"
  },
  {
    "code": "@@ -97,13 +100,25 @@ def __init__(self, *args, **kwargs): # Check whether the key is a component prop. if utils._issubclass(field_type, Var): - # Convert any constants into vars and make sure the types match. - kwargs[key] = Var.create(value) - passed_type = kwargs[key].type_ - expected_type = fields[key].outer_type_.__args__[0] - assert utils._issubclass( - passed_type, expected_type - ), f\"Invalid var passed for {key}, expected {expected_type}, got {passed_type}.\" + try: + # Try to create a var from the value. + kwargs[key] = Var.create(value) + + # Check that the var type is not None. + if kwargs[key] is None: + raise TypeError + + # Get the passed type and the var type. + passed_type = kwargs[key].type_ + expected_type = fields[key].outer_type_.__args__[0] + except TypeError: + # If it is not a valid var, check the base types. + passed_type = type(value) + expected_type = fields[key].outer_type_ + if not utils._issubclass(passed_type, expected_type): + raise TypeError( + f\"Invalid var passed for prop {key}, expected type {expected_type}, got value {value} of type {passed_type}.\"",
    "comment": "nice defiantly needed this!",
    "line_number": 120,
    "enriched": "File: pynecone/components/component.py\nCode: @@ -97,13 +100,25 @@ def __init__(self, *args, **kwargs):\n \n             # Check whether the key is a component prop.\n             if utils._issubclass(field_type, Var):\n-                # Convert any constants into vars and make sure the types match.\n-                kwargs[key] = Var.create(value)\n-                passed_type = kwargs[key].type_\n-                expected_type = fields[key].outer_type_.__args__[0]\n-                assert utils._issubclass(\n-                    passed_type, expected_type\n-                ), f\"Invalid var passed for {key}, expected {expected_type}, got {passed_type}.\"\n+                try:\n+                    # Try to create a var from the value.\n+                    kwargs[key] = Var.create(value)\n+\n+                    # Check that the var type is not None.\n+                    if kwargs[key] is None:\n+                        raise TypeError\n+\n+                    # Get the passed type and the var type.\n+                    passed_type = kwargs[key].type_\n+                    expected_type = fields[key].outer_type_.__args__[0]\n+                except TypeError:\n+                    # If it is not a valid var, check the base types.\n+                    passed_type = type(value)\n+                    expected_type = fields[key].outer_type_\n+                if not utils._issubclass(passed_type, expected_type):\n+                    raise TypeError(\n+                        f\"Invalid var passed for prop {key}, expected type {expected_type}, got value {value} of type {passed_type}.\"\nComment: Nice defiantly needed this!",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "pynecone/components/component.py",
    "pr_number": 84,
    "repo": "reflex",
    "owner": "reflex-dev",
    "comment_id": 1047545467,
    "comment_created_at": "2022-12-13T18:09:33Z"
  },
  {
    "code": "@@ -395,6 +395,8 @@ backends. * The new :meth:`Model._is_pk_set() <django.db.models.Model._is_pk_set>` method allows checking if a Model instance's primary key is defined. +* ``BaseDatabaseOperations.adapt_decimalfield_value()`` is now a no-op, simply + returning the given value.",
    "comment": "thanks for the notes!",
    "line_number": 399,
    "enriched": "File: docs/releases/5.2.txt\nCode: @@ -395,6 +395,8 @@ backends.\n * The new :meth:`Model._is_pk_set() <django.db.models.Model._is_pk_set>` method\n   allows checking if a Model instance's primary key is defined.\n \n+* ``BaseDatabaseOperations.adapt_decimalfield_value()`` is now a no-op, simply\n+  returning the given value.\nComment: Thanks for the notes!",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "docs/releases/5.2.txt",
    "pr_number": 18895,
    "repo": "django",
    "owner": "django",
    "comment_id": 1874516882,
    "comment_created_at": "2024-12-07T16:18:17Z"
  },
  {
    "code": "@@ -57,7 +58,9 @@ def _store_in_thread(self, file): @implementer(IFeedStorage) class StdoutFeedStorage(object): - def __init__(self, uri, _stdout=sys.stdout): + def __init__(self, uri, _stdout=None): + if not _stdout: + _stdout = sys.stdout if six.PY2 else sys.stdout.buffer",
    "comment": "nice catch! :+1:",
    "line_number": 63,
    "enriched": "File: scrapy/extensions/feedexport.py\nCode: @@ -57,7 +58,9 @@ def _store_in_thread(self, file):\n @implementer(IFeedStorage)\n class StdoutFeedStorage(object):\n \n-    def __init__(self, uri, _stdout=sys.stdout):\n+    def __init__(self, uri, _stdout=None):\n+        if not _stdout:\n+            _stdout = sys.stdout if six.PY2 else sys.stdout.buffer\nComment: Nice catch! :+1: \n",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "scrapy/extensions/feedexport.py",
    "pr_number": 1769,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 52111896,
    "comment_created_at": "2016-02-07T03:16:47Z"
  },
  {
    "code": "@@ -33,18 +33,12 @@ jobs: inputs: versionSpec: '3.9' - bash: | + source build_tools/shared.sh # Include pytest compatibility with mypy - pip install pytest flake8 mypy==0.961 black==22.3.0 + pip install pytest flake8 $(get_dep mypy min) $(get_dep black min)",
    "comment": "nice i was not aware these versions were in sklearn/_min_dependencies.py",
    "line_number": 38,
    "enriched": "File: azure-pipelines.yml\nCode: @@ -33,18 +33,12 @@ jobs:\n       inputs:\n         versionSpec: '3.9'\n     - bash: |\n+        source build_tools/shared.sh\n         # Include pytest compatibility with mypy\n-        pip install pytest flake8 mypy==0.961 black==22.3.0\n+        pip install pytest flake8 $(get_dep mypy min) $(get_dep black min)\nComment: Nice I was not aware these versions were in `sklearn/_min_dependencies.py`",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "azure-pipelines.yml",
    "pr_number": 25475,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1086639949,
    "comment_created_at": "2023-01-25T13:27:47Z"
  },
  {
    "code": "@@ -699,6 +714,30 @@ def test_html5_meta_charset(self): self._assert_response_values(r1, 'gb2312', body) +class JsonResponseTest(TextResponseTest): + + response_class = JsonResponse + + def test_js_encoding(self): + body = b\"[{\\\"id\\\": \\\"1234\\\", \\\"type\\\": \\\"event\\\"}]\"",
    "comment": "a little bit cleaner this way",
    "line_number": 722,
    "enriched": "File: tests/test_http_response.py\nCode: @@ -699,6 +714,30 @@ def test_html5_meta_charset(self):\n         self._assert_response_values(r1, 'gb2312', body)\n \n \n+class JsonResponseTest(TextResponseTest):\n+\n+    response_class = JsonResponse\n+\n+    def test_js_encoding(self):\n+        body = b\"[{\\\"id\\\": \\\"1234\\\", \\\"type\\\": \\\"event\\\"}]\"\nComment: ```suggestion\r\n        body = \"\"\"[{\"id\": \"1234\", \"type\": \"event\"}]\"\"\"\r\n```\r\nA little bit cleaner this way",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "tests/test_http_response.py",
    "pr_number": 4460,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 399618813,
    "comment_created_at": "2020-03-28T04:41:05Z"
  },
  {
    "code": "@@ -1195,9 +1195,19 @@ def english_upper(s): docstrings.get('numpy._core.umath.count'), None, ), +'index': + Ufunc(4, 1, None, + docstrings.get('numpy._core.umath.index'), + None, + ), +'rindex': + Ufunc(4, 1, None, + docstrings.get('numpy._core.umath.rindex'), + None, + ), '_replace': Ufunc(4, 1, None, - docstrings.get('numpy.core.umath._replace'), + docstrings.get('numpy._core.umath._replace'),",
    "comment": "good catch!",
    "line_number": 1210,
    "enriched": "File: numpy/_core/code_generators/generate_umath.py\nCode: @@ -1195,9 +1195,19 @@ def english_upper(s):\n           docstrings.get('numpy._core.umath.count'),\n           None,\n           ),\n+'index':\n+    Ufunc(4, 1, None,\n+          docstrings.get('numpy._core.umath.index'),\n+          None,\n+          ),\n+'rindex':\n+    Ufunc(4, 1, None,\n+          docstrings.get('numpy._core.umath.rindex'),\n+          None,\n+          ),\n '_replace':\n     Ufunc(4, 1, None,\n-          docstrings.get('numpy.core.umath._replace'),\n+          docstrings.get('numpy._core.umath._replace'),\nComment: good catch!",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "numpy/_core/code_generators/generate_umath.py",
    "pr_number": 25775,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 1483182928,
    "comment_created_at": "2024-02-08T15:38:46Z"
  },
  {
    "code": "@@ -4261,7 +4262,8 @@ def _assisted_decoding( added_len, is_decoder_attention=True, ) - else: + # some (V)LLMs have hard requirement on SDPA and thus never return attn + elif outputs.attentions[0] is not None:",
    "comment": "ouuu very nice!",
    "line_number": 4266,
    "enriched": "File: src/transformers/generation/utils.py\nCode: @@ -4261,7 +4262,8 @@ def _assisted_decoding(\n                             added_len,\n                             is_decoder_attention=True,\n                         )\n-                    else:\n+                    # some (V)LLMs have hard requirement on SDPA and thus never return attn\n+                    elif outputs.attentions[0] is not None:\nComment: ouuu very nice! ",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "src/transformers/generation/utils.py",
    "pr_number": 34062,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 1795374975,
    "comment_created_at": "2024-10-10T12:57:23Z"
  },
  {
    "code": "@@ -12,14 +12,14 @@ community build applications and use free, public APIs quickly and easily. Pull Current API entry format: -| API | Description | Auth | HTTPS | CORS | -| --- | --- | --- | --- | --- | -| API Title(Link to API documentation) | Description of API | Does this API require authentication? * | Does the API support HTTPS? | Does the API support [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)? * |",
    "comment": "good approach",
    "line_number": 17,
    "enriched": "File: CONTRIBUTING.md\nCode: @@ -12,14 +12,14 @@ community build applications and use free, public APIs quickly and easily. Pull\n \n Current API entry format:\n \n-| API | Description | Auth | HTTPS | CORS |\n-| --- | --- | --- | --- | --- |\n-| API Title(Link to API documentation) | Description of API | Does this API require authentication? * | Does the API support HTTPS? | Does the API support [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)? * |\nComment: good approach",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "CONTRIBUTING.md",
    "pr_number": 4382,
    "repo": "public-apis",
    "owner": "public-apis",
    "comment_id": 2435068624,
    "comment_created_at": "2025-10-16T08:39:28Z"
  },
  {
    "code": "@@ -180,3 +146,9 @@ repos: |^NOTICES$ |^proto/streamlit/proto/openmetrics_data_model.proto$ |\\.snap$ + - id: check-added-large-files + - id: check-json",
    "comment": "**praise:** nice, these are good additions!",
    "line_number": 150,
    "enriched": "File: .pre-commit-config.yaml\nCode: @@ -180,3 +146,9 @@ repos:\n           |^NOTICES$\n           |^proto/streamlit/proto/openmetrics_data_model.proto$\n           |\\.snap$\n+      - id: check-added-large-files\n+      - id: check-json\nComment: **praise:** Nice, these are good additions!",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": ".pre-commit-config.yaml",
    "pr_number": 9856,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1839336717,
    "comment_created_at": "2024-11-13T03:28:48Z"
  },
  {
    "code": "@@ -36,7 +36,9 @@ I'm getting a ``UnicodeDecodeError``. What am I doing wrong? This class of errors happen when a bytestring containing non-ASCII sequences is transformed into a Unicode string and the specified encoding is incorrect. The -output generally looks like this:: +output generally looks like this: + +.. code-block:: pytb",
    "comment": "nice, i didn't even know about pytb!",
    "line_number": 41,
    "enriched": "File: docs/faq/troubleshooting.txt\nCode: @@ -36,7 +36,9 @@ I'm getting a ``UnicodeDecodeError``. What am I doing wrong?\n \n This class of errors happen when a bytestring containing non-ASCII sequences is\n transformed into a Unicode string and the specified encoding is incorrect. The\n-output generally looks like this::\n+output generally looks like this:\n+\n+.. code-block:: pytb\nComment: Nice, I didn't even know about `pytb`!",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "docs/faq/troubleshooting.txt",
    "pr_number": 16537,
    "repo": "django",
    "owner": "django",
    "comment_id": 1101729092,
    "comment_created_at": "2023-02-09T16:27:50Z"
  },
  {
    "code": "@@ @@ -909,12 +910,22 @@ def _check_max_num(self, cls):\n         else:\n             return []\n \n+    def _check_min_num(self, cls):\n+        \"\"\" Check that min_num is an integer. \"\"\"\n+\n+        if cls.min_num is None:\n+            return []\n+        elif not isinstance(cls.min_num, int):\n+            return must_be('an integer', option='min_num', obj=cls, id='admin.E205')\n+        else:\n+            \n\u2026",
    "comment": "codes are documented in docs/ref/checks.txt. i guess if we backport this to 1.7 we can change it since it hasn't been released yet. need to add the one for min_num to the docs.",
    "line_number": 917,
    "enriched": "File: django/contrib/admin/checks.py\nCode: @@ @@ -909,12 +910,22 @@ def _check_max_num(self, cls):\n         else:\n             return []\n \n+    def _check_min_num(self, cls):\n+        \"\"\" Check that min_num is an integer. \"\"\"\n+\n+        if cls.min_num is None:\n+            return []\n+        elif not isinstance(cls.min_num, int):\n+            return must_be('an integer', option='min_num', obj=cls, id='admin.E205')\n+        else:\n+            \n\u2026\nComment: codes are documented in `docs/ref/checks.txt`. I guess if we backport this to 1.7 we can change it since it hasn't been released yet. Need to add the one for `min_num` to the docs.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "django/contrib/admin/checks.py",
    "pr_number": 2652,
    "repo": "django",
    "owner": "django",
    "comment_id": 12631735,
    "comment_created_at": "2014-05-14T11:51:07Z"
  },
  {
    "code": "@@ @@ -135,7 +136,8 @@ def __new__(cls, name, bases, attrs):\n \n         # Bail out early if we have already created this class.\n         m = get_model(new_class._meta.app_label, name,\n-                      seed_cache=False, only_installed=False)\n+                      seed_cache=False, only_installed=False,\n+                      module=proxy_model_module)",
    "comment": "i wonder if this actually works as intended - you now have two models with same app-label and name in app cache. iirc contenttypes for example are keyed by app-label and name, so other parts of django aren't necessary ready for this change.\n\nit might be better to enforce app_label, name key for models, that is it is illegal to have a proxy model with same name as the original model.",
    "line_number": 140,
    "enriched": "File: django/db/models/base.py\nCode: @@ @@ -135,7 +136,8 @@ def __new__(cls, name, bases, attrs):\n \n         # Bail out early if we have already created this class.\n         m = get_model(new_class._meta.app_label, name,\n-                      seed_cache=False, only_installed=False)\n+                      seed_cache=False, only_installed=False,\n+                      module=proxy_model_module)\nComment: I wonder if this actually works as intended - you now have two models with same app-label and name in app cache. IIRC contenttypes for example are keyed by app-label and name, so other parts of Django aren't necessary ready for this change.\n\nIt might be better to enforce app_label, name key for models, that is it is illegal to have a proxy model with same name as the original model.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "django/db/models/base.py",
    "pr_number": 1817,
    "repo": "django",
    "owner": "django",
    "comment_id": 7240699,
    "comment_created_at": "2013-10-28T06:08:03Z"
  },
  {
    "code": "@@ @@ -221,6 +223,62 @@ def test_custom_get_extra_form(self):\n         self.assertContains(response, max_forms_input % 2)\n         self.assertContains(response, total_forms_hidden)\n \n+    def test_min_num(self):\n+        \"\"\"\n+        Ensure that min_num and extra determine number of forms.\n+        \"\"\"\n+        class MinNumInline(TabularInline):\n+            model = BinaryTree\n+            min_num = \n\u2026",
    "comment": "it requires either making additional models and modeladmins (because the urls are not unique) _or_ dynamically changing the registered modeladmin for the model, then using reload(settings.root_urlconf) all over the place.\n\nmy impression was that adding models was something you wanted to avoid, since it slows down the tests, and reloading urlconfs is messy.",
    "line_number": 241,
    "enriched": "File: tests/admin_inlines/tests.py\nCode: @@ @@ -221,6 +223,62 @@ def test_custom_get_extra_form(self):\n         self.assertContains(response, max_forms_input % 2)\n         self.assertContains(response, total_forms_hidden)\n \n+    def test_min_num(self):\n+        \"\"\"\n+        Ensure that min_num and extra determine number of forms.\n+        \"\"\"\n+        class MinNumInline(TabularInline):\n+            model = BinaryTree\n+            min_num = \n\u2026\nComment: It requires either making additional models and modeladmins (because the URLs are not unique) _or_ dynamically changing the registered ModelAdmin for the model, then using `reload(settings.ROOT_URLCONF)` all over the place.\n\nMy impression was that adding models was something you wanted to avoid, since it slows down the tests, and reloading urlconfs is messy.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "tests/admin_inlines/tests.py",
    "pr_number": 2652,
    "repo": "django",
    "owner": "django",
    "comment_id": 12649893,
    "comment_created_at": "2014-05-14T17:45:13Z"
  },
  {
    "code": "@@ @@ -221,6 +223,62 @@ def test_custom_get_extra_form(self):\n         self.assertContains(response, max_forms_input % 2)\n         self.assertContains(response, total_forms_hidden)\n \n+    def test_min_num(self):\n+        \"\"\"\n+        Ensure that min_num and extra determine number of forms.\n+        \"\"\"\n+        class MinNumInline(TabularInline):\n+            model = BinaryTree\n+            min_num = \n\u2026",
    "comment": "gotcha, okay i think this is acceptable.",
    "line_number": 241,
    "enriched": "File: tests/admin_inlines/tests.py\nCode: @@ @@ -221,6 +223,62 @@ def test_custom_get_extra_form(self):\n         self.assertContains(response, max_forms_input % 2)\n         self.assertContains(response, total_forms_hidden)\n \n+    def test_min_num(self):\n+        \"\"\"\n+        Ensure that min_num and extra determine number of forms.\n+        \"\"\"\n+        class MinNumInline(TabularInline):\n+            model = BinaryTree\n+            min_num = \n\u2026\nComment: Gotcha, okay I think this is acceptable.",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "tests/admin_inlines/tests.py",
    "pr_number": 2652,
    "repo": "django",
    "owner": "django",
    "comment_id": 12650655,
    "comment_created_at": "2014-05-14T17:57:22Z"
  },
  {
    "code": "@@ @@ -238,6 +238,27 @@ Django quotes column and table names behind the scenes.\n     If ``proxy = True``, a model which subclasses another model will be treated as\n     a :ref:`proxy model <proxy-models>`.\n \n+``select_on_save``\n+------------------\n+\n+.. attribute:: Options.select_on_save\n+\n+    .. versionadded:: 1.6\n+\n+    Determines if Django will use the pre-1.6\n+    :meth:`django.db.models.Model.s\n\u2026",
    "comment": "algorithm",
    "line_number": 251,
    "enriched": "File: docs/ref/models/options.txt\nCode: @@ @@ -238,6 +238,27 @@ Django quotes column and table names behind the scenes.\n     If ``proxy = True``, a model which subclasses another model will be treated as\n     a :ref:`proxy model <proxy-models>`.\n \n+``select_on_save``\n+------------------\n+\n+.. attribute:: Options.select_on_save\n+\n+    .. versionadded:: 1.6\n+\n+    Determines if Django will use the pre-1.6\n+    :meth:`django.db.models.Model.s\n\u2026\nComment: algorithm",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/ref/models/options.txt",
    "pr_number": 1522,
    "repo": "django",
    "owner": "django",
    "comment_id": 6089851,
    "comment_created_at": "2013-08-30T16:13:56Z"
  },
  {
    "code": "@@ @@ -909,12 +910,22 @@ def _check_max_num(self, cls):\n         else:\n             return []\n \n+    def _check_min_num(self, cls):\n+        \"\"\" Check that min_num is an integer. \"\"\"\n+\n+        if cls.min_num is None:\n+            return []\n+        elif not isinstance(cls.min_num, int):\n+            return must_be('an integer', option='min_num', obj=cls, id='admin.E205')\n+        else:\n+            \n\u2026",
    "comment": "check. thanks for the heads up!",
    "line_number": 917,
    "enriched": "File: django/contrib/admin/checks.py\nCode: @@ @@ -909,12 +910,22 @@ def _check_max_num(self, cls):\n         else:\n             return []\n \n+    def _check_min_num(self, cls):\n+        \"\"\" Check that min_num is an integer. \"\"\"\n+\n+        if cls.min_num is None:\n+            return []\n+        elif not isinstance(cls.min_num, int):\n+            return must_be('an integer', option='min_num', obj=cls, id='admin.E205')\n+        else:\n+            \n\u2026\nComment: Check. Thanks for the heads up!",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "django/contrib/admin/checks.py",
    "pr_number": 2652,
    "repo": "django",
    "owner": "django",
    "comment_id": 12643862,
    "comment_created_at": "2014-05-14T15:53:20Z"
  },
  {
    "code": "@@ @@ -24,10 +24,13 @@\n     except ImportError:\n         raise ImportError('require setuptools/distribute for Py3k')\n     setuptools_args = {'use_2to3': True,\n-                       'zip_safe': False,}\n+                       'zip_safe': False,\n+                       'install_requires': ['python-dateutil > 2','numpy'],\n+                        }\n else:\n-    setuptools_args = {}\n-\n+    setuptools_ar\n\u2026",
    "comment": "very nice implementation",
    "line_number": 33,
    "enriched": "File: setup.py\nCode: @@ @@ -24,10 +24,13 @@\n     except ImportError:\n         raise ImportError('require setuptools/distribute for Py3k')\n     setuptools_args = {'use_2to3': True,\n-                       'zip_safe': False,}\n+                       'zip_safe': False,\n+                       'install_requires': ['python-dateutil > 2','numpy'],\n+                        }\n else:\n-    setuptools_args = {}\n-\n+    setuptools_ar\n\u2026\nComment: I don't think this will work because I don't use setuptools in Python 2.x-- something about setuptools messes up the Cython-ing step and I've never found the time to dig in and figure out why",
    "subcategory": "false positive",
    "category": "false positive",
    "file_path": "setup.py",
    "pr_number": 520,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 310136,
    "comment_created_at": "2011-12-21T21:50:17Z"
  },
  {
    "code": "@@ @@ -238,6 +238,27 @@ Django quotes column and table names behind the scenes.\n     If ``proxy = True``, a model which subclasses another model will be treated as\n     a :ref:`proxy model <proxy-models>`.\n \n+``select_on_save``\n+------------------\n+\n+.. attribute:: Options.select_on_save\n+\n+    .. versionadded:: 1.6\n+\n+    Determines if Django will use the pre-1.6\n+    :meth:`django.db.models.Model.s\n\u2026",
    "comment": "i'd prefer to use the style of the other tests (self.client.get) rather than request factory. is there any reason not to do that?",
    "line_number": 251,
    "enriched": "File: docs/ref/models/options.txt\nCode: @@ @@ -238,6 +238,27 @@ Django quotes column and table names behind the scenes.\n     If ``proxy = True``, a model which subclasses another model will be treated as\n     a :ref:`proxy model <proxy-models>`.\n \n+``select_on_save``\n+------------------\n+\n+.. attribute:: Options.select_on_save\n+\n+    .. versionadded:: 1.6\n+\n+    Determines if Django will use the pre-1.6\n+    :meth:`django.db.models.Model.s\n\u2026\nComment: algorithm",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "docs/ref/models/options.txt",
    "pr_number": 1522,
    "repo": "django",
    "owner": "django",
    "comment_id": 6089851,
    "comment_created_at": "2013-08-30T16:13:56Z"
  },
  {
    "code": "@@ @@ -461,12 +461,7 @@\n # response phase the middleware will be applied in reverse order.\n MIDDLEWARE_CLASSES = (\n     'django.middleware.common.CommonMiddleware',\n-    'django.contrib.sessions.middleware.SessionMiddleware',",
    "comment": "users (no apostrophe)",
    "line_number": 464,
    "enriched": "File: django/conf/global_settings.py\nCode: @@ @@ -461,12 +461,7 @@\n # response phase the middleware will be applied in reverse order.\n MIDDLEWARE_CLASSES = (\n     'django.middleware.common.CommonMiddleware',\n-    'django.contrib.sessions.middleware.SessionMiddleware',\nComment: I think that's a good idea but it's far more disruptive of a change. Unlike`MIDDLEWARE_CLASSES`  `startproject` doesn't define `TEMPLATE_CONTEXT_PROCESSORS` by default. Many project likely override the setting to add `django.core.context_processors.request` but it is far more likely for projects to rely on the defaults.",
    "subcategory": "false positive",
    "category": "false positive",
    "file_path": "django/conf/global_settings.py",
    "pr_number": 2591,
    "repo": "django",
    "owner": "django",
    "comment_id": 11808460,
    "comment_created_at": "2014-04-21T13:59:05Z"
  },
  {
    "code": "@@ -51,59 +53,47 @@ def get_debug(self) -> Dict[str, Any]: \"ids\": list(self._delta_index_map.keys()), } - def __iter__(self) -> Iterator[ForwardMsg]: - return iter(self._queue) -",
    "comment": "this was never safe in our multi-threaded queue world - but thankfully it was never used, either",
    "line_number": 56,
    "enriched": "File: lib/streamlit/forward_msg_queue.py\nCode: @@ -51,59 +53,47 @@ def get_debug(self) -> Dict[str, Any]:\n             \"ids\": list(self._delta_index_map.keys()),\n         }\n \n-    def __iter__(self) -> Iterator[ForwardMsg]:\n-        return iter(self._queue)\n-\nComment: This was never safe in our multi-threaded queue world - but thankfully it was never used, either",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "lib/streamlit/forward_msg_queue.py",
    "pr_number": 4568,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 838822101,
    "comment_created_at": "2022-03-30T18:00:13Z"
  },
  {
    "code": "@@ -24,6 +23,89 @@ logger = logging.getLogger(\"django.request\") +class ASGIStream: + \"\"\" + This class provides a minimalistic IO-like wrapper for an ASGI request + provided by a queue which is given as an asynchronous future + and its async event loop. + \"\"\" + + def __init__(self, receive): + self.buffer = bytearray() + self.receive = receive + self._has_more = True + + @property + def has_more(self): + return bool(self._has_more or self.buffer) + + def _receive_more_data(self): + if not self._has_more: + return b\"\" + + message = async_to_sync(self.receive)() + if message[\"type\"] == \"http.disconnect\": + # Early client disconnect. + self._has_more = False # safeguard against trying to call receive again + raise RequestAborted()",
    "comment": "this isn't triggered until the input queue is read at least once.",
    "line_number": 50,
    "enriched": "File: django/core/handlers/asgi.py\nCode: @@ -24,6 +23,89 @@\n logger = logging.getLogger(\"django.request\")\n \n \n+class ASGIStream:\n+    \"\"\"\n+    This class provides a minimalistic IO-like wrapper for an ASGI request\n+    provided by a queue which is given as an asynchronous future\n+    and its async event loop.\n+    \"\"\"\n+\n+    def __init__(self, receive):\n+        self.buffer = bytearray()\n+        self.receive = receive\n+        self._has_more = True\n+\n+    @property\n+    def has_more(self):\n+        return bool(self._has_more or self.buffer)\n+\n+    def _receive_more_data(self):\n+        if not self._has_more:\n+            return b\"\"\n+\n+        message = async_to_sync(self.receive)()\n+        if message[\"type\"] == \"http.disconnect\":\n+            # Early client disconnect.\n+            self._has_more = False  # safeguard against trying to call receive again\n+            raise RequestAborted()\nComment: This isn't triggered until the input queue is read at least once. ",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "django/core/handlers/asgi.py",
    "pr_number": 15704,
    "repo": "django",
    "owner": "django",
    "comment_id": 876824901,
    "comment_created_at": "2022-05-19T09:21:20Z"
  },
  {
    "code": "@@ -59,7 +59,7 @@ async def aparse_result( Returns: Structured output. \"\"\" - return await run_in_executor(None, self.parse_result, result) + return await run_in_executor(None, self.parse_result, result, partial=partial)",
    "comment": "bug fix ?",
    "line_number": 62,
    "enriched": "File: libs/core/langchain_core/output_parsers/base.py\nCode: @@ -59,7 +59,7 @@ async def aparse_result(\n         Returns:\n             Structured output.\n         \"\"\"\n-        return await run_in_executor(None, self.parse_result, result)\n+        return await run_in_executor(None, self.parse_result, result, partial=partial)\nComment: Bug fix ?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "libs/core/langchain_core/output_parsers/base.py",
    "pr_number": 30732,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 2033842368,
    "comment_created_at": "2025-04-08T18:47:37Z"
  },
  {
    "code": "@@ -35,6 +36,7 @@ def __init__(self, out_dir=None, max_keep_ckpts=-1, sync_buffer=False, + save_last=True,",
    "comment": "better to put save_last together with max_keep_ckpts.",
    "line_number": 39,
    "enriched": "File: mmcv/runner/hooks/checkpoint.py\nCode: @@ -35,6 +36,7 @@ def __init__(self,\n                  out_dir=None,\n                  max_keep_ckpts=-1,\n                  sync_buffer=False,\n+                 save_last=True,\nComment: Better to put `save_last` together with `max_keep_ckpts`.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "mmcv/runner/hooks/checkpoint.py",
    "pr_number": 853,
    "repo": "mmcv",
    "owner": "open-mmlab",
    "comment_id": 581883180,
    "comment_created_at": "2021-02-24T11:32:18Z"
  },
  {
    "code": "@@ -132,26 +143,26 @@ def get_table_description(self, cursor, table_name): END as is_autofield, CASE WHEN EXISTS ( - SELECT 1 + SELECT 1 FROM user_json_columns WHERE - user_json_columns.table_name = user_tab_cols.table_name AND + user_json_columns.table_name = user_tab_cols.table_name + AND user_json_columns.column_name = user_tab_cols.column_name ) THEN 1 ELSE 0 END as is_json, user_col_comments.comments as col_comment FROM user_tab_cols - LEFT OUTER JOIN - user_tables ON user_tables.table_name = user_tab_cols.table_name LEFT OUTER JOIN user_col_comments ON - user_col_comments.column_name = user_tab_cols.column_name AND - user_col_comments.table_name = user_tab_cols.table_name + user_col_comments.column_name = user_tab_cols.column_name + AND + user_col_comments.table_name = user_tab_cols.table_name",
    "comment": "please revert unrelated indentation changes.",
    "line_number": 162,
    "enriched": "File: django/db/backends/oracle/introspection.py\nCode: @@ -132,26 +143,26 @@ def get_table_description(self, cursor, table_name):\n                 END as is_autofield,\n                 CASE\n                     WHEN EXISTS (\n-                        SELECT  1\n+                        SELECT 1\n                         FROM user_json_columns\n                         WHERE\n-                            user_json_columns.table_name = user_tab_cols.table_name AND\n+                            user_json_columns.table_name = user_tab_cols.table_name\n+                            AND\n                             user_json_columns.column_name = user_tab_cols.column_name\n                     )\n                     THEN 1\n                     ELSE 0\n                 END as is_json,\n                 user_col_comments.comments as col_comment\n             FROM user_tab_cols\n-            LEFT OUTER JOIN\n-                user_tables ON user_tables.table_name = user_tab_cols.table_name\n             LEFT OUTER JOIN\n                 user_col_comments ON\n-                user_col_comments.column_name = user_tab_cols.column_name AND\n-                user_col_comments.table_name = user_tab_cols.table_name\n+                    user_col_comments.column_name = user_tab_cols.column_name\n+                    AND\n+                    user_col_comments.table_name = user_tab_cols.table_name\nComment: Please revert unrelated indentation changes.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "django/db/backends/oracle/introspection.py",
    "pr_number": 17003,
    "repo": "django",
    "owner": "django",
    "comment_id": 1241025427,
    "comment_created_at": "2023-06-25T04:25:21Z"
  },
  {
    "code": "@@ -435,7 +434,7 @@ JSON ==== JSON is the *lingua franca* of modern web services and it is also the -**implicit content type** HTTPie by default uses. +**implicit content type** HTTPie usesby default.",
    "comment": "missing space in \"usesby\"",
    "line_number": 437,
    "enriched": "File: README.rst\nCode: @@ -435,7 +434,7 @@ JSON\n ====\n \n JSON is the *lingua franca* of modern web services and it is also the\n-**implicit content type** HTTPie by default uses.\n+**implicit content type** HTTPie usesby default.\nComment: missing space in \"usesby\"",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "README.rst",
    "pr_number": 723,
    "repo": "httpie",
    "owner": "httpie",
    "comment_id": 229681647,
    "comment_created_at": "2018-10-31T12:58:54Z"
  },
  {
    "code": "@@ -44,10 +44,26 @@ export function sanitizeHtml(htmlString: string) { return xssFilter.process(htmlString); } +export function hasHtmlTagPattern(str: string): boolean {",
    "comment": "do we have a package or a function inside our current dependencies that provide this feature?",
    "line_number": 47,
    "enriched": "File: superset-frontend/packages/superset-ui-core/src/utils/html.tsx\nCode: @@ -44,10 +44,26 @@ export function sanitizeHtml(htmlString: string) {\n   return xssFilter.process(htmlString);\n }\n \n+export function hasHtmlTagPattern(str: string): boolean {\nComment: Do we have a package or a function inside our current dependencies that provide this feature?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "superset-frontend/packages/superset-ui-core/src/utils/html.tsx",
    "pr_number": 29321,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1649244593,
    "comment_created_at": "2024-06-21T17:22:52Z"
  },
  {
    "code": "@@ -91,11 +91,22 @@ def refresh_schema(self) -> None: .all() .result()[0] ) + edge_properties = ( + self.client.submit( + \"g.E().group().by(label)\" + \".by(project('inVLabel', 'outVLabel','properties')\" + \".by(inV().label()).by(outV().label()).by(properties().key().dedup()\" + \".fold()).dedup().fold())\" + ) + .all() + .result()[0]",
    "comment": "are there any possible circumstances in which the line above for vertex_properties executes fine, but this raises an error?",
    "line_number": 102,
    "enriched": "File: libs/community/langchain_community/graphs/gremlin_graph.py\nCode: @@ -91,11 +91,22 @@ def refresh_schema(self) -> None:\n             .all()\n             .result()[0]\n         )\n+        edge_properties = (\n+            self.client.submit(\n+                \"g.E().group().by(label)\"\n+                \".by(project('inVLabel', 'outVLabel','properties')\"\n+                \".by(inV().label()).by(outV().label()).by(properties().key().dedup()\"\n+                \".fold()).dedup().fold())\"\n+            )\n+            .all()\n+            .result()[0]\nComment: Are there any possible circumstances in which the line above for `vertex_properties` executes fine, but this raises an error?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "libs/community/langchain_community/graphs/gremlin_graph.py",
    "pr_number": 30449,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 2009191266,
    "comment_created_at": "2025-03-23T18:24:03Z"
  },
  {
    "code": "@@ -208,6 +208,16 @@ export const StyledSidebarNavLink = styled.a<StyledSidebarNavLinkProps>( export const StyledSidebarLinkText = styled.span<StyledSidebarNavLinkProps>( ({ isActive, theme }) => ({ color: isActive ? theme.colors.bodyText : theme.colors.fadedText60, + + // We only want it to to be truncated on desktop, + // where we can use the tooltip to show the whole content.",
    "comment": "> where we can use the tooltip to show the whole content.\r\n\r\nbut with this pr there isn't any tooltip anymore, or?",
    "line_number": 213,
    "enriched": "File: frontend/src/components/core/Sidebar/styled-components.ts\nCode: @@ -208,6 +208,16 @@ export const StyledSidebarNavLink = styled.a<StyledSidebarNavLinkProps>(\n export const StyledSidebarLinkText = styled.span<StyledSidebarNavLinkProps>(\n   ({ isActive, theme }) => ({\n     color: isActive ? theme.colors.bodyText : theme.colors.fadedText60,\n+\n+    // We only want it to to be truncated on desktop,\n+    // where we can use the tooltip to show the whole content.\nComment: > where we can use the tooltip to show the whole content.\r\n\r\nbut with this PR there isn't any tooltip anymore, or? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "frontend/src/components/core/Sidebar/styled-components.ts",
    "pr_number": 5538,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 997045011,
    "comment_created_at": "2022-10-17T13:13:15Z"
  },
  {
    "code": "@@ -34,12 +34,18 @@ short: * Use Django's test client to establish that the correct template is being rendered and that the template is passed the correct context data. +* The :class:`~django.test.RequestFactory` is a trimmed down version of the test client that bypasses the routing and middleware layers + so you can test your view as you would test any other function in python with known inputs, + testing for specific outputs. More detail on this can be found in :doc:`advanced testing topics</topics/testing/advanced>`. + * Use in-browser frameworks like Selenium_ to test *rendered* HTML and the *behavior* of web pages, namely JavaScript functionality. Django also provides special support for those frameworks; see the section on :class:`~django.test.LiveServerTestCase` for more details. -A comprehensive test suite should use a combination of both test types. + + +A comprehensive test suite should use a combination of all of these test types.",
    "comment": "chop unnecessary blank lines and wrap docs at 79 chars.",
    "line_number": 48,
    "enriched": "File: docs/topics/testing/tools.txt\nCode: @@ -34,12 +34,18 @@ short:\n * Use Django's test client to establish that the correct template is being\n   rendered and that the template is passed the correct context data.\n \n+* The :class:`~django.test.RequestFactory` is a trimmed down version of the test client that bypasses the routing and middleware layers\n+  so you can test your view as you would test any other function in python with known inputs, \n+  testing for specific outputs. More detail on this can be found in :doc:`advanced testing topics</topics/testing/advanced>`.\n+\n * Use in-browser frameworks like Selenium_ to test *rendered* HTML and the\n   *behavior* of web pages, namely JavaScript functionality. Django also\n   provides special support for those frameworks; see the section on\n   :class:`~django.test.LiveServerTestCase` for more details.\n \n-A comprehensive test suite should use a combination of both test types.\n+\n+\n+A comprehensive test suite should use a combination of all of these test types.\nComment: Chop unnecessary blank lines and wrap docs at 79 chars.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "docs/topics/testing/tools.txt",
    "pr_number": 16094,
    "repo": "django",
    "owner": "django",
    "comment_id": 979818047,
    "comment_created_at": "2022-09-26T10:06:17Z"
  },
  {
    "code": "@@ -0,0 +1,96 @@ +from collections.abc import Generator +\"\"\" + The class Node is used to create nodes of the Binary Tree. + + Agrs : Takes value x to create an object of Node with value x. + + Returns : Creates new object of Node. +\"\"\" +class Node: + def __init__(self, x: int):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide descriptive name for the parameter: x",
    "line_number": 10,
    "enriched": "File: data_structures/binary_tree/serialize_and_deserialize_binary_tree.py\nCode: @@ -0,0 +1,96 @@\n+from collections.abc import Generator\n+\"\"\"\n+    The class Node is used to create nodes of the Binary Tree.\n+    \n+    Agrs : Takes value x to create an object of Node with value x. \n+    \n+    Returns : Creates new object of Node.\n+\"\"\"\n+class Node:\n+    def __init__(self, x: int):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "data_structures/binary_tree/serialize_and_deserialize_binary_tree.py",
    "pr_number": 10128,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349740016,
    "comment_created_at": "2023-10-08T16:06:46Z"
  },
  {
    "code": "@@ -30,17 +30,31 @@ jobs: - name: Install and upgrade packaging tools run: python -m pip install --upgrade pip setuptools wheel - run: python -m pip install -r tests/requirements/py3.txt -e . + - name: Run Selenium tests with screenshots - id: generate-screenshots working-directory: ./tests/ + run: python -Wall runtests.py --verbosity=2 --noinput --selenium=chrome --headless --screenshots --settings=test_sqlite --parallel=2 + + - name: Cache oxipng + uses: actions/cache@v4 + with: + path: ~/.cargo/ + key: ${{ runner.os }}-cargo + + - name: Install oxipng + run: which oxipng || cargo install oxipng + + - name: Optimize screenshots",
    "comment": "this is a really nice addition \u2b50 thank you",
    "line_number": 47,
    "enriched": "File: .github/workflows/screenshots.yml\nCode: @@ -30,17 +30,31 @@ jobs:\n       - name: Install and upgrade packaging tools\n         run: python -m pip install --upgrade pip setuptools wheel\n       - run: python -m pip install -r tests/requirements/py3.txt -e .\n+\n       - name: Run Selenium tests with screenshots\n-        id: generate-screenshots\n         working-directory: ./tests/\n+        run: python -Wall runtests.py --verbosity=2 --noinput --selenium=chrome --headless --screenshots --settings=test_sqlite --parallel=2\n+\n+      - name: Cache oxipng\n+        uses: actions/cache@v4\n+        with:\n+          path: ~/.cargo/\n+          key: ${{ runner.os }}-cargo\n+\n+      - name: Install oxipng\n+        run: which oxipng || cargo install oxipng\n+\n+      - name: Optimize screenshots\nComment: This is a really nice addition \u2b50 thank you",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": ".github/workflows/screenshots.yml",
    "pr_number": 18149,
    "repo": "django",
    "owner": "django",
    "comment_id": 1595285181,
    "comment_created_at": "2024-05-09T10:42:45Z"
  },
  {
    "code": "@@ -512,6 +512,12 @@ def test_add_after_prefetch(self): a4.publications.add(self.p1) self.assertEqual(a4.publications.count(), 2) + def test_create_after_prefetch(self): + a4 = Article.objects.prefetch_related(\"publications\").get(id=self.a4.id) + self.assertSequenceEqual(a4.publications.all(), [self.p2]) + p5 = a4.publications.create(title=\"Django beats\") + self.assertCountEqual(a4.publications.all(), [self.p2, p5])",
    "comment": "ah so it works for m2m, nice \ud83d\udc4d",
    "line_number": 519,
    "enriched": "File: tests/many_to_many/tests.py\nCode: @@ -512,6 +512,12 @@ def test_add_after_prefetch(self):\n         a4.publications.add(self.p1)\n         self.assertEqual(a4.publications.count(), 2)\n \n+    def test_create_after_prefetch(self):\n+        a4 = Article.objects.prefetch_related(\"publications\").get(id=self.a4.id)\n+        self.assertSequenceEqual(a4.publications.all(), [self.p2])\n+        p5 = a4.publications.create(title=\"Django beats\")\n+        self.assertCountEqual(a4.publications.all(), [self.p2, p5])\nComment: Ah so it works for m2m, nice \ud83d\udc4d",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "tests/many_to_many/tests.py",
    "pr_number": 16945,
    "repo": "django",
    "owner": "django",
    "comment_id": 1218852741,
    "comment_created_at": "2023-06-06T04:00:07Z"
  },
  {
    "code": "@@ -1005,7 +1005,7 @@ deal with the management form: The above ends up calling the :meth:`BaseFormSet.render` method on the formset class. This renders the formset using the template specified by the :attr:`~BaseFormSet.template_name` attribute. Similar to forms, by default the -formset will be rendered ``as_table``, with other helper methods of ``as_p`` +formset will be rendered ``as_div``, with other helper methods of ``as_p`` and ``as_ul`` being available. The rendering of the formset can be customized by specifying the ``template_name`` attribute, or more generally by :ref:`overriding the default template <overriding-built-in-formset-templates>`.",
    "comment": "thanks, good catch :+1: i'd move as_table to the list of helpers, e.g.\r\ndiff\r\ndiff --git a/docs/topics/forms/formsets.txt b/docs/topics/forms/formsets.txt\r\nindex 3b68ed614c..14d4962eb6 100644\r\n--- a/docs/topics/forms/formsets.txt\r\n+++ b/docs/topics/forms/formsets.txt\r\n@@ -1005,10 +1005,11 @@ deal with the management form:\r\n the above ends up calling the :meth:baseformset.render method on the formset\r\n class. this renders the formset using the template specified by the\r\n :attr:~baseformset.template_name attribute. similar to forms, by default the\r\n-formset will be rendered as_table, with other helper methods of as_p\r\n-and as_ul being available. the rendering of the formset can be customized\r\n-by specifying the template_name attribute, or more generally by\r\n-:ref:overriding the default template <overriding-built-in-formset-templates>.\r\n+formset will be rendered as_div, with other helper methods of as_p,\r\n+as_ul, and as_table being available. the rendering of the formset can\r\n+be customized by specifying the template_name attribute, or more generally\r\n+by :ref:overriding the default template\r\n+<overriding-built-in-formset-templates>.\r\n \r\n .. _manually-rendered-can-delete-and-can-order:",
    "line_number": 1011,
    "enriched": "File: docs/topics/forms/formsets.txt\nCode: @@ -1005,7 +1005,7 @@ deal with the management form:\n The above ends up calling the :meth:`BaseFormSet.render` method on the formset\n class. This renders the formset using the template specified by the\n :attr:`~BaseFormSet.template_name` attribute. Similar to forms, by default the\n-formset will be rendered ``as_table``, with other helper methods of ``as_p``\n+formset will be rendered ``as_div``, with other helper methods of ``as_p``\n and ``as_ul`` being available. The rendering of the formset can be customized\n by specifying the ``template_name`` attribute, or more generally by\n :ref:`overriding the default template <overriding-built-in-formset-templates>`.\nComment: Thanks, good catch :+1: I'd move `as_table` to the list of helpers, e.g.\r\n```diff\r\ndiff --git a/docs/topics/forms/formsets.txt b/docs/topics/forms/formsets.txt\r\nindex 3b68ed614c..14d4962eb6 100644\r\n--- a/docs/topics/forms/formsets.txt\r\n+++ b/docs/topics/forms/formsets.txt\r\n@@ -1005,10 +1005,11 @@ deal with the management form:\r\n The above ends up calling the :meth:`BaseFormSet.render` method on the formset\r\n class. This renders the formset using the template specified by the\r\n :attr:`~BaseFormSet.template_name` attribute. Similar to forms, by default the\r\n-formset will be rendered ``as_table``, with other helper methods of ``as_p``\r\n-and ``as_ul`` being available. The rendering of the formset can be customized\r\n-by specifying the ``template_name`` attribute, or more generally by\r\n-:ref:`overriding the default template <overriding-built-in-formset-templates>`.\r\n+formset will be rendered ``as_div``, with other helper methods of ``as_p``,\r\n+``as_ul``, and ``as_table`` being available. The rendering of the formset can\r\n+be customized by specifying the ``template_name`` attribute, or more generally\r\n+by :ref:`overriding the default template\r\n+<overriding-built-in-formset-templates>`.\r\n \r\n .. _manually-rendered-can-delete-and-can-order:\r\n \r\n```",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "docs/topics/forms/formsets.txt",
    "pr_number": 18817,
    "repo": "django",
    "owner": "django",
    "comment_id": 1845197160,
    "comment_created_at": "2024-11-16T18:56:56Z"
  },
  {
    "code": "@@ -31,7 +31,7 @@ class DallEAPIWrapper(BaseModel): async_client: Any = Field(default=None, exclude=True) #: :meta private: model_name: str = Field(default=\"dall-e-2\", alias=\"model\") model_kwargs: Dict[str, Any] = Field(default_factory=dict) - openai_api_key: Secret[str] = Field( + openai_api_key: SecretStr = Field(",
    "comment": "thank you",
    "line_number": 34,
    "enriched": "File: libs/community/langchain_community/utilities/dalle_image_generator.py\nCode: @@ -31,7 +31,7 @@ class DallEAPIWrapper(BaseModel):\n     async_client: Any = Field(default=None, exclude=True)  #: :meta private:\n     model_name: str = Field(default=\"dall-e-2\", alias=\"model\")\n     model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n-    openai_api_key: Secret[str] = Field(\n+    openai_api_key: SecretStr = Field(\nComment: thank you ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "libs/community/langchain_community/utilities/dalle_image_generator.py",
    "pr_number": 26396,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 1759114760,
    "comment_created_at": "2024-09-13T15:58:35Z"
  },
  {
    "code": "@@ -275,6 +280,9 @@ def _enqueue_forward_msg(self, msg: ForwardMsg) -> None: if self._debug_last_backmsg_id: msg.debug_last_backmsg_id = self._debug_last_backmsg_id + msg.metadata.sequence_number = self._forward_msg_counter + self._forward_msg_counter += 1",
    "comment": "could this part be thread-unsafe, if user invokes threads from the streamlit app that sends forwardmsg-s (is it true in the case of st.fragment?)\r\n\r\nadding lock could be a solution\r\nalthough another idea could be to move it to self._browser_queue forwardmsgqueue() / runtime _send_message to have a sequential place where we handle this",
    "line_number": 284,
    "enriched": "File: lib/streamlit/runtime/app_session.py\nCode: @@ -275,6 +280,9 @@ def _enqueue_forward_msg(self, msg: ForwardMsg) -> None:\n         if self._debug_last_backmsg_id:\n             msg.debug_last_backmsg_id = self._debug_last_backmsg_id\n \n+        msg.metadata.sequence_number = self._forward_msg_counter\n+        self._forward_msg_counter += 1\nComment: Could this part be thread-unsafe, if user invokes threads from the streamlit app that sends ForwardMsg-s (Is it true in the case of st.fragment?)\r\n\r\nAdding Lock could be a solution\r\nAlthough another idea could be to move it to self._browser_queue ForwardMsgQueue() / runtime _send_message to have a sequential place where we handle this",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "lib/streamlit/runtime/app_session.py",
    "pr_number": 9117,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1686783435,
    "comment_created_at": "2024-07-22T15:57:23Z"
  },
  {
    "code": "@@ -263,37 +292,21 @@ def create_assistant( def invoke( self, input: dict, config: Optional[RunnableConfig] = None, **kwargs: Any ) -> OutputType: - \"\"\"Invoke assistant. + \"\"\"Invoke the assistant. Args: - input: Runnable input dict that can have: - content: User message when starting a new run. - thread_id: Existing thread to use. - run_id: Existing run to use. Should only be supplied when providing - the tool output for a required action after an initial invocation. - file_ids: (deprecated) File ids to include in new run. Use - 'attachments' instead - attachments: Assistant files to include in new run. (v2 API). - message_metadata: Metadata to associate with new message. - thread_metadata: Metadata to associate with new thread. Only relevant - when new thread being created. - instructions: Additional run instructions. - model: Override Assistant model for this run. - tools: Override Assistant tools for this run. - tool_resources: Override Assistant tool resources for this run (v2 API). - run_metadata: Metadata to associate with new run. - config: Runnable config: - - Return: - If self.as_agent, will return + input (dict): Input dictionary containing user message, thread ID, etc.",
    "comment": "why was this deleted?",
    "line_number": 298,
    "enriched": "File: libs/community/langchain_community/agents/openai_assistant/base.py\nCode: @@ -263,37 +292,21 @@ def create_assistant(\n     def invoke(\n         self, input: dict, config: Optional[RunnableConfig] = None, **kwargs: Any\n     ) -> OutputType:\n-        \"\"\"Invoke assistant.\n+        \"\"\"Invoke the assistant.\n \n         Args:\n-            input: Runnable input dict that can have:\n-                content: User message when starting a new run.\n-                thread_id: Existing thread to use.\n-                run_id: Existing run to use. Should only be supplied when providing\n-                    the tool output for a required action after an initial invocation.\n-                file_ids: (deprecated) File ids to include in new run. Use\n-                    'attachments' instead\n-                attachments: Assistant files to include in new run. (v2 API).\n-                message_metadata: Metadata to associate with new message.\n-                thread_metadata: Metadata to associate with new thread. Only relevant\n-                    when new thread being created.\n-                instructions: Additional run instructions.\n-                model: Override Assistant model for this run.\n-                tools: Override Assistant tools for this run.\n-                tool_resources: Override Assistant tool resources for this run (v2 API).\n-                run_metadata: Metadata to associate with new run.\n-            config: Runnable config:\n-\n-        Return:\n-            If self.as_agent, will return\n+            input (dict): Input dictionary containing user message, thread ID, etc.\nComment: why was this deleted?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "libs/community/langchain_community/agents/openai_assistant/base.py",
    "pr_number": 27402,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 1803637310,
    "comment_created_at": "2024-10-16T18:52:21Z"
  },
  {
    "code": "@@ -257,6 +258,16 @@ def init_connection_state(self): if not self.get_autocommit(): self.connection.commit() + # Set the role on the connection. This is useful if the + # credential used to login is not the same as the role that + # owns database resources. This scenario can occur when using + # temporary credentials generated by an external system like + # Vault. + options = self.settings_dict[\"OPTIONS\"] + if options.get(\"assume_role\"): + with self.connection.cursor() as cursor: + cursor.execute(\"SET ROLE %s\", (options[\"assume_role\"],))",
    "comment": "shouldn't this also utilize:\r\n\r\n            if not self.get_autocommit():\r\n                self.connection.commit()\r\n\r\n\r\nlike the timezone stuff above? i think the same argument holds true there.",
    "line_number": 269,
    "enriched": "File: django/db/backends/postgresql/base.py\nCode: @@ -257,6 +258,16 @@ def init_connection_state(self):\n             if not self.get_autocommit():\n                 self.connection.commit()\n \n+        # Set the role on the connection. This is useful if the\n+        # credential used to login is not the same as the role that\n+        # owns database resources. This scenario can occur when using\n+        # temporary credentials generated by an external system like\n+        # Vault.\n+        options = self.settings_dict[\"OPTIONS\"]\n+        if options.get(\"assume_role\"):\n+            with self.connection.cursor() as cursor:\n+                cursor.execute(\"SET ROLE %s\", (options[\"assume_role\"],))\nComment: Shouldn't this also utilize:\r\n```\r\n            if not self.get_autocommit():\r\n                self.connection.commit()\r\n```\r\n\r\nlike the timezone stuff above? I think the same argument holds true there.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "django/db/backends/postgresql/base.py",
    "pr_number": 16360,
    "repo": "django",
    "owner": "django",
    "comment_id": 1058877471,
    "comment_created_at": "2022-12-29T10:42:52Z"
  },
  {
    "code": "@@ -138,14 +146,30 @@ def modify_model_request( \"\"\"Logic to modify request kwargs before the model is called.\"\"\" return request + async def amodify_model_request( + self, + request: ModelRequest, + state: StateT, # noqa: ARG002 + runtime: Runtime[ContextT], # noqa: ARG002 + ) -> ModelRequest: + \"\"\"Async logic to modify request kwargs before the model is called.\"\"\" + return request",
    "comment": "the pattern we use in a lot of places it to have a default implementation which delegates to the sync implementation by launching it on a thread (so it's non blocking)\n\nfor example, https://github.com/langchain-ai/langchain/blob/f2bb1d35420a0adc777b846cebd34c22a6df6666/libs/core/langchain_core/runnables/base.py#l869-l869\n\ngit grep for run_in_executor to find the pattern throughout.\n\n\ni'd prefer doing this at the lowest level of the abstraction (not at a higher level), so the individual methods on the abstraction can be tested on their own (e.g., amodify_model_request on custom middleware can be tested and will behave correctly if modify_model_request has been defined)",
    "line_number": 156,
    "enriched": "File: libs/langchain_v1/langchain/agents/middleware/types.py\nCode: @@ -138,14 +146,30 @@ def modify_model_request(\n         \"\"\"Logic to modify request kwargs before the model is called.\"\"\"\n         return request\n \n+    async def amodify_model_request(\n+        self,\n+        request: ModelRequest,\n+        state: StateT,  # noqa: ARG002\n+        runtime: Runtime[ContextT],  # noqa: ARG002\n+    ) -> ModelRequest:\n+        \"\"\"Async logic to modify request kwargs before the model is called.\"\"\"\n+        return request\nComment: The pattern we use in a lot of places it to have a default implementation which delegates to the sync implementation by launching it on a thread (so it's non blocking)\n\nFor example, https://github.com/langchain-ai/langchain/blob/f2bb1d35420a0adc777b846cebd34c22a6df6666/libs/core/langchain_core/runnables/base.py#L869-L869\n\nGit grep for `run_in_executor` to find the pattern throughout.\n\n\nI'd prefer doing this at the lowest level of the abstraction (not at a higher level), so the individual methods on the abstraction can be tested on their own (e.g., `amodify_model_request` on custom middleware can be tested and will behave correctly if `modify_model_request` has been defined)",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "libs/langchain_v1/langchain/agents/middleware/types.py",
    "pr_number": 33164,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 2392823260,
    "comment_created_at": "2025-09-30T20:59:43Z"
  },
  {
    "code": "@@ -95,6 +140,39 @@ def save(self, must_create=False): raise UpdateError raise + async def asave(self, must_create=False): + \"\"\" + Save the current session data to the database. If 'must_create' is + True, raise a database error if the saving operation doesn't create a + new entry (as opposed to possibly updating an existing entry). + \"\"\" + if self.session_key is None: + return await self.acreate() + data = await self._aget_session(no_load=must_create) + obj = await self.acreate_model_instance(data) + using = router.db_for_write(self.model, instance=obj) + try: + # This code MOST run in a transaction, so it requires sync_to_async + # until `transaction.atomic` supports async + @sync_to_async + def sync_transaction(): + with transaction.atomic(using=using): + obj.save( + force_insert=must_create, + force_update=not must_create, + using=using, + ) + + await sync_transaction()",
    "comment": "this may need to be written with thread_sensitive=true with the sync_to_async",
    "line_number": 159,
    "enriched": "File: django/contrib/sessions/backends/db.py\nCode: @@ -95,6 +140,39 @@ def save(self, must_create=False):\n                 raise UpdateError\n             raise\n \n+    async def asave(self, must_create=False):\n+        \"\"\"\n+        Save the current session data to the database. If 'must_create' is\n+        True, raise a database error if the saving operation doesn't create a\n+        new entry (as opposed to possibly updating an existing entry).\n+        \"\"\"\n+        if self.session_key is None:\n+            return await self.acreate()\n+        data = await self._aget_session(no_load=must_create)\n+        obj = await self.acreate_model_instance(data)\n+        using = router.db_for_write(self.model, instance=obj)\n+        try:\n+            # This code MOST run in a transaction, so it requires sync_to_async\n+            # until `transaction.atomic` supports async\n+            @sync_to_async\n+            def sync_transaction():\n+                with transaction.atomic(using=using):\n+                    obj.save(\n+                        force_insert=must_create,\n+                        force_update=not must_create,\n+                        using=using,\n+                    )\n+\n+            await sync_transaction()\nComment: this may need to be written with `thread_sensitive=True` with the `sync_to_async`",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "django/contrib/sessions/backends/db.py",
    "pr_number": 17372,
    "repo": "django",
    "owner": "django",
    "comment_id": 1361474979,
    "comment_created_at": "2023-10-17T03:44:52Z"
  },
  {
    "code": "@@ -345,9 +343,11 @@ def check_credentials() -> None: check, since credential would be automatically set to an empty string. \"\"\" - from streamlit import config - if not _check_credential_file_exists() and config.get_option(\"server.headless\"): + if not _check_credential_file_exists() and (",
    "comment": "the import statement for config was removed but it's still being used in the conditional check. this will cause a nameerror when config.get_option() is called.",
    "line_number": 347,
    "enriched": "File: lib/streamlit/runtime/credentials.py\nCode: @@ -345,9 +343,11 @@ def check_credentials() -> None:\n     check, since credential would be automatically set to an empty string.\n \n     \"\"\"\n-    from streamlit import config\n \n-    if not _check_credential_file_exists() and config.get_option(\"server.headless\"):\n+    if not _check_credential_file_exists() and (\nComment: The import statement for `config` was removed but it's still being used in the conditional check. This will cause a NameError when `config.get_option()` is called.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "lib/streamlit/runtime/credentials.py",
    "pr_number": 12202,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 2274866808,
    "comment_created_at": "2025-08-13T23:39:24Z"
  },
  {
    "code": "@@ -209,9 +210,374 @@ def get_name(spec: type[BaseEngineSpec]) -> str: return spec.engine_name or spec.engine +def format_markdown_table(headers: list[str], rows: list[list[Any]]) -> str: + \"\"\" + Format headers and rows into a markdown table. + \"\"\" + lines = [] + lines.append(\"| \" + \" | \".join(headers) + \" |\") + lines.append(\"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\") + for row in rows: + lines.append(\"| \" + \" | \".join(str(col) for col in row) + \" |\") + return \"\\n\".join(lines) + + +def generate_focused_table( + info: dict[str, dict[str, Any]], + feature_keys: list[str], + column_labels: list[str], + filter_fn: Any = None, + value_extractor: Any = None, + preserve_order: bool = False, +) -> tuple[str, list[str]]: + \"\"\" + Generate a focused markdown table with databases as rows. + + Args: + info: Dictionary mapping database names to their feature info + feature_keys: List of feature keys to extract from db_info + column_labels: List of column header labels + filter_fn: Optional function to filter databases (receives db_info dict) + value_extractor: Optional function to extract value (receives db_info, key) + + Returns: + Tuple of (markdown table string, list of excluded database names) + \"\"\" + # Filter databases if filter function provided + filtered_info = {} + excluded_dbs = [] + + for db_name, db_info in info.items(): + if filter_fn is None or filter_fn(db_info): + filtered_info[db_name] = db_info + else: + excluded_dbs.append(db_name) + + if not filtered_info: + return \"\", excluded_dbs + + # Build headers: Database + feature columns + headers = [\"Database\"] + column_labels + + # Build rows + rows = [] + # Sort by database name unless preserve_order is True + db_names = ( + list(filtered_info.keys()) if preserve_order else sorted(filtered_info.keys()) + ) + + for db_name in db_names: + db_info = filtered_info[db_name] + row = [db_name] + + for key in feature_keys: + if value_extractor: + value = value_extractor(db_info, key) + else: + value = db_info.get(key, \"\") + row.append(value) + + rows.append(row) + + return format_markdown_table(headers, rows), excluded_dbs + + +def calculate_support_level(db_info: dict[str, Any], feature_keys: list[str]) -> str: + \"\"\" + Calculate support level for a group of features. + + Returns: \"Supported\", \"Partial\", or \"Not supported\" + \"\"\" + if not feature_keys: + return \"Not supported\" + + # Handle time grain features specially + if all(k.startswith(\"time_grains.\") for k in feature_keys): + grain_keys = [k.split(\".\", 1)[1] for k in feature_keys] + supported = sum( + 1 for grain in grain_keys if db_info[\"time_grains\"].get(grain, False) + ) + else: + supported = sum(1 for k in feature_keys if db_info.get(k, False)) + + total = len(feature_keys) + if supported == 0: + return \"Not supported\" + elif supported == total: + return \"Supported\" + else: + return \"Partial\" + + +def generate_feature_tables() -> str: + \"\"\" + Generate multiple focused markdown tables organized by feature categories. + + Returns a complete markdown document with 7 tables optimized for readability. + \"\"\" + info = {} + for spec in sorted(load_engine_specs(), key=get_name): + info[get_name(spec)] = diagnose(spec) + + # remove 3rd party DB engine specs + info = {k: v for k, v in info.items() if v[\"module\"].startswith(\"superset\")} + + # Sort by score descending for overview table + sorted_info = dict(sorted(info.items(), key=lambda x: x[1][\"score\"], reverse=True)) + + output = [] + + # Table 1: Feature Overview + output.append(\"### Feature Overview\\n\") + + # Define feature groups for summary + sql_basics = [ + \"joins\", + \"subqueries\", + \"alias_in_select\", + \"alias_in_orderby\", + \"cte_in_subquery\", + ] + advanced_sql = [ + \"time_groupby_inline\", + \"alias_to_source_column\", + \"order_by_not_in_select\", + \"expressions_in_orderby\", + ] + common_grains = [ + f\"time_grains.{g}\" + for g in [\"SECOND\", \"MINUTE\", \"HOUR\", \"DAY\", \"WEEK\", \"MONTH\", \"QUARTER\", \"YEAR\"] + ] + extended_grains = [ + f\"time_grains.{g}\" + for g in [ + \"FIVE_SECONDS\", + \"THIRTY_SECONDS\", + \"FIVE_MINUTES\", + \"TEN_MINUTES\", + \"FIFTEEN_MINUTES\", + \"THIRTY_MINUTES\", + \"HALF_HOUR\", + \"SIX_HOURS\", + \"WEEK_STARTING_SUNDAY\", + \"WEEK_STARTING_MONDAY\", + \"WEEK_ENDING_SATURDAY\", + \"WEEK_ENDING_SUNDAY\", + \"QUARTER_YEAR\", + ] + ] + integrations = [ + \"ssh_tunneling\", + \"query_cancelation\", + \"get_metrics\", + \"get_extra_table_metadata\", + \"dbapi_exception_mapping\", + \"custom_errors\", + \"dynamic_schema\", + \"where_latest_partition\", + ] + advanced_features = [ + \"user_impersonation\", + \"expand_data\", + \"query_cost_estimation\", + \"sql_validation\", + ] + + headers = [ + \"Database\", + \"Score\", + \"SQL Basics\", + \"Advanced SQL\", + \"Common Time Grains\", + \"Extended Time Grains\", + \"Integrations\", + \"Advanced Features\", + ] + rows = [] + for db_name, db_info in sorted_info.items(): + row = [ + db_name, + db_info[\"score\"], + calculate_support_level(db_info, sql_basics), + calculate_support_level(db_info, advanced_sql), + calculate_support_level(db_info, common_grains), + calculate_support_level(db_info, extended_grains), + calculate_support_level(db_info, integrations), + calculate_support_level(db_info, advanced_features), + ] + rows.append(row) + output.append(format_markdown_table(headers, rows)) + + # Table 2: Database Information + output.append(\"\\n### Database Information\\n\") + + # Custom value extractor for database info to handle limit_method enum + def extract_db_info(db_info: dict[str, Any], key: str) -> str: + if key == \"limit_method\": + # Convert enum value to name + from superset.sql.parse import LimitMethod + + return LimitMethod(db_info[key]).name + return db_info.get(key, \"\")",
    "comment": "### import statement inside frequently called function <sub>![category performance](https://img.shields.io/badge/performance-4f46e5)</sub>\n\n<details>\n  <summary>tell me more</summary>\n\n###### what is the issue?\nthe import statement for limitmethod is placed inside the function and executed on every call when key equals 'limit_method'.\n\n\n###### why this matters\nrepeated imports inside function calls create unnecessary overhead, especially when the function is called multiple times during table generation.\n\n###### suggested change \u2219 *feature preview*\nmove the import to the top of the function or module level:\n\npython\nfrom superset.sql.parse import limitmethod\n\ndef extract_db_info(db_info: dict[str, any], key: str) -> str:\n    if key == \"limit_method\":\n        return limitmethod(db_info[key]).name\n    return db_info.get(key, \"\")\n\n\n\n###### provide feedback to improve future suggestions\n[![nice catch](https://img.shields.io/badge/\ud83d\udc4d%20nice%20catch-71bc78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7927cbed-b54c-4152-af9d-03fd79593a31/upvote) [![incorrect](https://img.shields.io/badge/\ud83d\udc4e%20incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7927cbed-b54c-4152-af9d-03fd79593a31?what_not_true=true)  [![not in scope](https://img.shields.io/badge/\ud83d\udc4e%20out%20of%20pr%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7927cbed-b54c-4152-af9d-03fd79593a31?what_out_of_scope=true) [![not in coding standard](https://img.shields.io/badge/\ud83d\udc4e%20not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7927cbed-b54c-4152-af9d-03fd79593a31?what_not_in_standard=true) [![other](https://img.shields.io/badge/\ud83d\udc4e%20other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7927cbed-b54c-4152-af9d-03fd79593a31)\n</details>\n\n<sub>\n\n\ud83d\udcac looking for more details? reply to this comment to chat with korbit.\n</sub>\n\n<!--- korbi internal id:87e30e14-88dd-4cb8-a78e-c25a3095013f -->\n\n\n[](87e30e14-88dd-4cb8-a78e-c25a3095013f)",
    "line_number": 421,
    "enriched": "File: superset/db_engine_specs/lib.py\nCode: @@ -209,9 +210,374 @@ def get_name(spec: type[BaseEngineSpec]) -> str:\n     return spec.engine_name or spec.engine\n \n \n+def format_markdown_table(headers: list[str], rows: list[list[Any]]) -> str:\n+    \"\"\"\n+    Format headers and rows into a markdown table.\n+    \"\"\"\n+    lines = []\n+    lines.append(\"| \" + \" | \".join(headers) + \" |\")\n+    lines.append(\"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\")\n+    for row in rows:\n+        lines.append(\"| \" + \" | \".join(str(col) for col in row) + \" |\")\n+    return \"\\n\".join(lines)\n+\n+\n+def generate_focused_table(\n+    info: dict[str, dict[str, Any]],\n+    feature_keys: list[str],\n+    column_labels: list[str],\n+    filter_fn: Any = None,\n+    value_extractor: Any = None,\n+    preserve_order: bool = False,\n+) -> tuple[str, list[str]]:\n+    \"\"\"\n+    Generate a focused markdown table with databases as rows.\n+\n+    Args:\n+        info: Dictionary mapping database names to their feature info\n+        feature_keys: List of feature keys to extract from db_info\n+        column_labels: List of column header labels\n+        filter_fn: Optional function to filter databases (receives db_info dict)\n+        value_extractor: Optional function to extract value (receives db_info, key)\n+\n+    Returns:\n+        Tuple of (markdown table string, list of excluded database names)\n+    \"\"\"\n+    # Filter databases if filter function provided\n+    filtered_info = {}\n+    excluded_dbs = []\n+\n+    for db_name, db_info in info.items():\n+        if filter_fn is None or filter_fn(db_info):\n+            filtered_info[db_name] = db_info\n+        else:\n+            excluded_dbs.append(db_name)\n+\n+    if not filtered_info:\n+        return \"\", excluded_dbs\n+\n+    # Build headers: Database + feature columns\n+    headers = [\"Database\"] + column_labels\n+\n+    # Build rows\n+    rows = []\n+    # Sort by database name unless preserve_order is True\n+    db_names = (\n+        list(filtered_info.keys()) if preserve_order else sorted(filtered_info.keys())\n+    )\n+\n+    for db_name in db_names:\n+        db_info = filtered_info[db_name]\n+        row = [db_name]\n+\n+        for key in feature_keys:\n+            if value_extractor:\n+                value = value_extractor(db_info, key)\n+            else:\n+                value = db_info.get(key, \"\")\n+            row.append(value)\n+\n+        rows.append(row)\n+\n+    return format_markdown_table(headers, rows), excluded_dbs\n+\n+\n+def calculate_support_level(db_info: dict[str, Any], feature_keys: list[str]) -> str:\n+    \"\"\"\n+    Calculate support level for a group of features.\n+\n+    Returns: \"Supported\", \"Partial\", or \"Not supported\"\n+    \"\"\"\n+    if not feature_keys:\n+        return \"Not supported\"\n+\n+    # Handle time grain features specially\n+    if all(k.startswith(\"time_grains.\") for k in feature_keys):\n+        grain_keys = [k.split(\".\", 1)[1] for k in feature_keys]\n+        supported = sum(\n+            1 for grain in grain_keys if db_info[\"time_grains\"].get(grain, False)\n+        )\n+    else:\n+        supported = sum(1 for k in feature_keys if db_info.get(k, False))\n+\n+    total = len(feature_keys)\n+    if supported == 0:\n+        return \"Not supported\"\n+    elif supported == total:\n+        return \"Supported\"\n+    else:\n+        return \"Partial\"\n+\n+\n+def generate_feature_tables() -> str:\n+    \"\"\"\n+    Generate multiple focused markdown tables organized by feature categories.\n+\n+    Returns a complete markdown document with 7 tables optimized for readability.\n+    \"\"\"\n+    info = {}\n+    for spec in sorted(load_engine_specs(), key=get_name):\n+        info[get_name(spec)] = diagnose(spec)\n+\n+    # remove 3rd party DB engine specs\n+    info = {k: v for k, v in info.items() if v[\"module\"].startswith(\"superset\")}\n+\n+    # Sort by score descending for overview table\n+    sorted_info = dict(sorted(info.items(), key=lambda x: x[1][\"score\"], reverse=True))\n+\n+    output = []\n+\n+    # Table 1: Feature Overview\n+    output.append(\"### Feature Overview\\n\")\n+\n+    # Define feature groups for summary\n+    sql_basics = [\n+        \"joins\",\n+        \"subqueries\",\n+        \"alias_in_select\",\n+        \"alias_in_orderby\",\n+        \"cte_in_subquery\",\n+    ]\n+    advanced_sql = [\n+        \"time_groupby_inline\",\n+        \"alias_to_source_column\",\n+        \"order_by_not_in_select\",\n+        \"expressions_in_orderby\",\n+    ]\n+    common_grains = [\n+        f\"time_grains.{g}\"\n+        for g in [\"SECOND\", \"MINUTE\", \"HOUR\", \"DAY\", \"WEEK\", \"MONTH\", \"QUARTER\", \"YEAR\"]\n+    ]\n+    extended_grains = [\n+        f\"time_grains.{g}\"\n+        for g in [\n+            \"FIVE_SECONDS\",\n+            \"THIRTY_SECONDS\",\n+            \"FIVE_MINUTES\",\n+            \"TEN_MINUTES\",\n+            \"FIFTEEN_MINUTES\",\n+            \"THIRTY_MINUTES\",\n+            \"HALF_HOUR\",\n+            \"SIX_HOURS\",\n+            \"WEEK_STARTING_SUNDAY\",\n+            \"WEEK_STARTING_MONDAY\",\n+            \"WEEK_ENDING_SATURDAY\",\n+            \"WEEK_ENDING_SUNDAY\",\n+            \"QUARTER_YEAR\",\n+        ]\n+    ]\n+    integrations = [\n+        \"ssh_tunneling\",\n+        \"query_cancelation\",\n+        \"get_metrics\",\n+        \"get_extra_table_metadata\",\n+        \"dbapi_exception_mapping\",\n+        \"custom_errors\",\n+        \"dynamic_schema\",\n+        \"where_latest_partition\",\n+    ]\n+    advanced_features = [\n+        \"user_impersonation\",\n+        \"expand_data\",\n+        \"query_cost_estimation\",\n+        \"sql_validation\",\n+    ]\n+\n+    headers = [\n+        \"Database\",\n+        \"Score\",\n+        \"SQL Basics\",\n+        \"Advanced SQL\",\n+        \"Common Time Grains\",\n+        \"Extended Time Grains\",\n+        \"Integrations\",\n+        \"Advanced Features\",\n+    ]\n+    rows = []\n+    for db_name, db_info in sorted_info.items():\n+        row = [\n+            db_name,\n+            db_info[\"score\"],\n+            calculate_support_level(db_info, sql_basics),\n+            calculate_support_level(db_info, advanced_sql),\n+            calculate_support_level(db_info, common_grains),\n+            calculate_support_level(db_info, extended_grains),\n+            calculate_support_level(db_info, integrations),\n+            calculate_support_level(db_info, advanced_features),\n+        ]\n+        rows.append(row)\n+    output.append(format_markdown_table(headers, rows))\n+\n+    # Table 2: Database Information\n+    output.append(\"\\n### Database Information\\n\")\n+\n+    # Custom value extractor for database info to handle limit_method enum\n+    def extract_db_info(db_info: dict[str, Any], key: str) -> str:\n+        if key == \"limit_method\":\n+            # Convert enum value to name\n+            from superset.sql.parse import LimitMethod\n+\n+            return LimitMethod(db_info[key]).name\n+        return db_info.get(key, \"\")\nComment: ### Import statement inside frequently called function <sub>![category Performance](https://img.shields.io/badge/Performance-4f46e5)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe import statement for LimitMethod is placed inside the function and executed on every call when key equals 'limit_method'.\n\n\n###### Why this matters\nRepeated imports inside function calls create unnecessary overhead, especially when the function is called multiple times during table generation.\n\n###### Suggested change \u2219 *Feature Preview*\nMove the import to the top of the function or module level:\n\n```python\nfrom superset.sql.parse import LimitMethod\n\ndef extract_db_info(db_info: dict[str, Any], key: str) -> str:\n    if key == \"limit_method\":\n        return LimitMethod(db_info[key]).name\n    return db_info.get(key, \"\")\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/\ud83d\udc4d%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7927cbed-b54c-4152-af9d-03fd79593a31/upvote) [![Incorrect](https://img.shields.io/badge/\ud83d\udc4e%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7927cbed-b54c-4152-af9d-03fd79593a31?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/\ud83d\udc4e%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7927cbed-b54c-4152-af9d-03fd79593a31?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/\ud83d\udc4e%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7927cbed-b54c-4152-af9d-03fd79593a31?what_not_in_standard=true) [![Other](https://img.shields.io/badge/\ud83d\udc4e%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7927cbed-b54c-4152-af9d-03fd79593a31)\n</details>\n\n<sub>\n\n\ud83d\udcac Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:87e30e14-88dd-4cb8-a78e-c25a3095013f -->\n\n\n[](87e30e14-88dd-4cb8-a78e-c25a3095013f)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "superset/db_engine_specs/lib.py",
    "pr_number": 35809,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 2453597373,
    "comment_created_at": "2025-10-22T23:50:24Z"
  },
  {
    "code": "@@ -10,7 +10,7 @@ class DumpertIE(InfoExtractor): _VALID_URL = r'''(?x) (?P<protocol>https?)://(?:(?:www|legacy)\\.)?dumpert\\.nl(?: /(?:mediabase|embed|item)/| - (?:/toppers|/latest|/?)\\?selectedId= + .*\\?selectedId=",
    "comment": "isn't this more accurate?",
    "line_number": 13,
    "enriched": "File: yt_dlp/extractor/dumpert.py\nCode: @@ -10,7 +10,7 @@ class DumpertIE(InfoExtractor):\n     _VALID_URL = r'''(?x)\n         (?P<protocol>https?)://(?:(?:www|legacy)\\.)?dumpert\\.nl(?:\n             /(?:mediabase|embed|item)/|\n-            (?:/toppers|/latest|/?)\\?selectedId=\n+            .*\\?selectedId=\nComment: ```suggestion\r\n            [^#]+[?&]selectedId=\r\n```\r\n\r\nIsn't this more accurate?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "yt_dlp/extractor/dumpert.py",
    "pr_number": 9320,
    "repo": "yt-dlp",
    "owner": "yt-dlp",
    "comment_id": 1508743072,
    "comment_created_at": "2024-03-01T09:48:08Z"
  },
  {
    "code": "@@ -35,6 +35,9 @@ assists people when migrating to a new version. files for production use cases! While we never really supported or should have tried to support docker-compose for production use cases, we now actively have taken a stance against supporting it. See the PR for details. +- [27697](https://github.com/apache/superset/pull/27697) [minor] flask-session bump leads to them + deprecating `SESSION_USE_SIGNER`, check your configs as this flag won't do anything moving + forward.",
    "comment": "we have this deprecation also:\r\ndeprecate [flask_session.filesystem.filesystemsessioninterface](https://flask-session.readthedocs.io/en/latest/api.html#flask_session.filesystem.filesystemsessioninterface) in favor of the broader [flask_session.cachelib.cachelibsessioninterface](https://flask-session.readthedocs.io/en/latest/api.html#flask_session.cachelib.cachelibsessioninterface) ([2bc7df](https://github.com/pallets-eco/flask-session/commit/2bc7df1be7b8929e55cb25f13845caf0503630d8)).\r\n\r\nseems it will be replaced by cachelib so this impacts session_type",
    "line_number": 40,
    "enriched": "File: UPDATING.md\nCode: @@ -35,6 +35,9 @@ assists people when migrating to a new version.\n   files for production use cases! While we never really supported\n   or should have tried to support docker-compose for production use cases, we now actively\n   have taken a stance against supporting it. See the PR for details.\n+- [27697](https://github.com/apache/superset/pull/27697) [minor] flask-session bump leads to them\n+  deprecating `SESSION_USE_SIGNER`, check your configs as this flag won't do anything moving\n+  forward.\nComment: We have this deprecation also:\r\nDeprecate [flask_session.filesystem.FileSystemSessionInterface](https://flask-session.readthedocs.io/en/latest/api.html#flask_session.filesystem.FileSystemSessionInterface) in favor of the broader [flask_session.cachelib.CacheLibSessionInterface](https://flask-session.readthedocs.io/en/latest/api.html#flask_session.cachelib.CacheLibSessionInterface) ([2bc7df](https://github.com/pallets-eco/flask-session/commit/2bc7df1be7b8929e55cb25f13845caf0503630d8)).\r\n\r\nseems it will be replaced by `cachelib` so this impacts `SESSION_TYPE`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "UPDATING.md",
    "pr_number": 27697,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1540817881,
    "comment_created_at": "2024-03-27T10:18:25Z"
  },
  {
    "code": "@@ -286,3 +286,38 @@ def test_slider_tick_bar_visibility(app: Page, assert_snapshot: ImageCompareFunc expect(slider.get_by_test_id(\"stSliderTickBar\")).to_be_visible() assert_snapshot(slider, name=\"st_slider-tick_bar_visibility\") + + +def test_dynamic_slider_props(app: Page, assert_snapshot: ImageCompareFunction): + \"\"\"Test that the slider can be updated dynamically while keeping the state.\"\"\" + dynamic_slider = get_element_by_key(app, \"dynamic_slider_with_key\") + expect(dynamic_slider).to_be_visible() + + expect(dynamic_slider).to_contain_text(\"Initial dynamic slider\") + assert_snapshot(dynamic_slider, name=\"st_slider-dynamic_initial\") + + # Check that the help tooltip is correct: + expect_help_tooltip(app, dynamic_slider, \"initial help\") + + # Click to change value + dynamic_slider.click() + wait_for_app_run(app) + + expect_prefixed_markdown(app, \"Initial slider value:\", \"50\") + + # Click the toggle to update the slider props + from e2e_playwright.shared.app_utils import click_toggle",
    "comment": "import statements should be placed at the top of the file with other imports, not within a function. move this import to the top of the file with the existing imports.",
    "line_number": 309,
    "enriched": "File: e2e_playwright/st_slider_test.py\nCode: @@ -286,3 +286,38 @@ def test_slider_tick_bar_visibility(app: Page, assert_snapshot: ImageCompareFunc\n     expect(slider.get_by_test_id(\"stSliderTickBar\")).to_be_visible()\n \n     assert_snapshot(slider, name=\"st_slider-tick_bar_visibility\")\n+\n+\n+def test_dynamic_slider_props(app: Page, assert_snapshot: ImageCompareFunction):\n+    \"\"\"Test that the slider can be updated dynamically while keeping the state.\"\"\"\n+    dynamic_slider = get_element_by_key(app, \"dynamic_slider_with_key\")\n+    expect(dynamic_slider).to_be_visible()\n+\n+    expect(dynamic_slider).to_contain_text(\"Initial dynamic slider\")\n+    assert_snapshot(dynamic_slider, name=\"st_slider-dynamic_initial\")\n+\n+    # Check that the help tooltip is correct:\n+    expect_help_tooltip(app, dynamic_slider, \"initial help\")\n+\n+    # Click to change value\n+    dynamic_slider.click()\n+    wait_for_app_run(app)\n+\n+    expect_prefixed_markdown(app, \"Initial slider value:\", \"50\")\n+\n+    # Click the toggle to update the slider props\n+    from e2e_playwright.shared.app_utils import click_toggle\nComment: Import statements should be placed at the top of the file with other imports, not within a function. Move this import to the top of the file with the existing imports.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "e2e_playwright/st_slider_test.py",
    "pr_number": 12575,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 2364729519,
    "comment_created_at": "2025-09-19T22:44:40Z"
  },
  {
    "code": "@@ -26,6 +26,9 @@ on: branches: - main - maintenance/** + push: + branches: + - maintenance/**",
    "comment": "please check https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#onpushbranchestagsbranches-ignoretags-ignore, only defining branches will not triggered by the tag",
    "line_number": 31,
    "enriched": "File: .github/workflows/wheels.yml\nCode: @@ -26,6 +26,9 @@ on:\n     branches:\n       - main\n       - maintenance/**\n+  push:\n+    branches:\n+      - maintenance/**\nComment: ```suggestion\r\n    branches:\r\n      - maintenance/**\r\n    tags:\r\n      - v*\r\n```\r\n\r\nPlease check https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#onpushbranchestagsbranches-ignoretags-ignore, only defining `branches` will not triggered by the tag",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": ".github/workflows/wheels.yml",
    "pr_number": 25981,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 1518711284,
    "comment_created_at": "2024-03-10T03:05:20Z"
  },
  {
    "code": "@@ -741,7 +741,9 @@ def test(tmpl, expected, *, info=None, **params): test('%(duration_string)s', ('27:46:40', '27-46-40')) test('%(resolution)s', '1080p') test('%(playlist_index|)s', '001') - test('%(playlist_index&{}!)s', '1!') + test('%(playlist_index&{}!)s', '01!') # Should maintain padding with replacement",
    "comment": "this test is failing",
    "line_number": 744,
    "enriched": "File: test/test_YoutubeDL.py\nCode: @@ -741,7 +741,9 @@ def test(tmpl, expected, *, info=None, **params):\n         test('%(duration_string)s', ('27:46:40', '27-46-40'))\n         test('%(resolution)s', '1080p')\n         test('%(playlist_index|)s', '001')\n-        test('%(playlist_index&{}!)s', '1!')\n+        test('%(playlist_index&{}!)s', '01!')  # Should maintain padding with replacement\nComment: This test is failing",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "test/test_YoutubeDL.py",
    "pr_number": 11635,
    "repo": "yt-dlp",
    "owner": "yt-dlp",
    "comment_id": 1857475627,
    "comment_created_at": "2024-11-25T23:38:07Z"
  },
  {
    "code": "@@ -34,12 +36,12 @@ class DefaultRunnerConstructor: >>> runner = build_runner(runner_cfg) \"\"\" - def __init__(self, runner_cfg, default_args=None): + def __init__(self, runner_cfg: Dict, default_args: Optional[Dict] = None): if not isinstance(runner_cfg, dict): raise TypeError('runner_cfg should be a dict', f'but got {type(runner_cfg)}') self.runner_cfg = runner_cfg self.default_args = default_args - def __call__(self): + def __call__(self) -> Any:",
    "comment": "actually, the return type should be baserunner, but will this cause circular import?",
    "line_number": 46,
    "enriched": "File: mmcv/runner/default_constructor.py\nCode: @@ -34,12 +36,12 @@ class DefaultRunnerConstructor:\n         >>> runner = build_runner(runner_cfg)\n     \"\"\"\n \n-    def __init__(self, runner_cfg, default_args=None):\n+    def __init__(self, runner_cfg: Dict, default_args: Optional[Dict] = None):\n         if not isinstance(runner_cfg, dict):\n             raise TypeError('runner_cfg should be a dict',\n                             f'but got {type(runner_cfg)}')\n         self.runner_cfg = runner_cfg\n         self.default_args = default_args\n \n-    def __call__(self):\n+    def __call__(self) -> Any:\nComment: Actually, the return type should be `BaseRunner`, but will this cause circular import?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mmcv/runner/default_constructor.py",
    "pr_number": 2002,
    "repo": "mmcv",
    "owner": "open-mmlab",
    "comment_id": 884135580,
    "comment_created_at": "2022-05-28T13:32:16Z"
  },
  {
    "code": "@@ -20,37 +22,59 @@ click_form_button, expect_help_tooltip, get_element_by_key, + get_expander, + get_slider,",
    "comment": "import statements should import entire modules instead of single functions. consider importing the utilities module and accessing these functions through the module namespace.",
    "line_number": 26,
    "enriched": "File: e2e_playwright/st_select_slider_test.py\nCode: @@ -20,37 +22,59 @@\n     click_form_button,\n     expect_help_tooltip,\n     get_element_by_key,\n+    get_expander,\n+    get_slider,\nComment: Import statements should import entire modules instead of single functions. Consider importing the utilities module and accessing these functions through the module namespace.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "e2e_playwright/st_select_slider_test.py",
    "pr_number": 12490,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 2335967667,
    "comment_created_at": "2025-09-10T08:22:32Z"
  },
  {
    "code": "@@ -60,7 +60,6 @@ def run_cypress_for_test_file( f\"--browser {browser} \" f\"--record --group {group_id} --tag {REPO},{GITHUB_EVENT_NAME} \" f\"--ci-build-id {build_id} \" - f\"--wait-for-missing-groups \" f\"-- {chrome_flags}\"",
    "comment": "### parallel test synchronization issue <sub>![category functionality](https://img.shields.io/badge/functionality-0284c7)</sub>\n\n<details>\n  <summary>tell me more</summary>\n\n###### what is the issue?\nremoving the '--wait-for-missing-groups' flag in parallel cypress test execution could lead to test runs completing before all parallel groups are ready, potentially missing test failures in other groups.\n\n###### why this matters\nin a ci environment with parallel test execution, not waiting for all groups can result in false positives where the build is marked as successful even if other parallel groups later fail.\n\n###### suggested change \u2219 *feature preview*\nretain the '--wait-for-missing-groups' flag to ensure proper synchronization between parallel test groups:\npython\nf\"--browser {browser} \"\nf\"--record --group {group_id} --tag {repo},{github_event_name} \"\nf\"--ci-build-id {build_id} \"\nf\"--wait-for-missing-groups \"\nf\"-- {chrome_flags}\"\n\n\n\n</details>\n\n<sub>\n\n[![report a problem with this comment](https://img.shields.io/badge/report%20a%20problem%20with%20this%20comment-gray.svg?logo=data:image/svg+xml;base64,phn2zyb4bwxucz0iahr0cdovl3d3dy53my5vcmcvmjawmc9zdmciihdpzhropsiyncigagvpz2h0psiyncigdmlld0jved0imcawidi0idi0iibmawxspsjub25liibzdhjva2u9iinmnwvjmdaiihn0cm9rzs13awr0ad0imiigc3ryb2tllwxpbmvjyxa9injvdw5kiibzdhjva2utbgluzwpvaw49injvdw5kiibjbgfzcz0ibhvjawrligx1y2lkzs10cmlhbmdszs1hbgvydci+phbhdgggzd0ibtixljczide4ltgtmtrhmiayidagmcawltmundggmgwtocaxneeyidigmcawidagncaymwgxnmeyidigmcawidagms43my0zii8+phbhdgggzd0itteyidl2ncivpjxwyxroigq9ik0xmiaxn2gumdeilz48l3n2zz4=)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/ee41bf17-7e22-45e2-8a72-99c24881604d?suggestedfixenabled=true)\n\n\ud83d\udcac chat with korbit by mentioning @korbit-ai.\n</sub>\n\n<!--- korbi internal id:43b63b5d-da52-48fe-83fe-1df7d7f2a5c6 -->",
    "line_number": 63,
    "enriched": "File: scripts/cypress_run.py\nCode: @@ -60,7 +60,6 @@ def run_cypress_for_test_file(\n                 f\"--browser {browser} \"\n                 f\"--record --group {group_id} --tag {REPO},{GITHUB_EVENT_NAME} \"\n                 f\"--ci-build-id {build_id} \"\n-                f\"--wait-for-missing-groups \"\n                 f\"-- {chrome_flags}\"\nComment: ### Parallel Test Synchronization Issue <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nRemoving the '--wait-for-missing-groups' flag in parallel Cypress test execution could lead to test runs completing before all parallel groups are ready, potentially missing test failures in other groups.\n\n###### Why this matters\nIn a CI environment with parallel test execution, not waiting for all groups can result in false positives where the build is marked as successful even if other parallel groups later fail.\n\n###### Suggested change \u2219 *Feature Preview*\nRetain the '--wait-for-missing-groups' flag to ensure proper synchronization between parallel test groups:\n```python\nf\"--browser {browser} \"\nf\"--record --group {group_id} --tag {REPO},{GITHUB_EVENT_NAME} \"\nf\"--ci-build-id {build_id} \"\nf\"--wait-for-missing-groups \"\nf\"-- {chrome_flags}\"\n```\n\n\n</details>\n\n<sub>\n\n[![Report a problem with this comment](https://img.shields.io/badge/Report%20a%20problem%20with%20this%20comment-gray.svg?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmNWVjMDAiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIiBjbGFzcz0ibHVjaWRlIGx1Y2lkZS10cmlhbmdsZS1hbGVydCI+PHBhdGggZD0ibTIxLjczIDE4LTgtMTRhMiAyIDAgMCAwLTMuNDggMGwtOCAxNEEyIDIgMCAwIDAgNCAyMWgxNmEyIDIgMCAwIDAgMS43My0zIi8+PHBhdGggZD0iTTEyIDl2NCIvPjxwYXRoIGQ9Ik0xMiAxN2guMDEiLz48L3N2Zz4=)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/ee41bf17-7e22-45e2-8a72-99c24881604d?suggestedFixEnabled=true)\n\n\ud83d\udcac Chat with Korbit by mentioning @korbit-ai.\n</sub>\n\n<!--- korbi internal id:43b63b5d-da52-48fe-83fe-1df7d7f2a5c6 -->\n",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "scripts/cypress_run.py",
    "pr_number": 32045,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1935858000,
    "comment_created_at": "2025-01-30T15:53:47Z"
  },
  {
    "code": "@@ -1,18 +1,34 @@ import json -from typing import List +from typing import Any, Dict, List import requests from langchain_core.documents import Document -from pydantic import BaseModel +from langchain_core.utils import get_from_dict_or_env +from pydantic import BaseModel, ConfigDict, SecretStr, model_validator from yarl import URL class JinaSearchAPIWrapper(BaseModel): \"\"\"Wrapper around the Jina search engine.\"\"\" + jina_api_key: SecretStr",
    "comment": "can we make just api_key",
    "line_number": 14,
    "enriched": "File: libs/community/langchain_community/utilities/jina_search.py\nCode: @@ -1,18 +1,34 @@\n import json\n-from typing import List\n+from typing import Any, Dict, List\n \n import requests\n from langchain_core.documents import Document\n-from pydantic import BaseModel\n+from langchain_core.utils import get_from_dict_or_env\n+from pydantic import BaseModel, ConfigDict, SecretStr, model_validator\n from yarl import URL\n \n \n class JinaSearchAPIWrapper(BaseModel):\n     \"\"\"Wrapper around the Jina search engine.\"\"\"\n \n+    jina_api_key: SecretStr\nComment: can we make just `api_key`",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "libs/community/langchain_community/utilities/jina_search.py",
    "pr_number": 29622,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 1943860684,
    "comment_created_at": "2025-02-06T00:23:44Z"
  },
  {
    "code": "@@ -382,6 +382,19 @@ def _get_filepath_or_buffer( # urlopen function defined elsewhere in this module import urllib.request + # Fix for GH #55828 + parsed_url = parse_url(filepath_or_buffer) + if parsed_url.scheme == \"file\": + file_path = urllib.request.url2pathname(parsed_url.path) + file_path = os.path.normpath(file_path) + return IOArgs( + filepath_or_buffer=open(file_path, \"rb\"),",
    "comment": "shouldn't this still respect mode?",
    "line_number": 391,
    "enriched": "File: pandas/io/common.py\nCode: @@ -382,6 +382,19 @@ def _get_filepath_or_buffer(\n         # urlopen function defined elsewhere in this module\n         import urllib.request\n \n+        # Fix for GH #55828\n+        parsed_url = parse_url(filepath_or_buffer)\n+        if parsed_url.scheme == \"file\":\n+            file_path = urllib.request.url2pathname(parsed_url.path)\n+            file_path = os.path.normpath(file_path)\n+            return IOArgs(\n+                filepath_or_buffer=open(file_path, \"rb\"),\nComment: Shouldn't this still respect mode?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/io/common.py",
    "pr_number": 56309,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1414347655,
    "comment_created_at": "2023-12-04T18:54:24Z"
  },
  {
    "code": "@@ -236,6 +236,7 @@ def switch_page(page: str | Path | StreamlitPage) -> NoReturn: # type: ignore[m query_string=ctx.query_string, page_script_hash=page_script_hash, cached_message_hashes=ctx.cached_message_hashes, + context_info=ctx.context_info,",
    "comment": "**question:** currently, context_info is defined as context_info: contextinfo | none = none. should we make this context_info: contextinfo so that it's required? would that help us catch these issues at a type level in the future?",
    "line_number": 239,
    "enriched": "File: lib/streamlit/commands/execution_control.py\nCode: @@ -236,6 +236,7 @@ def switch_page(page: str | Path | StreamlitPage) -> NoReturn:  # type: ignore[m\n             query_string=ctx.query_string,\n             page_script_hash=page_script_hash,\n             cached_message_hashes=ctx.cached_message_hashes,\n+            context_info=ctx.context_info,\nComment: **question:** Currently, `context_info` is defined as `context_info: ContextInfo | None = None`. Should we make this `context_info: ContextInfo` so that it's required? Would that help us catch these issues at a type level in the future?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/streamlit/commands/execution_control.py",
    "pr_number": 11521,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 2132831282,
    "comment_created_at": "2025-06-06T19:52:10Z"
  },
  {
    "code": "@@ -526,8 +526,8 @@ def test_non_list_record_path_errors(self, value): test_input = {\"state\": \"Texas\", \"info\": parsed_value} test_path = \"info\" msg = ( - f\"{test_input} has non list value {parsed_value} for path {test_path}. \" - \"Must be list or null.\" + f\"Path must contain list or null, \" + f\"but got {type(test_input['info']).__name__} at {repr(test_path)}\"",
    "comment": "could you fully evaluate {type(test_input['info']).__name__} and {repr(test_path)} here?",
    "line_number": 530,
    "enriched": "File: pandas/tests/io/json/test_normalize.py\nCode: @@ -526,8 +526,8 @@ def test_non_list_record_path_errors(self, value):\n         test_input = {\"state\": \"Texas\", \"info\": parsed_value}\n         test_path = \"info\"\n         msg = (\n-            f\"{test_input} has non list value {parsed_value} for path {test_path}. \"\n-            \"Must be list or null.\"\n+            f\"Path must contain list or null, \"\n+            f\"but got {type(test_input['info']).__name__} at {repr(test_path)}\"\nComment: Could you fully evaluate `{type(test_input['info']).__name__}` and `{repr(test_path)}` here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/tests/io/json/test_normalize.py",
    "pr_number": 56802,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1446751240,
    "comment_created_at": "2024-01-10T00:14:22Z"
  },
  {
    "code": "@@ -192,6 +196,17 @@ def fit(self, X, y, sample_weight=None): when not None, `sample_weight` is passed to all underlying estimators + .. deprecated:: 1.5 + `sample_weight` is deprecated in 1.5 and will be removed in 1.7.",
    "comment": "in this class it's not deprecated though, it's simply removed.",
    "line_number": 200,
    "enriched": "File: sklearn/ensemble/_stacking.py\nCode: @@ -192,6 +196,17 @@ def fit(self, X, y, sample_weight=None):\n                when not None, `sample_weight` is passed to all underlying\n                estimators\n \n+            .. deprecated:: 1.5\n+                `sample_weight` is deprecated in 1.5 and will be removed in 1.7.\nComment: in this class it's not deprecated though, it's simply removed.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "sklearn/ensemble/_stacking.py",
    "pr_number": 28701,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1557825382,
    "comment_created_at": "2024-04-09T15:10:22Z"
  },
  {
    "code": "@@ -167,6 +173,132 @@ def compute( \"\"\" +class PairwiseDistances(BaseDistancesReductionDispatcher): + \"\"\"Compute the pairwise distances matrix for two sets of vectors. + + The distance function `dist` depends on the values of the `metric` + and `metric_kwargs` parameters. + + This class only computes the pairwise distances matrix without + applying any reduction on it. It shares most of the underlying + code infrastructure with reducing variants to leverage + cache-aware chunking and multi-thread parallelism. + + This class is not meant to be instantiated, one should only use + its :meth:`compute` classmethod which handles allocation and + deallocation consistently. + \"\"\" + + @classmethod + def is_usable_for(cls, X, Y, metric) -> bool: + Y = X if Y is None else Y + return metric != \"sqeuclidean\" and super().is_usable_for(X, Y, metric) + + @classmethod + def compute( + cls, + X, + Y, + metric=\"euclidean\", + chunk_size=None, + metric_kwargs=None, + strategy=None, + ): + \"\"\"Return pairwise distances matrix for the given arguments. + + Parameters + ---------- + X : ndarray or CSR matrix of shape (n_samples_X, n_features) + Input data. + + Y : ndarray or CSR matrix of shape (n_samples_Y, n_features) + Input data. + + metric : str, default='euclidean' + The distance metric to use. + For a list of available metrics, see the documentation of + :class:`~sklearn.metrics.DistanceMetric`. + + chunk_size : int, default=None, + The number of vectors per chunk. If None (default) looks-up in + scikit-learn configuration for `pairwise_dist_chunk_size`, + and use 256 if it is not set. + + metric_kwargs : dict, default=None + Keyword arguments to pass to specified metric function. + + strategy : str, {'auto', 'parallel_on_X', 'parallel_on_Y'}, default=None + The chunking strategy defining which dataset parallelization are made on. + + For both strategies the computations happens with two nested loops, + respectively on chunks of X and chunks of Y. + Strategies differs on which loop (outer or inner) is made to run + in parallel with the Cython `prange` construct: + + - 'parallel_on_X' dispatches chunks of X uniformly on threads. + Each thread then iterates on all the chunks of Y. This strategy is + embarrassingly parallel and comes with no datastructures + synchronisation. + + - 'parallel_on_Y' dispatches chunks of Y uniformly on threads. + Each thread processes all the chunks of X in turn. This strategy is + a sequence of embarrassingly parallel subtasks (the inner loop on Y + chunks) with intermediate datastructures synchronisation at each + iteration of the sequential outer loop on X chunks. + + - 'auto' relies on a simple heuristic to choose between + 'parallel_on_X' and 'parallel_on_Y': when `X.shape[0]` is large enough, + 'parallel_on_X' is usually the most efficient strategy. + When `X.shape[0]` is small but `Y.shape[0]` is large, 'parallel_on_Y' + brings more opportunity for parallelism and is therefore more efficient. + + - None (default) looks-up in scikit-learn configuration for + `pairwise_dist_parallel_strategy`, and use 'auto' if it is not set.",
    "comment": "i think this must be changed not to mention chunking. what do you think?",
    "line_number": 278,
    "enriched": "File: sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\nCode: @@ -167,6 +173,132 @@ def compute(\n         \"\"\"\n \n \n+class PairwiseDistances(BaseDistancesReductionDispatcher):\n+    \"\"\"Compute the pairwise distances matrix for two sets of vectors.\n+\n+    The distance function `dist` depends on the values of the `metric`\n+    and `metric_kwargs` parameters.\n+\n+    This class only computes the pairwise distances matrix without\n+    applying any reduction on it. It shares most of the underlying\n+    code infrastructure with reducing variants to leverage\n+    cache-aware chunking and multi-thread parallelism.\n+\n+    This class is not meant to be instantiated, one should only use\n+    its :meth:`compute` classmethod which handles allocation and\n+    deallocation consistently.\n+    \"\"\"\n+\n+    @classmethod\n+    def is_usable_for(cls, X, Y, metric) -> bool:\n+        Y = X if Y is None else Y\n+        return metric != \"sqeuclidean\" and super().is_usable_for(X, Y, metric)\n+\n+    @classmethod\n+    def compute(\n+        cls,\n+        X,\n+        Y,\n+        metric=\"euclidean\",\n+        chunk_size=None,\n+        metric_kwargs=None,\n+        strategy=None,\n+    ):\n+        \"\"\"Return pairwise distances matrix for the given arguments.\n+\n+        Parameters\n+        ----------\n+        X : ndarray or CSR matrix of shape (n_samples_X, n_features)\n+            Input data.\n+\n+        Y : ndarray or CSR matrix of shape (n_samples_Y, n_features)\n+            Input data.\n+\n+        metric : str, default='euclidean'\n+            The distance metric to use.\n+            For a list of available metrics, see the documentation of\n+            :class:`~sklearn.metrics.DistanceMetric`.\n+\n+        chunk_size : int, default=None,\n+            The number of vectors per chunk. If None (default) looks-up in\n+            scikit-learn configuration for `pairwise_dist_chunk_size`,\n+            and use 256 if it is not set.\n+\n+        metric_kwargs : dict, default=None\n+            Keyword arguments to pass to specified metric function.\n+\n+        strategy : str, {'auto', 'parallel_on_X', 'parallel_on_Y'}, default=None\n+            The chunking strategy defining which dataset parallelization are made on.\n+\n+            For both strategies the computations happens with two nested loops,\n+            respectively on chunks of X and chunks of Y.\n+            Strategies differs on which loop (outer or inner) is made to run\n+            in parallel with the Cython `prange` construct:\n+\n+              - 'parallel_on_X' dispatches chunks of X uniformly on threads.\n+                Each thread then iterates on all the chunks of Y. This strategy is\n+                embarrassingly parallel and comes with no datastructures\n+                synchronisation.\n+\n+              - 'parallel_on_Y' dispatches chunks of Y uniformly on threads.\n+                Each thread processes all the chunks of X in turn. This strategy is\n+                a sequence of embarrassingly parallel subtasks (the inner loop on Y\n+                chunks) with intermediate datastructures synchronisation at each\n+                iteration of the sequential outer loop on X chunks.\n+\n+              - 'auto' relies on a simple heuristic to choose between\n+                'parallel_on_X' and 'parallel_on_Y': when `X.shape[0]` is large enough,\n+                'parallel_on_X' is usually the most efficient strategy.\n+                When `X.shape[0]` is small but `Y.shape[0]` is large, 'parallel_on_Y'\n+                brings more opportunity for parallelism and is therefore more efficient.\n+\n+              - None (default) looks-up in scikit-learn configuration for\n+                `pairwise_dist_parallel_strategy`, and use 'auto' if it is not set.\nComment: I think this must be changed not to mention chunking. What do you think?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py",
    "pr_number": 25561,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1098634652,
    "comment_created_at": "2023-02-07T13:05:17Z"
  },
  {
    "code": "@@ -191,35 +188,41 @@ const config: ControlPanelConfig = { }, ], [ - hasGenericChartAxes && isAggMode - ? { - name: 'time_grain_sqla', - config: { - ...sharedControls.time_grain_sqla, - visibility: ({ controls }) => { - const dttmLookup = Object.fromEntries( - ensureIsArray(controls?.groupby?.options).map(option => [ - option.column_name, - option.is_dttm, - ]), - ); + { + name: 'time_grain_sqla', + config: { + ...sharedControls.time_grain_sqla, + visibility: ({ controls }) => {",
    "comment": "not related to this pr. there are bunch of repeated logics between pivot table, chart table, and box plot that we could probably centralize at some point",
    "line_number": 193,
    "enriched": "File: superset-frontend/plugins/plugin-chart-table/src/controlPanel.tsx\nCode: @@ -191,35 +188,41 @@ const config: ControlPanelConfig = {\n           },\n         ],\n         [\n-          hasGenericChartAxes && isAggMode\n-            ? {\n-                name: 'time_grain_sqla',\n-                config: {\n-                  ...sharedControls.time_grain_sqla,\n-                  visibility: ({ controls }) => {\n-                    const dttmLookup = Object.fromEntries(\n-                      ensureIsArray(controls?.groupby?.options).map(option => [\n-                        option.column_name,\n-                        option.is_dttm,\n-                      ]),\n-                    );\n+          {\n+            name: 'time_grain_sqla',\n+            config: {\n+              ...sharedControls.time_grain_sqla,\n+              visibility: ({ controls }) => {\nComment: Not related to this PR. There are bunch of repeated logics between pivot table, chart table, and Box Plot that we could probably centralize at some point",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "superset-frontend/plugins/plugin-chart-table/src/controlPanel.tsx",
    "pr_number": 26372,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1453720306,
    "comment_created_at": "2024-01-16T17:04:45Z"
  },
  {
    "code": "@@ -278,7 +278,7 @@ export default function DatabaseSelector({ isFetching: loadingCatalogs, refetch: refetchCatalogs, } = useCatalogs({ - dbId: currentDb?.value, + dbId: showCatalogSelector ? currentDb?.value : undefined,",
    "comment": "oh, nice! \u2764\ufe0f",
    "line_number": 281,
    "enriched": "File: superset-frontend/src/components/DatabaseSelector/index.tsx\nCode: @@ -278,7 +278,7 @@ export default function DatabaseSelector({\n     isFetching: loadingCatalogs,\n     refetch: refetchCatalogs,\n   } = useCatalogs({\n-    dbId: currentDb?.value,\n+    dbId: showCatalogSelector ? currentDb?.value : undefined,\nComment: Oh, nice! \u2764\ufe0f ",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "superset-frontend/src/components/DatabaseSelector/index.tsx",
    "pr_number": 28415,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1595924179,
    "comment_created_at": "2024-05-09T20:13:33Z"
  },
  {
    "code": "@@ -190,26 +190,6 @@ def _get_constrained_beam_kwargs(self, num_return_sequences=1): } return beam_kwargs - @staticmethod - def _get_encoder_outputs(",
    "comment": "_get_encoder_outputs is defined in several places, but never used",
    "line_number": 194,
    "enriched": "File: tests/generation/test_utils.py\nCode: @@ -190,26 +190,6 @@ def _get_constrained_beam_kwargs(self, num_return_sequences=1):\n         }\n         return beam_kwargs\n \n-    @staticmethod\n-    def _get_encoder_outputs(\nComment: `_get_encoder_outputs` is defined in several places, but never used",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "tests/generation/test_utils.py",
    "pr_number": 33663,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 1771489467,
    "comment_created_at": "2024-09-23T13:52:25Z"
  },
  {
    "code": "@@ -32,59 +35,57 @@ def __init__(self, save_optimizer=True, out_dir=None, max_keep_ckpts=-1, + sync_buffer=True, **kwargs): self.interval = interval self.by_epoch = by_epoch self.save_optimizer = save_optimizer self.out_dir = out_dir self.max_keep_ckpts = max_keep_ckpts self.args = kwargs + self.sync_buffer = sync_buffer + self.sync_buffer_hook = SyncBuffersHook() - @master_only def after_train_epoch(self, runner): if not self.by_epoch or not self.every_n_epochs(runner, self.interval): return runner.logger.info(f'Saving checkpoint at {runner.epoch + 1} epochs') + if self.sync_buffer: + self.sync_buffer_hook.sync_buffers(runner)",
    "comment": "i suggest adding a utility method sync_buffer(model) for both hooks, instead of using one hook inside another.",
    "line_number": 55,
    "enriched": "File: mmcv/runner/hooks/checkpoint.py\nCode: @@ -32,59 +35,57 @@ def __init__(self,\n                  save_optimizer=True,\n                  out_dir=None,\n                  max_keep_ckpts=-1,\n+                 sync_buffer=True,\n                  **kwargs):\n         self.interval = interval\n         self.by_epoch = by_epoch\n         self.save_optimizer = save_optimizer\n         self.out_dir = out_dir\n         self.max_keep_ckpts = max_keep_ckpts\n         self.args = kwargs\n+        self.sync_buffer = sync_buffer\n+        self.sync_buffer_hook = SyncBuffersHook()\n \n-    @master_only\n     def after_train_epoch(self, runner):\n         if not self.by_epoch or not self.every_n_epochs(runner, self.interval):\n             return\n \n         runner.logger.info(f'Saving checkpoint at {runner.epoch + 1} epochs')\n+        if self.sync_buffer:\n+            self.sync_buffer_hook.sync_buffers(runner)\nComment: I suggest adding a utility method `sync_buffer(model)` for both hooks, instead of using one hook inside another.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "mmcv/runner/hooks/checkpoint.py",
    "pr_number": 588,
    "repo": "mmcv",
    "owner": "open-mmlab",
    "comment_id": 495750167,
    "comment_created_at": "2020-09-28T07:49:47Z"
  },
  {
    "code": "@@ -769,16 +769,19 @@ def func_assert_same_pos(x, y, func=isnan, hasval='nan'): y_id = func(y) # We include work-arounds here to handle three types of slightly # pathological ndarray subclasses: - # (1) all() on `masked` array scalars can return masked arrays, so we - # use != True + # (1) all() on fully masked arrays returns np.ma.masked, so we use != True + # (np.ma.masked != True evaluates as np.ma.masked, which is falsy). # (2) __eq__ on some ndarray subclasses returns Python booleans - # instead of element-wise comparisons, so we cast to np.bool() and - # use isinstance(..., bool) checks + # instead of element-wise comparisons, so we cast to np.bool in + # that case. # (3) subclasses with bare-bones __array_function__ implementations may # not implement np.all(), so favor using the .all() method - # We are not committed to supporting such subclasses, but it's nice to + # We are not committed to supporting cases (2) and (3), but it's nice to # support them if possible. - if np.bool(x_id == y_id).all() != True: + result = x_id == y_id + if isinstance(result, bool):",
    "comment": "this feels pretty brittle to me. is there another way to do this without isinstance?",
    "line_number": 782,
    "enriched": "File: numpy/testing/_private/utils.py\nCode: @@ -769,16 +769,19 @@ def func_assert_same_pos(x, y, func=isnan, hasval='nan'):\n         y_id = func(y)\n         # We include work-arounds here to handle three types of slightly\n         # pathological ndarray subclasses:\n-        # (1) all() on `masked` array scalars can return masked arrays, so we\n-        #     use != True\n+        # (1) all() on fully masked arrays returns np.ma.masked, so we use != True\n+        #     (np.ma.masked != True evaluates as np.ma.masked, which is falsy).\n         # (2) __eq__ on some ndarray subclasses returns Python booleans\n-        #     instead of element-wise comparisons, so we cast to np.bool() and\n-        #     use isinstance(..., bool) checks\n+        #     instead of element-wise comparisons, so we cast to np.bool in\n+        #     that case.\n         # (3) subclasses with bare-bones __array_function__ implementations may\n         #     not implement np.all(), so favor using the .all() method\n-        # We are not committed to supporting such subclasses, but it's nice to\n+        # We are not committed to supporting cases (2) and (3), but it's nice to\n         # support them if possible.\n-        if np.bool(x_id == y_id).all() != True:\n+        result = x_id == y_id\n+        if isinstance(result, bool):\nComment: This feels pretty brittle to me. Is there another way to do this without `isinstance`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "numpy/testing/_private/utils.py",
    "pr_number": 29318,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2190975324,
    "comment_created_at": "2025-07-07T20:40:32Z"
  },
  {
    "code": "@@ -323,31 +356,45 @@ def test_no_na_values_no_keep_default(all_parsers): tm.assert_frame_equal(result, expected) -@xfail_pyarrow # ValueError: The pyarrow engine doesn't support passing a dict def test_no_keep_default_na_dict_na_values(all_parsers): # see gh-19227 data = \"a,b\\n,2\" parser = all_parsers + + if parser.engine == \"pyarrow\": + msg = \"The pyarrow engine doesn't support passing a dict for na_values\" + with pytest.raises(ValueError, match=msg): + parser.read_csv( + StringIO(data), na_values={\"b\": [\"2\"]}, keep_default_na=False + ) + return + result = parser.read_csv( StringIO(data), na_values={\"b\": [\"2\"]}, keep_default_na=False ) expected = DataFrame({\"a\": [\"\"], \"b\": [np.nan]}) tm.assert_frame_equal(result, expected) -@xfail_pyarrow # ValueError: The pyarrow engine doesn't support passing a dict def test_no_keep_default_na_dict_na_scalar_values(all_parsers): # see gh-19227 # # Scalar values shouldn't cause the parsing to crash or fail. data = \"a,b\\n1,2\" parser = all_parsers + + if parser.engine == \"pyarrow\": + msg = \"The pyarrow engine doesn't support passing a dict for na_values\" + with pytest.raises(ValueError, match=msg): + parser.read_csv(StringIO(data), na_values={\"b\": 2}, keep_default_na=False) + return + df = parser.read_csv(StringIO(data), na_values={\"b\": 2}, keep_default_na=False) expected = DataFrame({\"a\": [1], \"b\": [np.nan]}) tm.assert_frame_equal(df, expected) -@xfail_pyarrow # ValueError: The pyarrow engine doesn't support passing a dict +# @xfail_pyarrow # ValueError: The pyarrow engine doesn't support passing a dict",
    "comment": "can this be removed now?",
    "line_number": 397,
    "enriched": "File: pandas/tests/io/parser/test_na_values.py\nCode: @@ -323,31 +356,45 @@ def test_no_na_values_no_keep_default(all_parsers):\n     tm.assert_frame_equal(result, expected)\n \n \n-@xfail_pyarrow  # ValueError: The pyarrow engine doesn't support passing a dict\n def test_no_keep_default_na_dict_na_values(all_parsers):\n     # see gh-19227\n     data = \"a,b\\n,2\"\n     parser = all_parsers\n+\n+    if parser.engine == \"pyarrow\":\n+        msg = \"The pyarrow engine doesn't support passing a dict for na_values\"\n+        with pytest.raises(ValueError, match=msg):\n+            parser.read_csv(\n+                StringIO(data), na_values={\"b\": [\"2\"]}, keep_default_na=False\n+            )\n+        return\n+\n     result = parser.read_csv(\n         StringIO(data), na_values={\"b\": [\"2\"]}, keep_default_na=False\n     )\n     expected = DataFrame({\"a\": [\"\"], \"b\": [np.nan]})\n     tm.assert_frame_equal(result, expected)\n \n \n-@xfail_pyarrow  # ValueError: The pyarrow engine doesn't support passing a dict\n def test_no_keep_default_na_dict_na_scalar_values(all_parsers):\n     # see gh-19227\n     #\n     # Scalar values shouldn't cause the parsing to crash or fail.\n     data = \"a,b\\n1,2\"\n     parser = all_parsers\n+\n+    if parser.engine == \"pyarrow\":\n+        msg = \"The pyarrow engine doesn't support passing a dict for na_values\"\n+        with pytest.raises(ValueError, match=msg):\n+            parser.read_csv(StringIO(data), na_values={\"b\": 2}, keep_default_na=False)\n+        return\n+\n     df = parser.read_csv(StringIO(data), na_values={\"b\": 2}, keep_default_na=False)\n     expected = DataFrame({\"a\": [1], \"b\": [np.nan]})\n     tm.assert_frame_equal(df, expected)\n \n \n-@xfail_pyarrow  # ValueError: The pyarrow engine doesn't support passing a dict\n+# @xfail_pyarrow  # ValueError: The pyarrow engine doesn't support passing a dict\nComment: Can this be removed now?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/tests/io/parser/test_na_values.py",
    "pr_number": 56090,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1399928607,
    "comment_created_at": "2023-11-21T01:34:10Z"
  },
  {
    "code": "@@ -1,161 +1,64 @@ -import re +import json from .common import InfoExtractor -from ..utils import ( - determine_ext, - extract_attributes, - int_or_none, - str_to_int, - url_or_none, - urlencode_postdata, -) class ManyVidsIE(InfoExtractor): _VALID_URL = r'(?i)https?://(?:www\\.)?manyvids\\.com/video/(?P<id>\\d+)' _TESTS = [{ # preview video - 'url': 'https://www.manyvids.com/Video/133957/everthing-about-me/', - 'md5': '03f11bb21c52dd12a05be21a5c7dcc97', + 'url': 'https://www.manyvids.com/Video/1856601/Horny-Asian-Bunny-Needs-Cock/',",
    "comment": "don't replace tests, just add new ones. if the old tests are dead links then add skip",
    "line_number": 10,
    "enriched": "File: yt_dlp/extractor/manyvids.py\nCode: @@ -1,161 +1,64 @@\n-import re\n+import json\n \n from .common import InfoExtractor\n-from ..utils import (\n-    determine_ext,\n-    extract_attributes,\n-    int_or_none,\n-    str_to_int,\n-    url_or_none,\n-    urlencode_postdata,\n-)\n \n \n class ManyVidsIE(InfoExtractor):\n     _VALID_URL = r'(?i)https?://(?:www\\.)?manyvids\\.com/video/(?P<id>\\d+)'\n     _TESTS = [{\n         # preview video\n-        'url': 'https://www.manyvids.com/Video/133957/everthing-about-me/',\n-        'md5': '03f11bb21c52dd12a05be21a5c7dcc97',\n+        'url': 'https://www.manyvids.com/Video/1856601/Horny-Asian-Bunny-Needs-Cock/',\nComment: don't replace tests, just add new ones. if the old tests are dead links then add `skip`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "yt_dlp/extractor/manyvids.py",
    "pr_number": 9089,
    "repo": "yt-dlp",
    "owner": "yt-dlp",
    "comment_id": 1468655363,
    "comment_created_at": "2024-01-27T21:14:07Z"
  },
  {
    "code": "@@ -0,0 +1,74 @@ +name: integrations",
    "comment": "nit: shall we call it integration to match the file name?",
    "line_number": 1,
    "enriched": "File: .github/workflows/integration.yml\nCode: @@ -0,0 +1,74 @@\n+name: integrations\nComment: nit: shall we call it `integration` to match the file name?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".github/workflows/integration.yml",
    "pr_number": 152,
    "repo": "reflex",
    "owner": "reflex-dev",
    "comment_id": 1054016244,
    "comment_created_at": "2022-12-21T06:12:41Z"
  },
  {
    "code": "@@ -980,21 +980,10 @@ paperless will process in parallel on a single document.\n         process very large documents faster, use a higher thread per worker\n         count.\n \n-    The default is a balance between the two, according to your CPU core\n-    count, with a slight favor towards threads per worker:\n-\n-    | CPU core count | Workers | Threads |\n-    | -------------- | ------- | ------- |\n-    | > 1            | > 1     | > 1     |\n-    | > 2            | > 2     | > 1     |\n-    | > 4            | > 2     | > 2     |\n-    | > 6            | > 2     | > 3     |\n-    | > 8            | > 2     | > 4     |\n-    | > 12           | > 3     | > 4     |\n-    | > 16           | > 4     | > 4     |\n-\n-    If you only specify PAPERLESS_TASK_WORKERS, paperless will adjust\n-    PAPERLESS_THREADS_PER_WORKER automatically.\n+    The default depends on `PAPERLESS_TASK_WORKERS` and is\n+    calculated so that the total number of threads is at most\n+    equal to the total number of (logical) CPU cores (and possibly\n+    less due to rounding).",
    "comment": "i think this is wrong. paperless keeps at least one thread per worker, so when paperless_task_workers exceeds the logical core count it will still create more total threads than cores.\n\n\n",
    "line_number": 986,
    "enriched": "File: docs/configuration.md\nCode: @@ -980,21 +980,10 @@ paperless will process in parallel on a single document.\n         process very large documents faster, use a higher thread per worker\n         count.\n \n-    The default is a balance between the two, according to your CPU core\n-    count, with a slight favor towards threads per worker:\n-\n-    | CPU core count | Workers | Threads |\n-    | -------------- | ------- | ------- |\n-    | > 1            | > 1     | > 1     |\n-    | > 2            | > 2     | > 1     |\n-    | > 4            | > 2     | > 2     |\n-    | > 6            | > 2     | > 3     |\n-    | > 8            | > 2     | > 4     |\n-    | > 12           | > 3     | > 4     |\n-    | > 16           | > 4     | > 4     |\n-\n-    If you only specify PAPERLESS_TASK_WORKERS, paperless will adjust\n-    PAPERLESS_THREADS_PER_WORKER automatically.\n+    The default depends on `PAPERLESS_TASK_WORKERS` and is\n+    calculated so that the total number of threads is at most\n+    equal to the total number of (logical) CPU cores (and possibly\n+    less due to rounding).\nComment: I think this is wrong. Paperless keeps at least one thread per worker, so when PAPERLESS_TASK_WORKERS exceeds the logical core count it will still create more total threads than cores.\n\n\n```suggestion\n    If unset, Paperless uses `max(floor(cpu_count / PAPERLESS_TASK_WORKERS), 1)` threads per worker.\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/configuration.md",
    "pr_number": 11031,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2431075507,
    "comment_created_at": "2025-10-15T04:20:34Z"
  },
  {
    "code": "@@ -1089,6 +1117,20 @@ class ScheduleDateField(models.TextChoices):\n         verbose_name=_(\"has this storage path\"),\n     )\n \n+    filter_has_not_storage_paths = models.ManyToManyField(\n+        StoragePath,\n+        blank=True,\n+        related_name=\"workflowtriggers_has_not_storage_path\",\n+        verbose_name=_(\"does not have these storage path(s)\"),\n+    )\n+\n+    filter_custom_field_query = models.TextField(",
    "comment": "just a thought, not really looking at this, but why not a json field?",
    "line_number": 1127,
    "enriched": "File: src/documents/models.py\nCode: @@ -1089,6 +1117,20 @@ class ScheduleDateField(models.TextChoices):\n         verbose_name=_(\"has this storage path\"),\n     )\n \n+    filter_has_not_storage_paths = models.ManyToManyField(\n+        StoragePath,\n+        blank=True,\n+        related_name=\"workflowtriggers_has_not_storage_path\",\n+        verbose_name=_(\"does not have these storage path(s)\"),\n+    )\n+\n+    filter_custom_field_query = models.TextField(\nComment: Just a thought, not really looking at this, but why not a JSON field?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "src/documents/models.py",
    "pr_number": 11029,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2411967250,
    "comment_created_at": "2025-10-07T21:25:46Z"
  },
  {
    "code": "@@ -0,0 +1,11 @@ +- name: Check PR title",
    "comment": "what is that?",
    "line_number": 1,
    "enriched": "File: .github/workflows/test-workflow.yml\nCode: @@ -0,0 +1,11 @@\n+- name: Check PR title\nComment: What is that?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".github/workflows/test-workflow.yml",
    "pr_number": 28256,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1466173756,
    "comment_created_at": "2024-01-25T10:39:28Z"
  },
  {
    "code": "@@ -150,53 +146,22 @@ const TextArea: FC<Props> = ({ setFocused(true) }, []) - const onChange = useCallback( - (e: React.ChangeEvent<HTMLTextAreaElement>): void => { - const { value } = e.target - const { maxChars } = element - - if (maxChars !== 0 && value.length > maxChars) { - return - } - - // mark it dirty but don't update its value in the WidgetMgr - // This means that individual keypresses won't trigger a script re-run. - setUiValue(value) - setDirty(true) - }, - [element] + const onChange = useOnInputChange( + element.formId, + element.maxChars, + setDirty, + setUiValue, + setValueWithSource ) - const isEnterKeyPressed = ( - event: React.KeyboardEvent<HTMLTextAreaElement> - ): boolean => { - const { keyCode, key } = event - - // Using keyCode as well due to some different behaviors on Windows - // https://bugs.chromium.org/p/chromium/issues/detail?id=79407 - return ( - (key === \"Enter\" || keyCode === 13 || keyCode === 10) && - // Do not send the sentence being composed when Enter is typed into the IME. - !(event.nativeEvent?.isComposing === true) - ) - } - - const onKeyDown = useCallback( - (e: React.KeyboardEvent<HTMLTextAreaElement>): void => { - const { metaKey, ctrlKey } = e - const { formId } = element - const allowFormEnterToSubmit = widgetMgr.allowFormEnterToSubmit(formId) - - if (isEnterKeyPressed(e) && (ctrlKey || metaKey) && dirty) { - e.preventDefault() - - commitWidgetValue({ fromUi: true }) - if (allowFormEnterToSubmit) { - widgetMgr.submitForm(formId, fragmentId) - } - } - }, - [element, widgetMgr, dirty, commitWidgetValue, fragmentId] + console.log(\"widgetMgr\", widgetMgr)",
    "comment": "can this be removed?",
    "line_number": 157,
    "enriched": "File: frontend/lib/src/components/widgets/TextArea/TextArea.tsx\nCode: @@ -150,53 +146,22 @@ const TextArea: FC<Props> = ({\n     setFocused(true)\n   }, [])\n \n-  const onChange = useCallback(\n-    (e: React.ChangeEvent<HTMLTextAreaElement>): void => {\n-      const { value } = e.target\n-      const { maxChars } = element\n-\n-      if (maxChars !== 0 && value.length > maxChars) {\n-        return\n-      }\n-\n-      // mark it dirty but don't update its value in the WidgetMgr\n-      // This means that individual keypresses won't trigger a script re-run.\n-      setUiValue(value)\n-      setDirty(true)\n-    },\n-    [element]\n+  const onChange = useOnInputChange(\n+    element.formId,\n+    element.maxChars,\n+    setDirty,\n+    setUiValue,\n+    setValueWithSource\n   )\n \n-  const isEnterKeyPressed = (\n-    event: React.KeyboardEvent<HTMLTextAreaElement>\n-  ): boolean => {\n-    const { keyCode, key } = event\n-\n-    // Using keyCode as well due to some different behaviors on Windows\n-    // https://bugs.chromium.org/p/chromium/issues/detail?id=79407\n-    return (\n-      (key === \"Enter\" || keyCode === 13 || keyCode === 10) &&\n-      // Do not send the sentence being composed when Enter is typed into the IME.\n-      !(event.nativeEvent?.isComposing === true)\n-    )\n-  }\n-\n-  const onKeyDown = useCallback(\n-    (e: React.KeyboardEvent<HTMLTextAreaElement>): void => {\n-      const { metaKey, ctrlKey } = e\n-      const { formId } = element\n-      const allowFormEnterToSubmit = widgetMgr.allowFormEnterToSubmit(formId)\n-\n-      if (isEnterKeyPressed(e) && (ctrlKey || metaKey) && dirty) {\n-        e.preventDefault()\n-\n-        commitWidgetValue({ fromUi: true })\n-        if (allowFormEnterToSubmit) {\n-          widgetMgr.submitForm(formId, fragmentId)\n-        }\n-      }\n-    },\n-    [element, widgetMgr, dirty, commitWidgetValue, fragmentId]\n+  console.log(\"widgetMgr\", widgetMgr)\nComment: Can this be removed?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "frontend/lib/src/components/widgets/TextArea/TextArea.tsx",
    "pr_number": 9847,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1838372327,
    "comment_created_at": "2024-11-12T16:07:12Z"
  },
  {
    "code": "@@ -471,7 +472,7 @@ The following workflow action types are available:\n -   Tags, correspondent, document type and storage path\n -   Document owner\n -   View and / or edit permissions to users or groups\n--   Custom fields. Note that no value for the field will be set\n+-   Custom fields.",
    "comment": "revert this please",
    "line_number": 475,
    "enriched": "File: docs/usage.md\nCode: @@ -471,7 +472,7 @@ The following workflow action types are available:\n -   Tags, correspondent, document type and storage path\n -   Document owner\n -   View and / or edit permissions to users or groups\n--   Custom fields. Note that no value for the field will be set\n+-   Custom fields.\nComment: Revert this please",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/usage.md",
    "pr_number": 10771,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2335506691,
    "comment_created_at": "2025-09-10T04:19:42Z"
  },
  {
    "code": "@@ -503,9 +503,10 @@ for the possible codes and their meanings.\n \n ##### Date Localization\n \n-The `localize_date` filter formats a date or datetime object into a localized string using Babel internationalization.\n+The `localize_date` filter formats a date, datetime object into a localized string using Babel internationalization.",
    "comment": "this grammar doesn't make sense.",
    "line_number": 506,
    "enriched": "File: docs/advanced_usage.md\nCode: @@ -503,9 +503,10 @@ for the possible codes and their meanings.\n \n ##### Date Localization\n \n-The `localize_date` filter formats a date or datetime object into a localized string using Babel internationalization.\n+The `localize_date` filter formats a date, datetime object into a localized string using Babel internationalization.\nComment: This grammar doesn't make sense.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/advanced_usage.md",
    "pr_number": 10700,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2310398090,
    "comment_created_at": "2025-08-29T14:55:13Z"
  },
  {
    "code": "@@ -78,5 +78,6 @@\n       \"lmdb\",\n       \"msgpackr-extract\"\n     ]\n-  }\n+  },\n+  \"packageManager\": \"pnpm@10.15.0+sha512.486ebc259d3e999a4e8691ce03b5cac4a71cbeca39372a9b762cb500cfdf0873e2cb16abe3d951b1ee2cf012503f027b98b6584e4df22524e0c7450d9ec7aa7b\"\n }",
    "comment": "please dont include unrelated changes",
    "line_number": 83,
    "enriched": "File: src-ui/package.json\nCode: @@ -78,5 +78,6 @@\n       \"lmdb\",\n       \"msgpackr-extract\"\n     ]\n-  }\n+  },\n+  \"packageManager\": \"pnpm@10.15.0+sha512.486ebc259d3e999a4e8691ce03b5cac4a71cbeca39372a9b762cb500cfdf0873e2cb16abe3d951b1ee2cf012503f027b98b6584e4df22524e0c7450d9ec7aa7b\"\n }\nComment: Please dont include unrelated changes",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src-ui/package.json",
    "pr_number": 10671,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2296637522,
    "comment_created_at": "2025-08-24T12:14:51Z"
  },
  {
    "code": "@@ -310,31 +306,31 @@ def handle_inotify(self, directory, recursive, *, is_testing: bool):\n                 try:\n                     for event in inotify.read(timeout=timeout_ms):\n                         path = inotify.get_path(event.wd) if recursive else directory\n-                        filepath = os.path.join(path, event.name)\n+                        filepath = Path(path) / event.name\n+                        filepath_str = str(filepath)",
    "comment": "why the conversion back to a string here?  i believe a path is hashable?",
    "line_number": 310,
    "enriched": "File: src/documents/management/commands/document_consumer.py\nCode: @@ -310,31 +306,31 @@ def handle_inotify(self, directory, recursive, *, is_testing: bool):\n                 try:\n                     for event in inotify.read(timeout=timeout_ms):\n                         path = inotify.get_path(event.wd) if recursive else directory\n-                        filepath = os.path.join(path, event.name)\n+                        filepath = Path(path) / event.name\n+                        filepath_str = str(filepath)\nComment: Why the conversion back to a string here?  I believe a `Path` is hashable?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "src/documents/management/commands/document_consumer.py",
    "pr_number": 10539,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2310565219,
    "comment_created_at": "2025-08-29T16:13:17Z"
  },
  {
    "code": "@@ -179,10 +179,14 @@ following:\n \n ### Database Upgrades\n \n-In general, paperless does not require a specific version of PostgreSQL or MariaDB and it is\n+In general, Paperless-ngx supports current version of PostgreSQL and MariaDB and it is generally",
    "comment": "current versions?  maybe \"supported versions\"?  i assume django isn't dropping support until a product has gone eol?",
    "line_number": 182,
    "enriched": "File: docs/administration.md\nCode: @@ -179,10 +179,14 @@ following:\n \n ### Database Upgrades\n \n-In general, paperless does not require a specific version of PostgreSQL or MariaDB and it is\n+In general, Paperless-ngx supports current version of PostgreSQL and MariaDB and it is generally\nComment: Current versions?  Maybe \"supported versions\"?  I assume Django isn't dropping support until a product has gone EoL?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/administration.md",
    "pr_number": 10538,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2267442614,
    "comment_created_at": "2025-08-11T16:57:18Z"
  },
  {
    "code": "@@ -35,3 +36,21 @@\n \n         assert text_parser.get_text() == \"Pantothens\ufffdure\\n\"\n         assert text_parser.get_archive_path() is None\n+\n+    def test_thumbnail_large_file(self, text_parser: TextDocumentParser):\n+        \"\"\"\n+        GIVEN:\n+            - A very large text file (>50MB)\n+        WHEN:\n+            - A thumbnail is requested\n+        THEN:\n+            - A thumbnail is created without reading the entire file into memory\n+        \"\"\"\n+        large_file = Path(tempfile.mktemp(suffix=\".txt\"))",
    "comment": "## insecure temporary file\n\ncall to deprecated function tempfile.mktemp may be insecure.\n\n[show more details](https://github.com/paperless-ngx/paperless-ngx/security/code-scanning/28)",
    "line_number": 49,
    "enriched": "File: src/paperless_text/tests/test_parser.py\nCode: @@ -35,3 +36,21 @@\n \n         assert text_parser.get_text() == \"Pantothens\ufffdure\\n\"\n         assert text_parser.get_archive_path() is None\n+\n+    def test_thumbnail_large_file(self, text_parser: TextDocumentParser):\n+        \"\"\"\n+        GIVEN:\n+            - A very large text file (>50MB)\n+        WHEN:\n+            - A thumbnail is requested\n+        THEN:\n+            - A thumbnail is created without reading the entire file into memory\n+        \"\"\"\n+        large_file = Path(tempfile.mktemp(suffix=\".txt\"))\nComment: ## Insecure temporary file\n\nCall to deprecated function tempfile.mktemp may be insecure.\n\n[Show more details](https://github.com/paperless-ngx/paperless-ngx/security/code-scanning/28)",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/paperless_text/tests/test_parser.py",
    "pr_number": 10483,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2246396284,
    "comment_created_at": "2025-07-31T21:18:05Z"
  },
  {
    "code": "@@ -52,4 +60,66 @@ def parse_w_workflow_placeholders(\n         formatting.update({\"doc_title\": doc_title})\n     if doc_url is not None:\n         formatting.update({\"doc_url\": doc_url})\n-    return text.format(**formatting).strip()\n+    \n+    # Handle enhanced placeholders by pre-processing the text\n+    def replace_placeholder(match):\n+        \"\"\"Replace a single placeholder match with its processed value.\"\"\"\n+        placeholder_content = match.group(1)  # Content inside the braces\n+        \n+        # Regex pattern to parse enhanced placeholder syntax:\n+        # - {field_name:s/pattern/replacement/flags}\n+        # - Supports escaped slashes in pattern/replacement\n+        # - Flags (optional): any characters, but only i and s are supported\n+        regex_match = re.match(\n+            r'^(?P<field_name>[^:]+):\\s*s/(?P<pattern>(?:[^/\\\\]|\\\\.)*)/(?P<replacement>(?:[^/\\\\]|\\\\.)*)(?:/(?P<flags>[a-zA-Z]*))?/?$',\n+            placeholder_content\n+        )\n+        \n+        if regex_match:\n+            field_name = regex_match.group(\"field_name\")\n+            pattern = regex_match.group(\"pattern\")\n+            replacement = regex_match.group(\"replacement\")\n+            flags_str = regex_match.group(\"flags\") or ''\n+            \n+            # Validate flags and filter out unsupported ones\n+            supported_flags = {'i', 's'}\n+            # Only use supported flags, silently ignore unsupported ones\n+            valid_flags = ''.join(char for char in flags_str if char in supported_flags)\n+            \n+            # Convert flags string to re flags\n+            flags = 0\n+            if 'i' in valid_flags:\n+                flags |= re.IGNORECASE\n+            if 's' in valid_flags:\n+                flags |= re.DOTALL\n+            \n+            # Get the original value\n+            original_value = formatting.get(field_name, '')\n+            if isinstance(original_value, str):\n+                try:\n+                    # Convert sed-style backreferences ($1, $2) to Python style (\\1, \\2)\n+                    python_replacement = re.sub(r'\\$(\\d+)', r'\\\\\\1', replacement)",
    "comment": "the regex pattern for converting sed-style backreferences has excessive escaping that makes it hard to read and maintain. consider using raw strings more effectively or simplifying the pattern.\n",
    "line_number": 101,
    "enriched": "File: src/documents/templating/workflows.py\nCode: @@ -52,4 +60,66 @@ def parse_w_workflow_placeholders(\n         formatting.update({\"doc_title\": doc_title})\n     if doc_url is not None:\n         formatting.update({\"doc_url\": doc_url})\n-    return text.format(**formatting).strip()\n+    \n+    # Handle enhanced placeholders by pre-processing the text\n+    def replace_placeholder(match):\n+        \"\"\"Replace a single placeholder match with its processed value.\"\"\"\n+        placeholder_content = match.group(1)  # Content inside the braces\n+        \n+        # Regex pattern to parse enhanced placeholder syntax:\n+        # - {field_name:s/pattern/replacement/flags}\n+        # - Supports escaped slashes in pattern/replacement\n+        # - Flags (optional): any characters, but only i and s are supported\n+        regex_match = re.match(\n+            r'^(?P<field_name>[^:]+):\\s*s/(?P<pattern>(?:[^/\\\\]|\\\\.)*)/(?P<replacement>(?:[^/\\\\]|\\\\.)*)(?:/(?P<flags>[a-zA-Z]*))?/?$',\n+            placeholder_content\n+        )\n+        \n+        if regex_match:\n+            field_name = regex_match.group(\"field_name\")\n+            pattern = regex_match.group(\"pattern\")\n+            replacement = regex_match.group(\"replacement\")\n+            flags_str = regex_match.group(\"flags\") or ''\n+            \n+            # Validate flags and filter out unsupported ones\n+            supported_flags = {'i', 's'}\n+            # Only use supported flags, silently ignore unsupported ones\n+            valid_flags = ''.join(char for char in flags_str if char in supported_flags)\n+            \n+            # Convert flags string to re flags\n+            flags = 0\n+            if 'i' in valid_flags:\n+                flags |= re.IGNORECASE\n+            if 's' in valid_flags:\n+                flags |= re.DOTALL\n+            \n+            # Get the original value\n+            original_value = formatting.get(field_name, '')\n+            if isinstance(original_value, str):\n+                try:\n+                    # Convert sed-style backreferences ($1, $2) to Python style (\\1, \\2)\n+                    python_replacement = re.sub(r'\\$(\\d+)', r'\\\\\\1', replacement)\nComment: The regex pattern for converting sed-style backreferences has excessive escaping that makes it hard to read and maintain. Consider using raw strings more effectively or simplifying the pattern.\n```suggestion\n                    python_replacement = re.sub(r'\\$(\\d+)', r'\\\\1', replacement)\n```",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "src/documents/templating/workflows.py",
    "pr_number": 10418,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2217954328,
    "comment_created_at": "2025-07-20T20:32:31Z"
  },
  {
    "code": "@@ -442,10 +512,8 @@ def predict_document_type(self, content: str) -> int | None:\n             return None\n \n     def predict_tags(self, content: str) -> list[int]:\n-        from sklearn.utils.multiclass import type_of_target",
    "comment": "imports like these were deliberate.  not every path needed them, so it's a minor optimization to import only when needed",
    "line_number": 445,
    "enriched": "File: src/documents/classifier.py\nCode: @@ -442,10 +512,8 @@ def predict_document_type(self, content: str) -> int | None:\n             return None\n \n     def predict_tags(self, content: str) -> list[int]:\n-        from sklearn.utils.multiclass import type_of_target\nComment: Imports like these were deliberate.  Not every path needed them, so it's a minor optimization to import only when needed",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "src/documents/classifier.py",
    "pr_number": 10363,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2246328877,
    "comment_created_at": "2025-07-31T20:36:21Z"
  },
  {
    "code": "@@ -305,6 +307,9 @@ psycopg-c = [\n   { url = \"https://github.com/paperless-ngx/builder/releases/download/psycopg-3.2.9/psycopg_c-3.2.9-cp312-cp312-linux_x86_64.whl\", marker = \"sys_platform == 'linux' and platform_machine == 'x86_64' and python_version == '3.12'\" },\n   { url = \"https://github.com/paperless-ngx/builder/releases/download/psycopg-3.2.9/psycopg_c-3.2.9-cp312-cp312-linux_aarch64.whl\", marker = \"sys_platform == 'linux' and platform_machine == 'aarch64' and python_version == '3.12'\" },\n ]\n+psycopg-pool = [",
    "comment": "no need to get this from the builder releases.  it's plain python",
    "line_number": 310,
    "enriched": "File: pyproject.toml\nCode: @@ -305,6 +307,9 @@ psycopg-c = [\n   { url = \"https://github.com/paperless-ngx/builder/releases/download/psycopg-3.2.9/psycopg_c-3.2.9-cp312-cp312-linux_x86_64.whl\", marker = \"sys_platform == 'linux' and platform_machine == 'x86_64' and python_version == '3.12'\" },\n   { url = \"https://github.com/paperless-ngx/builder/releases/download/psycopg-3.2.9/psycopg_c-3.2.9-cp312-cp312-linux_aarch64.whl\", marker = \"sys_platform == 'linux' and platform_machine == 'aarch64' and python_version == '3.12'\" },\n ]\n+psycopg-pool = [\nComment: No need to get this from the builder releases.  it's plain Python",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pyproject.toml",
    "pr_number": 10354,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2193460989,
    "comment_created_at": "2025-07-08T21:19:04Z"
  },
  {
    "code": "@@ -83,7 +83,8 @@ RUN set -eux \\\n     && apt-get update \\\n     && apt-get install --yes --quiet ${PYTHON_PACKAGES}\n \n-COPY --from=ghcr.io/astral-sh/uv:0.6 /uv /bin/uv\n+COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/",
    "comment": "is there a reason you\u2019re adding uvx here?",
    "line_number": 86,
    "enriched": "File: .devcontainer/Dockerfile\nCode: @@ -83,7 +83,8 @@ RUN set -eux \\\n     && apt-get update \\\n     && apt-get install --yes --quiet ${PYTHON_PACKAGES}\n \n-COPY --from=ghcr.io/astral-sh/uv:0.6 /uv /bin/uv\n+COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/\nComment: Is there a reason you\u2019re adding uvx here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".devcontainer/Dockerfile",
    "pr_number": 10081,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2116867623,
    "comment_created_at": "2025-05-30T23:42:25Z"
  },
  {
    "code": "@@ -1189,17 +1189,17 @@\n \n         filename = self.get_log_filename(log_file)\n \n-        if not os.path.isfile(filename):\n+        if not Path(filename).is_file():",
    "comment": "## uncontrolled data used in path expression\n\nthis path depends on a [user-provided value](1).\n\n[show more details](https://github.com/paperless-ngx/paperless-ngx/security/code-scanning/26)",
    "line_number": 1192,
    "enriched": "File: src/documents/views.py\nCode: @@ -1189,17 +1189,17 @@\n \n         filename = self.get_log_filename(log_file)\n \n-        if not os.path.isfile(filename):\n+        if not Path(filename).is_file():\nComment: ## Uncontrolled data used in path expression\n\nThis path depends on a [user-provided value](1).\n\n[Show more details](https://github.com/paperless-ngx/paperless-ngx/security/code-scanning/26)",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/documents/views.py",
    "pr_number": 9933,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 2089426525,
    "comment_created_at": "2025-05-14T17:36:03Z"
  },
  {
    "code": "@@ -75,7 +75,7 @@ first-time setup.\n 4.  Install the Python dependencies:\n \n     ```bash\n-    $ uv sync --dev\n+    $ uv sync",
    "comment": "i think we should keep this explicit with --dev, to ensure those are installed",
    "line_number": 78,
    "enriched": "File: docs/development.md\nCode: @@ -75,7 +75,7 @@ first-time setup.\n 4.  Install the Python dependencies:\n \n     ```bash\n-    $ uv sync --dev\n+    $ uv sync\nComment: I think we should keep this explicit with `--dev`, to ensure those are installed",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/development.md",
    "pr_number": 9309,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 1982375209,
    "comment_created_at": "2025-03-06T00:27:34Z"
  },
  {
    "code": "@@ -220,4 +221,22 @@ describe(\"AppView element\", () => { expect(wrapper.find(\"StyledAppViewBlockSpacer\").exists()).toBe(false) expect(wrapper.find(\"StyledAppViewFooter\").exists()).toBe(false) }) + + describe(\"when window.location.hash changes\", () => { + let originalLocation: Location + beforeEach(() => (originalLocation = window.location)) + afterEach(() => (window.location = originalLocation)) + + it(\"sends UPDATE_HASH message to host\", () => {",
    "comment": "this test was missing before",
    "line_number": 230,
    "enriched": "File: frontend/src/components/core/AppView/AppView.test.tsx\nCode: @@ -220,4 +221,22 @@ describe(\"AppView element\", () => {\n     expect(wrapper.find(\"StyledAppViewBlockSpacer\").exists()).toBe(false)\n     expect(wrapper.find(\"StyledAppViewFooter\").exists()).toBe(false)\n   })\n+\n+  describe(\"when window.location.hash changes\", () => {\n+    let originalLocation: Location\n+    beforeEach(() => (originalLocation = window.location))\n+    afterEach(() => (window.location = originalLocation))\n+\n+    it(\"sends UPDATE_HASH message to host\", () => {\nComment: This test was missing before",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "frontend/src/components/core/AppView/AppView.test.tsx",
    "pr_number": 6345,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1142771094,
    "comment_created_at": "2023-03-20T23:33:53Z"
  },
  {
    "code": "@@ -159,18 +159,22 @@ def __eq__(self, other): def convert_to_tensor(x, dtype=None, sparse=None): if sparse: raise ValueError(\"`sparse=True` is not supported with torch backend\") + if type(x) is Variable: + # We cannot use `isinstance(x, Variable)` due to the failure of",
    "comment": "hi hongyu,\r\n\r\nhow do you reproduce this issue? i need to change this line and i want to make sure i don't create a regression here.\r\n\r\nthanks!",
    "line_number": 163,
    "enriched": "File: keras/src/backend/torch/core.py\nCode: @@ -159,18 +159,22 @@ def __eq__(self, other):\n def convert_to_tensor(x, dtype=None, sparse=None):\n     if sparse:\n         raise ValueError(\"`sparse=True` is not supported with torch backend\")\n+    if type(x) is Variable:\n+        # We cannot use `isinstance(x, Variable)` due to the failure of\nComment: Hi Hongyu,\r\n\r\nHow do you reproduce this issue? I need to change this line and I want to make sure I don't create a regression here.\r\n\r\nThanks!",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "keras/src/backend/torch/core.py",
    "pr_number": 19955,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 1790558245,
    "comment_created_at": "2024-10-07T16:42:02Z"
  },
  {
    "code": "@@ -320,7 +320,11 @@ def _save_chunk(self, start_i: int, end_i: int) -> None: res = df._get_values_for_csv(**self._number_format) data = list(res._iter_column_arrays()) - ix = self.data_index[slicer]._get_values_for_csv(**self._number_format) + ix = ( + self.data_index[slicer]._get_values_for_csv(**self._number_format) + if self.nlevels != 0 + else np.full(end_i - start_i, None)",
    "comment": "can you use np.empty instead?",
    "line_number": 326,
    "enriched": "File: pandas/io/formats/csvs.py\nCode: @@ -320,7 +320,11 @@ def _save_chunk(self, start_i: int, end_i: int) -> None:\n         res = df._get_values_for_csv(**self._number_format)\n         data = list(res._iter_column_arrays())\n \n-        ix = self.data_index[slicer]._get_values_for_csv(**self._number_format)\n+        ix = (\n+            self.data_index[slicer]._get_values_for_csv(**self._number_format)\n+            if self.nlevels != 0\n+            else np.full(end_i - start_i, None)\nComment: Can you use `np.empty` instead?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/io/formats/csvs.py",
    "pr_number": 59608,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1731547414,
    "comment_created_at": "2024-08-26T17:14:32Z"
  },
  {
    "code": "@@ -1553,6 +1553,28 @@ def test_custom_renderer(self): self.assertEqual(formset.non_form_errors().renderer, renderer) self.assertEqual(formset.empty_form.renderer, renderer) + def test_form_default_renderer(self): + \"\"\" + In the absense of a renderer passed to formset_factory, the default_renderer + attribute of the Form class should be respected.",
    "comment": "wrap at 79 chars.",
    "line_number": 1559,
    "enriched": "File: tests/forms_tests/tests/test_formsets.py\nCode: @@ -1553,6 +1553,28 @@ def test_custom_renderer(self):\n         self.assertEqual(formset.non_form_errors().renderer, renderer)\n         self.assertEqual(formset.empty_form.renderer, renderer)\n \n+    def test_form_default_renderer(self):\n+        \"\"\"\n+        In the absense of a renderer passed to formset_factory, the default_renderer\n+        attribute of the Form class should be respected.\nComment: Wrap at 79 chars.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "tests/forms_tests/tests/test_formsets.py",
    "pr_number": 16916,
    "repo": "django",
    "owner": "django",
    "comment_id": 1214025691,
    "comment_created_at": "2023-06-02T07:31:42Z"
  },
  {
    "code": "@@ -440,6 +441,27 @@ def test_nan_in_nested_tuple(self): table.get_item(other) assert str(error.value) == str(other) + def test_nan_in_namedtuple(self): + T = namedtuple(\"T\", [\"x\"]) + nan1 = T(float(\"nan\")) + nan2 = T(float(\"nan\")) + assert nan1.x is not nan2.x + table = ht.PyObjectHashTable() + table.set_item(nan1, 42) + assert table.get_item(nan2) == 42 + + def test_nan_in_nested_namedtuple(self): + T = namedtuple(\"T\", [\"x\", \"y\"]) + nan1 = T(1, (2, (float(\"nan\"),))) + nan2 = T(1, (2, (float(\"nan\"),))) + other = T(1, 2) + table = ht.PyObjectHashTable() + table.set_item(nan1, 42) + assert table.get_item(nan2) == 42 + with pytest.raises(KeyError, match=None) as error: + table.get_item(other) + assert str(error.value) == str(other)",
    "comment": "instead could you use the match argument instead of setting it to none?",
    "line_number": 463,
    "enriched": "File: pandas/tests/libs/test_hashtable.py\nCode: @@ -440,6 +441,27 @@ def test_nan_in_nested_tuple(self):\n             table.get_item(other)\n         assert str(error.value) == str(other)\n \n+    def test_nan_in_namedtuple(self):\n+        T = namedtuple(\"T\", [\"x\"])\n+        nan1 = T(float(\"nan\"))\n+        nan2 = T(float(\"nan\"))\n+        assert nan1.x is not nan2.x\n+        table = ht.PyObjectHashTable()\n+        table.set_item(nan1, 42)\n+        assert table.get_item(nan2) == 42\n+\n+    def test_nan_in_nested_namedtuple(self):\n+        T = namedtuple(\"T\", [\"x\", \"y\"])\n+        nan1 = T(1, (2, (float(\"nan\"),)))\n+        nan2 = T(1, (2, (float(\"nan\"),)))\n+        other = T(1, 2)\n+        table = ht.PyObjectHashTable()\n+        table.set_item(nan1, 42)\n+        assert table.get_item(nan2) == 42\n+        with pytest.raises(KeyError, match=None) as error:\n+            table.get_item(other)\n+        assert str(error.value) == str(other)\nComment: Instead could you use the `match` argument instead of setting it to `None`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/tests/libs/test_hashtable.py",
    "pr_number": 59286,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1686907942,
    "comment_created_at": "2024-07-22T17:38:40Z"
  },
  {
    "code": "@@ -0,0 +1,38 @@ +name: Check Reflex run on Node.js Latest + +on: + push: + branches: + - main + pull_request: + branches: + - main + +env: + TELEMETRY_ENABLED: false + +jobs: + check_latest_node: + runs-on: ubuntu-latest + strategy: + matrix: + python-version: ['3.12'] + node-version: ['node'] + steps: + - uses: actions/checkout@v4 + - uses: ./.github/actions/setup_build_env + with: + python-version: ${{ matrix.python-version }} + run-poetry-install: true + create-venv-at-path: .venv + - uses: actions/setup-node@v4 + with: + node-version: ${{ matrix.node-version }} + - run: | + poetry run uv pip install playwright pytest-playwright pyvirtualdisplay pillow + poetry run playwright install --with-deps + - run: | + poetry run pytest integration/test_node_version.py",
    "comment": "can we run _all_ of the integration tests with latest node to improve our coverage and catch more potential errors?",
    "line_number": 35,
    "enriched": "File: .github/workflows/check_node_latest.yml\nCode: @@ -0,0 +1,38 @@\n+name: Check Reflex run on Node.js Latest\n+\n+on:\n+    push:\n+        branches:\n+            - main\n+    pull_request:\n+        branches:\n+            - main\n+\n+env:\n+    TELEMETRY_ENABLED: false\n+\n+jobs:\n+    check_latest_node:\n+        runs-on: ubuntu-latest\n+        strategy:\n+            matrix:\n+                python-version: ['3.12']\n+                node-version: ['node']\n+        steps:\n+            - uses: actions/checkout@v4\n+            - uses: ./.github/actions/setup_build_env\n+              with:\n+                python-version: ${{ matrix.python-version }}\n+                run-poetry-install: true\n+                create-venv-at-path: .venv\n+            - uses: actions/setup-node@v4\n+              with:\n+                node-version: ${{ matrix.node-version }}\n+            - run: |\n+                poetry run uv pip install playwright pytest-playwright pyvirtualdisplay pillow\n+                poetry run playwright install --with-deps\n+            - run: |\n+                poetry run pytest integration/test_node_version.py\nComment: ```suggestion\r\n                poetry run pytest integration/\r\n```\r\n\r\ncan we run _all_ of the integration tests with latest node to improve our coverage and catch more potential errors?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".github/workflows/check_node_latest.yml",
    "pr_number": 3981,
    "repo": "reflex",
    "owner": "reflex-dev",
    "comment_id": 1774216546,
    "comment_created_at": "2024-09-24T23:25:28Z"
  },
  {
    "code": "@@ -69,9 +69,11 @@ Here's an example spider using BeautifulSoup API, with ``lxml`` as the HTML pars What Python versions does Scrapy support? ----------------------------------------- -Scrapy is supported under Python 2.7 and Python 3.4+. +Scrapy is supported under Python 2.7 and Python 3.4+ +under CPython (default Python implementation) and PyPy (only for Python 2.7).",
    "comment": "do you know what's the reason pypy3 is not supported? is it just us not running tests for it, or are there things which work in pypy2, but not in pypy3?",
    "line_number": 73,
    "enriched": "File: docs/faq.rst\nCode: @@ -69,9 +69,11 @@ Here's an example spider using BeautifulSoup API, with ``lxml`` as the HTML pars\n What Python versions does Scrapy support?\n -----------------------------------------\n \n-Scrapy is supported under Python 2.7 and Python 3.4+.\n+Scrapy is supported under Python 2.7 and Python 3.4+\n+under CPython (default Python implementation) and PyPy (only for Python 2.7).\nComment: Do you know what's the reason pypy3 is not supported? Is it just us not running tests for it, or are there things which work in pypy2, but not in pypy3?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/faq.rst",
    "pr_number": 3048,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 158634341,
    "comment_created_at": "2017-12-25T09:46:16Z"
  },
  {
    "code": "@@ -102,8 +102,9 @@ def pivot_table( on the rows and columns. dropna : bool, default True Do not include columns whose entries are all NaN. If True, - rows with a NaN value in any column will be omitted before - computing margins. + - rows with a NaN value in any column will be omitted before computing margins, + - index/column keys containing NA values will be dropped (see ``dropna`` + parameter in ``pandas.DataFrame.groupby``).",
    "comment": "you'll also need to update the entry in shared_docs within pandas.core.frame.",
    "line_number": 107,
    "enriched": "File: pandas/core/reshape/pivot.py\nCode: @@ -102,8 +102,9 @@ def pivot_table(\n         on the rows and columns.\n     dropna : bool, default True\n         Do not include columns whose entries are all NaN. If True,\n-        rows with a NaN value in any column will be omitted before\n-        computing margins.\n+        - rows with a NaN value in any column will be omitted before computing margins,\n+        - index/column keys containing NA values will be dropped (see ``dropna``\n+        parameter in ``pandas.DataFrame.groupby``).\nComment: You'll also need to update the entry in `shared_docs` within `pandas.core.frame`.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/core/reshape/pivot.py",
    "pr_number": 61184,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2019176926,
    "comment_created_at": "2025-03-28T18:54:05Z"
  },
  {
    "code": "@@ -167,3 +167,13 @@ def test_load_llmchain_with_non_serializable_arg() -> None: chain_obj = dumpd(chain) with pytest.raises(NotImplementedError): load(chain_obj, secrets_map={\"OPENAI_API_KEY\": \"hello\"}) + + +@pytest.mark.requires(\"openai\", \"langchain_openai\") +def test_loads_with_missing_secrets() -> None: + import openai + + llm_string = '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"llms\", \"openai\", \"OpenAI\"], \"kwargs\": {\"model_name\": \"davinci\", \"temperature\": 0.5, \"max_tokens\": 256, \"top_p\": 0.8, \"n\": 1, \"best_of\": 1, \"openai_api_key\": {\"lc\": 1, \"type\": \"secret\", \"id\": [\"OPENAI_API_KEY\"]}, \"batch_size\": 20, \"max_retries\": 2, \"disallowed_special\": \"all\"}, \"name\": \"OpenAI\"}' # noqa: E501 + # Should throw on instantiation, not deserialization + with pytest.raises(openai.OpenAIError): + loads(llm_string, secrets_map={\"OPENAI_API_KEY\": \"hello\"})",
    "comment": "the key isn't missing here is it?",
    "line_number": 179,
    "enriched": "File: libs/langchain/tests/unit_tests/load/test_load.py\nCode: @@ -167,3 +167,13 @@ def test_load_llmchain_with_non_serializable_arg() -> None:\n     chain_obj = dumpd(chain)\n     with pytest.raises(NotImplementedError):\n         load(chain_obj, secrets_map={\"OPENAI_API_KEY\": \"hello\"})\n+\n+\n+@pytest.mark.requires(\"openai\", \"langchain_openai\")\n+def test_loads_with_missing_secrets() -> None:\n+    import openai\n+\n+    llm_string = '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"llms\", \"openai\", \"OpenAI\"], \"kwargs\": {\"model_name\": \"davinci\", \"temperature\": 0.5, \"max_tokens\": 256, \"top_p\": 0.8, \"n\": 1, \"best_of\": 1, \"openai_api_key\": {\"lc\": 1, \"type\": \"secret\", \"id\": [\"OPENAI_API_KEY\"]}, \"batch_size\": 20, \"max_retries\": 2, \"disallowed_special\": \"all\"}, \"name\": \"OpenAI\"}'  # noqa: E501\n+    # Should throw on instantiation, not deserialization\n+    with pytest.raises(openai.OpenAIError):\n+        loads(llm_string, secrets_map={\"OPENAI_API_KEY\": \"hello\"})\nComment: the key isn't missing here is it?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "libs/langchain/tests/unit_tests/load/test_load.py",
    "pr_number": 30252,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 1992926051,
    "comment_created_at": "2025-03-13T07:23:33Z"
  },
  {
    "code": "@@ -388,7 +388,10 @@ def run(self, data_loaders, workflow, max_epochs, **kwargs): def register_lr_hook(self, lr_config): if isinstance(lr_config, dict): assert 'policy' in lr_config - hook_type = lr_config.pop('policy').title() + 'LrUpdaterHook' + policy_type = lr_config.pop('policy') + if policy_type == policy_type.lower(): + policy_type = policy_type.title() + hook_type = policy_type + 'LrUpdaterHook'",
    "comment": "add some comments for the above logic, otherwise users may get confused.",
    "line_number": 394,
    "enriched": "File: mmcv/runner/runner.py\nCode: @@ -388,7 +388,10 @@ def run(self, data_loaders, workflow, max_epochs, **kwargs):\n     def register_lr_hook(self, lr_config):\n         if isinstance(lr_config, dict):\n             assert 'policy' in lr_config\n-            hook_type = lr_config.pop('policy').title() + 'LrUpdaterHook'\n+            policy_type = lr_config.pop('policy')\n+            if policy_type == policy_type.lower():\n+                policy_type = policy_type.title()\n+            hook_type = policy_type + 'LrUpdaterHook'\nComment: Add some comments for the above logic, otherwise users may get confused.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "mmcv/runner/runner.py",
    "pr_number": 265,
    "repo": "mmcv",
    "owner": "open-mmlab",
    "comment_id": 419110617,
    "comment_created_at": "2020-05-03T14:22:34Z"
  },
  {
    "code": "@@ -68,15 +69,51 @@ def __setattr__(self, key: str, value: Union[str, Iterable[str]]) -> None: @gather_metrics(\"query_params.get_all\") def get_all(self, key: str) -> List[str]: + \"\"\" + Get a list of all query parameter values associated to a given key. + + When a key is repeated as a query parameter within the URL, this method + allows all values to be obtained. In contrast, dict-like methods only + retireve tha last value when a key is repeated in the URL.",
    "comment": "typo:",
    "line_number": 77,
    "enriched": "File: lib/streamlit/runtime/state/query_params_proxy.py\nCode: @@ -68,15 +69,51 @@ def __setattr__(self, key: str, value: Union[str, Iterable[str]]) -> None:\n \n     @gather_metrics(\"query_params.get_all\")\n     def get_all(self, key: str) -> List[str]:\n+        \"\"\"\n+        Get a list of all query parameter values associated to a given key.\n+\n+        When a key is repeated as a query parameter within the URL, this method\n+        allows all values to be obtained. In contrast, dict-like methods only\n+        retireve tha last value when a key is repeated in the URL.\nComment: typo:\r\n```suggestion\r\n        retrieve the last value when a key is repeated in the URL.\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "lib/streamlit/runtime/state/query_params_proxy.py",
    "pr_number": 7916,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1445414819,
    "comment_created_at": "2024-01-08T22:25:57Z"
  },
  {
    "code": "@@ -36,3 +36,11 @@ - Explore a [New York City rideshare dataset](https://github.com/streamlit/demo-uber-nyc-pickups) \"\"\" ) + +# Preload Python modules that take a while to compile in a new venv. +# Otherwise, when users switch to another page, it seems that Streamlit +# is slow, when in reality this is just an artifact of loading/compiling +# large modules from zero. +with st.spinner(\"Preloading Python modules for other pages...\"): + import numpy # noqa: ICN001 F401 + import pandas # noqa: ICN001 F401",
    "comment": "the python guide states 'prefer importing entire modules instead of single functions: from streamlit import mymodule over from streamlit.mymodule import internal_function'. the current imports import numpy and import pandas should be changed to use full module names like import numpy as np and import pandas as pd to follow the established convention, or alternatively use from numpy import numpy and from pandas import pandas if the intent is to import the modules themselves.\n\n  \n\n*spotted by [diamond](https://app.graphite.dev/diamond/?org=streamlit&ref=ai-review-comment) (based on custom rule: [python guide](https://app.graphite.dev/diamond/?org=streamlit&ref=ai-review-comment&view=settings&rule=gt_reviewer_rule_01jxr02cezfzgrwjc9mh1q0250))*<i class='graphite__hidden'><br /><br /><a href=\"https://app.graphite.dev/github/pr/streamlit/streamlit/12617?chatwithgeneratedcomment=09f35748-34ce-4e59-8fde-3dee1cec56dc\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://static.graphite.dev/github-diamond-fix-in-graphite-dark.svg\"><source media=\"(prefers-color-scheme: light)\" srcset=\"https://static.graphite.dev/github-diamond-fix-in-graphite-light.svg\"><img alt=\"fix in graphite\" src=\"https://static.graphite.dev/github-diamond-fix-in-graphite-dark.svg\"></picture></a></i><i class='graphite__hidden'><br /><br />is this helpful? react \ud83d\udc4d or \ud83d\udc4e to let us know.</i>",
    "line_number": 46,
    "enriched": "File: lib/streamlit/hello/hello.py\nCode: @@ -36,3 +36,11 @@\n     - Explore a [New York City rideshare dataset](https://github.com/streamlit/demo-uber-nyc-pickups)\n     \"\"\"\n )\n+\n+# Preload Python modules that take a while to compile in a new venv.\n+# Otherwise, when users switch to another page, it seems that Streamlit\n+# is slow, when in reality this is just an artifact of loading/compiling\n+# large modules from zero.\n+with st.spinner(\"Preloading Python modules for other pages...\"):\n+    import numpy  # noqa: ICN001 F401\n+    import pandas  # noqa: ICN001 F401\nComment: The Python Guide states 'Prefer importing entire modules instead of single functions: `from streamlit import mymodule` over `from streamlit.mymodule import internal_function`'. The current imports `import numpy` and `import pandas` should be changed to use full module names like `import numpy as np` and `import pandas as pd` to follow the established convention, or alternatively use `from numpy import numpy` and `from pandas import pandas` if the intent is to import the modules themselves.\n```suggestion\n    import numpy as np  # noqa: ICN001 F401\n    import pandas as pd  # noqa: ICN001 F401\n```\n  \n\n*Spotted by [Diamond](https://app.graphite.dev/diamond/?org=streamlit&ref=ai-review-comment) (based on custom rule: [Python Guide](https://app.graphite.dev/diamond/?org=streamlit&ref=ai-review-comment&view=settings&rule=gt_reviewer_rule_01jxr02cezfzgrwjc9mh1q0250))*<i class='graphite__hidden'><br /><br /><a href=\"https://app.graphite.dev/github/pr/streamlit/streamlit/12617?chatWithGeneratedComment=09f35748-34ce-4e59-8fde-3dee1cec56dc\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://static.graphite.dev/github-diamond-fix-in-graphite-dark.svg\"><source media=\"(prefers-color-scheme: light)\" srcset=\"https://static.graphite.dev/github-diamond-fix-in-graphite-light.svg\"><img alt=\"Fix in Graphite\" src=\"https://static.graphite.dev/github-diamond-fix-in-graphite-dark.svg\"></picture></a></i><i class='graphite__hidden'><br /><br />Is this helpful? React \ud83d\udc4d or \ud83d\udc4e to let us know.</i>",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "lib/streamlit/hello/hello.py",
    "pr_number": 12617,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 2373805552,
    "comment_created_at": "2025-09-24T01:10:34Z"
  },
  {
    "code": "@@ -323,28 +323,41 @@ def load_from_pavi(filename, map_location=None): @CheckpointLoader.register_scheme(prefixes='s3://') -def load_from_ceph(filename, map_location=None, backend='ceph'): +def load_from_ceph(filename, map_location=None, backend='petrel'): \"\"\"load checkpoint through the file path prefixed with s3. In distributed setting, this function download ckpt at all ranks to different temporary directories. Args: filename (str): checkpoint file path with s3 prefix map_location (str, optional): Same as :func:`torch.load`. - backend (str): The storage backend type. Options are \"disk\", \"ceph\", - \"memcached\" and \"lmdb\". Default: 'ceph' + backend (str): The storage backend type. Options are 'ceph', 'petrel'. + Default: 'petrel'. + + ..warning:: + :class:`CephBackend` is deprecated using :class:`PetrelBackend` instead",
    "comment": ":class:cephbackend will be deprecated, please use :class:petrelbackend instead",
    "line_number": 338,
    "enriched": "File: mmcv/runner/checkpoint.py\nCode: @@ -323,28 +323,41 @@ def load_from_pavi(filename, map_location=None):\n \n \n @CheckpointLoader.register_scheme(prefixes='s3://')\n-def load_from_ceph(filename, map_location=None, backend='ceph'):\n+def load_from_ceph(filename, map_location=None, backend='petrel'):\n     \"\"\"load checkpoint through the file path prefixed with s3.  In distributed\n     setting, this function download ckpt at all ranks to different temporary\n     directories.\n \n     Args:\n         filename (str): checkpoint file path with s3 prefix\n         map_location (str, optional): Same as :func:`torch.load`.\n-        backend (str): The storage backend type. Options are \"disk\", \"ceph\",\n-            \"memcached\" and \"lmdb\". Default: 'ceph'\n+        backend (str): The storage backend type. Options are 'ceph', 'petrel'.\n+            Default: 'petrel'.\n+\n+    ..warning::\n+        :class:`CephBackend` is deprecated using :class:`PetrelBackend` instead\nComment: ```\r\n:class:`CephBackend` will be deprecated, please use :class:`PetrelBackend` instead\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "mmcv/runner/checkpoint.py",
    "pr_number": 1375,
    "repo": "mmcv",
    "owner": "open-mmlab",
    "comment_id": 724662302,
    "comment_created_at": "2021-10-08T02:48:05Z"
  },
  {
    "code": "@@ -12,14 +12,13 @@ Branching Make the branch --------------- -This is only needed when starting a new maintenance branch. Because -NumPy now depends on tags to determine the version, the start of a new -development cycle in the main branch needs an annotated tag. That is done +This is only needed when starting a new maintenance branch. The start of a new +development cycle in the main branch should get an annotated tag. That is done as follows:: $ git checkout main $ git pull upstream main - $ git commit --allow-empty -m'REL: Begin NumPy 1.22.0 development' + $ git commit --allow-empty -m'REL: Begin NumPy 2.4.0 development' $ git push upstream HEAD",
    "comment": "i've changed this in practice, but not yet here. i just make it an ordinary pr, when it goes in marks the beginning of new development. i did that because pushing directly to the branch bothers me, and a pr is more flexible. i can make those modifications after this goes in.\r\n\r\ni also use that pr to set the development version in pyproject.toml, which is all that is needed since i removed pavement.py and put write_release in tools.",
    "line_number": 23,
    "enriched": "File: doc/BRANCH_WALKTHROUGH.rst\nCode: @@ -12,14 +12,13 @@ Branching\n Make the branch\n ---------------\n \n-This is only needed when starting a new maintenance branch. Because\n-NumPy now depends on tags to determine the version, the start of a new\n-development cycle in the main branch needs an annotated tag. That is done\n+This is only needed when starting a new maintenance branch. The start of a new\n+development cycle in the main branch should get an annotated tag. That is done\n as follows::\n \n     $ git checkout main\n     $ git pull upstream main\n-    $ git commit --allow-empty -m'REL: Begin NumPy 1.22.0 development'\n+    $ git commit --allow-empty -m'REL: Begin NumPy 2.4.0 development'\n     $ git push upstream HEAD\n \nComment: I've changed this in practice, but not yet here. I just make it an ordinary PR, when it goes in marks the beginning of new development. I did that because pushing directly to the branch bothers me, and a PR is more flexible. I can make those modifications after this goes in.\r\n\r\nI also use that PR to set the development version in pyproject.toml, which is all that is needed since I removed `pavement.py` and put `write_release` in `tools`.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "doc/BRANCH_WALKTHROUGH.rst",
    "pr_number": 29732,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2337005554,
    "comment_created_at": "2025-09-10T14:42:08Z"
  },
  {
    "code": "@@ -1642,6 +1642,26 @@ def test_replace_dumps_kwargs(self): self.assertEqual(kwargs[\"ensure_ascii\"], True) self.assertEqual(kwargs[\"allow_nan\"], True) + def test_replacement_both_body_and_data_warns(self): + \"\"\"Test that we can get a warning if both body and data are passed for branch coverage\"\"\" + body1 = None + body2 = b\"body\" + data1 = { + \"name1\": \"value1\", + } + data2 = { + \"name2\": \"value2\", + } + r1 = self.request_class(url=\"http://www.example.com/\", data=data1, body=body1) + + with mock.patch(\"warnings.warn\") as mock_warn:",
    "comment": "https://docs.pytest.org/en/latest/how-to/capture-warnings.html",
    "line_number": 1657,
    "enriched": "File: tests/test_http_request.py\nCode: @@ -1642,6 +1642,26 @@ def test_replace_dumps_kwargs(self):\n             self.assertEqual(kwargs[\"ensure_ascii\"], True)\n             self.assertEqual(kwargs[\"allow_nan\"], True)\n \n+    def test_replacement_both_body_and_data_warns(self):\n+        \"\"\"Test that we can get a warning if both body and data are passed for branch coverage\"\"\"\n+        body1 = None\n+        body2 = b\"body\"\n+        data1 = {\n+            \"name1\": \"value1\",\n+        }\n+        data2 = {\n+            \"name2\": \"value2\",\n+        }\n+        r1 = self.request_class(url=\"http://www.example.com/\", data=data1, body=body1)\n+\n+        with mock.patch(\"warnings.warn\") as mock_warn:\nComment: https://docs.pytest.org/en/latest/how-to/capture-warnings.html",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "tests/test_http_request.py",
    "pr_number": 6236,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 1497380068,
    "comment_created_at": "2024-02-21T11:44:03Z"
  },
  {
    "code": "@@ -154,6 +154,8 @@ def __init__(self, settings): self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None uripar = settings['FEED_URI_PARAMS'] self._uripar = load_object(uripar) if uripar else lambda x, y: None + self.batch_size = settings['FEED_BATCH_SIZE'] + self.start = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')",
    "comment": "here we defined a variable to group all batches in a directory.\n\nexample : \n\n\nstorage uri : /tmp/%(start)s/%(time)s.json\n\n/tmp\n   |_ 2015-07-03t20-29-01 (start)\n      |_ 2015-07-03t20-29-04.json (time)\n      |_ 2015-07-03t20-52-06.json (time)",
    "line_number": 158,
    "enriched": "File: scrapy/extensions/feedexport.py\nCode: @@ -154,6 +154,8 @@ def __init__(self, settings):\n         self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None\n         uripar = settings['FEED_URI_PARAMS']\n         self._uripar = load_object(uripar) if uripar else lambda x, y: None\n+        self.batch_size = settings['FEED_BATCH_SIZE']\n+        self.start = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')\nComment: Here we defined a variable to group all batches in a directory.\n\nExample : \n\n```\nStorage URI : /tmp/%(start)s/%(time)s.json\n\n/tmp\n   |_ 2015-07-03T20-29-01 (start)\n      |_ 2015-07-03T20-29-04.json (time)\n      |_ 2015-07-03T20-52-06.json (time)\n```\n",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "scrapy/extensions/feedexport.py",
    "pr_number": 1340,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 33898286,
    "comment_created_at": "2015-07-05T19:55:14Z"
  },
  {
    "code": "@@ -0,0 +1,88 @@ +#define _UMATHMODULE +#define _MULTIARRAYMODULE +#define NPY_NO_DEPRECATED_API NPY_API_VERSION + +#define PY_SSIZE_T_CLEAN +#include <Python.h> + +#undef HWY_TARGET_INCLUDE +#define HWY_TARGET_INCLUDE \"absolute.cpp\" // this file +#include <hwy/foreach_target.h> // must come before highway.h +#include <hwy/highway.h> + +#include \"numpy/ndarraytypes.h\" +#include \"numpy/npy_common.h\" +#include \"numpy/npy_math.h\" +#include \"numpy/utils.h\" + +namespace numpy { +namespace HWY_NAMESPACE { // required: unique per target + +// Can skip hn:: prefixes if already inside hwy::HWY_NAMESPACE. +namespace hn = hwy::HWY_NAMESPACE; + +// Alternative to per-function HWY_ATTR: see HWY_BEFORE_NAMESPACE +template <typename T> +HWY_ATTR void SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) { + const T* HWY_RESTRICT input_array = (const T*) args[0]; + T* HWY_RESTRICT output_array = (T*) args[1]; + const size_t size = dimensions[0]; + const hn::ScalableTag<T> d; + + for (size_t i = 0; i < size; i += hn::Lanes(d)) { + const auto in = hn::Load(d, input_array + i); + auto x = hn::Abs(in); + hn::Store(x, d, output_array + i); + } +} + +HWY_ATTR void INT_SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) { + SuperAbsolute<npy_int>(args, dimensions, steps); +} + +HWY_ATTR void DOUBLE_SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) { + SuperAbsolute<npy_double>(args, dimensions, steps); +} + +HWY_ATTR void FLOAT_SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) { + SuperAbsolute<npy_float>(args, dimensions, steps); +} + +} +} + +#if HWY_ONCE +namespace numpy { + +HWY_EXPORT(INT_SuperAbsolute); +HWY_EXPORT(FLOAT_SuperAbsolute); +HWY_EXPORT(DOUBLE_SuperAbsolute); + +extern \"C\" { + +NPY_NO_EXPORT void +INT_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)) +{ + static auto dispatcher = HWY_DYNAMIC_DISPATCH(INT_SuperAbsolute); + return dispatcher(args, dimensions, steps); +} + +NPY_NO_EXPORT void +DOUBLE_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)) +{ + static auto dispatcher = HWY_DYNAMIC_DISPATCH(DOUBLE_SuperAbsolute); + return dispatcher(args, dimensions, steps); +} + +NPY_NO_EXPORT void +FLOAT_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)) +{ + static auto dispatcher = HWY_DYNAMIC_DISPATCH(FLOAT_SuperAbsolute);",
    "comment": "looks simple but i'm afraid of c++ evilness may involved since static initialization requires thread safety so this call usually will be warped in between a thread guard which make it actually slower than uses local variable if highway cached cpuid calls similar to what numpy does.\r\nhere a pesudo code:\r\npython\r\nstatic the_dudced_type dispatcher;\r\nstatic bool dispatcher_once = false\r\ncall cx_gaurd_lock\r\nif not dispatcher_once\r\n   dispatcher = hwy_dynamic_dispatch(float_superabsolute)\r\n   dispatcher_once = true\r\nendif\r\ncall cx_gaurd_unlock",
    "line_number": 80,
    "enriched": "File: numpy/core/src/umath/absolute.cpp\nCode: @@ -0,0 +1,88 @@\n+#define _UMATHMODULE\n+#define _MULTIARRAYMODULE\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+\n+#define PY_SSIZE_T_CLEAN\n+#include <Python.h>\n+\n+#undef HWY_TARGET_INCLUDE\n+#define HWY_TARGET_INCLUDE \"absolute.cpp\"  // this file\n+#include <hwy/foreach_target.h>  // must come before highway.h\n+#include <hwy/highway.h>\n+\n+#include \"numpy/ndarraytypes.h\"\n+#include \"numpy/npy_common.h\"\n+#include \"numpy/npy_math.h\"\n+#include \"numpy/utils.h\"\n+\n+namespace numpy {\n+namespace HWY_NAMESPACE {  // required: unique per target\n+\n+// Can skip hn:: prefixes if already inside hwy::HWY_NAMESPACE.\n+namespace hn = hwy::HWY_NAMESPACE;\n+\n+// Alternative to per-function HWY_ATTR: see HWY_BEFORE_NAMESPACE\n+template <typename T>\n+HWY_ATTR void SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) {\n+  const T* HWY_RESTRICT input_array = (const T*) args[0];\n+  T* HWY_RESTRICT output_array = (T*) args[1];\n+  const size_t size = dimensions[0];\n+  const hn::ScalableTag<T> d;\n+  \n+  for (size_t i = 0; i < size; i += hn::Lanes(d)) {\n+    const auto in = hn::Load(d, input_array + i);\n+    auto x = hn::Abs(in);\n+    hn::Store(x, d, output_array + i);\n+  }\n+}\n+\n+HWY_ATTR void INT_SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) {\n+  SuperAbsolute<npy_int>(args, dimensions, steps);\n+}\n+\n+HWY_ATTR void DOUBLE_SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) {\n+  SuperAbsolute<npy_double>(args, dimensions, steps);\n+}\n+\n+HWY_ATTR void FLOAT_SuperAbsolute(char **args, npy_intp const *dimensions, npy_intp const *steps) {\n+  SuperAbsolute<npy_float>(args, dimensions, steps);\n+}\n+\n+}\n+}\n+\n+#if HWY_ONCE\n+namespace numpy {\n+\n+HWY_EXPORT(INT_SuperAbsolute);\n+HWY_EXPORT(FLOAT_SuperAbsolute);\n+HWY_EXPORT(DOUBLE_SuperAbsolute);\n+\n+extern \"C\" {\n+\n+NPY_NO_EXPORT void\n+INT_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+  static auto dispatcher = HWY_DYNAMIC_DISPATCH(INT_SuperAbsolute);\n+  return dispatcher(args, dimensions, steps);\n+}\n+\n+NPY_NO_EXPORT void\n+DOUBLE_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+  static auto dispatcher = HWY_DYNAMIC_DISPATCH(DOUBLE_SuperAbsolute);\n+  return dispatcher(args, dimensions, steps);\n+}\n+\n+NPY_NO_EXPORT void\n+FLOAT_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+  static auto dispatcher = HWY_DYNAMIC_DISPATCH(FLOAT_SuperAbsolute);\nComment: ```suggestion\r\n  auto dispatcher = HWY_DYNAMIC_DISPATCH(FLOAT_SuperAbsolute);\r\n```\r\nLooks simple but I'm afraid of C++ evilness may involved since static initialization requires thread safety so this call usually will be warped in between a thread guard which make it actually slower than uses local variable if Highway cached CPUID calls similar to what NumPy does.\r\nHere a pesudo code:\r\n```Python\r\nstatic the_dudced_type dispatcher;\r\nstatic bool dispatcher_once = false\r\ncall cx_gaurd_lock\r\nif not dispatcher_once\r\n   dispatcher = HWY_DYNAMIC_DISPATCH(FLOAT_SuperAbsolute)\r\n   dispatcher_once = true\r\nendif\r\ncall cx_gaurd_unlock\r\n```\r\n",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "numpy/core/src/umath/absolute.cpp",
    "pr_number": 24384,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 1292330069,
    "comment_created_at": "2023-08-12T14:32:47Z"
  },
  {
    "code": "@@ -228,34 +236,82 @@ const DatasourceModal: FunctionComponent<DatasourceModalProps> = ({ setErrors(err); }; - const renderSaveDialog = () => ( - <div> - <Alert - css={theme => ({ - marginTop: theme.gridUnit * 4, - marginBottom: theme.gridUnit * 4, - })} - type=\"warning\" - showIcon - message={t(`The dataset configuration exposed here + const getSaveDialog = useCallback( + () => ( + <div> + <Alert + css={theme => ({ + marginTop: theme.gridUnit * 4, + marginBottom: theme.gridUnit * 4, + })} + type=\"warning\" + showIcon={false} + message={t(`The dataset configuration exposed here affects all the charts using this dataset. Be mindful that changing settings here may affect other charts in undesirable ways.`)} - /> - {t('Are you sure you want to save and apply changes?')} - </div> + /> + {datasource.sql !== currentDatasource.sql && ( + <> + <Alert + css={theme => ({ + marginTop: theme.gridUnit * 4, + marginBottom: theme.gridUnit * 4, + })} + type=\"info\" + showIcon={false} + message={t(`The dataset columns will be automatically synced + given the changes in your SQL query. If your changes don't + impact columns, you might want to skip this step.`)} + /> + <Checkbox + checked={syncColumnsRef.current} + onChange={() => { + syncColumnsRef.current = !syncColumnsRef.current; + if (confirmModal) { + confirmModal.update({ + content: getSaveDialog(), + }); + } + }} + /> + <span className=\"m-l-5\">{t('Automatically sync columns')}</span> + <br /> + <br /> + </> + )} + {t('Are you sure you want to save and apply changes?')} + </div> + ), + [currentDatasource.sql, datasource.sql, confirmModal], ); + useEffect(() => { + if (confirmModal) { + confirmModal.update({ + content: getSaveDialog(), + }); + } + }, [confirmModal, getSaveDialog]); + + useEffect(() => { + if (datasource.sql !== currentDatasource.sql) { + syncColumnsRef.current = true; + }",
    "comment": "### sync columns checkbox state not reset <sub>![category functionality](https://img.shields.io/badge/functionality-0284c7)</sub>\n\n<details>\n  <summary>tell me more</summary>\n\n###### what is the issue?\nthe sync columns checkbox is automatically checked when sql changes, but it doesn't get unchecked when sql reverts to its original state.\n\n\n###### why this matters\nusers may be confused when the checkbox remains checked even after reverting their sql changes, potentially leading to unintended column synchronization.\n\n###### suggested change \u2219 *feature preview*\nupdate the useeffect to handle sql reversion:\ntypescript\nuseeffect(() => {\n  synccolumnsref.current = datasource.sql !== currentdatasource.sql;\n}, [datasource.sql, currentdatasource.sql]);\n\n\n\n###### provide feedback to improve future suggestions\n[![nice catch](https://img.shields.io/badge/\ud83d\udc4d%20nice%20catch-71bc78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/17a10732-fadf-46cb-92ff-c1a6a6564fd6/upvote) [![incorrect](https://img.shields.io/badge/\ud83d\udc4e%20incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/17a10732-fadf-46cb-92ff-c1a6a6564fd6?what_not_true=true)  [![not in scope](https://img.shields.io/badge/\ud83d\udc4e%20out%20of%20pr%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/17a10732-fadf-46cb-92ff-c1a6a6564fd6?what_out_of_scope=true) [![not in coding standard](https://img.shields.io/badge/\ud83d\udc4e%20not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/17a10732-fadf-46cb-92ff-c1a6a6564fd6?what_not_in_standard=true) [![other](https://img.shields.io/badge/\ud83d\udc4e%20other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/17a10732-fadf-46cb-92ff-c1a6a6564fd6)\n</details>\n\n<sub>\n\n\ud83d\udcac looking for more details? reply to this comment to chat with korbit.\n</sub>\n\n<!--- korbi internal id:24eb6fe7-a336-4371-a72d-67cbc4a22535 -->\n\n\n[](24eb6fe7-a336-4371-a72d-67cbc4a22535)",
    "line_number": 301,
    "enriched": "File: superset-frontend/src/components/Datasource/DatasourceModal.tsx\nCode: @@ -228,34 +236,82 @@ const DatasourceModal: FunctionComponent<DatasourceModalProps> = ({\n     setErrors(err);\n   };\n \n-  const renderSaveDialog = () => (\n-    <div>\n-      <Alert\n-        css={theme => ({\n-          marginTop: theme.gridUnit * 4,\n-          marginBottom: theme.gridUnit * 4,\n-        })}\n-        type=\"warning\"\n-        showIcon\n-        message={t(`The dataset configuration exposed here\n+  const getSaveDialog = useCallback(\n+    () => (\n+      <div>\n+        <Alert\n+          css={theme => ({\n+            marginTop: theme.gridUnit * 4,\n+            marginBottom: theme.gridUnit * 4,\n+          })}\n+          type=\"warning\"\n+          showIcon={false}\n+          message={t(`The dataset configuration exposed here\n                 affects all the charts using this dataset.\n                 Be mindful that changing settings\n                 here may affect other charts\n                 in undesirable ways.`)}\n-      />\n-      {t('Are you sure you want to save and apply changes?')}\n-    </div>\n+        />\n+        {datasource.sql !== currentDatasource.sql && (\n+          <>\n+            <Alert\n+              css={theme => ({\n+                marginTop: theme.gridUnit * 4,\n+                marginBottom: theme.gridUnit * 4,\n+              })}\n+              type=\"info\"\n+              showIcon={false}\n+              message={t(`The dataset columns will be automatically synced\n+              given the changes in your SQL query. If your changes don't\n+              impact columns, you might want to skip this step.`)}\n+            />\n+            <Checkbox\n+              checked={syncColumnsRef.current}\n+              onChange={() => {\n+                syncColumnsRef.current = !syncColumnsRef.current;\n+                if (confirmModal) {\n+                  confirmModal.update({\n+                    content: getSaveDialog(),\n+                  });\n+                }\n+              }}\n+            />\n+            <span className=\"m-l-5\">{t('Automatically sync columns')}</span>\n+            <br />\n+            <br />\n+          </>\n+        )}\n+        {t('Are you sure you want to save and apply changes?')}\n+      </div>\n+    ),\n+    [currentDatasource.sql, datasource.sql, confirmModal],\n   );\n \n+  useEffect(() => {\n+    if (confirmModal) {\n+      confirmModal.update({\n+        content: getSaveDialog(),\n+      });\n+    }\n+  }, [confirmModal, getSaveDialog]);\n+\n+  useEffect(() => {\n+    if (datasource.sql !== currentDatasource.sql) {\n+      syncColumnsRef.current = true;\n+    }\nComment: ### Sync Columns Checkbox State Not Reset <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe sync columns checkbox is automatically checked when SQL changes, but it doesn't get unchecked when SQL reverts to its original state.\n\n\n###### Why this matters\nUsers may be confused when the checkbox remains checked even after reverting their SQL changes, potentially leading to unintended column synchronization.\n\n###### Suggested change \u2219 *Feature Preview*\nUpdate the useEffect to handle SQL reversion:\n```typescript\nuseEffect(() => {\n  syncColumnsRef.current = datasource.sql !== currentDatasource.sql;\n}, [datasource.sql, currentDatasource.sql]);\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/\ud83d\udc4d%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/17a10732-fadf-46cb-92ff-c1a6a6564fd6/upvote) [![Incorrect](https://img.shields.io/badge/\ud83d\udc4e%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/17a10732-fadf-46cb-92ff-c1a6a6564fd6?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/\ud83d\udc4e%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/17a10732-fadf-46cb-92ff-c1a6a6564fd6?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/\ud83d\udc4e%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/17a10732-fadf-46cb-92ff-c1a6a6564fd6?what_not_in_standard=true) [![Other](https://img.shields.io/badge/\ud83d\udc4e%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/17a10732-fadf-46cb-92ff-c1a6a6564fd6)\n</details>\n\n<sub>\n\n\ud83d\udcac Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:24eb6fe7-a336-4371-a72d-67cbc4a22535 -->\n\n\n[](24eb6fe7-a336-4371-a72d-67cbc4a22535)",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "superset-frontend/src/components/Datasource/DatasourceModal.tsx",
    "pr_number": 34509,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 2248816437,
    "comment_created_at": "2025-08-01T20:28:50Z"
  },
  {
    "code": "@@ -9,6 +9,8 @@ Version 2.2.3 Unreleased +- Autoescaping is now enabled by default for ``.svg`` files. Inside + templates this behavior can be changed with the ``autoescape`` tag.",
    "comment": "could you reference this pr (see other examples in this file).",
    "line_number": 13,
    "enriched": "File: CHANGES.rst\nCode: @@ -9,6 +9,8 @@ Version 2.2.3\n \n Unreleased\n \n+-   Autoescaping is now enabled by default for ``.svg`` files. Inside\n+    templates this behavior can be changed with the ``autoescape`` tag.\nComment: Could you reference this PR (see other examples in this file).",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "CHANGES.rst",
    "pr_number": 4840,
    "repo": "flask",
    "owner": "pallets",
    "comment_id": 998516191,
    "comment_created_at": "2022-10-18T17:23:46Z"
  },
  {
    "code": "@@ -4861,6 +4861,24 @@ def test_add_db_comment_charfield(self): comment, ) + @skipUnlessDBFeature(\"supports_comments\")",
    "comment": "some databases don't support stored generated columns",
    "line_number": 4864,
    "enriched": "File: tests/schema/tests.py\nCode: @@ -4861,6 +4861,24 @@ def test_add_db_comment_charfield(self):\n             comment,\n         )\n \n+    @skipUnlessDBFeature(\"supports_comments\")\nComment: Some databases don't support stored generated columns\r\n```suggestion\r\n    @skipUnlessDBFeature(\"supports_comments\", \"supports_stored_generated_columns\")\r\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "tests/schema/tests.py",
    "pr_number": 19199,
    "repo": "django",
    "owner": "django",
    "comment_id": 1966150591,
    "comment_created_at": "2025-02-21T20:48:04Z"
  },
  {
    "code": "@@ -82,17 +84,7 @@ def generate_regular_range( \"at least 'start' or 'end' should be specified if a 'period' is given.\" ) - with np.errstate(over=\"raise\"): - # If the range is sufficiently large, np.arange may overflow - # and incorrectly return an empty array if not caught. - try: - values = np.arange(b, e, stride, dtype=np.int64) - except FloatingPointError:",
    "comment": "do we have coverage here?",
    "line_number": 90,
    "enriched": "File: pandas/core/arrays/_ranges.py\nCode: @@ -82,17 +84,7 @@ def generate_regular_range(\n             \"at least 'start' or 'end' should be specified if a 'period' is given.\"\n         )\n \n-    with np.errstate(over=\"raise\"):\n-        # If the range is sufficiently large, np.arange may overflow\n-        #  and incorrectly return an empty array if not caught.\n-        try:\n-            values = np.arange(b, e, stride, dtype=np.int64)\n-        except FloatingPointError:\nComment: Do we have coverage here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/core/arrays/_ranges.py",
    "pr_number": 59672,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1739518197,
    "comment_created_at": "2024-08-30T23:38:27Z"
  },
  {
    "code": "@@ -489,12 +486,13 @@ def init_offset(self): self.conv_offset.bias.data.zero_() def forward(self, x: Tensor) -> Tensor: # type: ignore + assert digit_version(torchvision.__version__) >= digit_version( + '0.10.0a0'), 'the version of torchvision should be >= 0.10.0'",
    "comment": "suggest moving it to __init__.",
    "line_number": 490,
    "enriched": "File: mmcv/ops/deform_conv.py\nCode: @@ -489,12 +486,13 @@ def init_offset(self):\n             self.conv_offset.bias.data.zero_()\n \n         def forward(self, x: Tensor) -> Tensor:  # type: ignore\n+            assert digit_version(torchvision.__version__) >= digit_version(\n+                '0.10.0a0'), 'the version of torchvision should be >= 0.10.0'\nComment: suggest moving it to `__init__`.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "mmcv/ops/deform_conv.py",
    "pr_number": 2556,
    "repo": "mmcv",
    "owner": "open-mmlab",
    "comment_id": 1071913515,
    "comment_created_at": "2023-01-17T08:44:10Z"
  },
  {
    "code": "@@ -994,6 +996,78 @@ def gradient_proba( ) +class ExponentialLoss(BaseLoss): + \"\"\"Exponential loss with (half) logit link, for binary classification. + + This is also know as boosting loss. + + Domain: + y_true in [0, 1], i.e. regression on the unit interval + y_pred in (0, 1), i.e. boundaries excluded",
    "comment": "for my education: why are 0 and 1 excluded for y_pred?",
    "line_number": 1006,
    "enriched": "File: sklearn/_loss/loss.py\nCode: @@ -994,6 +996,78 @@ def gradient_proba(\n         )\n \n \n+class ExponentialLoss(BaseLoss):\n+    \"\"\"Exponential loss with (half) logit link, for binary classification.\n+\n+    This is also know as boosting loss.\n+\n+    Domain:\n+    y_true in [0, 1], i.e. regression on the unit interval\n+    y_pred in (0, 1), i.e. boundaries excluded\nComment: For my education: why are 0 and 1 excluded for `y_pred`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "sklearn/_loss/loss.py",
    "pr_number": 25965,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1156793223,
    "comment_created_at": "2023-04-04T06:36:39Z"
  },
  {
    "code": "@@ -87,17 +88,29 @@ def configure_logging(settings=None, install_root_handler=True): observer = twisted_log.PythonLoggingObserver('twisted') observer.start() - dictConfig(DEFAULT_LOGGING) - if isinstance(settings, dict) or settings is None: settings = Settings(settings) + LOG_CONFIG_FILE_LOADED = False + LOG_CONFIG_FILE = settings.get(\"LOG_CONFIG_FILE\") + if LOG_CONFIG_FILE: + if LOG_CONFIG_FILE[-3:] == \"yml\": + with open(LOG_CONFIG_FILE) as f: + config = yaml.load(f) + config.setdefault('version', 1) + dictConfig(config) + LOG_CONFIG_FILE_LOADED = True + if not LOG_CONFIG_FILE_LOADED: + dictConfig(DEFAULT_LOGGING) + if settings.getbool('LOG_STDOUT'): sys.stdout = StreamLogger(logging.getLogger('stdout')) if install_root_handler: install_scrapy_root_handler(settings) + if not LOG_CONFIG_FILE_LOADED: + logger.error(\"LOG_CONFIG_FILE from settings could not be loaded. Make sure it's a valid YAML file.\")",
    "comment": "only log when log_config_file is provided.",
    "line_number": 113,
    "enriched": "File: scrapy/utils/log.py\nCode: @@ -87,17 +88,29 @@ def configure_logging(settings=None, install_root_handler=True):\n     observer = twisted_log.PythonLoggingObserver('twisted')\n     observer.start()\n \n-    dictConfig(DEFAULT_LOGGING)\n-\n     if isinstance(settings, dict) or settings is None:\n         settings = Settings(settings)\n \n+    LOG_CONFIG_FILE_LOADED = False\n+    LOG_CONFIG_FILE = settings.get(\"LOG_CONFIG_FILE\")\n+    if LOG_CONFIG_FILE:\n+        if LOG_CONFIG_FILE[-3:] == \"yml\":\n+            with open(LOG_CONFIG_FILE) as f:\n+                config = yaml.load(f)\n+                config.setdefault('version', 1)\n+                dictConfig(config)\n+                LOG_CONFIG_FILE_LOADED = True\n+    if not LOG_CONFIG_FILE_LOADED:\n+        dictConfig(DEFAULT_LOGGING)\n+\n     if settings.getbool('LOG_STDOUT'):\n         sys.stdout = StreamLogger(logging.getLogger('stdout'))\n \n     if install_root_handler:\n         install_scrapy_root_handler(settings)\n \n+    if not LOG_CONFIG_FILE_LOADED:\n+        logger.error(\"LOG_CONFIG_FILE from settings could not be loaded. Make sure it's a valid YAML file.\")\nComment: Only log when LOG_CONFIG_FILE is provided.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "scrapy/utils/log.py",
    "pr_number": 2827,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 129832794,
    "comment_created_at": "2017-07-27T12:55:33Z"
  },
  {
    "code": "@@ -178,6 +178,51 @@ def stop(self) -> Generator[Deferred, Any, None]: assert self.engine yield maybeDeferred(self.engine.stop) + @staticmethod + def _get_component(component_class, components): + for component in components: + if ( + type(component) # pylint: disable=unidiomatic-typecheck + is component_class",
    "comment": "i think it'd be good to check what's the best behavor here, based on the use cases in the wild: should the default be to match the type exactly, or to use ininstance (i.e. allow subclasses).",
    "line_number": 186,
    "enriched": "File: scrapy/crawler.py\nCode: @@ -178,6 +178,51 @@ def stop(self) -> Generator[Deferred, Any, None]:\n             assert self.engine\n             yield maybeDeferred(self.engine.stop)\n \n+    @staticmethod\n+    def _get_component(component_class, components):\n+        for component in components:\n+            if (\n+                type(component)  # pylint: disable=unidiomatic-typecheck\n+                is component_class\nComment: I think it'd be good to check what's the best behavor here, based on the use cases in the wild: should the default be to match the type exactly, or to use ininstance (i.e. allow subclasses).",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "scrapy/crawler.py",
    "pr_number": 6181,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 1432994575,
    "comment_created_at": "2023-12-20T17:26:51Z"
  },
  {
    "code": "@@ -50,143 +57,147 @@ const codeBlockStyle = (theme: Theme): CSSObject => ({ margin: 0, overflowX: \"auto\", padding: 0, - whiteSpace: \"pre\", - wordBreak: \"normal\", + whiteSpace: wrapLines ? \"pre-wrap\" : undefined, + wordBreak: wrapLines ? \"break-word\" : undefined,",
    "comment": "when wraplines is false, whitespace and wordbreak are unset, removing the original preformatted behavior. to preserve default formatting, use whitespace: wraplines ? 'pre-wrap' : 'pre' and wordbreak: wraplines ? 'break-word' : 'normal'.",
    "line_number": 61,
    "enriched": "File: frontend/lib/src/components/elements/CodeBlock/styled-components.ts\nCode: @@ -50,143 +57,147 @@ const codeBlockStyle = (theme: Theme): CSSObject => ({\n   margin: 0,\n   overflowX: \"auto\",\n   padding: 0,\n-  whiteSpace: \"pre\",\n-  wordBreak: \"normal\",\n+  whiteSpace: wrapLines ? \"pre-wrap\" : undefined,\n+  wordBreak: wrapLines ? \"break-word\" : undefined,\nComment: When `wrapLines` is false, `whiteSpace` and `wordBreak` are unset, removing the original preformatted behavior. To preserve default formatting, use `whiteSpace: wrapLines ? 'pre-wrap' : 'pre'` and `wordBreak: wrapLines ? 'break-word' : 'normal'`.\n```suggestion\n  whiteSpace: wrapLines ? \"pre-wrap\" : \"pre\",\n  wordBreak: wrapLines ? \"break-word\" : \"normal\",\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "frontend/lib/src/components/elements/CodeBlock/styled-components.ts",
    "pr_number": 10969,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 2143756808,
    "comment_created_at": "2025-06-12T22:27:33Z"
  },
  {
    "code": "@@ -2272,7 +2281,7 @@ class RidgeCV( Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`. - If using Leave-One-Out cross-validation, alphas must be positive. + If using Leave-One-Out cross-validation, alphas must be > 0.",
    "comment": "or\r\n\r\ni find the bare > not optimal to read - personal taste. same in ridgeclassifiercv.",
    "line_number": 2284,
    "enriched": "File: sklearn/linear_model/_ridge.py\nCode: @@ -2272,7 +2281,7 @@ class RidgeCV(\n         Alpha corresponds to ``1 / (2C)`` in other linear models such as\n         :class:`~sklearn.linear_model.LogisticRegression` or\n         :class:`~sklearn.svm.LinearSVC`.\n-        If using Leave-One-Out cross-validation, alphas must be positive.\n+        If using Leave-One-Out cross-validation, alphas must be > 0.\nComment: ```suggestion\r\n        If using Leave-One-Out cross-validation, alphas must be strictly positive.\r\n```\r\nor\r\n```suggestion\r\n        If using Leave-One-Out cross-validation, `alphas > 0` is required.\r\n```\r\nI find the bare > not optimal to read - personal taste. Same in `RidgeClassifierCV`.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "sklearn/linear_model/_ridge.py",
    "pr_number": 28425,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1490892343,
    "comment_created_at": "2024-02-15T11:48:58Z"
  },
  {
    "code": "@@ -436,6 +452,19 @@ def convert_anything_to_pandas_df( data = data.copy(deep=True) return data.to_series().to_frame() + if is_dask_object(data): + data = data.head(max_unevaluated_rows, compute=True) + + if isinstance(data, (pd.Series, pd.Index)):",
    "comment": "how can data here be of type pd.series or pd.index when we are inside of the is_dask_object function?",
    "line_number": 458,
    "enriched": "File: lib/streamlit/dataframe_util.py\nCode: @@ -436,6 +452,19 @@ def convert_anything_to_pandas_df(\n             data = data.copy(deep=True)\n         return data.to_series().to_frame()\n \n+    if is_dask_object(data):\n+        data = data.head(max_unevaluated_rows, compute=True)\n+\n+        if isinstance(data, (pd.Series, pd.Index)):\nComment: how can `data` here be of type `pd.Series` or `pd.Index` when we are inside of the `is_dask_object` function?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/streamlit/dataframe_util.py",
    "pr_number": 9240,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1711712356,
    "comment_created_at": "2024-08-09T15:51:32Z"
  },
  {
    "code": "@@ -2373,6 +2399,14 @@ def to_sql( cur.execute(f\"DROP TABLE {table_name}\")",
    "comment": "i was wondering if we should use mode=replace instead of running a manual drop table statement. i can have other pr for this patch if it makes sense...",
    "line_number": 2408,
    "enriched": "File: pandas/io/sql.py\nCode: @@ -2373,6 +2399,14 @@ def to_sql(\n                     cur.execute(f\"DROP TABLE {table_name}\")\nComment: I was wondering if we should use `mode=replace` instead of running a manual `DROP TABLE` statement. I can have other PR for this patch if it makes sense...",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/io/sql.py",
    "pr_number": 59391,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1702207401,
    "comment_created_at": "2024-08-02T18:51:55Z"
  },
  {
    "code": "@@ -3487,7 +3487,7 @@ def _to_bool_indexer(indexer) -> npt.NDArray[np.bool_]: \"cannot index with a boolean indexer that \" \"is not the same length as the index\" ) - lvl_indexer = np.asarray(k) + lvl_indexer = np.asarray(k).copy()",
    "comment": "not copying here propagates all the way to the input mask",
    "line_number": 3490,
    "enriched": "File: pandas/core/indexes/multi.py\nCode: @@ -3487,7 +3487,7 @@ def _to_bool_indexer(indexer) -> npt.NDArray[np.bool_]:\n                         \"cannot index with a boolean indexer that \"\n                         \"is not the same length as the index\"\n                     )\n-                lvl_indexer = np.asarray(k)\n+                lvl_indexer = np.asarray(k).copy()\nComment: Not copying here propagates all the way to the input mask",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "pandas/core/indexes/multi.py",
    "pr_number": 56635,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1437223779,
    "comment_created_at": "2023-12-27T19:45:20Z"
  },
  {
    "code": "@@ -270,6 +270,16 @@ HOST_DEVICE_INLINE int convex_hull_graham(const Point<T> (&p)[24], return m; } +template <typename T> +HOST_DEVICE_INLINE T quadri_box_area(const Point<T> (&q)[4]) { + T area = 0; + for (int i = 1; i < 3; i++) {",
    "comment": "this loop can be unrolled:\n\nc++\n#pragma unroll\nfor(...)",
    "line_number": 276,
    "enriched": "File: mmcv/ops/csrc/common/box_iou_rotated_utils.hpp\nCode: @@ -270,6 +270,16 @@ HOST_DEVICE_INLINE int convex_hull_graham(const Point<T> (&p)[24],\n   return m;\n }\n \n+template <typename T>\n+HOST_DEVICE_INLINE T quadri_box_area(const Point<T> (&q)[4]) {\n+  T area = 0;\n+  for (int i = 1; i < 3; i++) {\nComment: This loop can be unrolled:\n\n```c++\n#pragma unroll\nfor(...)\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "mmcv/ops/csrc/common/box_iou_rotated_utils.hpp",
    "pr_number": 2277,
    "repo": "mmcv",
    "owner": "open-mmlab",
    "comment_id": 979511975,
    "comment_created_at": "2022-09-26T02:19:07Z"
  },
  {
    "code": "@@ -709,28 +746,28 @@ def test_nested_load_item(self): class SelectJmesTestCase(unittest.TestCase): - test_list_equals = {",
    "comment": ":lipstick: i would avoid these indentation fixes, being unrelated to the actual changes.",
    "line_number": 712,
    "enriched": "File: tests/test_loader.py\nCode: @@ -709,28 +746,28 @@ def test_nested_load_item(self):\n \n \n class SelectJmesTestCase(unittest.TestCase):\n-        test_list_equals = {\nComment: :lipstick: I would avoid these indentation fixes, being unrelated to the actual changes.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "tests/test_loader.py",
    "pr_number": 3819,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 292387793,
    "comment_created_at": "2019-06-11T10:33:44Z"
  },
  {
    "code": "@@ -954,10 +972,31 @@ class InjectedToolArg: \"\"\"Annotation for a Tool arg that is **not** meant to be generated by a model.\"\"\" -def _is_injected_arg_type(type_: type) -> bool: +class InjectedToolCallId(InjectedToolArg):",
    "comment": "could we export this one from langchain_core.tools?",
    "line_number": 975,
    "enriched": "File: libs/core/langchain_core/tools/base.py\nCode: @@ -954,10 +972,31 @@ class InjectedToolArg:\n     \"\"\"Annotation for a Tool arg that is **not** meant to be generated by a model.\"\"\"\n \n \n-def _is_injected_arg_type(type_: type) -> bool:\n+class InjectedToolCallId(InjectedToolArg):\nComment: could we export this one from `langchain_core.tools`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "libs/core/langchain_core/tools/base.py",
    "pr_number": 28605,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 1877166761,
    "comment_created_at": "2024-12-10T02:45:10Z"
  },
  {
    "code": "@@ -739,7 +739,7 @@ def _convert_non_tensor(x): inputs = tf.nest.map_structure(_convert_non_tensor, inputs) input_list = tf.nest.flatten(inputs) - # Handle `mask` propagation from previous layer to current layer. Masks + # Handle `mask` propagation from the previous layer to the current layer. Masks",
    "comment": "please shorten line",
    "line_number": 742,
    "enriched": "File: keras/engine/base_layer_v1.py\nCode: @@ -739,7 +739,7 @@ def _convert_non_tensor(x):\n             inputs = tf.nest.map_structure(_convert_non_tensor, inputs)\n             input_list = tf.nest.flatten(inputs)\n \n-        # Handle `mask` propagation from previous layer to current layer. Masks\n+        # Handle `mask` propagation from the previous layer to the current layer. Masks\nComment: Please shorten line",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "keras/engine/base_layer_v1.py",
    "pr_number": 17246,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 1023396126,
    "comment_created_at": "2022-11-16T00:51:07Z"
  },
  {
    "code": "@@ -100,21 +99,19 @@ def test_filtered_out_level(self): assert self.crawler.stats.get_value(\"log_count/INFO\") is None -class StreamLoggerTest(unittest.TestCase): - def setUp(self):",
    "comment": "pytest overrides sys.stdout after the fixtures etc. run, so merging the code into the test method was the easiest way to reimplement this",
    "line_number": 104,
    "enriched": "File: tests/test_utils_log.py\nCode: @@ -100,21 +99,19 @@ def test_filtered_out_level(self):\n         assert self.crawler.stats.get_value(\"log_count/INFO\") is None\n \n \n-class StreamLoggerTest(unittest.TestCase):\n-    def setUp(self):\nComment: pytest overrides `sys.stdout` after the fixtures etc. run, so merging the code into the test method was the easiest way to reimplement this",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/test_utils_log.py",
    "pr_number": 6873,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2132860636,
    "comment_created_at": "2025-06-06T20:18:34Z"
  },
  {
    "code": "@@ -370,6 +370,7 @@ def set_related_perm(_mapper: Mapper, _connection: Connection, target: Slice) -> if ds: target.perm = ds.perm target.schema_perm = ds.schema_perm + target.catalog_perm = ds.catalog_perm",
    "comment": "til about this function!",
    "line_number": 373,
    "enriched": "File: superset/models/slice.py\nCode: @@ -370,6 +370,7 @@ def set_related_perm(_mapper: Mapper, _connection: Connection, target: Slice) ->\n         if ds:\n             target.perm = ds.perm\n             target.schema_perm = ds.schema_perm\n+            target.catalog_perm = ds.catalog_perm\nComment: TIL about this function!",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "superset/models/slice.py",
    "pr_number": 29608,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1680123459,
    "comment_created_at": "2024-07-16T22:17:52Z"
  },
  {
    "code": "@@ -177,7 +177,7 @@ def mean_shift( n_jobs : int, default=None The number of jobs to use for the computation. This works by computing - each of the n_init runs in parallel. + each of the n_jobs runs in parallel.",
    "comment": "the number of jobs to use for the computation. the following tasks benefit\r\n        from the parallelization: the search of nearest neighbors for bandwidth\r\n        estimation and label assignments, and the hill climbing optimization for all\r\n        seeds.",
    "line_number": 180,
    "enriched": "File: sklearn/cluster/_mean_shift.py\nCode: @@ -177,7 +177,7 @@ def mean_shift(\n \n     n_jobs : int, default=None\n         The number of jobs to use for the computation. This works by computing\n-        each of the n_init runs in parallel.\n+        each of the n_jobs runs in parallel.\nComment: ```\r\n        The number of jobs to use for the computation. The following tasks benefit\r\n        from the parallelization: the search of nearest neighbors for bandwidth\r\n        estimation and label assignments, and the hill climbing optimization for all\r\n        seeds.\r\n```",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "sklearn/cluster/_mean_shift.py",
    "pr_number": 25083,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1036958287,
    "comment_created_at": "2022-12-01T10:45:03Z"
  },
  {
    "code": "@@ -86,9 +86,9 @@ Please report unacceptable behavior to opensource@scrapinghub.com. Companies using Scrapy ====================== -See https://scrapy.org/companies/ +See https://scrapy.org/companies/. Commercial Support ================== -See https://scrapy.org/support/ +See https://scrapy.org/support/.",
    "comment": "i think that's fine not to have dot in the end of urls, because it can get tricky with highlighting - is a dot a part of url, or a part of a sentence? in rest is may depend on a punctuation symbol",
    "line_number": 94,
    "enriched": "File: README.rst\nCode: @@ -86,9 +86,9 @@ Please report unacceptable behavior to opensource@scrapinghub.com.\n Companies using Scrapy\n ======================\n \n-See https://scrapy.org/companies/\n+See https://scrapy.org/companies/.\n \n Commercial Support\n ==================\n \n-See https://scrapy.org/support/\n+See https://scrapy.org/support/.\nComment: I think that's fine not to have dot in the end of URLs, because it can get tricky with highlighting - is a dot a part of URL, or a part of a sentence? In ReST is may depend on a punctuation symbol",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "README.rst",
    "pr_number": 4059,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 332230471,
    "comment_created_at": "2019-10-07T20:52:06Z"
  },
  {
    "code": "@@ -23,6 +23,8 @@ matrix: env: TOXENV=py36 - python: 3.6 env: TOXENV=docs + - python: 3.7-dev",
    "comment": "i think now, as 3.7 is released, it may be better to use python 3.7 relase (though it may be not straightforward); according to https://github.com/travis-ci/travis-ci/issues/9815 3.7-dev is an old beta.",
    "line_number": 26,
    "enriched": "File: .travis.yml\nCode: @@ -23,6 +23,8 @@ matrix:\n       env: TOXENV=py36\n     - python: 3.6\n       env: TOXENV=docs\n+    - python: 3.7-dev\nComment: I think now, as 3.7 is released, it may be better to use Python 3.7 relase (though it may be not straightforward); according to https://github.com/travis-ci/travis-ci/issues/9815 3.7-dev is an old beta.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": ".travis.yml",
    "pr_number": 3150,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 200169747,
    "comment_created_at": "2018-07-04T16:20:14Z"
  },
  {
    "code": "@@ -152,6 +154,7 @@ def __init__( session: Optional[requests.Session] = None, oauth2: Optional[dict] = None, token: Optional[str] = None, + cookies: Optional[dict] = None,",
    "comment": "can we make this parameter keyword-only (move below the *), in case users are passing in parameters positionally?",
    "line_number": 157,
    "enriched": "File: libs/community/langchain_community/document_loaders/confluence.py\nCode: @@ -152,6 +154,7 @@ def __init__(\n         session: Optional[requests.Session] = None,\n         oauth2: Optional[dict] = None,\n         token: Optional[str] = None,\n+        cookies: Optional[dict] = None,\nComment: Can we make this parameter keyword-only (move below the `*`), in case users are passing in parameters positionally?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "libs/community/langchain_community/document_loaders/confluence.py",
    "pr_number": 28760,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 1888792538,
    "comment_created_at": "2024-12-17T16:03:40Z"
  },
  {
    "code": "@@ -121,49 +120,59 @@ def _smacof_single( \"init matrix should be of shape (%d, %d)\" % (n_samples, n_components) ) X = init + dis = euclidean_distances(X) + + # Out of bounds condition cannot happen because we are transforming + # the training set here, but does sometimes get triggered in + # practice due to machine precision issues. Hence \"clip\". + ir = IsotonicRegression(out_of_bounds=\"clip\") old_stress = None - ir = IsotonicRegression() for it in range(max_iter): # Compute distance and monotonic regression - dis = euclidean_distances(X) - if metric: disparities = dissimilarities else: dis_flat = dis.ravel() # dissimilarities with 0 are considered as missing values dis_flat_w = dis_flat[sim_flat != 0] - # Compute the disparities using a monotonic regression - disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w) - disparities = dis_flat.copy() + # Compute the disparities using isotonic regression. + # For the first SMACOF iteration, use scaled original dissimilarities. + if it < 1: + disparities_flat = sim_flat_w + else: + disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w) + disparities = np.zeros_like(dis_flat) disparities[sim_flat != 0] = disparities_flat disparities = disparities.reshape((n_samples, n_samples)) disparities *= np.sqrt( (n_samples * (n_samples - 1) / 2) / (disparities**2).sum() ) + disparities = disparities + disparities.T",
    "comment": "i think the normalization of the disparities to n_samples * (n_samples - 1) / 2  should happen after fixing the lower triangular part.",
    "line_number": 152,
    "enriched": "File: sklearn/manifold/_mds.py\nCode: @@ -121,49 +120,59 @@ def _smacof_single(\n                 \"init matrix should be of shape (%d, %d)\" % (n_samples, n_components)\n             )\n         X = init\n+    dis = euclidean_distances(X)\n+\n+    # Out of bounds condition cannot happen because we are transforming\n+    # the training set here, but does sometimes get triggered in\n+    # practice due to machine precision issues. Hence \"clip\".\n+    ir = IsotonicRegression(out_of_bounds=\"clip\")\n \n     old_stress = None\n-    ir = IsotonicRegression()\n     for it in range(max_iter):\n         # Compute distance and monotonic regression\n-        dis = euclidean_distances(X)\n-\n         if metric:\n             disparities = dissimilarities\n         else:\n             dis_flat = dis.ravel()\n             # dissimilarities with 0 are considered as missing values\n             dis_flat_w = dis_flat[sim_flat != 0]\n \n-            # Compute the disparities using a monotonic regression\n-            disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)\n-            disparities = dis_flat.copy()\n+            # Compute the disparities using isotonic regression.\n+            # For the first SMACOF iteration, use scaled original dissimilarities.\n+            if it < 1:\n+                disparities_flat = sim_flat_w\n+            else:\n+                disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)\n+            disparities = np.zeros_like(dis_flat)\n             disparities[sim_flat != 0] = disparities_flat\n             disparities = disparities.reshape((n_samples, n_samples))\n             disparities *= np.sqrt(\n                 (n_samples * (n_samples - 1) / 2) / (disparities**2).sum()\n             )\n+            disparities = disparities + disparities.T\nComment: I think the normalization of the `disparities` to `n_samples * (n_samples - 1) / 2`  should happen after fixing the lower triangular part.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "sklearn/manifold/_mds.py",
    "pr_number": 30514,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2007946353,
    "comment_created_at": "2025-03-21T16:38:14Z"
  },
  {
    "code": "@@ -26,13 +26,11 @@ } function addPopupIndex(name) { - name = name + \"__\" + (popupIndex + 1); - return name; + return name + \"__\" + (popupIndex + 1); } function removePopupIndex(name) { - name = name.replace(new RegExp(\"__\" + (popupIndex + 1) + \"$\"), ''); - return name; + return name.replace(new RegExp(\"__\" + (popupIndex + 1) + \"$\"), '');",
    "comment": "is it worth splitting this into its own commit? \ud83e\udd14\r\n\r\nedit: there's also some additional inlining further down that could be grouped with this if separate commits are preferred.",
    "line_number": 33,
    "enriched": "File: django/contrib/admin/static/admin/js/admin/RelatedObjectLookups.js\nCode: @@ -26,13 +26,11 @@\n     }\n \n     function addPopupIndex(name) {\n-        name = name + \"__\" + (popupIndex + 1);\n-        return name;\n+        return name + \"__\" + (popupIndex + 1);\n     }\n \n     function removePopupIndex(name) {\n-        name = name.replace(new RegExp(\"__\" + (popupIndex + 1) + \"$\"), '');\n-        return name;\n+        return name.replace(new RegExp(\"__\" + (popupIndex + 1) + \"$\"), '');\nComment: Is it worth splitting this into its own commit? \ud83e\udd14\r\n\r\nEdit: There's also some additional inlining further down that could be grouped with this if separate commits are preferred.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "django/contrib/admin/static/admin/js/admin/RelatedObjectLookups.js",
    "pr_number": 16188,
    "repo": "django",
    "owner": "django",
    "comment_id": 999180737,
    "comment_created_at": "2022-10-19T09:24:16Z"
  },
  {
    "code": "@@ -15,6 +16,8 @@ def __init__(self, interval=10, ignore_last=True, reset_flag=False): def before_run(self, runner): super(TextLoggerHook, self).before_run(runner) self.start_iter = runner.iter + self.json_log_path = '{}/{}.{}'.format(runner.work_dir, + runner.timestamp, 'log.json')",
    "comment": "it's more safe to use osp.join(runner.work_dir, '{}.log.json'.format(runner.timestamp)).",
    "line_number": 20,
    "enriched": "File: mmcv/runner/hooks/logger/text.py\nCode: @@ -15,6 +16,8 @@ def __init__(self, interval=10, ignore_last=True, reset_flag=False):\n     def before_run(self, runner):\n         super(TextLoggerHook, self).before_run(runner)\n         self.start_iter = runner.iter\n+        self.json_log_path = '{}/{}.{}'.format(runner.work_dir,\n+                                               runner.timestamp, 'log.json')\nComment: It's more safe to use `osp.join(runner.work_dir, '{}.log.json'.format(runner.timestamp))`.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "mmcv/runner/hooks/logger/text.py",
    "pr_number": 51,
    "repo": "mmcv",
    "owner": "open-mmlab",
    "comment_id": 275136464,
    "comment_created_at": "2019-04-14T00:15:09Z"
  },
  {
    "code": "@@ -1010,6 +1011,11 @@ def bar_chart( as the number of y values (e.g. ``color=[\"#fd0\", \"#f0f\", \"#04f\"]`` for three lines). + horizontal : bool + Determines the orientation of the chart: + * True: Displays the chart horizontally, with the x-axis and y-axis swapped. + * False: Displays the chart vertically (default).",
    "comment": "nit: i think our docstring convention for boolean flags is not to use bullets, but @sfc-gh-dmatthews is probably doing another iteration on that before release",
    "line_number": 1017,
    "enriched": "File: lib/streamlit/elements/vega_charts.py\nCode: @@ -1010,6 +1011,11 @@ def bar_chart(\n               as the number of y values (e.g. ``color=[\"#fd0\", \"#f0f\", \"#04f\"]``\n               for three lines).\n \n+        horizontal : bool\n+            Determines the orientation of the chart:\n+            * True: Displays the chart horizontally, with the x-axis and y-axis swapped.\n+            * False: Displays the chart vertically (default).\nComment: nit: I think our docstring convention for boolean flags is not to use bullets, but @sfc-gh-dmatthews is probably doing another iteration on that before release ",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "lib/streamlit/elements/vega_charts.py",
    "pr_number": 8877,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1637048156,
    "comment_created_at": "2024-06-12T20:33:16Z"
  },
  {
    "code": "@@ -275,9 +275,9 @@ class Template(str, Enum): class Endpoint(Enum): \"\"\"Endpoints for the reflex backend API.\"\"\" - PING = \"ping\" - EVENT = \"event\" - UPLOAD = \"upload\" + PING = \"_ping\"",
    "comment": "i think some users use the /ping endpoint to do health checks currently. maybe we can keep this one as is? the rest should be good to rename.",
    "line_number": 278,
    "enriched": "File: reflex/constants.py\nCode: @@ -275,9 +275,9 @@ class Template(str, Enum):\n class Endpoint(Enum):\n     \"\"\"Endpoints for the reflex backend API.\"\"\"\n \n-    PING = \"ping\"\n-    EVENT = \"event\"\n-    UPLOAD = \"upload\"\n+    PING = \"_ping\"\nComment: I think some users use the `/ping` endpoint to do health checks currently. Maybe we can keep this one as is? The rest should be good to rename.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "reflex/constants.py",
    "pr_number": 1542,
    "repo": "reflex",
    "owner": "reflex-dev",
    "comment_id": 1289517711,
    "comment_created_at": "2023-08-10T04:00:51Z"
  },
  {
    "code": "@@ -139,7 +139,10 @@ def compute(self, method: str) -> Series: # arr passed into kth_smallest must be contiguous. We copy # here because kth_smallest will modify its input - kth_val = libalgos.kth_smallest(arr.copy(order=\"C\"), n - 1) + if n == 0: # avoid OOB access with kth_smallest_c + kth_val = 0 + else: + kth_val = libalgos.kth_smallest(arr.copy(order=\"C\"), n - 1)",
    "comment": "i would be more comfortable with this change. since kth_val is a value in the array and comparisons are made with this value, this may be incorrect if values in arr were all negative for example",
    "line_number": 145,
    "enriched": "File: pandas/core/methods/selectn.py\nCode: @@ -139,7 +139,10 @@ def compute(self, method: str) -> Series:\n \n         # arr passed into kth_smallest must be contiguous. We copy\n         # here because kth_smallest will modify its input\n-        kth_val = libalgos.kth_smallest(arr.copy(order=\"C\"), n - 1)\n+        if n == 0:  # avoid OOB access with kth_smallest_c\n+            kth_val = 0\n+        else:\n+            kth_val = libalgos.kth_smallest(arr.copy(order=\"C\"), n - 1)\nComment: ```suggestion\r\n        # avoid OOB access with kth_smallest_c\r\n        kth_val = libalgos.kth_smallest(arr.copy(order=\"C\"), max(n - 1, 0))\r\n```\r\n\r\nI would be more comfortable with this change. Since `kth_val` is a value in the array and comparisons are made with this value, this may be incorrect if values in `arr` were all negative for example",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/core/methods/selectn.py",
    "pr_number": 56000,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1397742093,
    "comment_created_at": "2023-11-17T18:53:06Z"
  },
  {
    "code": "@@ -325,7 +325,7 @@ Properties is simple if and only if it does not intersect itself (except at boundary points). For example, a :class:`LineString` object is not simple if it intersects itself. Thus, :class:`LinearRing` and :class:`Polygon` objects - are always simple because they do cannot intersect themselves, by + are always simple because they cannot intersect themselves, by",
    "comment": "nice catch \ud83c\udfc6\r\n\r\ndoc line limit is 79 and lines should be stretched out to that limit meaning that the next word needs to be brought up.  just fyi editors should be able to do this for your too.",
    "line_number": 328,
    "enriched": "File: docs/ref/contrib/gis/geos.txt\nCode: @@ -325,7 +325,7 @@ Properties\n     is simple if and only if it does not intersect itself (except at boundary\n     points).  For example, a :class:`LineString` object is not simple if it\n     intersects itself. Thus, :class:`LinearRing` and :class:`Polygon` objects\n-    are always simple because they do cannot intersect themselves, by\n+    are always simple because they cannot intersect themselves, by\nComment: Nice catch \ud83c\udfc6\r\n\r\nDoc line limit is 79 and lines should be stretched out to that limit meaning that the next word needs to be brought up.  Just FYI editors should be able to do this for your too.",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "docs/ref/contrib/gis/geos.txt",
    "pr_number": 17411,
    "repo": "django",
    "owner": "django",
    "comment_id": 1373943926,
    "comment_created_at": "2023-10-27T00:10:02Z"
  },
  {
    "code": "@@ -188,7 +188,11 @@ export class SegmentMetricsManager implements MetricsManager { if (IS_DEV_ENV) { logAlways(\"[Dev mode] Not tracking stat datapoint: \", evName, data) } else { - this.track(evName, data) + this.track(evName, data, { + context: { + ip: \"0.0.0.0\",",
    "comment": "nit: maybe can you add a comment here to make it more explicit why we are doing this. something like:\r\n\r\n// segment automatically attaches the ip address. but we don't use, process, or store ip addresses for our telemetry. to make this more explicit, we are overwriting this here so that it is never even sent to segment.",
    "line_number": 193,
    "enriched": "File: frontend/src/lib/SegmentMetricsManager.ts\nCode: @@ -188,7 +188,11 @@ export class SegmentMetricsManager implements MetricsManager {\n     if (IS_DEV_ENV) {\n       logAlways(\"[Dev mode] Not tracking stat datapoint: \", evName, data)\n     } else {\n-      this.track(evName, data)\n+      this.track(evName, data, {\n+        context: {\n+          ip: \"0.0.0.0\",\nComment: nit: Maybe can you add a comment here to make it more explicit why we are doing this. Something like:\r\n\r\n// Segment automatically attaches the IP address. But we don't use, process, or store IP addresses for our telemetry. To make this more explicit, we are overwriting this here so that it is never even sent to Segment.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "frontend/src/lib/SegmentMetricsManager.ts",
    "pr_number": 6463,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1173879797,
    "comment_created_at": "2023-04-21T15:00:44Z"
  },
  {
    "code": "@@ -24,7 +24,14 @@ def scatter_map(obj): if obj.cpu_only: return obj.data else: - return Scatter.forward(target_gpus, obj.data) + data = obj.data + if isinstance(data, torch.Tensor):",
    "comment": "should update the scatter rather than this function.",
    "line_number": 28,
    "enriched": "File: mmcv/parallel/scatter_gather.py\nCode: @@ -24,7 +24,14 @@ def scatter_map(obj):\n             if obj.cpu_only:\n                 return obj.data\n             else:\n-                return Scatter.forward(target_gpus, obj.data)\n+                data = obj.data\n+                if isinstance(data, torch.Tensor):\nComment: Should update the Scatter rather than this function.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "mmcv/parallel/scatter_gather.py",
    "pr_number": 1236,
    "repo": "mmcv",
    "owner": "open-mmlab",
    "comment_id": 683085684,
    "comment_created_at": "2021-08-05T02:38:26Z"
  },
  {
    "code": "@@ -1,13 +1,24 @@ +import importlib + import pytest from dirty_equals import IsDict from fastapi.testclient import TestClient +from tests.utils import needs_py39",
    "comment": "shouldn't this be a relative import?\r\n(same comment in the other 2 files)",
    "line_number": 7,
    "enriched": "File: tests/test_tutorial/test_request_form_models/test_tutorial001.py\nCode: @@ -1,13 +1,24 @@\n+import importlib\n+\n import pytest\n from dirty_equals import IsDict\n from fastapi.testclient import TestClient\n \n+from tests.utils import needs_py39\nComment: Shouldn't this be a relative import?\r\n(same comment in the other 2 files)",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/test_tutorial/test_request_form_models/test_tutorial001.py",
    "pr_number": 13183,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 1910427984,
    "comment_created_at": "2025-01-10T14:10:32Z"
  },
  {
    "code": "@@ -65,6 +68,13 @@ def connection( ... +@overload +def connection( + connection_class: Literal[\"file\"], name: str = \"default\", **kwargs +) -> \"File\":",
    "comment": "nit: i think this can be file (without the quotes) now that it's always imported rather than living in the if type_checking block (same thing with the \"snowpark\" and \"sql\" -- i just wrote those @overloads while the imports still lived in the if statement)",
    "line_number": 74,
    "enriched": "File: lib/streamlit/connections/connection_factory.py\nCode: @@ -65,6 +68,13 @@ def connection(\n     ...\n \n \n+@overload\n+def connection(\n+    connection_class: Literal[\"file\"], name: str = \"default\", **kwargs\n+) -> \"File\":\nComment: nit: I think this can be `File` (without the quotes) now that it's always imported rather than living in the `if TYPE_CHECKING` block (same thing with the `\"Snowpark\"` and `\"SQL\"` -- I just wrote those `@overloads` while the imports still lived in the if statement)",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "lib/streamlit/connections/connection_factory.py",
    "pr_number": 6049,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1093867051,
    "comment_created_at": "2023-02-02T00:05:45Z"
  },
  {
    "code": "@@ -65,6 +65,9 @@ typedef int (PyUFunc_TypeResolutionFunc)( PyObject *type_tup, PyArray_Descr **out_dtypes); +typedef int (PyUFunc_ProcessCoreDimsFunc)(",
    "comment": "need to add a bit of docs here.  also need to add the same/similar docs for to the proper ufunc documentation.",
    "line_number": 68,
    "enriched": "File: numpy/_core/include/numpy/ufuncobject.h\nCode: @@ -65,6 +65,9 @@ typedef int (PyUFunc_TypeResolutionFunc)(\n                                 PyObject *type_tup,\n                                 PyArray_Descr **out_dtypes);\n \n+typedef int (PyUFunc_ProcessCoreDimsFunc)(\nComment: Need to add a bit of docs here.  Also need to add the same/similar docs for to the proper ufunc documentation.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "numpy/_core/include/numpy/ufuncobject.h",
    "pr_number": 26908,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 1676856165,
    "comment_created_at": "2024-07-13T16:42:22Z"
  },
  {
    "code": "@@ -14,18 +14,11 @@ * limitations under the License. */ -/** - * When in dev mode, this is the port used to connect to the web server that is - * serving the current page (i.e. the actual web page server, not the API - * server, which in dev are actually different servers.) - */ -export const WWW_PORT_DEV = 3000",
    "comment": "www_port_dev seems to have been unused previously?",
    "line_number": 22,
    "enriched": "File: frontend/lib/src/baseconsts.ts\nCode: @@ -14,18 +14,11 @@\n  * limitations under the License.\n  */\n \n-/**\n- * When in dev mode, this is the port used to connect to the web server that is\n- * serving the current page (i.e. the actual web page server, not the API\n- * server, which in dev are actually different servers.)\n- */\n-export const WWW_PORT_DEV = 3000\nComment: `WWW_PORT_DEV` seems to have been unused previously? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "frontend/lib/src/baseconsts.ts",
    "pr_number": 10293,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1935397801,
    "comment_created_at": "2025-01-30T10:48:34Z"
  },
  {
    "code": "@@ -109,6 +110,17 @@ def _mode(a, axis=0): return scipy.stats.mode(a, axis=axis) +# TODO: Remove when Scipy 1.12 is the minimum supported version +if parse_version(scipy.__version__) >= parse_version(\"1.12.0.dev0\"):",
    "comment": "i think checking base versions works with the dev builds:",
    "line_number": 114,
    "enriched": "File: sklearn/utils/fixes.py\nCode: @@ -109,6 +110,17 @@ def _mode(a, axis=0):\n     return scipy.stats.mode(a, axis=axis)\n \n \n+# TODO: Remove when Scipy 1.12 is the minimum supported version\n+if parse_version(scipy.__version__) >= parse_version(\"1.12.0.dev0\"):\nComment: I think checking base versions works with the dev builds:\r\n\r\n```suggestion\r\nif sp_base_version >= parse_version(\"1.12.0\"):\r\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "sklearn/utils/fixes.py",
    "pr_number": 26814,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1258898693,
    "comment_created_at": "2023-07-10T20:47:04Z"
  },
  {
    "code": "@@ -1257,3 +1244,21 @@ def _make_message_chunk_from_anthropic_event( @deprecated(since=\"0.1.0\", removal=\"0.3.0\", alternative=\"ChatAnthropic\") class ChatAnthropicMessages(ChatAnthropic): pass + + +def _create_usage_metadata(anthropic_usage: BaseModel) -> UsageMetadata:",
    "comment": "why is anthropic usage a base model?",
    "line_number": 1249,
    "enriched": "File: libs/partners/anthropic/langchain_anthropic/chat_models.py\nCode: @@ -1257,3 +1244,21 @@ def _make_message_chunk_from_anthropic_event(\n @deprecated(since=\"0.1.0\", removal=\"0.3.0\", alternative=\"ChatAnthropic\")\n class ChatAnthropicMessages(ChatAnthropic):\n     pass\n+\n+\n+def _create_usage_metadata(anthropic_usage: BaseModel) -> UsageMetadata:\nComment: Why is anthropic usage a base model?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "libs/partners/anthropic/langchain_anthropic/chat_models.py",
    "pr_number": 27087,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 1786906646,
    "comment_created_at": "2024-10-03T22:29:03Z"
  },
  {
    "code": "@@ -458,11 +458,12 @@ def nms_rotated(dets: Tensor, input_labels = scores.new_empty(0, dtype=torch.int) else: input_labels = labels - if dets.device.type == 'npu': + if dets.device.type in ('npu', 'mlu'): order = scores.new_empty(0, dtype=torch.long) coefficient = 57.29578 # 180 / PI - for i in range(dets.size()[0]): - dets_cw[i][4] *= coefficient # radians to angle + if dets.device.type == 'npu': + for i in range(dets.size()[0]): + dets_cw[i][4] *= coefficient # radians to angle",
    "comment": "coefficient = 57.29578 # 180 / pi can be moved to line 465.",
    "line_number": 466,
    "enriched": "File: mmcv/ops/nms.py\nCode: @@ -458,11 +458,12 @@ def nms_rotated(dets: Tensor,\n         input_labels = scores.new_empty(0, dtype=torch.int)\n     else:\n         input_labels = labels\n-    if dets.device.type == 'npu':\n+    if dets.device.type in ('npu', 'mlu'):\n         order = scores.new_empty(0, dtype=torch.long)\n         coefficient = 57.29578  # 180 / PI\n-        for i in range(dets.size()[0]):\n-            dets_cw[i][4] *= coefficient  # radians to angle\n+        if dets.device.type == 'npu':\n+            for i in range(dets.size()[0]):\n+                dets_cw[i][4] *= coefficient  # radians to angle\nComment: `coefficient = 57.29578 # 180 / PI` can be moved to line 465.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "mmcv/ops/nms.py",
    "pr_number": 2643,
    "repo": "mmcv",
    "owner": "open-mmlab",
    "comment_id": 1144749587,
    "comment_created_at": "2023-03-22T12:43:02Z"
  },
  {
    "code": "@@ -1,57 +1,70 @@ -### Translating the Transformers documentation into your language +# Translating the Transformers Documentation into Your Language",
    "comment": "not necessary to capitalize the other letters here",
    "line_number": 1,
    "enriched": "File: docs/TRANSLATING.md\nCode: @@ -1,57 +1,70 @@\n-### Translating the Transformers documentation into your language\n+# Translating the Transformers Documentation into Your Language\nComment: Not necessary to capitalize the other letters here",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/TRANSLATING.md",
    "pr_number": 34226,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 1823079807,
    "comment_created_at": "2024-10-30T17:18:35Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@ +--- +title: Installation Methods +hide_title: true +sidebar_position: 2 +version: 1 +--- + +import useBaseUrl from \"@docusaurus/useBaseUrl\"; + +# Installation Methods + +How should you install Superset? Here's a comparison of the different options. It will help if you've first read the [Architecture](/docs/installation/architecture.mdx) page to understand Superset's different components.",
    "comment": "no need for it now, but i guess that link is where @mistercrunch 's diagram could go.",
    "line_number": 12,
    "enriched": "File: docs/docs/installation/installation-methods.mdx\nCode: @@ -0,0 +1,56 @@\n+---\n+title: Installation Methods\n+hide_title: true\n+sidebar_position: 2\n+version: 1\n+---\n+\n+import useBaseUrl from \"@docusaurus/useBaseUrl\";\n+\n+# Installation Methods\n+\n+How should you install Superset? Here's a comparison of the different options. It will help if you've first read the [Architecture](/docs/installation/architecture.mdx) page to understand Superset's different components.\nComment: No need for it now, but I guess that link is where @mistercrunch 's diagram could go.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/docs/installation/installation-methods.mdx",
    "pr_number": 33137,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 2049004402,
    "comment_created_at": "2025-04-17T13:52:53Z"
  },
  {
    "code": "@@ -68,7 +68,8 @@ def format( str_number = \"{:f}\".format(number) else: str_number = str(number) - if str_number[0] == \"-\": + # Test for sign but also database null received as empty string + if str_number and str_number[0] == \"-\":",
    "comment": "would not it be better to check this at the very beginning and return none or \"\" immediately?\r\npython\r\ndef format(\r\n    ...\r\n):\r\n    if not str_number:\r\n        return str_number\r\n    use_grouping = (\r\n        use_l10n or (use_l10n is none and settings.use_l10n)\r\n    ) and settings.use_thousand_separator\r\n    ...",
    "line_number": 72,
    "enriched": "File: django/utils/numberformat.py\nCode: @@ -68,7 +68,8 @@ def format(\n             str_number = \"{:f}\".format(number)\n     else:\n         str_number = str(number)\n-    if str_number[0] == \"-\":\n+    # Test for sign but also database null received as empty string\n+    if str_number and str_number[0] == \"-\":\nComment: Would not it be better to check this at the very beginning and return `None` or `\"\"` immediately?\r\n```python\r\ndef format(\r\n    ...\r\n):\r\n    if not str_number:\r\n        return str_number\r\n    use_grouping = (\r\n        use_l10n or (use_l10n is None and settings.USE_L10N)\r\n    ) and settings.USE_THOUSAND_SEPARATOR\r\n    ...\r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "django/utils/numberformat.py",
    "pr_number": 16046,
    "repo": "django",
    "owner": "django",
    "comment_id": 967955272,
    "comment_created_at": "2022-09-12T03:17:26Z"
  },
  {
    "code": "@@ -1,35 +1,152 @@ -from keras.src import layers +import copy +import re +import warnings + +import torch + +from keras.src import backend +from keras.src import ops from keras.src import tree +from keras.src.utils.module_utils import tensorflow as tf +from keras.src.utils.module_utils import torch_xla class TorchExportArchive: def track(self, resource): - if not isinstance(resource, layers.Layer): - raise ValueError( - \"Invalid resource type. Expected an instance of a \" - \"JAX-based Keras `Layer` or `Model`. \" - f\"Received instead an object of type '{type(resource)}'. \" - f\"Object received: {resource}\" - ) + raise NotImplementedError( + \"`track` is not implemented in the torch backend. Use\" + \"`track_and_add_endpoint` instead.\" + ) + + def add_endpoint(self, name, fn, input_signature, **kwargs): + raise NotImplementedError( + \"`add_endpoint` is not implemented in the torch backend. Use\" + \"`track_and_add_endpoint` instead.\" + ) + + def track_and_add_endpoint(self, name, resource, input_signature, **kwargs):",
    "comment": "why was the new api needed? can't we just make track and add_endpoint pure config ops, and make write do all the actual export logic?",
    "line_number": 27,
    "enriched": "File: keras/src/backend/torch/export.py\nCode: @@ -1,35 +1,152 @@\n-from keras.src import layers\n+import copy\n+import re\n+import warnings\n+\n+import torch\n+\n+from keras.src import backend\n+from keras.src import ops\n from keras.src import tree\n+from keras.src.utils.module_utils import tensorflow as tf\n+from keras.src.utils.module_utils import torch_xla\n \n \n class TorchExportArchive:\n     def track(self, resource):\n-        if not isinstance(resource, layers.Layer):\n-            raise ValueError(\n-                \"Invalid resource type. Expected an instance of a \"\n-                \"JAX-based Keras `Layer` or `Model`. \"\n-                f\"Received instead an object of type '{type(resource)}'. \"\n-                f\"Object received: {resource}\"\n-            )\n+        raise NotImplementedError(\n+            \"`track` is not implemented in the torch backend. Use\"\n+            \"`track_and_add_endpoint` instead.\"\n+        )\n+\n+    def add_endpoint(self, name, fn, input_signature, **kwargs):\n+        raise NotImplementedError(\n+            \"`add_endpoint` is not implemented in the torch backend. Use\"\n+            \"`track_and_add_endpoint` instead.\"\n+        )\n+\n+    def track_and_add_endpoint(self, name, resource, input_signature, **kwargs):\nComment: Why was the new API needed? Can't we just make `track` and `add_endpoint` pure config ops, and make `write` do all the actual export logic?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "keras/src/backend/torch/export.py",
    "pr_number": 20685,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 1897018893,
    "comment_created_at": "2024-12-25T00:52:37Z"
  },
  {
    "code": "@@ -14,7 +14,7 @@ ray duckdb # Used for testing of st.connection -sqlalchemy[mypy]>=1.4.25, <2 +sqlalchemy[mypy]>=2.0.0",
    "comment": "there's a discrepancy between the pr title and the actual dependency change. the title indicates updating to >=1.4.25,<3 (which would maintain compatibility with both sqlalchemy 1.x and 2.x), but the code change sets it to >=2.0.0 (which would require sqlalchemy 2.x and break compatibility with 1.x).\n\nif the intention is to support both major versions as suggested by the pr title, please update the requirement to >=1.4.25,<3 instead of >=2.0.0.\n\n  \n\n*spotted by [diamond](https://app.graphite.dev/diamond/?org=streamlit&ref=ai-review-comment)*<i class='graphite__hidden'><br /><br />is this helpful? react \ud83d\udc4d or \ud83d\udc4e to let us know.</i>",
    "line_number": 17,
    "enriched": "File: lib/integration-requirements.txt\nCode: @@ -14,7 +14,7 @@ ray\n duckdb\n \n # Used for testing of st.connection\n-sqlalchemy[mypy]>=1.4.25, <2\n+sqlalchemy[mypy]>=2.0.0\nComment: There's a discrepancy between the PR title and the actual dependency change. The title indicates updating to `>=1.4.25,<3` (which would maintain compatibility with both SQLAlchemy 1.x and 2.x), but the code change sets it to `>=2.0.0` (which would require SQLAlchemy 2.x and break compatibility with 1.x).\n\nIf the intention is to support both major versions as suggested by the PR title, please update the requirement to `>=1.4.25,<3` instead of `>=2.0.0`.\n```suggestion\nsqlalchemy[mypy]>=1.4.25,<3\n```\n  \n\n*Spotted by [Diamond](https://app.graphite.dev/diamond/?org=streamlit&ref=ai-review-comment)*<i class='graphite__hidden'><br /><br />Is this helpful? React \ud83d\udc4d or \ud83d\udc4e to let us know.</i>",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "lib/integration-requirements.txt",
    "pr_number": 11199,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 2085616843,
    "comment_created_at": "2025-05-12T22:22:06Z"
  },
  {
    "code": "@@ -33,7 +33,7 @@ Bug fixes Other ~~~~~ -- +- :meth:`DataFrame.cum*` methods now have a ``numeric_only`` parameter (:issue:`53072`)",
    "comment": "could you put this under other enhancements?",
    "line_number": 36,
    "enriched": "File: doc/source/whatsnew/v2.2.2.rst\nCode: @@ -33,7 +33,7 @@ Bug fixes\n \n Other\n ~~~~~\n--\n+- :meth:`DataFrame.cum*` methods now have a ``numeric_only`` parameter (:issue:`53072`)\nComment: Could you put this under `Other Enhancements`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "doc/source/whatsnew/v2.2.2.rst",
    "pr_number": 58172,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1557993258,
    "comment_created_at": "2024-04-09T16:47:56Z"
  },
  {
    "code": "@@ -434,3 +434,49 @@ def test_reindex_expand_nonnano_nat(dtype): np.array([1, getattr(np, dtype)(\"nat\", \"s\")], dtype=f\"{dtype}[s]\") ) tm.assert_series_equal(result, expected) + + +@pytest.mark.parametrize( + \"idx,expected_match_level_a\", + [ + # Source index has matching name - should match level \"a\" + (Index([81, 82], name=\"a\"), True),",
    "comment": "instead of idx could you parameterize on name instead since the data is the same?",
    "line_number": 443,
    "enriched": "File: pandas/tests/series/methods/test_reindex.py\nCode: @@ -434,3 +434,49 @@ def test_reindex_expand_nonnano_nat(dtype):\n         np.array([1, getattr(np, dtype)(\"nat\", \"s\")], dtype=f\"{dtype}[s]\")\n     )\n     tm.assert_series_equal(result, expected)\n+\n+\n+@pytest.mark.parametrize(\n+    \"idx,expected_match_level_a\",\n+    [\n+        # Source index has matching name - should match level \"a\"\n+        (Index([81, 82], name=\"a\"), True),\nComment: Instead of `idx` could you parameterize on `name` instead since the data is the same?",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/tests/series/methods/test_reindex.py",
    "pr_number": 61969,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2237247440,
    "comment_created_at": "2025-07-28T16:47:35Z"
  },
  {
    "code": "@@ -18,6 +18,7 @@ ) import numpy as np +import pyarrow as pa",
    "comment": "pyarrow cannot be imported like this since it's not a required dependency",
    "line_number": 21,
    "enriched": "File: pandas/core/groupby/ops.py\nCode: @@ -18,6 +18,7 @@\n )\n \n import numpy as np\n+import pyarrow as pa\nComment: pyarrow cannot be imported like this since it's not a required dependency",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "pandas/core/groupby/ops.py",
    "pr_number": 58129,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1550204865,
    "comment_created_at": "2024-04-03T17:48:36Z"
  },
  {
    "code": "@@ -18,15 +18,18 @@ from __future__ import annotations import logging -from typing import TYPE_CHECKING +from typing import Optional, TYPE_CHECKING",
    "comment": "for new code it'd be nice if we always use from __future__ import annotations and then use | none notation, rather than optional[]",
    "line_number": 21,
    "enriched": "File: superset/thumbnails/digest.py\nCode: @@ -18,15 +18,18 @@\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING\n+from typing import Optional, TYPE_CHECKING\nComment: For new code it'd be nice if we always use `from __future__ import annotations` and then use `| None` notation, rather than `Optional[]`",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "superset/thumbnails/digest.py",
    "pr_number": 30336,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1768747734,
    "comment_created_at": "2024-09-20T14:38:57Z"
  },
  {
    "code": "@@ -11,13 +15,35 @@ from scrapy.utils.python import to_bytes +def basic_auth_header(username, password, auth_encoding):",
    "comment": "+1 to extract it, though i think it is better to refactor code to use w3lib's [basic_auth_header](https://github.com/scrapy/w3lib/blob/c1a030582ec30423c40215fcd159bc951c851ed7/w3lib/http.py#l81) function instead.",
    "line_number": 18,
    "enriched": "File: scrapy/downloadermiddlewares/httpproxy.py\nCode: @@ -11,13 +15,35 @@\n from scrapy.utils.python import to_bytes\n \n \n+def basic_auth_header(username, password, auth_encoding):\nComment: +1 to extract it, though I think it is better to refactor code to use w3lib's [basic_auth_header](https://github.com/scrapy/w3lib/blob/c1a030582ec30423c40215fcd159bc951c851ed7/w3lib/http.py#L81) function instead. ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "scrapy/downloadermiddlewares/httpproxy.py",
    "pr_number": 3316,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 200180841,
    "comment_created_at": "2018-07-04T17:41:40Z"
  },
  {
    "code": "@@ -1,78 +1,16 @@ \"\"\"Test CloudflareWorkersAI Chat API wrapper.\"\"\" -from typing import Any, Dict, List, Type - -import pytest -from langchain_core.language_models import BaseChatModel from langchain_core.messages import ( - AIMessage, - BaseMessage, HumanMessage, - SystemMessage, - ToolMessage, ) -from langchain_tests.unit_tests import ChatModelUnitTests from langchain_community.chat_models.cloudflare_workersai import ( - ChatCloudflareWorkersAI, - _convert_messages_to_cloudflare_messages, + _convert_message_to_dict, ) -class TestChatCloudflareWorkersAI(ChatModelUnitTests):",
    "comment": "why are we deleting the standard tests?",
    "line_number": 22,
    "enriched": "File: libs/community/tests/unit_tests/chat_models/test_cloudflare_workersai.py\nCode: @@ -1,78 +1,16 @@\n \"\"\"Test CloudflareWorkersAI Chat API wrapper.\"\"\"\n \n-from typing import Any, Dict, List, Type\n-\n-import pytest\n-from langchain_core.language_models import BaseChatModel\n from langchain_core.messages import (\n-    AIMessage,\n-    BaseMessage,\n     HumanMessage,\n-    SystemMessage,\n-    ToolMessage,\n )\n-from langchain_tests.unit_tests import ChatModelUnitTests\n \n from langchain_community.chat_models.cloudflare_workersai import (\n-    ChatCloudflareWorkersAI,\n-    _convert_messages_to_cloudflare_messages,\n+    _convert_message_to_dict,\n )\n \n \n-class TestChatCloudflareWorkersAI(ChatModelUnitTests):\nComment: Why are we deleting the standard tests?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "libs/community/tests/unit_tests/chat_models/test_cloudflare_workersai.py",
    "pr_number": 30634,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 2029107507,
    "comment_created_at": "2025-04-04T16:21:18Z"
  },
  {
    "code": "@@ -102,8 +102,11 @@ export const onDidChangeEditorDatabase: typeof sqlLabType.onDidChangeEditorDatab createActionListener( predicate(QUERY_EDITOR_SETDB), listener, - (action: { type: string; queryEditor: { dbId: number } }) => - action.queryEditor.dbId, + (action: { + type: string; + dbId?: number; + queryEditor: { dbId: number }; + }) => action.dbId || action.queryEditor.dbId,",
    "comment": "### incorrect falsy value handling in database id selection <sub>![category functionality](https://img.shields.io/badge/functionality-0284c7)</sub>\n\n<details>\n  <summary>tell me more</summary>\n\n###### what is the issue?\nthe or operator (||) will evaluate falsy values like 0 as false, causing it to incorrectly fallback to queryeditor.dbid when dbid is 0\n\n\n###### why this matters\nif dbid is 0 (which is a valid database id), the code will incorrectly use queryeditor.dbid instead, potentially connecting to the wrong database\n\n###### suggested change \u2219 *feature preview*\nuse nullish coalescing (??) instead of or operator to only fallback when dbid is null or undefined:\ntypescript\n(action: {\n        type: string;\n        dbid?: number;\n        queryeditor: { dbid: number };\n      }) => action.dbid ?? action.queryeditor.dbid\n\n\n\n###### provide feedback to improve future suggestions\n[![nice catch](https://img.shields.io/badge/\ud83d\udc4d%20nice%20catch-71bc78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/227851c7-9e56-4c50-ab41-bd3b882c355c/upvote) [![incorrect](https://img.shields.io/badge/\ud83d\udc4e%20incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/227851c7-9e56-4c50-ab41-bd3b882c355c?what_not_true=true)  [![not in scope](https://img.shields.io/badge/\ud83d\udc4e%20out%20of%20pr%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/227851c7-9e56-4c50-ab41-bd3b882c355c?what_out_of_scope=true) [![not in coding standard](https://img.shields.io/badge/\ud83d\udc4e%20not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/227851c7-9e56-4c50-ab41-bd3b882c355c?what_not_in_standard=true) [![other](https://img.shields.io/badge/\ud83d\udc4e%20other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/227851c7-9e56-4c50-ab41-bd3b882c355c)\n</details>\n\n<sub>\n\n\ud83d\udcac looking for more details? reply to this comment to chat with korbit.\n</sub>\n\n<!--- korbi internal id:0a4fb6bd-f8be-4e59-902d-1a27a8dac397 -->\n\n\n[](0a4fb6bd-f8be-4e59-902d-1a27a8dac397)",
    "line_number": 109,
    "enriched": "File: superset-frontend/src/core/sqlLab.ts\nCode: @@ -102,8 +102,11 @@ export const onDidChangeEditorDatabase: typeof sqlLabType.onDidChangeEditorDatab\n     createActionListener(\n       predicate(QUERY_EDITOR_SETDB),\n       listener,\n-      (action: { type: string; queryEditor: { dbId: number } }) =>\n-        action.queryEditor.dbId,\n+      (action: {\n+        type: string;\n+        dbId?: number;\n+        queryEditor: { dbId: number };\n+      }) => action.dbId || action.queryEditor.dbId,\nComment: ### Incorrect falsy value handling in database ID selection <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe OR operator (||) will evaluate falsy values like 0 as false, causing it to incorrectly fallback to queryEditor.dbId when dbId is 0\n\n\n###### Why this matters\nIf dbId is 0 (which is a valid database ID), the code will incorrectly use queryEditor.dbId instead, potentially connecting to the wrong database\n\n###### Suggested change \u2219 *Feature Preview*\nUse nullish coalescing (??) instead of OR operator to only fallback when dbId is null or undefined:\n```typescript\n(action: {\n        type: string;\n        dbId?: number;\n        queryEditor: { dbId: number };\n      }) => action.dbId ?? action.queryEditor.dbId\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/\ud83d\udc4d%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/227851c7-9e56-4c50-ab41-bd3b882c355c/upvote) [![Incorrect](https://img.shields.io/badge/\ud83d\udc4e%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/227851c7-9e56-4c50-ab41-bd3b882c355c?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/\ud83d\udc4e%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/227851c7-9e56-4c50-ab41-bd3b882c355c?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/\ud83d\udc4e%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/227851c7-9e56-4c50-ab41-bd3b882c355c?what_not_in_standard=true) [![Other](https://img.shields.io/badge/\ud83d\udc4e%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/227851c7-9e56-4c50-ab41-bd3b882c355c)\n</details>\n\n<sub>\n\n\ud83d\udcac Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:0a4fb6bd-f8be-4e59-902d-1a27a8dac397 -->\n\n\n[](0a4fb6bd-f8be-4e59-902d-1a27a8dac397)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "superset-frontend/src/core/sqlLab.ts",
    "pr_number": 35071,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 2333573464,
    "comment_created_at": "2025-09-09T13:20:01Z"
  },
  {
    "code": "@@ -316,6 +316,9 @@ class SpectralCoclustering(BaseSpectral): array([0, 0], dtype=int32) >>> clustering SpectralCoclustering(n_clusters=2, random_state=0) + + For a more detailed example, see",
    "comment": "this looks good but can you correct the indentation?",
    "line_number": 320,
    "enriched": "File: sklearn/cluster/_bicluster.py\nCode: @@ -316,6 +316,9 @@ class SpectralCoclustering(BaseSpectral):\n     array([0, 0], dtype=int32)\n     >>> clustering\n     SpectralCoclustering(n_clusters=2, random_state=0)\n+\n+        For a more detailed example, see\nComment: This looks good but can you correct the indentation? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "sklearn/cluster/_bicluster.py",
    "pr_number": 29606,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1720758963,
    "comment_created_at": "2024-08-17T11:04:50Z"
  },
  {
    "code": "@@ -313,6 +313,12 @@ export const globalStyles = (theme: EmotionTheme): SerializedStyles => css` font-weight: ${theme.fontWeights.bold}; } + // Override h1 font weight to default weight + h1 b, + h1 strong { + font-weight: ${theme.fontWeights.extrabold}; + }",
    "comment": "**question:** is this intentionally modifying a global css selector? i'm asking because the pr description and title both refer to \"bold in markdown,\" but this will apply even outside of the markdown component such as st.html(\"<h1>hello <strong>world</strong></h1>\"). that's great if that's expected, i just wanted to check!",
    "line_number": 320,
    "enriched": "File: frontend/lib/src/theme/globalStyles.ts\nCode: @@ -313,6 +313,12 @@ export const globalStyles = (theme: EmotionTheme): SerializedStyles => css`\n     font-weight: ${theme.fontWeights.bold};\n   }\n \n+  // Override h1 font weight to default weight\n+  h1 b,\n+  h1 strong {\n+    font-weight: ${theme.fontWeights.extrabold};\n+  }\nComment: **question:** Is this intentionally modifying a global CSS selector? I'm asking because the PR description and title both refer to \"bold in Markdown,\" but this will apply even outside of the Markdown component such as `st.html(\"<h1>Hello <strong>world</strong></h1>\")`. That's great if that's expected, I just wanted to check!",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "frontend/lib/src/theme/globalStyles.ts",
    "pr_number": 9395,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1745825644,
    "comment_created_at": "2024-09-05T16:11:27Z"
  },
  {
    "code": "@@ -1,12 +1,12 @@ autodoc_pydantic>=2,<3 sphinx>=8,<9 -myst-parser>=3 sphinx-autobuild>=2024 +sphinx-design +sphinx-copybutton",
    "comment": "this is just a resort i think?",
    "line_number": 5,
    "enriched": "File: docs/api_reference/requirements.txt\nCode: @@ -1,12 +1,12 @@\n autodoc_pydantic>=2,<3\n sphinx>=8,<9\n-myst-parser>=3\n sphinx-autobuild>=2024\n+sphinx-design\n+sphinx-copybutton\nComment: this is just a resort I think?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/api_reference/requirements.txt",
    "pr_number": 33057,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 2373268744,
    "comment_created_at": "2025-09-23T19:32:36Z"
  },
  {
    "code": "@@ -83,7 +83,7 @@ permissions. ``you@example.com`` is the email address associated with the key you want to use. -* A clean Python virtual environment per Django version being released, with +* A clean Python virtual environment (Python 3.9+) to build artifacts, with",
    "comment": "to build artifacts, we need to use python 3.9+ so setuptools >= 75.8.1 is used, ensuring proper artifact names (lowercase d in django) for both the tarball and wheel. this will also be the case when ticket-35961 gets worked on/solved, since the new project.license and project.license-files are available on setuptools>=77.0.0 which supports python 3.9+.",
    "line_number": 86,
    "enriched": "File: docs/internals/howto-release-django.txt\nCode: @@ -83,7 +83,7 @@ permissions.\n     ``you@example.com`` is the email address associated with the key you want\n     to use.\n \n-* A clean Python virtual environment per Django version being released, with\n+* A clean Python virtual environment (Python 3.9+) to build artifacts, with\nComment: To build artifacts, we need to use Python 3.9+ so setuptools >= 75.8.1 is used, ensuring proper artifact names (lowercase `d` in `django`) for both the tarball and wheel. This will also be the case when ticket-35961 gets worked on/solved, since the new `project.license` and `project.license-files` are available on setuptools>=77.0.0 which supports Python 3.9+.\r\n",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "docs/internals/howto-release-django.txt",
    "pr_number": 19443,
    "repo": "django",
    "owner": "django",
    "comment_id": 2075339421,
    "comment_created_at": "2025-05-06T12:14:00Z"
  },
  {
    "code": "@@ -58,9 +58,16 @@ once):: If you installed Python some other way than conda, first install `virtualenv`_ (optionally use `virtualenvwrapper`_), then create your -virtualenv (named ``numpy-dev`` here) with:: +virtualenv (named ``numpy-dev`` here), activate it, and install all project +dependencies with:: $ virtualenv numpy-dev + $ source numpy-dev/bin/activate # activate virtual environment + $ python -m pip install -r build_requirements.txt + $ python -m pip install -r doc_requirements.txt + $ python -m pip install -r linter_requirements.txt + $ python -m pip install -r release_requirements.txt + $ python -m pip install -r test_requirements.txt",
    "comment": "requirements files support referencing other requirements files (https://pip.pypa.io/en/stable/reference/requirements-file-format/#requirements-file-format). i suggest to create a file all_requirements.txt that collects all the individual requirements. \r\n\r\n\r\n-r build_requirements.txt\r\n-r doc_requirements.txt\r\n-r linter_requirements.txt\r\n-r release_requirements.txt\r\n-r test_requirements.txt\r\n\r\n\r\nso that one can do\r\n\r\n$ python -m pip install -r all_requirements.txt",
    "line_number": 70,
    "enriched": "File: doc/source/dev/development_environment.rst\nCode: @@ -58,9 +58,16 @@ once)::\n \n If you installed Python some other way than conda, first install\n `virtualenv`_ (optionally use `virtualenvwrapper`_), then create your\n-virtualenv (named ``numpy-dev`` here) with::\n+virtualenv (named ``numpy-dev`` here), activate it, and install all project \n+dependencies with::\n \n     $ virtualenv numpy-dev\n+    $ source numpy-dev/bin/activate # activate virtual environment\n+    $ python -m pip install -r build_requirements.txt\n+    $ python -m pip install -r doc_requirements.txt\n+    $ python -m pip install -r linter_requirements.txt\n+    $ python -m pip install -r release_requirements.txt\n+    $ python -m pip install -r test_requirements.txt\nComment: Requirements files support referencing other requirements files (https://pip.pypa.io/en/stable/reference/requirements-file-format/#requirements-file-format). I suggest to create a file `all_requirements.txt` that collects all the individual requirements. \r\n\r\n```\r\n-r build_requirements.txt\r\n-r doc_requirements.txt\r\n-r linter_requirements.txt\r\n-r release_requirements.txt\r\n-r test_requirements.txt\r\n```\r\n\r\nSo that one can do\r\n```\r\n$ python -m pip install -r all_requirements.txt\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "doc/source/dev/development_environment.rst",
    "pr_number": 25302,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 1413598578,
    "comment_created_at": "2023-12-04T09:34:47Z"
  },
  {
    "code": "@@ -990,7 +990,7 @@ CSRF protection extended to PUT and DELETE Previously, Django's :doc:`CSRF protection </ref/csrf/>` provided protection only against POST requests. Since use of PUT and DELETE methods in AJAX applications is becoming more common, we now protect all methods not -defined as safe by :rfc:`2616` -- i.e., we exempt GET, HEAD, OPTIONS and TRACE, +defined as safe by :rfc:`9110` -- i.e., we exempt GET, HEAD, OPTIONS and TRACE,",
    "comment": "i'd revert all changes to release notes as they are confusing, e.g. rfc 9110 didn't exist in 2012 when django 1.4 was released.",
    "line_number": 993,
    "enriched": "File: docs/releases/1.4.txt\nCode: @@ -990,7 +990,7 @@ CSRF protection extended to PUT and DELETE\n Previously, Django's :doc:`CSRF protection </ref/csrf/>` provided\n protection only against POST requests. Since use of PUT and DELETE methods in\n AJAX applications is becoming more common, we now protect all methods not\n-defined as safe by :rfc:`2616` -- i.e., we exempt GET, HEAD, OPTIONS and TRACE,\n+defined as safe by :rfc:`9110` -- i.e., we exempt GET, HEAD, OPTIONS and TRACE,\nComment: I'd revert all changes to release notes as they are confusing, e.g. RFC 9110 didn't exist in 2012 when Django 1.4 was released.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/releases/1.4.txt",
    "pr_number": 16257,
    "repo": "django",
    "owner": "django",
    "comment_id": 1018985268,
    "comment_created_at": "2022-11-10T11:29:08Z"
  },
  {
    "code": "@@ -237,7 +237,7 @@ After you have ensured this site is distributing its content legally, you can fo In any case, thank you very much for your contributions! -**Tip:** To test extractors that require login information, create a file `test/local_parameters.json` and add `\"usenetrc\": true` or your username and password in it: ++**Tip:** To test extractors that require login information, create a file `test/local_parameters.json` and add `\"usenetrc\": true` or your `username`&`password` or `cookiefile`/`cookiesfrombrowser` in it:",
    "comment": "im assuming the + was not intentional",
    "line_number": 240,
    "enriched": "File: CONTRIBUTING.md\nCode: @@ -237,7 +237,7 @@ After you have ensured this site is distributing its content legally, you can fo\n \n In any case, thank you very much for your contributions!\n \n-**Tip:** To test extractors that require login information, create a file `test/local_parameters.json` and add `\"usenetrc\": true` or your username and password in it:\n++**Tip:** To test extractors that require login information, create a file `test/local_parameters.json` and add `\"usenetrc\": true` or your `username`&`password` or `cookiefile`/`cookiesfrombrowser` in it:\nComment: Im assuming the `+` was not intentional\r\n```suggestion\r\n**Tip:** To test extractors that require login information, create a file `test/local_parameters.json` and add `\"usenetrc\": true` or your `username`&`password` or `cookiefile`/`cookiesfrombrowser` in it:\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "CONTRIBUTING.md",
    "pr_number": 8977,
    "repo": "yt-dlp",
    "owner": "yt-dlp",
    "comment_id": 1468432694,
    "comment_created_at": "2024-01-27T09:51:03Z"
  },
  {
    "code": "@@ -707,6 +707,7 @@ def on_disconnect(self, sid): pass async def on_event(self, sid, data): + # TODO: should this be front-end websocket events below?",
    "comment": "nit: either put this comment above the function declaration or after the docstring, not between",
    "line_number": 710,
    "enriched": "File: reflex/app.py\nCode: @@ -707,6 +707,7 @@ def on_disconnect(self, sid):\n         pass\n \n     async def on_event(self, sid, data):\n+        # TODO: should this be front-end websocket events below?\nComment: nit: either put this comment above the function declaration or after the docstring, not between",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "reflex/app.py",
    "pr_number": 1614,
    "repo": "reflex",
    "owner": "reflex-dev",
    "comment_id": 1297834060,
    "comment_created_at": "2023-08-17T23:11:18Z"
  },
  {
    "code": "@@ -16,6 +16,7 @@ _ALL_COMPONENTS = [ # Core + \"Cond\",",
    "comment": "why do we need these? i thought we decided to keep the component classes out of the rx. namespace",
    "line_number": 19,
    "enriched": "File: reflex/__init__.py\nCode: @@ -16,6 +16,7 @@\n \n _ALL_COMPONENTS = [\n     # Core\n+    \"Cond\",\nComment: Why do we need these? I thought we decided to keep the component classes out of the `rx.` namespace",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "reflex/__init__.py",
    "pr_number": 2523,
    "repo": "reflex",
    "owner": "reflex-dev",
    "comment_id": 1477469951,
    "comment_created_at": "2024-02-04T22:45:28Z"
  },
  {
    "code": "@@ -0,0 +1,8 @@ +# Discord save the times at which a message is sent as \"Snowflake\", this is a code to convert the Snowflake into the actual timestamp. +import math +import time + +con = input(\"SnowflakeID \") +dec = int(con) / 4194304 +ms = math.trunc(int(dec) + 1420070400000) +print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ms / 1000.0)))",
    "comment": "could you make these functions?",
    "line_number": 8,
    "enriched": "File: conversions/SnowflakeTimestamp_Fn.py\nCode: @@ -0,0 +1,8 @@\n+# Discord save the times at which a message is sent as \"Snowflake\", this is a code to convert the Snowflake into the actual timestamp.\n+import math\n+import time\n+\n+con = input(\"SnowflakeID \")\n+dec = int(con) / 4194304\n+ms = math.trunc(int(dec) + 1420070400000)\n+print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ms / 1000.0)))\nComment: Could you make these functions?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "conversions/SnowflakeTimestamp_Fn.py",
    "pr_number": 7397,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004992173,
    "comment_created_at": "2022-10-25T21:46:42Z"
  },
  {
    "code": "@@ -76,7 +76,7 @@ test = [ ] codespell = [\"codespell<3.0.0,>=2.2.0\"] test_integration = [ - \"pytest-vcr<2.0.0,>=1.0.2\",",
    "comment": "about this unexpected change:  \r\n\r\n\r\nafter bumping langsmith i hit this integration test [error](https://github.com/langchain-ai/langchain/actions/runs/15622944899/job/44011626693?pr=31594) in ci/cd.\r\n\r\nmy best-effort attempt to fix is remove pytest-vcr and use vcrpy directly after looking online.\r\n\r\nthis is a conflict between pytest-recording and pytest-vcr: internalerror> runtimeerror: pytest-recording is incompatible with pytest-vcr. please, uninstall pytest-vcr in order to use pytest-recording.",
    "line_number": 79,
    "enriched": "File: libs/langchain/pyproject.toml\nCode: @@ -76,7 +76,7 @@ test = [\n ]\n codespell = [\"codespell<3.0.0,>=2.2.0\"]\n test_integration = [\n-    \"pytest-vcr<2.0.0,>=1.0.2\",\nComment: About this unexpected change:  \r\n\r\n\r\nAfter bumping langsmith i hit this Integration Test [error](https://github.com/langchain-ai/langchain/actions/runs/15622944899/job/44011626693?pr=31594) in ci/cd.\r\n\r\nMy best-effort attempt to fix is remove pytest-vcr and use vcrpy directly after looking online.\r\n\r\nThis is a conflict between pytest-recording and pytest-vcr: INTERNALERROR> RuntimeError: `pytest-recording` is incompatible with `pytest-vcr`. Please, uninstall `pytest-vcr` in order to use `pytest-recording`.\r\n\r\n\r\n",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "libs/langchain/pyproject.toml",
    "pr_number": 31594,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 2143848147,
    "comment_created_at": "2025-06-12T23:59:02Z"
  },
  {
    "code": "@@ -455,6 +457,9 @@ def convert_anything_to_df( if is_type(df, \"numpy.ndarray\") and len(df.shape) == 0: return pd.DataFrame([]) + if is_type(df, _PYSPARK_DF_TYPE_STR) and callable(getattr(df, \"toPandas\")): + df = df.toPandas()",
    "comment": "aren't pyspark dataframes also uneveluated? maybe we should apply the limit here as well?",
    "line_number": 461,
    "enriched": "File: lib/streamlit/type_util.py\nCode: @@ -455,6 +457,9 @@ def convert_anything_to_df(\n     if is_type(df, \"numpy.ndarray\") and len(df.shape) == 0:\n         return pd.DataFrame([])\n \n+    if is_type(df, _PYSPARK_DF_TYPE_STR) and callable(getattr(df, \"toPandas\")):\n+        df = df.toPandas()\nComment: Aren't Pyspark dataframes also uneveluated? Maybe we should apply the limit here as well?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/streamlit/type_util.py",
    "pr_number": 5624,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1008819044,
    "comment_created_at": "2022-10-30T09:19:17Z"
  },
  {
    "code": "@@ -390,7 +390,11 @@ def extract_metadata(webpage): k == 'media' and str(v['id']) == video_id and v['__typename'] == 'Video')), expected_type=dict) title = get_first(media, ('title', 'text')) description = get_first(media, ('creation_story', 'comet_sections', 'message', 'story', 'message', 'text')) - uploader_data = get_first(media, 'owner') or get_first(post, ('node', 'actors', ...)) or {} + uploader_data = ( + get_first(post, (..., ..., lambda k, v: k == 'owner' and v.get('name')), expected_type=dict) + or get_first(media, 'owner') + or get_first(post, ('node', 'actors', ...)) + or {})",
    "comment": "..., ... seems maybe too lax for the first traversal path? i.e. it could branch out and return the wrong dict instead of falling back to the other get_firsts. is all that branching necessary? or can we be more specific?\r\n\r\nif you think we need ..., ..., suggestion:",
    "line_number": 397,
    "enriched": "File: yt_dlp/extractor/facebook.py\nCode: @@ -390,7 +390,11 @@ def extract_metadata(webpage):\n                 k == 'media' and str(v['id']) == video_id and v['__typename'] == 'Video')), expected_type=dict)\n             title = get_first(media, ('title', 'text'))\n             description = get_first(media, ('creation_story', 'comet_sections', 'message', 'story', 'message', 'text'))\n-            uploader_data = get_first(media, 'owner') or get_first(post, ('node', 'actors', ...)) or {}\n+            uploader_data = (\n+                get_first(post, (..., ..., lambda k, v: k == 'owner' and v.get('name')), expected_type=dict)\n+                or get_first(media, 'owner')\n+                or get_first(post, ('node', 'actors', ...))\n+                or {})\nComment: `..., ...` seems maybe too lax for the first traversal path? i.e. it could branch out and return the wrong dict instead of falling back to the other `get_first`s. Is all that branching necessary? Or can we be more specific?\r\n\r\nIf you think we need `..., ...`, suggestion:\r\n```suggestion\r\n            uploader_data = (\r\n                get_first(post, (..., ..., lambda k, v: k == 'owner' and v['name']))\r\n                or get_first(media, ('owner', {dict}))\r\n                or get_first(post, ('node', 'actors', ..., {dict})) or {})\r\n```",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "yt_dlp/extractor/facebook.py",
    "pr_number": 6856,
    "repo": "yt-dlp",
    "owner": "yt-dlp",
    "comment_id": 1180881032,
    "comment_created_at": "2023-04-28T23:32:03Z"
  },
  {
    "code": "@@ -45,7 +45,9 @@ def show_error(msg):\n-    print(f\"Error: {msg}\")\n+    print(f\"ERROR: {msg}. Please contact support.\")",
    "comment": "add guidance in the error message so users know what to do next.",
    "line_number": 46,
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "app/errors.py",
    "pr_number": 101,
    "repo": "web-app",
    "owner": "example-org",
    "comment_id": 3001,
    "comment_created_at": "2024-03-10T14:22:03Z"
  },
  {
    "code": "@@ -88,7 +88,9 @@ def alert_user():\n-    return \"Something went wrong\"\n+    return \"Unexpected error occurred. Retry or contact admin.\"",
    "comment": "make the alert message actionable instead of vague.",
    "line_number": 89,
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "alerts/handler.py",
    "pr_number": 102,
    "repo": "alert-system",
    "owner": "infra-team",
    "comment_id": 3002,
    "comment_created_at": "2024-03-15T09:15:00Z"
  },
  {
    "code": "@@ -120,7 +120,9 @@ def toast_message():\n-    return \"Saved\"\n+    return \"\u2705 Changes saved successfully\"",
    "comment": "add a success indicator emoji for better ux in toast messages.",
    "line_number": 121,
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "ui/toast.py",
    "pr_number": 103,
    "repo": "frontend-app",
    "owner": "ui-team",
    "comment_id": 3003,
    "comment_created_at": "2024-03-20T11:45:00Z"
  },
  {
    "code": "@@ -67,7 +67,9 @@ def invalid_input_alert():\n-    return \"Invalid input\"\n+    return \"Invalid input: Please enter a valid email address.\"",
    "comment": "provide specific guidance in the error message for better usability.",
    "line_number": 68,
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "forms/validation.py",
    "pr_number": 104,
    "repo": "form-handler",
    "owner": "backend-team",
    "comment_id": 3004,
    "comment_created_at": "2024-03-25T10:00:00Z"
  },
  {
    "code": "@@ -98,7 +98,9 @@ def show_warning():\n-    print(\"Warning\")\n+    print(\"\u26a0\ufe0f Warning: Low disk space\")",
    "comment": "add context to the warning message so users know what the issue is.",
    "line_number": 99,
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "system/warnings.py",
    "pr_number": 105,
    "repo": "infra-monitor",
    "owner": "ops-team",
    "comment_id": 3005,
    "comment_created_at": "2024-03-30T14:00:00Z"
  },
  {
    "code": "@@ -150,7 +150,9 @@ def error_popup():\n-    return \"Error occurred\"\n+    return \"Error occurred while saving. Please try again.\"",
    "comment": "make the popup message more descriptive and actionable.",
    "line_number": 151,
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "ui/popups.py",
    "pr_number": 106,
    "repo": "desktop-app",
    "owner": "desktop-team",
    "comment_id": 3006,
    "comment_created_at": "2024-04-02T12:45:00Z"
  },
  {
    "code": "@@ -33,7 +33,9 @@ def alert_connection_error():\n-    return \"Connection failed\"\n+    return \"Connection failed. Check your internet and retry.\"",
    "comment": "add troubleshooting steps to the alert message.",
    "line_number": 34,
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "network/alerts.py",
    "pr_number": 107,
    "repo": "network-utils",
    "owner": "infra-team",
    "comment_id": 3007,
    "comment_created_at": "2024-04-05T16:30:00Z"
  },
  {
    "code": "@@ -72,7 +72,9 @@ def toast_upload():\n-    return \"Upload complete\"\n+    return \"\u2705 Upload complete. File saved to /uploads.\"",
    "comment": "include file path in the success toast for clarity.",
    "line_number": 73,
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "uploads/toast.py",
    "pr_number": 108,
    "repo": "file-manager",
    "owner": "storage-team",
    "comment_id": 3008,
    "comment_created_at": "2024-04-10T09:30:00Z"
  },
  {
    "code": "@@ -110,7 +110,9 @@ def show_error_dialog():\n-    return \"Error\"\n+    return \"Error: Unable to load configuration file.\"",
    "comment": "specify what caused the error in the dialog message.",
    "line_number": 111,
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "config/dialogs.py",
    "pr_number": 109,
    "repo": "config-tool",
    "owner": "dev-tools",
    "comment_id": 3009,
    "comment_created_at": "2024-04-15T17:00:00Z"
  },
  {
    "code": "@@ -58,7 +58,9 @@ def alert_timeout():\n-    return \"Timeout\"\n+    return \"Request timed out. Please refresh the page.\"",
    "comment": "add actionable advice to the timeout alert.",
    "line_number": 59,
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "network/timeout.py",
    "pr_number": 110,
    "repo": "web-client",
    "owner": "frontend-team",
    "comment_id": 3010,
    "comment_created_at": "2024-04-20T18:00:00Z"
  },
  {
    "code": "@@ -120,7 +120,9 @@ def read_file(path):\n-    f = open(path)\n+    f = open(path)\n+    data = f.read()\n+    f.close()",
    "comment": "file is opened but never closed in the original code. this can lead to resource leaks.",
    "line_number": 121,
    "subcategory": "resource",
    "category": "functional",
    "file_path": "utils/file_reader.py",
    "pr_number": 201,
    "repo": "file-utils",
    "owner": "infra-team",
    "comment_id": 4001,
    "comment_created_at": "2024-05-10T14:22:03Z"
  },
  {
    "code": "@@ -88,7 +88,9 @@ def process_data():\n-    buffer = []\n+    buffer = []  # Consider limiting size or using deque for large data",
    "comment": "using an unbounded list as a buffer can cause memory issues for large datasets.",
    "line_number": 89,
    "subcategory": "resource",
    "category": "functional",
    "file_path": "data/processor.py",
    "pr_number": 202,
    "repo": "data-pipeline",
    "owner": "analytics-team",
    "comment_id": 4002,
    "comment_created_at": "2024-05-15T09:15:00Z"
  },
  {
    "code": "@@ -45,7 +45,9 @@ def write_temp_file():\n-    open('temp.txt', 'w').write('data')\n+    with open('temp.txt', 'w') as f:\n+        f.write('data')",
    "comment": "use context manager to ensure file is properly closed even if an exception occurs.",
    "line_number": 46,
    "subcategory": "resource",
    "category": "functional",
    "file_path": "temp/file_ops.py",
    "pr_number": 203,
    "repo": "file-handler",
    "owner": "backend-team",
    "comment_id": 4003,
    "comment_created_at": "2024-05-20T11:45:00Z"
  },
  {
    "code": "@@ -67,7 +67,9 @@ def load_image():\n-    img = Image.open('image.png')\n+    img = Image.open('image.png')\n+    img.load()",
    "comment": "image object should be fully loaded before processing to avoid lazy loading issues.",
    "line_number": 68,
    "subcategory": "resource",
    "category": "functional",
    "file_path": "graphics/image_loader.py",
    "pr_number": 204,
    "repo": "image-utils",
    "owner": "graphics-team",
    "comment_id": 4004,
    "comment_created_at": "2024-05-25T10:00:00Z"
  },
  {
    "code": "@@ -98,7 +98,9 @@ def handle_socket():\n-    s = socket.socket()\n+    s = socket.socket()\n+    s.settimeout(10)",
    "comment": "socket should have a timeout to prevent hanging indefinitely.",
    "line_number": 99,
    "subcategory": "resource",
    "category": "functional",
    "file_path": "network/socket_handler.py",
    "pr_number": 205,
    "repo": "network-utils",
    "owner": "infra-team",
    "comment_id": 4005,
    "comment_created_at": "2024-05-30T14:00:00Z"
  },
  {
    "code": "@@ -150,7 +150,9 @@ def read_large_file():\n-    data = f.read()\n+    for chunk in iter(lambda: f.read(4096), ''):\n+        process(chunk)",
    "comment": "reading entire file into memory can cause oom errors. use chunked reading instead.",
    "line_number": 151,
    "subcategory": "resource",
    "category": "functional",
    "file_path": "io/file_reader.py",
    "pr_number": 206,
    "repo": "big-data-tools",
    "owner": "data-team",
    "comment_id": 4006,
    "comment_created_at": "2024-06-02T12:45:00Z"
  },
  {
    "code": "@@ -33,7 +33,9 @@ def cache_results():\n-    cache = {}\n+    cache = LRUCache(maxsize=1000)",
    "comment": "unbounded dictionary for caching can lead to memory leaks. use lru cache with size limit.",
    "line_number": 34,
    "subcategory": "resource",
    "category": "functional",
    "file_path": "cache/cache_manager.py",
    "pr_number": 207,
    "repo": "performance-utils",
    "owner": "backend-team",
    "comment_id": 4007,
    "comment_created_at": "2024-06-05T16:30:00Z"
  },
  {
    "code": "@@ -72,7 +72,9 @@ def temp_dir():\n-    os.mkdir('tmp')\n+    tempfile.mkdtemp()",
    "comment": "use tempfile.mkdtemp() for secure temporary directory creation instead of hardcoded path.",
    "line_number": 73,
    "subcategory": "resource",
    "category": "functional",
    "file_path": "utils/temp.py",
    "pr_number": 208,
    "repo": "system-utils",
    "owner": "infra-team",
    "comment_id": 4008,
    "comment_created_at": "2024-06-10T09:30:00Z"
  },
  {
    "code": "@@ -110,7 +110,9 @@ def close_connection(conn):\n-    pass\n+    conn.close()",
    "comment": "database connection should be explicitly closed to free resources.",
    "line_number": 111,
    "subcategory": "resource",
    "category": "functional",
    "file_path": "db/connection.py",
    "pr_number": 209,
    "repo": "db-utils",
    "owner": "data-team",
    "comment_id": 4009,
    "comment_created_at": "2024-06-15T17:00:00Z"
  },
  {
    "code": "@@ -58,7 +58,9 @@ def allocate_array():\n-    arr = [0] * 10000000\n+    arr = array('i', [0]) * 10000000",
    "comment": "consider using array module for memory efficiency instead of python list for large numeric data.",
    "line_number": 59,
    "subcategory": "resource",
    "category": "functional",
    "file_path": "memory/allocator.py",
    "pr_number": 210,
    "repo": "performance-lib",
    "owner": "perf-team",
    "comment_id": 4010,
    "comment_created_at": "2024-06-20T18:00:00Z"
  },
  {
    "code": "@@ -1361,18 +1362,36 @@ def nankurt( m2 = adjusted2.sum(axis, dtype=np.float64) m4 = adjusted4.sum(axis, dtype=np.float64) + # Several floating point errors may occur during the summation due to rounding. + # We need to estimate an upper bound to the error to consider the data constant.",
    "comment": "could you also mention and maybe link to where scipy does a similar calculation?",
    "line_number": 1366,
    "enriched": "File: pandas/core/nanops.py\nCode: @@ -1361,18 +1362,36 @@ def nankurt(\n     m2 = adjusted2.sum(axis, dtype=np.float64)\n     m4 = adjusted4.sum(axis, dtype=np.float64)\n \n+    # Several floating point errors may occur during the summation due to rounding.\n+    # We need to estimate an upper bound to the error to consider the data constant.\nComment: Could you also mention and maybe link to where Scipy does a similar calculation?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/core/nanops.py",
    "pr_number": 62405,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2369370358,
    "comment_created_at": "2025-09-22T16:44:19Z"
  },
  {
    "code": "@@ -1851,6 +1851,9 @@ The following extractors use this feature: #### soundcloud * `formats`: Formats to request from the API. Requested values should be in the format of `{protocol}_{extension}` (omitting the bitrate), e.g. `hls_opus,http_aac`. The `*` character functions as a wildcard, e.g. `*_mp3`, and can passed by itself to request all formats. Known protocols include `http`, `hls` and `hls-aes`; known extensions include `aac`, `opus` and `mp3`. Original `download` formats are always extracted. Default is `http_aac,hls_aac,http_opus,hls_opus,http_mp3,hls_mp3` +#### orfon +* `prefer_segments_playlist`: Some videos are available both as a single file and as a segmented playlist. This option will return the segmented playlist when using `orfon:prefer_segments_playlist`",
    "comment": "should also note that --concat-playlist never is needed if you want the individual segments",
    "line_number": 1855,
    "enriched": "File: README.md\nCode: @@ -1851,6 +1851,9 @@ The following extractors use this feature:\n #### soundcloud\n * `formats`: Formats to request from the API. Requested values should be in the format of `{protocol}_{extension}` (omitting the bitrate), e.g. `hls_opus,http_aac`. The `*` character functions as a wildcard, e.g. `*_mp3`, and can passed by itself to request all formats. Known protocols include `http`, `hls` and `hls-aes`; known extensions include `aac`, `opus` and `mp3`. Original `download` formats are always extracted. Default is `http_aac,hls_aac,http_opus,hls_opus,http_mp3,hls_mp3`\n \n+#### orfon\n+* `prefer_segments_playlist`: Some videos are available both as a single file and as a segmented playlist. This option will return the segmented playlist when using `orfon:prefer_segments_playlist`\nComment: should also note that `--concat-playlist never` is needed if you want the individual segments",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "README.md",
    "pr_number": 10314,
    "repo": "yt-dlp",
    "owner": "yt-dlp",
    "comment_id": 1660239586,
    "comment_created_at": "2024-06-30T19:37:52Z"
  },
  {
    "code": "@@ -120,7 +120,9 @@ class UserManager:\n-    def create_user(self, name):\n+    def create_user(self, name, role):",
    "comment": "should we consider using a factory pattern here instead of adding more parameters to create_user? it would make user creation more extensible.",
    "line_number": 121,
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "auth/user_manager.py",
    "pr_number": 301,
    "repo": "auth-service",
    "owner": "example-org",
    "comment_id": 5001,
    "comment_created_at": "2024-07-10T14:22:03Z"
  },
  {
    "code": "@@ -88,7 +88,9 @@ def get_data():\n-    return requests.get(url)\n+    return http_client.get(url)",
    "comment": "instead of directly using http_client, should we introduce an adapter layer? this would make switching http libraries easier in the future.",
    "line_number": 89,
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "network/api.py",
    "pr_number": 302,
    "repo": "network-utils",
    "owner": "infra-team",
    "comment_id": 5002,
    "comment_created_at": "2024-07-15T09:15:00Z"
  },
  {
    "code": "@@ -45,7 +45,9 @@ class PaymentProcessor:\n-    def process(self, amount):\n+    def process(self, amount, currency):",
    "comment": "adding more parameters might make this class harder to maintain. should we consider a strategy pattern for handling different currencies?",
    "line_number": 46,
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "payments/processor.py",
    "pr_number": 303,
    "repo": "payment-service",
    "owner": "finance-team",
    "comment_id": 5003,
    "comment_created_at": "2024-07-20T11:45:00Z"
  },
  {
    "code": "@@ -67,7 +67,9 @@ def handle_event(event):\n-    if event.type == 'click':\n+    if event.type in ['click', 'hover']:",
    "comment": "this conditional logic is growing. should we refactor to use an observer pattern for event handling?",
    "line_number": 68,
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "ui/event_handler.py",
    "pr_number": 304,
    "repo": "frontend-app",
    "owner": "ui-team",
    "comment_id": 5004,
    "comment_created_at": "2024-07-25T10:00:00Z"
  },
  {
    "code": "@@ -98,7 +98,9 @@ def save_to_db(data):\n-    db.insert(data)\n+    db.save(data)",
    "comment": "should we abstract database operations behind a repository pattern? this would make testing easier and reduce coupling.",
    "line_number": 99,
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "db/operations.py",
    "pr_number": 305,
    "repo": "data-service",
    "owner": "backend-team",
    "comment_id": 5005,
    "comment_created_at": "2024-07-30T14:00:00Z"
  },
  {
    "code": "@@ -150,7 +150,9 @@ def send_email():\n-    smtp.send(msg)\n+    email_client.send(msg)",
    "comment": "should we introduce a facade for email sending? it would simplify the interface and hide smtp details.",
    "line_number": 151,
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "notifications/email.py",
    "pr_number": 306,
    "repo": "notification-service",
    "owner": "comm-team",
    "comment_id": 5006,
    "comment_created_at": "2024-08-02T12:45:00Z"
  },
  {
    "code": "@@ -33,7 +33,9 @@ def calculate_discount():\n-    return price * 0.9\n+    return price * discount_rate",
    "comment": "should we encapsulate discount logic in a separate class? this would make it easier to add new discount strategies later.",
    "line_number": 34,
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "pricing/discount.py",
    "pr_number": 307,
    "repo": "ecommerce-app",
    "owner": "sales-team",
    "comment_id": 5007,
    "comment_created_at": "2024-08-05T16:30:00Z"
  },
  {
    "code": "@@ -72,7 +72,9 @@ def authenticate_user():\n-    return check_password(user, pwd)\n+    return auth_service.verify(user, pwd)",
    "comment": "should we consider using dependency injection for auth_service instead of hardcoding it?",
    "line_number": 73,
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "auth/authenticator.py",
    "pr_number": 308,
    "repo": "auth-service",
    "owner": "security-team",
    "comment_id": 5008,
    "comment_created_at": "2024-08-10T09:30:00Z"
  },
  {
    "code": "@@ -110,7 +110,9 @@ def generate_report():\n-    return json.dumps(data)\n+    return report_builder.build(data)",
    "comment": "should we use a builder pattern for report generation? it would make adding new formats easier.",
    "line_number": 111,
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "reports/generator.py",
    "pr_number": 309,
    "repo": "analytics-tool",
    "owner": "data-team",
    "comment_id": 5009,
    "comment_created_at": "2024-08-15T17:00:00Z"
  },
  {
    "code": "@@ -58,7 +58,9 @@ def log_event():\n-    print(event)\n+    logger.log(event)",
    "comment": "should we centralize logging using a singleton logger instance? this would ensure consistency across modules.",
    "line_number": 59,
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "logging/logger.py",
    "pr_number": 310,
    "repo": "infra-tools",
    "owner": "ops-team",
    "comment_id": 5010,
    "comment_created_at": "2024-08-20T18:00:00Z"
  },
  {
    "code": "@@ -117,3 +118,70 @@ export const lightAlgorithm: MappingAlgorithm = seedToken => { lineWidthBold: supersetTheme.gridUnit / 2, }; }; + +export const lightConfig: ThemeConfig = {",
    "comment": "### simplify the lightconfig object structure. <sub>![category design patterns](https://img.shields.io/badge/design%20patterns-0d9488)</sub>\n\n<details>\n  <summary>tell me more</summary>\n&#8203;\n\nit appears that the lightconfig object is becoming quite complex and monolithic with maintaining colors, themes, and configuration components inside one large object. this could limit the scalability of your theme setup. i suggest you consider breaking down the lightconfig object into smaller, more manageable configuration units. this will increase the maintainability and simplicity of the configuration code. an appropriate design pattern like the module or factory pattern can be employed to achieve this modularity.\n\n\n</details>\n\n###### chat with korbit by mentioning @korbit-ai, and give a \ud83d\udc4d or \ud83d\udc4e to help korbit improve your reviews.\n\n<!--- korbi internal id:3466a5e7-ea68-44b6-b92d-260a6b1493a9 -->",
    "line_number": 122,
    "enriched": "File: superset-frontend/src/theme/light.ts\nCode: @@ -117,3 +118,70 @@ export const lightAlgorithm: MappingAlgorithm = seedToken => {\n     lineWidthBold: supersetTheme.gridUnit / 2,\n   };\n };\n+\n+export const lightConfig: ThemeConfig = {\nComment: ### Simplify the `lightConfig` object structure. <sub>![category Design Patterns](https://img.shields.io/badge/Design%20Patterns-0d9488)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n&#8203;\n\nIt appears that the `lightConfig` object is becoming quite complex and monolithic with maintaining colors, themes, and configuration components inside one large object. This could limit the scalability of your theme setup. I suggest you consider breaking down the `lightConfig` object into smaller, more manageable configuration units. This will increase the maintainability and simplicity of the configuration code. An appropriate design pattern like the Module or Factory pattern can be employed to achieve this modularity.\n\n\n</details>\n\n###### Chat with Korbit by mentioning @korbit-ai, and give a \ud83d\udc4d or \ud83d\udc4e to help Korbit improve your reviews.\n\n<!--- korbi internal id:3466a5e7-ea68-44b6-b92d-260a6b1493a9 -->\n",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "superset-frontend/src/theme/light.ts",
    "pr_number": 31719,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1903915903,
    "comment_created_at": "2025-01-06T09:32:21Z"
  },
  {
    "code": "@@ -323,6 +323,9 @@ def evaluate(self, runner, results): \"\"\" eval_res = self.dataloader.dataset.evaluate( results, logger=runner.logger, **self.eval_kwargs) + + assert eval_res, '`eval_res` should not be a null dict.'",
    "comment": "a null dict -> an empty dict",
    "line_number": 327,
    "enriched": "File: mmcv/runner/hooks/evaluation.py\nCode: @@ -323,6 +323,9 @@ def evaluate(self, runner, results):\n         \"\"\"\n         eval_res = self.dataloader.dataset.evaluate(\n             results, logger=runner.logger, **self.eval_kwargs)\n+\n+        assert eval_res, '`eval_res` should not be a null dict.'\nComment: a null dict -> an empty dict",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "mmcv/runner/hooks/evaluation.py",
    "pr_number": 1398,
    "repo": "mmcv",
    "owner": "open-mmlab",
    "comment_id": 728037199,
    "comment_created_at": "2021-10-13T13:01:32Z"
  },
  {
    "code": "@@ -163,8 +163,13 @@ if [[ \"$(uname)\" != Darwin ]]; then MEMORY_LIMIT_MAX_JOBS=12 NUM_CPUS=$(( $(nproc) - 2 )) - # Defaults here for **binary** linux builds so they can be changed in one place - export MAX_JOBS=${MAX_JOBS:-$(( ${NUM_CPUS} > ${MEMORY_LIMIT_MAX_JOBS} ? ${MEMORY_LIMIT_MAX_JOBS} : ${NUM_CPUS} ))} + if [[ \"$(uname)\" == Linux ]]; then + # Defaults here for **binary** linux builds so they can be changed in one place + export MAX_JOBS=${MAX_JOBS:-$(( ${NUM_CPUS} > ${MEMORY_LIMIT_MAX_JOBS} ? ${MEMORY_LIMIT_MAX_JOBS} : ${NUM_CPUS} ))} + else + # For Windows builds",
    "comment": "this statement is wrong, it also affects macos builds, isn't it?",
    "line_number": 170,
    "enriched": "File: .circleci/scripts/binary_populate_env.sh\nCode: @@ -163,8 +163,13 @@ if [[ \"$(uname)\" != Darwin ]]; then\n   MEMORY_LIMIT_MAX_JOBS=12\n   NUM_CPUS=$(( $(nproc) - 2 ))\n \n-  # Defaults here for **binary** linux builds so they can be changed in one place\n-  export MAX_JOBS=${MAX_JOBS:-$(( ${NUM_CPUS} > ${MEMORY_LIMIT_MAX_JOBS} ? ${MEMORY_LIMIT_MAX_JOBS} : ${NUM_CPUS} ))}\n+  if [[ \"$(uname)\" == Linux ]]; then\n+    # Defaults here for **binary** linux builds so they can be changed in one place\n+    export MAX_JOBS=${MAX_JOBS:-$(( ${NUM_CPUS} > ${MEMORY_LIMIT_MAX_JOBS} ? ${MEMORY_LIMIT_MAX_JOBS} : ${NUM_CPUS} ))}\n+  else\n+    # For Windows builds\nComment: This statement is wrong, it also affects MacOS builds, isn't it?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".circleci/scripts/binary_populate_env.sh",
    "pr_number": 165287,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2446575550,
    "comment_created_at": "2025-10-21T02:37:46Z"
  },
  {
    "code": "@@ -390,7 +390,13 @@ Each plugin is a class that must implement the following methods: .. method:: close(self) - Close the target file object. + Clean up the plugin. + + For example, you might want to close and file wrapper that you might have",
    "comment": "there is some typo here.",
    "line_number": 395,
    "enriched": "File: docs/topics/feed-exports.rst\nCode: @@ -390,7 +390,13 @@ Each plugin is a class that must implement the following methods:\n \n .. method:: close(self)\n \n-    Close the target file object.\n+    Clean up the plugin.\n+\n+    For example, you might want to close and file wrapper that you might have\nComment: There is some typo here.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/topics/feed-exports.rst",
    "pr_number": 6239,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 1497246112,
    "comment_created_at": "2024-02-21T10:17:03Z"
  },
  {
    "code": "@@ -753,25 +754,20 @@ def test_iloc_mask(self): (None, \".iloc\"): \"0b1100\", (\"index\", \"\"): \"0b11\", (\"index\", \".loc\"): \"0b11\", - (\"index\", \".iloc\"): ( - \"iLocation based boolean indexing cannot use an indexable as a mask\" - ), - (\"locs\", \"\"): \"Unalignable boolean Series provided as indexer \" - \"(index of the boolean Series and of the indexed \" - \"object do not match).\", - (\"locs\", \".loc\"): \"Unalignable boolean Series provided as indexer \" - \"(index of the boolean Series and of the \" - \"indexed object do not match).\", - (\"locs\", \".iloc\"): ( - \"iLocation based boolean indexing on an integer type is not available\" - ), + ( + \"index\", + \".iloc\", + ): \"iLocation based boolean indexing cannot use an indexable as a mask\", + (\"locs\", \"\"): \"Unalignable boolean Series provided as indexer\", + (\"locs\", \".loc\"): \"Unalignable boolean Series provided as indexer\", + (\"locs\", \".iloc\"): \"Unalignable boolean Series provided as indexer\", } # UserWarnings from reindex of a boolean mask for idx in [None, \"index\", \"locs\"]: mask = (df.nums > 2).values if idx: - mask_index = getattr(df, idx)[::-1] + mask_index = getattr(df, idx if idx == \"index\" else \"locs\")[::-1]",
    "comment": "since idx is not none here, i think this can be reverted.",
    "line_number": 770,
    "enriched": "File: pandas/tests/indexing/test_iloc.py\nCode: @@ -753,25 +754,20 @@ def test_iloc_mask(self):\n             (None, \".iloc\"): \"0b1100\",\n             (\"index\", \"\"): \"0b11\",\n             (\"index\", \".loc\"): \"0b11\",\n-            (\"index\", \".iloc\"): (\n-                \"iLocation based boolean indexing cannot use an indexable as a mask\"\n-            ),\n-            (\"locs\", \"\"): \"Unalignable boolean Series provided as indexer \"\n-            \"(index of the boolean Series and of the indexed \"\n-            \"object do not match).\",\n-            (\"locs\", \".loc\"): \"Unalignable boolean Series provided as indexer \"\n-            \"(index of the boolean Series and of the \"\n-            \"indexed object do not match).\",\n-            (\"locs\", \".iloc\"): (\n-                \"iLocation based boolean indexing on an integer type is not available\"\n-            ),\n+            (\n+                \"index\",\n+                \".iloc\",\n+            ): \"iLocation based boolean indexing cannot use an indexable as a mask\",\n+            (\"locs\", \"\"): \"Unalignable boolean Series provided as indexer\",\n+            (\"locs\", \".loc\"): \"Unalignable boolean Series provided as indexer\",\n+            (\"locs\", \".iloc\"): \"Unalignable boolean Series provided as indexer\",\n         }\n \n         # UserWarnings from reindex of a boolean mask\n         for idx in [None, \"index\", \"locs\"]:\n             mask = (df.nums > 2).values\n             if idx:\n-                mask_index = getattr(df, idx)[::-1]\n+                mask_index = getattr(df, idx if idx == \"index\" else \"locs\")[::-1]\nComment: Since `idx` is not `None` here, I think this can be reverted.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/tests/indexing/test_iloc.py",
    "pr_number": 61162,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2019968167,
    "comment_created_at": "2025-03-29T19:49:43Z"
  },
  {
    "code": "@@ -62,7 +62,7 @@ def _get_queryset(klass): return klass -def get_object_or_404(klass, *args, **kwargs): +def get_object_or_404(klass, exception_detail: str = None, *args, **kwargs):",
    "comment": "django does not use type annotations, so i don't think we'd want this here.",
    "line_number": 65,
    "enriched": "File: django/shortcuts.py\nCode: @@ -62,7 +62,7 @@ def _get_queryset(klass):\n     return klass\n \n \n-def get_object_or_404(klass, *args, **kwargs):\n+def get_object_or_404(klass, exception_detail: str = None, *args, **kwargs):\nComment: Django does not use type annotations, so I don't think we'd want this here.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "django/shortcuts.py",
    "pr_number": 18788,
    "repo": "django",
    "owner": "django",
    "comment_id": 1835443206,
    "comment_created_at": "2024-11-09T16:15:57Z"
  },
  {
    "code": "@@ -169,6 +169,7 @@ def __init__( secret_key=None, acl=None, endpoint_url=None, + region_name=None, *,",
    "comment": "i wonder if it should be before or after *",
    "line_number": 172,
    "enriched": "File: scrapy/extensions/feedexport.py\nCode: @@ -169,6 +169,7 @@ def __init__(\n         secret_key=None,\n         acl=None,\n         endpoint_url=None,\n+        region_name=None,\n         *,\nComment: I wonder if it should be before or after *\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "scrapy/extensions/feedexport.py",
    "pr_number": 5980,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 1269355524,
    "comment_created_at": "2023-07-20T11:54:33Z"
  },
  {
    "code": "@@ -2271,6 +2271,9 @@ def construct_from_string(cls, string: str) -> ArrowDtype: \"instance with specific parameters.\" ) from err raise TypeError(f\"'{base_type}' is not a valid pyarrow data type.\") from err + except NameError as err:",
    "comment": "i think this is a fairly fragile fix if a refactor happens. somewhere import_optional_dependency should be used instead",
    "line_number": 2274,
    "enriched": "File: pandas/core/dtypes/dtypes.py\nCode: @@ -2271,6 +2271,9 @@ def construct_from_string(cls, string: str) -> ArrowDtype:\n                     \"instance with specific parameters.\"\n                 ) from err\n             raise TypeError(f\"'{base_type}' is not a valid pyarrow data type.\") from err\n+        except NameError as err:\nComment: I think this is a fairly fragile fix if a refactor happens. Somewhere import_optional_dependency should be used instead",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "pandas/core/dtypes/dtypes.py",
    "pr_number": 58046,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1546650473,
    "comment_created_at": "2024-04-01T17:57:14Z"
  },
  {
    "code": "@@ -16,12 +16,15 @@ import json from enum import Enum -from typing import Any, Dict, List, Optional, Union +from typing import Any, Dict, Final, List, Optional, Union",
    "comment": "## unused import\n\nimport of 'any' is not used.\n\n[show more details](https://github.com/streamlit/streamlit/security/code-scanning/7359)",
    "line_number": 19,
    "enriched": "File: lib/streamlit/elements/lib/column_config_utils.py\nCode: @@ -16,12 +16,15 @@\n \n import json\n from enum import Enum\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Dict, Final, List, Optional, Union\nComment: ## Unused import\n\nImport of 'Any' is not used.\n\n[Show more details](https://github.com/streamlit/streamlit/security/code-scanning/7359)",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "lib/streamlit/elements/lib/column_config_utils.py",
    "pr_number": 6598,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1196789694,
    "comment_created_at": "2023-05-17T16:31:41Z"
  },
  {
    "code": "@@ -117,6 +117,11 @@ def get_reactor_settings() -> dict[str, Any]: settings, so tests that run the crawler in the current process may need to pass a correct ``\"TWISTED_REACTOR\"`` setting value when creating it. \"\"\" + if not is_reactor_installed(): + raise RuntimeError(",
    "comment": "writing a test for this would need running a standalone script, but i checked this with scrapinghub-entrypoint-scrapy before https://github.com/scrapinghub/scrapinghub-entrypoint-scrapy/pull/94 and it produces the correct exception.",
    "line_number": 121,
    "enriched": "File: scrapy/utils/test.py\nCode: @@ -117,6 +117,11 @@ def get_reactor_settings() -> dict[str, Any]:\n     settings, so tests that run the crawler in the current process may need to\n     pass a correct ``\"TWISTED_REACTOR\"`` setting value when creating it.\n     \"\"\"\n+    if not is_reactor_installed():\n+        raise RuntimeError(\nComment: Writing a test for this would need running a standalone script, but I checked this with `scrapinghub-entrypoint-scrapy` before https://github.com/scrapinghub/scrapinghub-entrypoint-scrapy/pull/94 and it produces the correct exception.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "scrapy/utils/test.py",
    "pr_number": 6866,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2131938592,
    "comment_created_at": "2025-06-06T10:21:59Z"
  },
  {
    "code": "@@ -323,6 +336,7 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False): model_size = m.group().strip(\"-\") # only keeps `7b` if architecture + model_size not in GGUF_SUPPORTED_ARCHITECTURES: + print(GGUF_SUPPORTED_ARCHITECTURES)",
    "comment": "remove",
    "line_number": 339,
    "enriched": "File: src/transformers/modeling_gguf_pytorch_utils.py\nCode: @@ -323,6 +336,7 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n         model_size = m.group().strip(\"-\")  # only keeps `7b`\n \n     if architecture + model_size not in GGUF_SUPPORTED_ARCHITECTURES:\n+        print(GGUF_SUPPORTED_ARCHITECTURES)\nComment: remove ",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/modeling_gguf_pytorch_utils.py",
    "pr_number": 34002,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 1899596498,
    "comment_created_at": "2024-12-30T14:52:41Z"
  },
  {
    "code": "@@ -496,6 +504,8 @@ class D3Format(TypedDict, total=False): \"PLAYWRIGHT_REPORTS_AND_THUMBNAILS\": False, # Set to True to enable experimental chart plugins \"CHART_PLUGINS_EXPERIMENTAL\": False, + # Set to True to limit the number of users who can certify datasets + \"LIMIT_CERTIFICATION\": False,",
    "comment": "couldn't this be controlled via superset's/fab's existing permission/view model? this feels more consistent/coherent than creating more custom logic in the security manager.",
    "line_number": 508,
    "enriched": "File: superset/config.py\nCode: @@ -496,6 +504,8 @@ class D3Format(TypedDict, total=False):\n     \"PLAYWRIGHT_REPORTS_AND_THUMBNAILS\": False,\n     # Set to True to enable experimental chart plugins\n     \"CHART_PLUGINS_EXPERIMENTAL\": False,\n+    # Set to True to limit the number of users who can certify datasets\n+    \"LIMIT_CERTIFICATION\": False,\nComment: Couldn't this be controlled via Superset's/FAB's existing permission/view model? This feels more consistent/coherent than creating more custom logic in the security manager. ",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "superset/config.py",
    "pr_number": 28781,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1621187049,
    "comment_created_at": "2024-05-30T17:31:09Z"
  },
  {
    "code": "@@ -3705,7 +3705,7 @@ def transpose( nv.validate_transpose(args, {}) # construct the args - dtypes = list(self.dtypes) + first_dtype = next(iter(self.dtypes), None)",
    "comment": "can't this just be self.dtypes[0] if self.dtypes else none? going through the iteration protocol seems overkill",
    "line_number": 3708,
    "enriched": "File: pandas/core/frame.py\nCode: @@ -3705,7 +3705,7 @@ def transpose(\n         nv.validate_transpose(args, {})\n         # construct the args\n \n-        dtypes = list(self.dtypes)\n+        first_dtype = next(iter(self.dtypes), None)\nComment: Can't this just be `self.dtypes[0] if self.dtypes else None`? Going through the iteration protocol seems overkill",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "pandas/core/frame.py",
    "pr_number": 58267,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1567717185,
    "comment_created_at": "2024-04-16T17:24:16Z"
  },
  {
    "code": "@@ -0,0 +1,291 @@ +import numpy as np",
    "comment": "in tests.frame and tests.series with have test_cumulative.py.  can we use that pattern?  and if we're really trying to follow the pattern, that file goes outside the methods/ directory",
    "line_number": 1,
    "enriched": "File: pandas/tests/groupby/methods/test_cum.py\nCode: @@ -0,0 +1,291 @@\n+import numpy as np\nComment: in tests.frame and tests.series with have test_cumulative.py.  can we use that pattern?  and if we're really trying to follow the pattern, that file goes outside the methods/ directory",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "pandas/tests/groupby/methods/test_cum.py",
    "pr_number": 55312,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1349543613,
    "comment_created_at": "2023-10-07T16:39:52Z"
  },
  {
    "code": "@@ -1073,6 +1073,8 @@ In order to use this parser: * Set :setting:`ROBOTSTXT_PARSER` setting to ``scrapy.robotstxt.ReppyRobotParser`` +* Only works with python 3.8 and earlier",
    "comment": "i suggest we put this at the beginning of the section, as a warning:\r\n\r\nrst\r\n.. warning:: does not support python 3.9+",
    "line_number": 1076,
    "enriched": "File: docs/topics/downloader-middleware.rst\nCode: @@ -1073,6 +1073,8 @@ In order to use this parser:\n * Set :setting:`ROBOTSTXT_PARSER` setting to\n   ``scrapy.robotstxt.ReppyRobotParser``\n \n+* Only works with python 3.8 and earlier\nComment: I suggest we put this at the beginning of the section, as a warning:\r\n\r\n```rst\r\n.. warning:: Does not support Python 3.9+\r\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "docs/topics/downloader-middleware.rst",
    "pr_number": 5231,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 689458024,
    "comment_created_at": "2021-08-16T11:34:30Z"
  },
  {
    "code": "@@ -0,0 +1,25 @@ +import pytest +import threading +import requests + +from tests.testserver.server import Server, consume_socket_content",
    "comment": "i'm confused how this import is working right now. maybe it's too late for me to be reviewing code?",
    "line_number": 5,
    "enriched": "File: tests/testserver.py\nCode: @@ -0,0 +1,25 @@\n+import pytest\n+import threading\n+import requests\n+\n+from tests.testserver.server import Server, consume_socket_content\nComment: I'm confused how this import is working right now. Maybe it's too late for me to be reviewing code?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/testserver.py",
    "pr_number": 4452,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 160065925,
    "comment_created_at": "2018-01-08T00:50:30Z"
  },
  {
    "code": "@@ -0,0 +1,262 @@ +{ + \"cells\": [ + { + \"cell_type\": \"markdown\", + \"id\": \"245a954a\", + \"metadata\": {}, + \"source\": [ + \"# Naver Search\\n\", + \"\\n\", + \"## Overview\\n\", + \"\\n\", + \"### Integration details\\n\", + \"\\n\", + \"| Class | Package | Serializable | JS support | Package latest |\\n\", + \"| :--- | :--- | :---: | :---: | :---: |\\n\", + \"| NaverSearchResults | [langchain-naver-community](https://pypi.org/project/langchain-naver-community/) | \u274c | \u274c | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-naver-community?style=flat-square&label=%20) |\\n\", + \"\\n\", + \"### Tool features\\n\", + \"\\n\", + \"**Search** : The Naver Search Tool provides a simple interface to search Naver and get results: \\n\",",
    "comment": "this sentence ends abruptly.",
    "line_number": 20,
    "enriched": "File: docs/docs/integrations/tools/naver_search.ipynb\nCode: @@ -0,0 +1,262 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"245a954a\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# Naver Search\\n\",\n+    \"\\n\",\n+    \"## Overview\\n\",\n+    \"\\n\",\n+    \"### Integration details\\n\",\n+    \"\\n\",\n+    \"| Class | Package | Serializable | JS support |  Package latest |\\n\",\n+    \"| :--- | :--- | :---: | :---: | :---: |\\n\",\n+    \"| NaverSearchResults | [langchain-naver-community](https://pypi.org/project/langchain-naver-community/) | \u274c | \u274c |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-naver-community?style=flat-square&label=%20) |\\n\",\n+    \"\\n\",\n+    \"### Tool features\\n\",\n+    \"\\n\",\n+    \"**Search** : The Naver Search Tool provides a simple interface to search Naver and get results: \\n\",\nComment: This sentence ends abruptly.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/docs/integrations/tools/naver_search.ipynb",
    "pr_number": 30392,
    "repo": "langchain",
    "owner": "langchain-ai",
    "comment_id": 2009147164,
    "comment_created_at": "2025-03-23T15:34:53Z"
  },
  {
  "code": "@@ -2569,13 +2569,13 @@ class ndarray(_ArrayOrScalarCommon, Generic[_ShapeT_co, _DTypeT_co]): copy: builtins.bool | _CopyMode = ..., ) -> ndarray[_ShapeT_co, dtype]: ... - # + # Keep in sync with `MaskedArray.view` @overload # () def view(self, /) -> Self: ... @overload # (dtype: T) def view(self, /, dtype: _DTypeT | _HasDType[_DTypeT]) -> ndarray[_ShapeT_co, _DTypeT]: ... @overload # (dtype: dtype[T]) - def view(self, /, dtype: _DTypeLike[_ScalarT]) -> NDArray[_ScalarT]: ... + def view(self, /, dtype: _DTypeLike[_ScalarT]) -> ndarray[_ShapeT_co, dtype[_ScalarT]]: ...",
  "comment": "nice :)",
  "line_number": 2578,
  "enriched": "File: numpy/__init__.pyi\nCode: @@ -2569,13 +2569,13 @@ class ndarray(_ArrayOrScalarCommon, Generic[_ShapeT_co, _DTypeT_co]):\n         copy: builtins.bool | _CopyMode = ...,\n     ) -> ndarray[_ShapeT_co, dtype]: ...\n \n-    #\n+    # Keep in sync with `MaskedArray.view`\n     @overload  # ()\n     def view(self, /) -> Self: ...\n     @overload  # (dtype: T)\n     def view(self, /, dtype: _DTypeT | _HasDType[_DTypeT]) -> ndarray[_ShapeT_co, _DTypeT]: ...\n     @overload  # (dtype: dtype[T])\n-    def view(self, /, dtype: _DTypeLike[_ScalarT]) -> NDArray[_ScalarT]: ...\n+    def view(self, /, dtype: _DTypeLike[_ScalarT]) -> ndarray[_ShapeT_co, dtype[_ScalarT]]: ...\nComment: nice :)",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "numpy/__init__.pyi",
  "pr_number": 29383,
  "repo": "numpy",
  "owner": "numpy",
  "comment_id": 2208841892,
  "comment_created_at": "2025-07-15T22:38:06Z"
},
{
  "code": "@@ -266,3 +267,66 @@ def inc2_cfloat_struct(cnp.ndarray[cnp.cfloat_t] arr): # This works in both modes arr[1].real = arr[1].real + 1 arr[1].imag = arr[1].imag + 1 + + +def npystring_pack(arr): + cdef char *string = \"Hello world\" + cdef size_t size = 11 + + allocator = cnp.NpyString_acquire_allocator( + <cnp.PyArray_StringDTypeObject *>cnp.PyArray_DESCR(arr) + ) + + # copy string->packed_string, the pointer to the underlying array buffer + if cnp.NpyString_pack( + allocator, <cnp.npy_packed_static_string *>cnp.PyArray_DATA(arr), string, size, + ) == -1: + return -1 + + cnp.NpyString_release_allocator(allocator) + return 0 + + +def npystring_load(arr): + allocator = cnp.NpyString_acquire_allocator( + <cnp.PyArray_StringDTypeObject *>cnp.PyArray_DESCR(arr) + ) + + cdef cnp.npy_static_string sdata + sdata.size = 0 + sdata.buf = NULL + + cdef cnp.npy_packed_static_string *packed_string = <cnp.npy_packed_static_string *>cnp.PyArray_DATA(arr) + cdef int is_null = cnp.NpyString_load(allocator, packed_string, &sdata) + cnp.NpyString_release_allocator(allocator) + if is_null == -1: + raise ValueError(\"String unpacking failed.\") + elif is_null == 1: + # String in the array buffer is the null string + return \"\" + else: + # Cython syntax for copying a c string to python bytestring: + # slice the char * by the length of the string + return sdata.buf[:sdata.size].decode('utf-8')",
  "comment": "neat, thanks for the comment",
  "line_number": 310,
  "enriched": "File: numpy/_core/tests/examples/cython/checks.pyx\nCode: @@ -266,3 +267,66 @@ def inc2_cfloat_struct(cnp.ndarray[cnp.cfloat_t] arr):\n     # This works in both modes\n     arr[1].real = arr[1].real + 1\n     arr[1].imag = arr[1].imag + 1\n+\n+\n+def npystring_pack(arr):\n+    cdef char *string = \"Hello world\"\n+    cdef size_t size = 11\n+\n+    allocator = cnp.NpyString_acquire_allocator(\n+        <cnp.PyArray_StringDTypeObject *>cnp.PyArray_DESCR(arr)\n+    )\n+\n+    # copy string->packed_string, the pointer to the underlying array buffer\n+    if cnp.NpyString_pack(\n+        allocator, <cnp.npy_packed_static_string *>cnp.PyArray_DATA(arr), string, size,\n+    ) == -1:\n+        return -1\n+\n+    cnp.NpyString_release_allocator(allocator)\n+    return 0\n+\n+\n+def npystring_load(arr):\n+    allocator = cnp.NpyString_acquire_allocator(\n+        <cnp.PyArray_StringDTypeObject *>cnp.PyArray_DESCR(arr)\n+    )\n+\n+    cdef cnp.npy_static_string sdata\n+    sdata.size = 0\n+    sdata.buf = NULL\n+\n+    cdef cnp.npy_packed_static_string *packed_string = <cnp.npy_packed_static_string *>cnp.PyArray_DATA(arr)\n+    cdef int is_null = cnp.NpyString_load(allocator, packed_string, &sdata)\n+    cnp.NpyString_release_allocator(allocator)\n+    if is_null == -1:\n+        raise ValueError(\"String unpacking failed.\")\n+    elif is_null == 1:\n+        # String in the array buffer is the null string\n+        return \"\"\n+    else:\n+        # Cython syntax for copying a c string to python bytestring:\n+        # slice the char * by the length of the string\n+        return sdata.buf[:sdata.size].decode('utf-8')\nComment: neat, thanks for the comment",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "numpy/_core/tests/examples/cython/checks.pyx",
  "pr_number": 27923,
  "repo": "numpy",
  "owner": "numpy",
  "comment_id": 1876302650,
  "comment_created_at": "2024-12-09T16:33:43Z"
},
{
  "code": "@@ -0,0 +1,155 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, + * software distributed under the License is distributed on an + * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY + * KIND, either express or implied. See the License for the + * specific language governing permissions and limitations + * under the License. + */ +import { Metric, t } from '@superset-ui/core'; +import { + ColumnItem, + DatasourceFolder, + DatasourcePanelColumn, + Folder, + MetricItem, +} from './types'; + +const transformToFolderStructure = (",
  "comment": "nice! ",
  "line_number": 28,
  "enriched": "File: superset-frontend/src/explore/components/DatasourcePanel/transformDatasourceFolders.ts\nCode: @@ -0,0 +1,155 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+import { Metric, t } from '@superset-ui/core';\n+import {\n+  ColumnItem,\n+  DatasourceFolder,\n+  DatasourcePanelColumn,\n+  Folder,\n+  MetricItem,\n+} from './types';\n+\n+const transformToFolderStructure = (\nComment: Nice!  ",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "superset-frontend/src/explore/components/DatasourcePanel/transformDatasourceFolders.ts",
  "pr_number": 32548,
  "repo": "superset",
  "owner": "apache",
  "comment_id": 1985858714,
  "comment_created_at": "2025-03-07T23:59:27Z"
},
{
  "code": "@@ -593,33 +634,7 @@ const DashboardBuilder: FC<DashboardBuilderProps> = () => { maxWidth={OPEN_FILTER_BAR_MAX_WIDTH} initialWidth={OPEN_FILTER_BAR_WIDTH} > - {adjustedWidth => { - const filterBarWidth = dashboardFiltersOpen - ? adjustedWidth - : CLOSED_FILTER_BAR_WIDTH; - return ( - <FiltersPanel - width={filterBarWidth} - hidden={isReport} - data-test=\"dashboard-filters-panel\" - > - <StickyPanel ref={containerRef} width={filterBarWidth}> - <ErrorBoundary> - <FilterBar - orientation={FilterBarOrientation.Vertical} - verticalConfig={{ - filtersOpen: dashboardFiltersOpen, - toggleFiltersBar: toggleDashboardFiltersOpen, - width: filterBarWidth, - height: filterBarHeight, - offset: filterBarOffset, - }} - /> - </ErrorBoundary> - </StickyPanel> - </FiltersPanel> - ); - }} + {renderChild}",
  "comment": "nice!",
  "line_number": 637,
  "enriched": "File: superset-frontend/src/dashboard/components/DashboardBuilder/DashboardBuilder.tsx\nCode: @@ -593,33 +634,7 @@ const DashboardBuilder: FC<DashboardBuilderProps> = () => {\n               maxWidth={OPEN_FILTER_BAR_MAX_WIDTH}\n               initialWidth={OPEN_FILTER_BAR_WIDTH}\n             >\n-              {adjustedWidth => {\n-                const filterBarWidth = dashboardFiltersOpen\n-                  ? adjustedWidth\n-                  : CLOSED_FILTER_BAR_WIDTH;\n-                return (\n-                  <FiltersPanel\n-                    width={filterBarWidth}\n-                    hidden={isReport}\n-                    data-test=\"dashboard-filters-panel\"\n-                  >\n-                    <StickyPanel ref={containerRef} width={filterBarWidth}>\n-                      <ErrorBoundary>\n-                        <FilterBar\n-                          orientation={FilterBarOrientation.Vertical}\n-                          verticalConfig={{\n-                            filtersOpen: dashboardFiltersOpen,\n-                            toggleFiltersBar: toggleDashboardFiltersOpen,\n-                            width: filterBarWidth,\n-                            height: filterBarHeight,\n-                            offset: filterBarOffset,\n-                          }}\n-                        />\n-                      </ErrorBoundary>\n-                    </StickyPanel>\n-                  </FiltersPanel>\n-                );\n-              }}\n+              {renderChild}\nComment: Nice!",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "superset-frontend/src/dashboard/components/DashboardBuilder/DashboardBuilder.tsx",
  "pr_number": 30958,
  "repo": "superset",
  "owner": "apache",
  "comment_id": 1846881121,
  "comment_created_at": "2024-11-18T16:15:01Z"
},
{
  "code": "@@ -461,7 +461,7 @@ def get_reverse_path_info(self, filtered_relation=None): to_opts=opts, target_fields=(opts.pk,), join_field=self, - m2m=not self.unique, + m2m=False,",
  "comment": "good catch :dart: looks like it was copied from foreignobject.get_reverse_path_info().",
  "line_number": 464,
  "enriched": "File: django/contrib/contenttypes/fields.py\nCode: @@ -461,7 +461,7 @@ def get_reverse_path_info(self, filtered_relation=None):\n                 to_opts=opts,\n                 target_fields=(opts.pk,),\n                 join_field=self,\n-                m2m=not self.unique,\n+                m2m=False,\nComment: Good catch :dart: Looks like it was copied from `ForeignObject.get_reverse_path_info().`",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "django/contrib/contenttypes/fields.py",
  "pr_number": 16305,
  "repo": "django",
  "owner": "django",
  "comment_id": 1026157804,
  "comment_created_at": "2022-11-18T08:41:27Z"
},
{
  "code": "@@ -93,22 +92,22 @@ def run_backend( Args: host: The app host - app_name: The app name. port: The app port loglevel: The log level. \"\"\" + config = get_config() + app_module = f\"{config.app_name}.{config.app_name}:{constants.APP_VAR}\" uvicorn.run( - app=f\"{app_name}:{constants.APP_VAR}.{constants.API_VAR}\", + app=f\"{app_module}.{constants.API_VAR}\",",
  "comment": "nice!",
  "line_number": 101,
  "enriched": "File: reflex/utils/exec.py\nCode: @@ -93,22 +92,22 @@ def run_backend(\n \n     Args:\n         host: The app host\n-        app_name: The app name.\n         port: The app port\n         loglevel: The log level.\n     \"\"\"\n+    config = get_config()\n+    app_module = f\"{config.app_name}.{config.app_name}:{constants.APP_VAR}\"\n     uvicorn.run(\n-        app=f\"{app_name}:{constants.APP_VAR}.{constants.API_VAR}\",\n+        app=f\"{app_module}.{constants.API_VAR}\",\nComment: nice!",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "reflex/utils/exec.py",
  "pr_number": 1665,
  "repo": "reflex",
  "owner": "reflex-dev",
  "comment_id": 1306066229,
  "comment_created_at": "2023-08-25T19:17:46Z"
},
{
  "code": "@@ -228,7 +228,7 @@ def add_page( component = component if isinstance(component, Component) else component(*args) # Add the title to the component. - compiler_utils.add_meta(component, title, description, image)",
  "comment": "good catch here",
  "line_number": 231,
  "enriched": "File: pynecone/app.py\nCode: @@ -228,7 +228,7 @@ def add_page(\n         component = component if isinstance(component, Component) else component(*args)\n \n         # Add the title to the component.\n-        compiler_utils.add_meta(component, title, description, image)\nComment: Good catch here",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "pynecone/app.py",
  "pr_number": 108,
  "repo": "reflex",
  "owner": "reflex-dev",
  "comment_id": 1049418850,
  "comment_created_at": "2022-12-15T09:39:22Z"
},
{
  "code": "@@ -131,18 +131,18 @@ The code below is equivalent to the code above:: .. _field-checking: -Field, constraint, model, manager, template engine, and database checks ------------------------------------------------------------------------ +Field, constraint, model, manager, template engine, task, and database checks",
  "comment": "good catch! though i'm not sure we should keep this given we are removing all inbuilt checks for tasks :thinking:",
  "line_number": 134,
  "enriched": "File: docs/topics/checks.txt\nCode: @@ -131,18 +131,18 @@ The code below is equivalent to the code above::\n \n .. _field-checking:\n \n-Field, constraint, model, manager, template engine, and database checks\n------------------------------------------------------------------------\n+Field, constraint, model, manager, template engine, task, and database checks\nComment: Good catch! Though I'm not sure we should keep this given we are removing all inbuilt checks for tasks :thinking: ",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "docs/topics/checks.txt",
  "pr_number": 19876,
  "repo": "django",
  "owner": "django",
  "comment_id": 2355590967,
  "comment_created_at": "2025-09-17T13:47:35Z"
},
{
  "code": "@@ -19,3 +19,37 @@ def handle(self, **options): options[\"secret_key\"] = SECRET_KEY_INSECURE_PREFIX + get_random_secret_key() super().handle(\"project\", project_name, target, **options) + + if target is None: + # Use current working directory if the target is not given + target = os.getcwd() + else: + # Get an absolute path of a directory + target = os.path.abspath(target) + + target = os.path.join(target, project_name)",
  "comment": "nice",
  "line_number": 30,
  "enriched": "File: django/core/management/commands/startproject.py\nCode: @@ -19,3 +19,37 @@ def handle(self, **options):\n         options[\"secret_key\"] = SECRET_KEY_INSECURE_PREFIX + get_random_secret_key()\n \n         super().handle(\"project\", project_name, target, **options)\n+\n+        if target is None:\n+            # Use current working directory if the target is not given\n+            target = os.getcwd()\n+        else:\n+            # Get an absolute path of a directory\n+            target = os.path.abspath(target)\n+        \n+        target = os.path.join(target, project_name)\nComment: nice",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "django/core/management/commands/startproject.py",
  "pr_number": 17521,
  "repo": "django",
  "owner": "django",
  "comment_id": 1404362273,
  "comment_created_at": "2023-11-24T13:35:42Z"
},
{
  "code": "@@ -21,6 +21,7 @@ def get_overridable_numpy_ufuncs(): \"\"\" ufuncs = {obj for obj in _umath.__dict__.values() if isinstance(obj, _ufunc)} + return ufuncs",
  "comment": "hahaha, nice :).",
  "line_number": 24,
  "enriched": "File: numpy/testing/overrides.py\nCode: @@ -21,6 +21,7 @@ def get_overridable_numpy_ufuncs():\n     \"\"\"\n     ufuncs = {obj for obj in _umath.__dict__.values()\n               if isinstance(obj, _ufunc)}\n+    return ufuncs\nComment: Hahaha, nice :).",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "numpy/testing/overrides.py",
  "pr_number": 22879,
  "repo": "numpy",
  "owner": "numpy",
  "comment_id": 1056538925,
  "comment_created_at": "2022-12-23T18:05:05Z"
},
{
  "code": "@@ -1,7 +1,13 @@ import io import os.path +import sys from typing import List, Tuple, Type +if sys.version_info.major >= 3 and sys.version_info.minor > 7:",
  "comment": "nice workaround",
  "line_number": 6,
  "enriched": "File: tests/test_app.py\nCode: @@ -1,7 +1,13 @@\n import io\n import os.path\n+import sys\n from typing import List, Tuple, Type\n \n+if sys.version_info.major >= 3 and sys.version_info.minor > 7:\nComment: Nice workaround",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "tests/test_app.py",
  "pr_number": 1163,
  "repo": "reflex",
  "owner": "reflex-dev",
  "comment_id": 1222024244,
  "comment_created_at": "2023-06-07T18:46:09Z"
},
{
  "code": "@@ -158,20 +136,6 @@ async function showRandomAnnouncement(groupId, timeInterval) { } async function main() { - if (div) { - data = await getData() - div.innerHTML = '<ul></ul>' - const ul = document.querySelector('.github-topic-projects ul') - data.forEach(v => { - if (v.full_name === 'fastapi/fastapi') { - return - } - const li = document.createElement('li') - li.innerHTML = `<a href=\"${v.html_url}\" target=\"_blank\"> ${v.stargazers_count} - ${v.full_name}</a> by <a href=\"${v.owner.html_url}\" target=\"_blank\">@${v.owner.login}</a>` - ul.append(li) - }) - }",
  "comment": "nice! i wanted to get rid of this. :fire:",
  "line_number": 173,
  "enriched": "File: docs/en/docs/js/custom.js\nCode: @@ -158,20 +136,6 @@ async function showRandomAnnouncement(groupId, timeInterval) {\n }\n \n async function main() {\n-    if (div) {\n-        data = await getData()\n-        div.innerHTML = '<ul></ul>'\n-        const ul = document.querySelector('.github-topic-projects ul')\n-        data.forEach(v => {\n-            if (v.full_name === 'fastapi/fastapi') {\n-                return\n-            }\n-            const li = document.createElement('li')\n-            li.innerHTML = `<a href=\"${v.html_url}\" target=\"_blank\"> ${v.stargazers_count} - ${v.full_name}</a> by <a href=\"${v.owner.html_url}\" target=\"_blank\">@${v.owner.login}</a>`\n-            ul.append(li)\n-        })\n-    }\nComment: Nice! I wanted to get rid of this. :fire: ",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "docs/en/docs/js/custom.js",
  "pr_number": 13135,
  "repo": "fastapi",
  "owner": "tiangolo",
  "comment_id": 1901237015,
  "comment_created_at": "2025-01-02T20:29:55Z"
},
{
  "code": "@@ -40,9 +44,10 @@ def test_displays_correct_number_of_elements(app: Page): @pytest.mark.only_browser(\"chromium\") def test_captures_photo(app: Page): \"\"\"Test camera_input captures photo when 'Take photo' button clicked.\"\"\" - # Wait for some timeout, until fake video stream available for camera_input - app.wait_for_timeout(3000) - take_photo_button = app.get_by_test_id(\"stCameraInputButton\").first + camera = get_camera_input(app, \"Label1\") + take_photo_button = camera.get_by_test_id(\"stCameraInputButton\").first + # Wait until the fake video stream is ready (button becomes enabled) + expect(take_photo_button).to_be_enabled()",
  "comment": "nice, this looks like a great improvement over the arbitrary wait_for_timeout!",
  "line_number": 50,
  "enriched": "File: e2e_playwright/st_camera_input_test.py\nCode: @@ -40,9 +44,10 @@ def test_displays_correct_number_of_elements(app: Page):\n @pytest.mark.only_browser(\"chromium\")\n def test_captures_photo(app: Page):\n     \"\"\"Test camera_input captures photo when 'Take photo' button clicked.\"\"\"\n-    # Wait for some timeout, until fake video stream available for camera_input\n-    app.wait_for_timeout(3000)\n-    take_photo_button = app.get_by_test_id(\"stCameraInputButton\").first\n+    camera = get_camera_input(app, \"Label1\")\n+    take_photo_button = camera.get_by_test_id(\"stCameraInputButton\").first\n+    # Wait until the fake video stream is ready (button becomes enabled)\n+    expect(take_photo_button).to_be_enabled()\nComment: Nice, this looks like a great improvement over the arbitrary `wait_for_timeout`!",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "e2e_playwright/st_camera_input_test.py",
  "pr_number": 12712,
  "repo": "streamlit",
  "owner": "streamlit",
  "comment_id": 2414440078,
  "comment_created_at": "2025-10-08T16:35:53Z"
},
{
  "code": "@@ -67,6 +66,6 @@ def validate(self) -> None: def get(self) -> Optional[Any]: filter_ = get_filter(self.resource, self.key) entry = db.session.query(KeyValueEntry).filter_by(**filter_).first() - if entry and (entry.expires_on is None or entry.expires_on > datetime.now()): + if entry and not entry.is_expired():",
  "comment": "nice!",
  "line_number": 69,
  "enriched": "File: superset/commands/key_value/get.py\nCode: @@ -67,6 +66,6 @@ def validate(self) -> None:\n     def get(self) -> Optional[Any]:\n         filter_ = get_filter(self.resource, self.key)\n         entry = db.session.query(KeyValueEntry).filter_by(**filter_).first()\n-        if entry and (entry.expires_on is None or entry.expires_on > datetime.now()):\n+        if entry and not entry.is_expired():\nComment: Nice!",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "superset/commands/key_value/get.py",
  "pr_number": 29302,
  "repo": "superset",
  "owner": "apache",
  "comment_id": 1647499844,
  "comment_created_at": "2024-06-20T12:31:27Z"
},
{
  "code": "@@ -48,6 +48,6 @@ def upgrade(): def downgrade(): + downgrade_catalog_perms(engines={\"postgresql\"})",
  "comment": "oh, good catch!",
  "line_number": 51,
  "enriched": "File: superset/migrations/versions/2024-05-01_10-52_58d051681a3b_add_catalog_perm_to_tables.py\nCode: @@ -48,6 +48,6 @@ def upgrade():\n \n \n def downgrade():\n+    downgrade_catalog_perms(engines={\"postgresql\"})\nComment: Oh, good catch!",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "superset/migrations/versions/2024-05-01_10-52_58d051681a3b_add_catalog_perm_to_tables.py",
  "pr_number": 29906,
  "repo": "superset",
  "owner": "apache",
  "comment_id": 1711883866,
  "comment_created_at": "2024-08-09T17:49:03Z"
},
{
  "code": "@@ -1295,7 +1295,7 @@ API | Description | Auth | HTTPS | CORS | | [Wikidata](https://www.wikidata.org/w/api.php?action=help) | Collaboratively edited knowledge base operated by the Wikimedia Foundation | `OAuth` | Yes | Unknown | | [Wikipedia](https://www.mediawiki.org/wiki/API:Main_page) | Mediawiki Encyclopedia | No | Yes | Unknown |",
  "comment": "wooooo",
  "line_number": 1296,
  "enriched": "File: README.md\nCode: @@ -1295,7 +1295,7 @@ API | Description | Auth | HTTPS | CORS |\n | [Wikidata](https://www.wikidata.org/w/api.php?action=help) | Collaboratively edited knowledge base operated by the Wikimedia Foundation | `OAuth` | Yes | Unknown |\n | [Wikipedia](https://www.mediawiki.org/wiki/API:Main_page) | Mediawiki Encyclopedia | No | Yes | Unknown |\nComment: Wooooo\n",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "README.md",
  "pr_number": 4479,
  "repo": "public-apis",
  "owner": "public-apis",
  "comment_id": 2255062511,
  "comment_created_at": "2025-08-05T18:38:37Z"
},
{
  "code": "@@ -730,8 +730,14 @@ async def _process_event( # Handle regular generators. elif inspect.isgenerator(events): - for event in events: - yield event, False + try: + while True: + yield next(events), False + except StopIteration as si:",
  "comment": "nice catch",
  "line_number": 736,
  "enriched": "File: reflex/state.py\nCode: @@ -730,8 +730,14 @@ async def _process_event(\n \n             # Handle regular generators.\n             elif inspect.isgenerator(events):\n-                for event in events:\n-                    yield event, False\n+                try:\n+                    while True:\n+                        yield next(events), False\n+                except StopIteration as si:\nComment: nice catch",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "reflex/state.py",
  "pr_number": 1390,
  "repo": "reflex",
  "owner": "reflex-dev",
  "comment_id": 1270982268,
  "comment_created_at": "2023-07-21T18:46:34Z"
},
{
  "code": "@@ -25,6 +25,18 @@ def get_english_count(message: str) -> float: def remove_non_letters(message: str) -> str:",
  "comment": "nice",
  "line_number": 27,
  "enriched": "File: strings/detecting_english_programmatically.py\nCode: @@ -25,6 +25,18 @@ def get_english_count(message: str) -> float:\n \n \n def remove_non_letters(message: str) -> str:\nComment: Nice",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "strings/detecting_english_programmatically.py",
  "pr_number": 11135,
  "repo": "Python",
  "owner": "TheAlgorithms",
  "comment_id": 1380414335,
  "comment_created_at": "2023-11-02T16:21:23Z"
},
{
  "code": "@@ -137,6 +137,7 @@ \"shillelagh\": Dialects.SQLITE, \"snowflake\": Dialects.SNOWFLAKE, # \"solr\": ??? + \"spark\": Dialects.SPARK,",
  "comment": "nice!",
  "line_number": 140,
  "enriched": "File: superset/sql_parse.py\nCode: @@ -137,6 +137,7 @@\n     \"shillelagh\": Dialects.SQLITE,\n     \"snowflake\": Dialects.SNOWFLAKE,\n     # \"solr\": ???\n+    \"spark\": Dialects.SPARK,\nComment: Nice!",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "superset/sql_parse.py",
  "pr_number": 28322,
  "repo": "superset",
  "owner": "apache",
  "comment_id": 1587961953,
  "comment_created_at": "2024-05-02T16:45:52Z"
},
{
  "code": "@@ -490,69 +486,15 @@ def get_transformed_features(\n             )\n \n     def infer_features(self) -> None:\n-        if self.mode in {\"pandas\", \"substrait\"}:\n-            self._infer_features_df()\n-        elif self.mode == \"python\":\n-            self._infer_features_dict()\n-        else:\n-            raise Exception(\n-                f'Invalid OnDemandFeatureMode: {self.mode}. Expected one of \"pandas\" or \"python\".'\n-            )\n-\n-    def _infer_features_dict(self):\n-        \"\"\"\n-        Infers the set of features associated to this feature view from the input source.\n-\n-        Raises:\n-            RegistryInferenceFailure: The set of features could not be inferred.\n-        \"\"\"\n-        rand_dict_value: Dict[str, Any] = {\n-            \"float\": [1.0],\n-            \"int\": [1],\n-            \"str\": [\"hello world\"],\n-            \"bytes\": [str.encode(\"hello world\")],\n-            \"bool\": [True],\n-            \"datetime64[ns]\": [datetime.utcnow()],\n-        }\n-\n-        feature_dict = {}\n-        for feature_view_projection in self.source_feature_view_projections.values():\n-            for feature in feature_view_projection.features:\n-                dtype = feast_value_type_to_pandas_type(feature.dtype.to_value_type())\n-                feature_dict[f\"{feature_view_projection.name}__{feature.name}\"] = (\n-                    rand_dict_value[dtype] if dtype in rand_dict_value else [None]\n-                )\n-                feature_dict[f\"{feature.name}\"] = (\n-                    rand_dict_value[dtype] if dtype in rand_dict_value else [None]\n-                )\n-        for request_data in self.source_request_sources.values():\n-            for field in request_data.schema:\n-                dtype = feast_value_type_to_pandas_type(field.dtype.to_value_type())\n-                feature_dict[f\"{field.name}\"] = (\n-                    rand_dict_value[dtype] if dtype in rand_dict_value else [None]\n-                )\n-\n-        output_dict: Dict[str, List[Any]] = self.feature_transformation.transform(\n-            feature_dict\n+        inferred_features = self.feature_transformation.infer_features(",
  "comment": "very nice",
  "line_number": 489,
  "enriched": "File: sdk/python/feast/on_demand_feature_view.py\nCode: @@ -490,69 +486,15 @@ def get_transformed_features(\n             )\n \n     def infer_features(self) -> None:\n-        if self.mode in {\"pandas\", \"substrait\"}:\n-            self._infer_features_df()\n-        elif self.mode == \"python\":\n-            self._infer_features_dict()\n-        else:\n-            raise Exception(\n-                f'Invalid OnDemandFeatureMode: {self.mode}. Expected one of \"pandas\" or \"python\".'\n-            )\n-\n-    def _infer_features_dict(self):\n-        \"\"\"\n-        Infers the set of features associated to this feature view from the input source.\n-\n-        Raises:\n-            RegistryInferenceFailure: The set of features could not be inferred.\n-        \"\"\"\n-        rand_dict_value: Dict[str, Any] = {\n-            \"float\": [1.0],\n-            \"int\": [1],\n-            \"str\": [\"hello world\"],\n-            \"bytes\": [str.encode(\"hello world\")],\n-            \"bool\": [True],\n-            \"datetime64[ns]\": [datetime.utcnow()],\n-        }\n-\n-        feature_dict = {}\n-        for feature_view_projection in self.source_feature_view_projections.values():\n-            for feature in feature_view_projection.features:\n-                dtype = feast_value_type_to_pandas_type(feature.dtype.to_value_type())\n-                feature_dict[f\"{feature_view_projection.name}__{feature.name}\"] = (\n-                    rand_dict_value[dtype] if dtype in rand_dict_value else [None]\n-                )\n-                feature_dict[f\"{feature.name}\"] = (\n-                    rand_dict_value[dtype] if dtype in rand_dict_value else [None]\n-                )\n-        for request_data in self.source_request_sources.values():\n-            for field in request_data.schema:\n-                dtype = feast_value_type_to_pandas_type(field.dtype.to_value_type())\n-                feature_dict[f\"{field.name}\"] = (\n-                    rand_dict_value[dtype] if dtype in rand_dict_value else [None]\n-                )\n-\n-        output_dict: Dict[str, List[Any]] = self.feature_transformation.transform(\n-            feature_dict\n+        inferred_features = self.feature_transformation.infer_features(\nComment: very nice",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "sdk/python/feast/on_demand_feature_view.py",
  "pr_number": 4076,
  "repo": "feast",
  "owner": "feast-dev",
  "comment_id": 1552244832,
  "comment_created_at": "2024-04-04T18:45:18Z"
},
{
  "code": "@@ -0,0 +1,140 @@\n+import numpy as np\n+import pandas as pd\n+\n+from feast.offline_store import ENTITY_DF_EVENT_TIMESTAMP_COL\n+\n+\n+def create_orders_df(\n+    customers, drivers, start_date, end_date, order_count\n+) -> pd.DataFrame:\n+    \"\"\"\n+    Example df generated by this function:\n+\n+    | order_id | driver_id | customer_id | order_is_success |    event_timestamp  |\n+    +----------+-----------+-------------+------------------+---------------------+\n+    |      100 |      5004 |        1007 |                0 | 2021-03-10 19:31:15 |\n+    |      101 |      5003 |        1006 |                0 | 2021-03-11 22:02:50 |\n+    |      102 |      5010 |        1005 |                0 | 2021-03-13 00:34:24 |\n+    |      103 |      5010 |        1001 |                1 | 2021-03-14 03:05:59 |\n+    \"\"\"\n+    df = pd.DataFrame()\n+    df[\"order_id\"] = [order_id for order_id in range(100, 100 + order_count)]\n+    df[\"driver_id\"] = np.random.choice(drivers, order_count)\n+    df[\"customer_id\"] = np.random.choice(customers, order_count)\n+    df[\"order_is_success\"] = np.random.randint(0, 2, size=order_count).astype(np.int32)\n+    df[ENTITY_DF_EVENT_TIMESTAMP_COL] = [\n+        pd.Timestamp(dt, unit=\"ms\", tz=\"UTC\").round(\"ms\")\n+        for dt in pd.date_range(start=start_date, end=end_date, periods=order_count)\n+    ]\n+    df.sort_values(\n+        by=[ENTITY_DF_EVENT_TIMESTAMP_COL, \"order_id\", \"driver_id\", \"customer_id\"],\n+        inplace=True,\n+    )\n+    return df\n+\n+\n+def create_driver_hourly_stats_df(drivers, start_date, end_date) -> pd.DataFrame:\n+    \"\"\"\n+    Example df generated by this function:\n+\n+    | datetime         | driver_id | conv_rate | acc_rate | avg_daily_trips | created          |\n+    |------------------+-----------+-----------+----------+-----------------+------------------|\n+    | 2021-03-17 19:31 |     5010  | 0.229297  | 0.685843 | 861             | 2021-03-24 19:34 |\n+    | 2021-03-17 20:31 |     5010  | 0.781655  | 0.861280 | 769             | 2021-03-24 19:34 |\n+    | 2021-03-17 21:31 |     5010  | 0.150333  | 0.525581 | 778             | 2021-03-24 19:34 |\n+    | 2021-03-17 22:31 |     5010  | 0.951701  | 0.228883 | 570             | 2021-03-24 19:34 |\n+    | 2021-03-17 23:31 |     5010  | 0.819598  | 0.262503 | 473             | 2021-03-24 19:34 |\n+    |                  |      ...  |      ...  |      ... | ...             |                  |\n+    | 2021-03-24 16:31 |     5001  | 0.061585  | 0.658140 | 477             | 2021-03-24 19:34 |\n+    | 2021-03-24 17:31 |     5001  | 0.088949  | 0.303897 | 618             | 2021-03-24 19:34 |\n+    | 2021-03-24 18:31 |     5001  | 0.096652  | 0.747421 | 480             | 2021-03-24 19:34 |\n+    | 2021-03-17 19:31 |     5005  | 0.142936  | 0.707596 | 466             | 2021-03-24 19:34 |\n+    | 2021-03-17 19:31 |     5005  | 0.142936  | 0.707596 | 466             | 2021-03-24 19:34 |\n+    \"\"\"\n+    df_hourly = pd.DataFrame(\n+        {\n+            \"datetime\": [\n+                pd.Timestamp(dt, unit=\"ms\", tz=\"UTC\").round(\"ms\")\n+                for dt in pd.date_range(\n+                    start=start_date, end=end_date, freq=\"1H\", closed=\"left\"\n+                )\n+            ]\n+        }\n+    )\n+    df_all_drivers = pd.DataFrame()\n+\n+    for driver in drivers:\n+        df_hourly_copy = df_hourly.copy()\n+        df_hourly_copy[\"driver_id\"] = driver\n+        df_all_drivers = pd.concat([df_hourly_copy, df_all_drivers])\n+\n+    df_all_drivers.reset_index(drop=True, inplace=True)\n+    rows = df_all_drivers[\"datetime\"].count()\n+\n+    df_all_drivers[\"conv_rate\"] = np.random.random(size=rows).astype(np.float32)\n+    df_all_drivers[\"acc_rate\"] = np.random.random(size=rows).astype(np.float32)\n+    df_all_drivers[\"avg_daily_trips\"] = np.random.randint(0, 1000, size=rows).astype(\n+        np.int32\n+    )\n+    df_all_drivers[\"created\"] = pd.to_datetime(pd.Timestamp.now(tz=None).round(\"ms\"))\n+\n+    # Create duplicate rows that should be filtered by created timestamp\n+    # TODO: These duplicate rows area indirectly being filtered out by the point in time join already. We need to\n+    #  inject a bad row at a timestamp where we know it will get joined to the entity dataframe, and then test that\n+    #  we are actually filtering it with the created timestamp\n+    late_row = df_all_drivers.iloc[int(rows / 2)]\n+    df_all_drivers = df_all_drivers.append(late_row).append(late_row)\n+\n+    return df_all_drivers\n+\n+\n+def create_customer_daily_profile_df(customers, start_date, end_date) -> pd.DataFrame:\n+    \"\"\"\n+    Example df generated by this function:",
  "comment": "so nice :)",
  "line_number": 93,
  "enriched": "File: sdk/python/tests/driver_test_data.py\nCode: @@ -0,0 +1,140 @@\n+import numpy as np\n+import pandas as pd\n+\n+from feast.offline_store import ENTITY_DF_EVENT_TIMESTAMP_COL\n+\n+\n+def create_orders_df(\n+    customers, drivers, start_date, end_date, order_count\n+) -> pd.DataFrame:\n+    \"\"\"\n+    Example df generated by this function:\n+\n+    | order_id | driver_id | customer_id | order_is_success |    event_timestamp  |\n+    +----------+-----------+-------------+------------------+---------------------+\n+    |      100 |      5004 |        1007 |                0 | 2021-03-10 19:31:15 |\n+    |      101 |      5003 |        1006 |                0 | 2021-03-11 22:02:50 |\n+    |      102 |      5010 |        1005 |                0 | 2021-03-13 00:34:24 |\n+    |      103 |      5010 |        1001 |                1 | 2021-03-14 03:05:59 |\n+    \"\"\"\n+    df = pd.DataFrame()\n+    df[\"order_id\"] = [order_id for order_id in range(100, 100 + order_count)]\n+    df[\"driver_id\"] = np.random.choice(drivers, order_count)\n+    df[\"customer_id\"] = np.random.choice(customers, order_count)\n+    df[\"order_is_success\"] = np.random.randint(0, 2, size=order_count).astype(np.int32)\n+    df[ENTITY_DF_EVENT_TIMESTAMP_COL] = [\n+        pd.Timestamp(dt, unit=\"ms\", tz=\"UTC\").round(\"ms\")\n+        for dt in pd.date_range(start=start_date, end=end_date, periods=order_count)\n+    ]\n+    df.sort_values(\n+        by=[ENTITY_DF_EVENT_TIMESTAMP_COL, \"order_id\", \"driver_id\", \"customer_id\"],\n+        inplace=True,\n+    )\n+    return df\n+\n+\n+def create_driver_hourly_stats_df(drivers, start_date, end_date) -> pd.DataFrame:\n+    \"\"\"\n+    Example df generated by this function:\n+\n+    | datetime         | driver_id | conv_rate | acc_rate | avg_daily_trips | created          |\n+    |------------------+-----------+-----------+----------+-----------------+------------------|\n+    | 2021-03-17 19:31 |     5010  | 0.229297  | 0.685843 | 861             | 2021-03-24 19:34 |\n+    | 2021-03-17 20:31 |     5010  | 0.781655  | 0.861280 | 769             | 2021-03-24 19:34 |\n+    | 2021-03-17 21:31 |     5010  | 0.150333  | 0.525581 | 778             | 2021-03-24 19:34 |\n+    | 2021-03-17 22:31 |     5010  | 0.951701  | 0.228883 | 570             | 2021-03-24 19:34 |\n+    | 2021-03-17 23:31 |     5010  | 0.819598  | 0.262503 | 473             | 2021-03-24 19:34 |\n+    |                  |      ...  |      ...  |      ... | ...             |                  |\n+    | 2021-03-24 16:31 |     5001  | 0.061585  | 0.658140 | 477             | 2021-03-24 19:34 |\n+    | 2021-03-24 17:31 |     5001  | 0.088949  | 0.303897 | 618             | 2021-03-24 19:34 |\n+    | 2021-03-24 18:31 |     5001  | 0.096652  | 0.747421 | 480             | 2021-03-24 19:34 |\n+    | 2021-03-17 19:31 |     5005  | 0.142936  | 0.707596 | 466             | 2021-03-24 19:34 |\n+    | 2021-03-17 19:31 |     5005  | 0.142936  | 0.707596 | 466             | 2021-03-24 19:34 |\n+    \"\"\"\n+    df_hourly = pd.DataFrame(\n+        {\n+            \"datetime\": [\n+                pd.Timestamp(dt, unit=\"ms\", tz=\"UTC\").round(\"ms\")\n+                for dt in pd.date_range(\n+                    start=start_date, end=end_date, freq=\"1H\", closed=\"left\"\n+                )\n+            ]\n+        }\n+    )\n+    df_all_drivers = pd.DataFrame()\n+\n+    for driver in drivers:\n+        df_hourly_copy = df_hourly.copy()\n+        df_hourly_copy[\"driver_id\"] = driver\n+        df_all_drivers = pd.concat([df_hourly_copy, df_all_drivers])\n+\n+    df_all_drivers.reset_index(drop=True, inplace=True)\n+    rows = df_all_drivers[\"datetime\"].count()\n+\n+    df_all_drivers[\"conv_rate\"] = np.random.random(size=rows).astype(np.float32)\n+    df_all_drivers[\"acc_rate\"] = np.random.random(size=rows).astype(np.float32)\n+    df_all_drivers[\"avg_daily_trips\"] = np.random.randint(0, 1000, size=rows).astype(\n+        np.int32\n+    )\n+    df_all_drivers[\"created\"] = pd.to_datetime(pd.Timestamp.now(tz=None).round(\"ms\"))\n+\n+    # Create duplicate rows that should be filtered by created timestamp\n+    # TODO: These duplicate rows area indirectly being filtered out by the point in time join already. We need to\n+    #  inject a bad row at a timestamp where we know it will get joined to the entity dataframe, and then test that\n+    #  we are actually filtering it with the created timestamp\n+    late_row = df_all_drivers.iloc[int(rows / 2)]\n+    df_all_drivers = df_all_drivers.append(late_row).append(late_row)\n+\n+    return df_all_drivers\n+\n+\n+def create_customer_daily_profile_df(customers, start_date, end_date) -> pd.DataFrame:\n+    \"\"\"\n+    Example df generated by this function:\nComment: so nice :)",
  "subcategory": "praise",
  "category": "discussion",
  "file_path": "sdk/python/tests/driver_test_data.py",
  "pr_number": 1410,
  "repo": "feast",
  "owner": "feast-dev",
  "comment_id": 601040776,
  "comment_created_at": "2021-03-25T04:55:43Z"
  },
  {
    "code": "@@ -4210,13 +4210,13 @@\n         </context-group>\n         <target state=\"final\">Keine Elemente gefunden</target>\n       </trans-unit>\n-      <trans-unit id=\"6541407358060244620\" datatype=\"html\">\n+      <trans-unit id=\"6541407358060244620\" datatype=\"html\" approved=\"yes\">\n         <source>Note: value has not yet been set and will not apply until explicitly changed</source>\n         <context-group purpose=\"location\">\n           <context context-type=\"sourcefile\">src/app/components/common/input/switch/switch.component.html</context>\n           <context context-type=\"linenumber\">45</context>\n         </context-group>\n-        <target state=\"needs-translation\">Note: value has not yet been set and will not apply until explicitly changed</target>\n+        <target state=\"final\">Heinweis: der Wert wurde nicht gesetzt und wird nicht angewendet, bis er explizit gendert wurde</target>",
    "comment": "Typo: \"Hinweis\" ",
    "line_number": 4278,
    "enriched": "File: src-ui/src/locale/messages.de_DE.xlf\nCode: @@ -4210,13 +4210,13 @@\n         </context-group>\n         <target state=\"final\">Keine Elemente gefunden</target>\n       </trans-unit>\n-      <trans-unit id=\"6541407358060244620\" datatype=\"html\">\n+      <trans-unit id=\"6541407358060244620\" datatype=\"html\" approved=\"yes\">\n         <source>Note: value has not yet been set and will not apply until explicitly changed</source>\n         <context-group purpose=\"location\">\n           <context context-type=\"sourcefile\">src/app/components/common/input/switch/switch.component.html</context>\n           <context context-type=\"linenumber\">45</context>\n         </context-group>\n-        <target state=\"needs-translation\">Note: value has not yet been set and will not apply until explicitly changed</target>\n+        <target state=\"final\">Heinweis: der Wert wurde nicht gesetzt und wird nicht angewendet, bis er explizit gendert wurde</target>\nComment: Typo: \"Hinweis\" ",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "src-ui/src/locale/messages.de_DE.xlf",
    "pr_number": 5349,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 1448872360,
    "comment_created_at": "2024-01-11T13:32:20Z"
  },
  {
    "code": "@@ -270,7 +270,7 @@ def _get_query_filter(self):\n                     query.DateRange(field, start=isoparse(value), end=None),\n                 )\n             elif query_filter == \"icontains\":\n-                criterias.append(\n+                  criterias.append(",
    "comment": "Revert this change",
    "line_number": 273,
    "enriched": "File: src/documents/index.py\nCode: @@ -270,7 +270,7 @@ def _get_query_filter(self):\n                     query.DateRange(field, start=isoparse(value), end=None),\n                 )\n             elif query_filter == \"icontains\":\n-                criterias.append(\n+                  criterias.append(\nComment: Revert this change",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/documents/index.py",
    "pr_number": 5319,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 1444771754,
    "comment_created_at": "2024-01-08T14:51:13Z"
  },
  {
    "code": "@@ -997,6 +998,39 @@ def test_document_updated_workflow(self):\n \n         self.assertEqual(doc.custom_fields.all().count(), 1)\n \n+    def test_document_updated_workflow_existing_custom_field(self):",
    "comment": "Maybe add the given/when/then if you can",
    "line_number": 1001,
    "enriched": "File: src/documents/tests/test_workflows.py\nCode: @@ -997,6 +998,39 @@ def test_document_updated_workflow(self):\n \n         self.assertEqual(doc.custom_fields.all().count(), 1)\n \n+    def test_document_updated_workflow_existing_custom_field(self):\nComment: Maybe add the given/when/then if you can",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/documents/tests/test_workflows.py",
    "pr_number": 5302,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 1444059714,
    "comment_created_at": "2024-01-07T19:29:10Z"
  },
  {
    "code": "@@ -726,12 +726,17 @@ def _store(\n \n         storage_type = Document.STORAGE_TYPE_UNENCRYPTED\n \n+        title = file_info.title[:127]\n+        if self.override_title is not None:\n+            try:\n+                title = self._parse_title_placeholders(self.override_title)\n+            except Exception:",
    "comment": "I think it would be good to log the error here as well, to help debugging",
    "line_number": 733,
    "enriched": "File: src/documents/consumer.py\nCode: @@ -726,12 +726,17 @@ def _store(\n \n         storage_type = Document.STORAGE_TYPE_UNENCRYPTED\n \n+        title = file_info.title[:127]\n+        if self.override_title is not None:\n+            try:\n+                title = self._parse_title_placeholders(self.override_title)\n+            except Exception:\nComment: I think it would be good to log the error here as well, to help debugging",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/documents/consumer.py",
    "pr_number": 5282,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 1446461239,
    "comment_created_at": "2024-01-09T18:33:10Z"
  },
   {
    "code": "@@ -32,8 +32,7 @@ do\n \t\t\texport \"${non_file_env_name}\"=\"${val}\"\n \n \t\telse\n-\t\t\techo \"File ${env_value} doesn't exist\"\n-\t\t\texit 1\n+\t\t\techo \"File ${env_value} referenced ${env_name} by doesn't exist\"",
    "comment": "Think this is a typo =)",
    "line_number": 35,
    "enriched": "File: docker/env-from-file.sh\nCode: @@ -32,8 +32,7 @@ do\n \t\t\texport \"${non_file_env_name}\"=\"${val}\"\n \n \t\telse\n-\t\t\techo \"File ${env_value} doesn't exist\"\n-\t\t\texit 1\n+\t\t\techo \"File ${env_value} referenced ${env_name} by doesn't exist\"\nComment: Think this is a typo =)",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docker/env-from-file.sh",
    "pr_number": 2368,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 1064020544,
    "comment_created_at": "2023-01-07T16:14:33Z"
  },
  {
    "code": "@@ -90,6 +92,7 @@ def open_index_searcher():\n def update_document(writer, doc):\n     tags = \",\".join([t.name for t in doc.tags.all()])\n     tags_ids = \",\".join([str(t.id) for t in doc.tags.all()])\n+    comments = \",\".join([str(c.comment) for c in Comment.objects.filter(document=doc)])",
    "comment": "I haven't looked at the Whoosh docs yet, but since a comment might contain commas, could there be a better separator to use?",
    "line_number": 95,
    "enriched": "File: src/documents/index.py\nCode: @@ -90,6 +92,7 @@ def open_index_searcher():\n def update_document(writer, doc):\n     tags = \",\".join([t.name for t in doc.tags.all()])\n     tags_ids = \",\".join([str(t.id) for t in doc.tags.all()])\n+    comments = \",\".join([str(c.comment) for c in Comment.objects.filter(document=doc)])\nComment: I haven't looked at the Whoosh docs yet, but since a comment might contain commas, could there be a better separator to use?",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "src/documents/index.py",
    "pr_number": 2351,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 1062597321,
    "comment_created_at": "2023-01-05T15:25:29Z"
  },
  {
    "code": "@@ -11,7 +11,15 @@ FROM ubuntu:20.04 AS jbig2enc\n \n WORKDIR /usr/src/jbig2enc\n \n-RUN apt-get update && apt-get install -y --no-install-recommends build-essential automake libtool libleptonica-dev zlib1g-dev git ca-certificates\n+RUN apt-get update \\\n+  && apt-get install -y --no-install-recommends build-essential \\\n+    automake \\",
    "comment": "Indentation is a bit weird here. Is this intended?",
    "line_number": 16,
    "enriched": "File: Dockerfile\nCode: @@ -11,7 +11,15 @@ FROM ubuntu:20.04 AS jbig2enc\n \n WORKDIR /usr/src/jbig2enc\n \n-RUN apt-get update && apt-get install -y --no-install-recommends build-essential automake libtool libleptonica-dev zlib1g-dev git ca-certificates\n+RUN apt-get update \\\n+  && apt-get install -y --no-install-recommends build-essential \\\n+    automake \\\nComment: Indentation is a bit weird here. Is this intended?",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "Dockerfile",
    "pr_number": 454,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 830672207,
    "comment_created_at": "2022-03-20T20:58:04Z"
  },
  {
    "code": "@@ -127,4 +127,4 @@ All team members are notified when mentioned or assigned to a relevant issue or\n \n We are not overly strict with inviting people to the organization. If you have read the [team permissions](#permissions) and think having additional access would enhance your contributions, please reach out to an [admin](#structure) of the team.\n \n-The admins occasionally invite contributors directly if we believe having them on a team will accelerate their work.\n\\ No newline at end of file\n+The admins occasionally invite contributors directly if we believe having them on a team will accelerate their work.",
    "comment": "Edit: as in, why is this a change, I dont see any difference",
    "line_number": 130,
    "enriched": "File: CONTRIBUTING.md\nCode: @@ -127,4 +127,4 @@ All team members are notified when mentioned or assigned to a relevant issue or\n \n We are not overly strict with inviting people to the organization. If you have read the [team permissions](#permissions) and think having additional access would enhance your contributions, please reach out to an [admin](#structure) of the team.\n \n-The admins occasionally invite contributors directly if we believe having them on a team will accelerate their work.\n\\ No newline at end of file\n+The admins occasionally invite contributors directly if we believe having them on a team will accelerate their work.\nComment: Edit: as in, why is this a change, I dont see any difference",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "CONTRIBUTING.md",
    "pr_number": 446,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 830511461,
    "comment_created_at": "2022-03-19T18:08:07Z"
  },
  {
    "code": "@@ -1947,23 +1947,23 @@\n           <context context-type=\"sourcefile\">src/app/components/document-list/bulk-editor/bulk-editor.component.html</context>\n           <context context-type=\"linenumber\">78,82</context>\n         </context-group>\n-        <target state=\"needs-translation\"> Download <x id=\"START_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;div *ngIf=&quot;awaitingDownload&quot; class=&quot;spinner-border spinner-border-sm&quot; role=&quot;status&quot;&gt;\"/><x id=\"START_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;span class=&quot;visually-hidden&quot;&gt;\"/>Preparing download...<x id=\"CLOSE_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;/span&gt;\"/><x id=\"CLOSE_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;/div&gt;\"/></target>\n+        <target state=\"translated\"> Originale herunterladen <x id=\"START_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;div *ngIf=&quot;awaitingDownload&quot; class=&quot;spinner-border spinner-border-sm&quot; role=&quot;status&quot;&gt;\"/><x id=\"START_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;span class=&quot;visually-hidden&quot;&gt;\"/>Download wird vorbereitet...<x id=\"CLOSE_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;/span&gt;\"/><x id=\"CLOSE_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;/div&gt;\"/></target>\n       </trans-unit>\n       <trans-unit id=\"9202029122138685465\" datatype=\"html\">\n         <source> Download originals <x id=\"START_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;div *ngIf=&quot;awaitingDownload&quot; class=&quot;spinner-border spinner-border-sm&quot; role=&quot;status&quot;&gt;\"/><x id=\"START_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;span class=&quot;visually-hidden&quot;&gt;\"/>Preparing download...<x id=\"CLOSE_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;/span&gt;\"/><x id=\"CLOSE_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;/div&gt;\"/></source>\n         <context-group purpose=\"location\">\n           <context context-type=\"sourcefile\">src/app/components/document-list/bulk-editor/bulk-editor.component.html</context>\n           <context context-type=\"linenumber\">84,88</context>\n         </context-group>\n-        <target state=\"needs-translation\"> Download originals <x id=\"START_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;div *ngIf=&quot;awaitingDownload&quot; class=&quot;spinner-border spinner-border-sm&quot; role=&quot;status&quot;&gt;\"/><x id=\"START_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;span class=&quot;visually-hidden&quot;&gt;\"/>Preparing download...<x id=\"CLOSE_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;/span&gt;\"/><x id=\"CLOSE_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;/div&gt;\"/></target>\n+        <target state=\"translated\"> Originale herunterladen <x id=\"START_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;div *ngIf=&quot;awaitingDownload&quot; class=&quot;spinner-border spinner-border-sm&quot; role=&quot;status&quot;&gt;\"/><x id=\"START_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;span class=&quot;visually-hidden&quot;&gt;\"/>Download wird vorbereitet...<x id=\"CLOSE_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;/span&gt;\"/><x id=\"CLOSE_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;/div&gt;\"/></target>\n       </trans-unit>\n       <trans-unit id=\"8312409092917397847\" datatype=\"html\">\n         <source>Redo OCR</source>\n         <context-group purpose=\"location\">\n           <context context-type=\"sourcefile\">src/app/components/document-list/bulk-editor/bulk-editor.component.html</context>\n           <context context-type=\"linenumber\">90</context>\n         </context-group>\n-        <target state=\"needs-translation\">Redo OCR</target>\n+        <target state=\"translated\">PCR wiederholen</target>",
    "comment": "Typo: OCR wiederholen",
    "line_number": 1966,
    "enriched": "File: src-ui/src/locale/messages.de_DE.xlf\nCode: @@ -1947,23 +1947,23 @@\n           <context context-type=\"sourcefile\">src/app/components/document-list/bulk-editor/bulk-editor.component.html</context>\n           <context context-type=\"linenumber\">78,82</context>\n         </context-group>\n-        <target state=\"needs-translation\"> Download <x id=\"START_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;div *ngIf=&quot;awaitingDownload&quot; class=&quot;spinner-border spinner-border-sm&quot; role=&quot;status&quot;&gt;\"/><x id=\"START_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;span class=&quot;visually-hidden&quot;&gt;\"/>Preparing download...<x id=\"CLOSE_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;/span&gt;\"/><x id=\"CLOSE_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;/div&gt;\"/></target>\n+        <target state=\"translated\"> Originale herunterladen <x id=\"START_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;div *ngIf=&quot;awaitingDownload&quot; class=&quot;spinner-border spinner-border-sm&quot; role=&quot;status&quot;&gt;\"/><x id=\"START_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;span class=&quot;visually-hidden&quot;&gt;\"/>Download wird vorbereitet...<x id=\"CLOSE_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;/span&gt;\"/><x id=\"CLOSE_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;/div&gt;\"/></target>\n       </trans-unit>\n       <trans-unit id=\"9202029122138685465\" datatype=\"html\">\n         <source> Download originals <x id=\"START_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;div *ngIf=&quot;awaitingDownload&quot; class=&quot;spinner-border spinner-border-sm&quot; role=&quot;status&quot;&gt;\"/><x id=\"START_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;span class=&quot;visually-hidden&quot;&gt;\"/>Preparing download...<x id=\"CLOSE_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;/span&gt;\"/><x id=\"CLOSE_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;/div&gt;\"/></source>\n         <context-group purpose=\"location\">\n           <context context-type=\"sourcefile\">src/app/components/document-list/bulk-editor/bulk-editor.component.html</context>\n           <context context-type=\"linenumber\">84,88</context>\n         </context-group>\n-        <target state=\"needs-translation\"> Download originals <x id=\"START_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;div *ngIf=&quot;awaitingDownload&quot; class=&quot;spinner-border spinner-border-sm&quot; role=&quot;status&quot;&gt;\"/><x id=\"START_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;span class=&quot;visually-hidden&quot;&gt;\"/>Preparing download...<x id=\"CLOSE_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;/span&gt;\"/><x id=\"CLOSE_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;/div&gt;\"/></target>\n+        <target state=\"translated\"> Originale herunterladen <x id=\"START_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;div *ngIf=&quot;awaitingDownload&quot; class=&quot;spinner-border spinner-border-sm&quot; role=&quot;status&quot;&gt;\"/><x id=\"START_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;span class=&quot;visually-hidden&quot;&gt;\"/>Download wird vorbereitet...<x id=\"CLOSE_TAG_SPAN\" ctype=\"x-span\" equiv-text=\"&lt;/span&gt;\"/><x id=\"CLOSE_TAG_DIV\" ctype=\"x-div\" equiv-text=\"&lt;/div&gt;\"/></target>\n       </trans-unit>\n       <trans-unit id=\"8312409092917397847\" datatype=\"html\">\n         <source>Redo OCR</source>\n         <context-group purpose=\"location\">\n           <context context-type=\"sourcefile\">src/app/components/document-list/bulk-editor/bulk-editor.component.html</context>\n           <context context-type=\"linenumber\">90</context>\n         </context-group>\n-        <target state=\"needs-translation\">Redo OCR</target>\n+        <target state=\"translated\">PCR wiederholen</target>\nComment: Typo: OCR wiederholen",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "src-ui/src/locale/messages.de_DE.xlf",
    "pr_number": 1238,
    "repo": "paperless-ngx",
    "owner": "paperless-ngx",
    "comment_id": 922795596,
    "comment_created_at": "2022-07-17T08:30:24Z"
  },
  {
    "code": "@@ -51,9 +52,9 @@ def _real_extract(self, url): nextjs_data = self._search_nextjs_data(webpage, video_id) media_data = ( - traverse_obj( - nextjs_data, ('props', 'pageProps', ('data', 'OpenGraphParameters')), get_all=False) - or self._download_json(f'https://yappy.media/api/video/{video_id}', video_id)) + traverse_obj( + nextjs_data, ('props', 'pageProps', ('data', 'OpenGraphParameters')), get_all=False) + or self._download_json(f'https://yappy.media/api/video/{video_id}', video_id))",
    "comment": "revert indentation, this is what the linter was complaining about",
    "line_number": 57,
    "enriched": "File: yt_dlp/extractor/yappy.py\nCode: @@ -51,9 +52,9 @@ def _real_extract(self, url):\n         nextjs_data = self._search_nextjs_data(webpage, video_id)\n \n         media_data = (\n-            traverse_obj(\n-                nextjs_data, ('props', 'pageProps', ('data', 'OpenGraphParameters')), get_all=False)\n-            or self._download_json(f'https://yappy.media/api/video/{video_id}', video_id))\n+                traverse_obj(\n+                    nextjs_data, ('props', 'pageProps', ('data', 'OpenGraphParameters')), get_all=False)\n+                or self._download_json(f'https://yappy.media/api/video/{video_id}', video_id))\nComment: revert indentation, this is what the linter was complaining about",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "yt_dlp/extractor/yappy.py",
    "pr_number": 7346,
    "repo": "yt-dlp",
    "owner": "yt-dlp",
    "comment_id": 1233338884,
    "comment_created_at": "2023-06-18T16:53:01Z"
  },
  {
    "code": "@@ -22,11 +22,11 @@ import RowCountLabel from 'src/explore/components/RowCountLabel'; import CachedLabel from 'src/components/CachedLabel'; import Timer from 'src/components/Timer'; -enum CHART_STATUS_MAP { - failed = 'danger', - loading = 'warning', - success = 'success', -} +const CHART_STATUS_MAP = { + failed: 'danger', + loading: 'warning', + success: 'success', +};",
    "comment": "this was really not a proper enum or used as such, so i changed it to a regular object.",
    "line_number": 30,
    "enriched": "File: superset-frontend/src/explore/components/ChartPills.tsx\nCode: @@ -22,11 +22,11 @@ import RowCountLabel from 'src/explore/components/RowCountLabel';\n import CachedLabel from 'src/components/CachedLabel';\n import Timer from 'src/components/Timer';\n \n-enum CHART_STATUS_MAP {\n-  failed = 'danger',\n-  loading = 'warning',\n-  success = 'success',\n-}\n+const CHART_STATUS_MAP = {\n+  failed: 'danger',\n+  loading: 'warning',\n+  success: 'success',\n+};\nComment: This was really not a proper enum or used as such, so I changed it to a regular object.",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "superset-frontend/src/explore/components/ChartPills.tsx",
    "pr_number": 26875,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1472202832,
    "comment_created_at": "2024-01-31T01:38:02Z"
  },
  {
    "code": "@@ -1,91 +1,79 @@ import torch import torch.nn as nn import torch.nn.functional as F -from torch.autograd.function import once_differentiable +from einops import rearrange -from mmcv.cnn import Scale -from ..utils import ext_loader -ext_module = ext_loader.load_ext( - '_ext', ['ca_forward', 'ca_backward', 'ca_map_forward', 'ca_map_backward']) +def NEG_INF_DIAG(n, device): + \"\"\"Returns a diagonal matrix of size [n, n]. - -class CAWeightFunction(torch.autograd.Function): - - @staticmethod - def symbolic(g, t, f): - return g.op('MMCVCAWeight', t, f) - - @staticmethod - def forward(ctx, t, f): - n, c, h, w = t.size() - weight = torch.zeros(n, h + w - 1, h, w).to(t.device) - ext_module.ca_forward(t, f, weight) - - ctx.save_for_backward(t, f) - - return weight - - @staticmethod - @once_differentiable - def backward(ctx, dw): - t, f = ctx.saved_tensors - dt = torch.zeros_like(t) - df = torch.zeros_like(f) - ext_module.ca_backward(dw, t, f, dt, df) - return dt, df - - -class CAMapFunction(torch.autograd.Function): - - @staticmethod - def symbolic(g, weight, v): - return g.op('MMCVCAMap', weight, v) - - @staticmethod - def forward(ctx, weight, v): - out = torch.zeros_like(v) - ext_module.ca_map_forward(weight, v, out) - - ctx.save_for_backward(weight, v) - - return out - - @staticmethod - @once_differentiable - def backward(ctx, dout): - weight, v = ctx.saved_tensors - dw = torch.zeros_like(weight) - dv = torch.zeros_like(v) - ext_module.ca_map_backward(dout, weight, v, dw, dv) - - return dw, dv - - -ca_weight = CAWeightFunction.apply -ca_map = CAMapFunction.apply + The diagonal are all \"-inf\". This is for avoiding calculating the + overlapped element in the Criss-Cross twice. + \"\"\" + return torch.diag(torch.tensor(float('-inf')).to(device).repeat(n), 0) class CrissCrossAttention(nn.Module): - \"\"\"Criss-Cross Attention Module.\"\"\" + \"\"\"Criss-Cross Attention Module. +",
    "comment": "suggest to also update arguments in the docstring.",
    "line_number": 18,
    "enriched": "File: mmcv/ops/cc_attention.py\nCode: @@ -1,91 +1,79 @@\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n-from torch.autograd.function import once_differentiable\n+from einops import rearrange\n \n-from mmcv.cnn import Scale\n-from ..utils import ext_loader\n \n-ext_module = ext_loader.load_ext(\n-    '_ext', ['ca_forward', 'ca_backward', 'ca_map_forward', 'ca_map_backward'])\n+def NEG_INF_DIAG(n, device):\n+    \"\"\"Returns a diagonal matrix of size [n, n].\n \n-\n-class CAWeightFunction(torch.autograd.Function):\n-\n-    @staticmethod\n-    def symbolic(g, t, f):\n-        return g.op('MMCVCAWeight', t, f)\n-\n-    @staticmethod\n-    def forward(ctx, t, f):\n-        n, c, h, w = t.size()\n-        weight = torch.zeros(n, h + w - 1, h, w).to(t.device)\n-        ext_module.ca_forward(t, f, weight)\n-\n-        ctx.save_for_backward(t, f)\n-\n-        return weight\n-\n-    @staticmethod\n-    @once_differentiable\n-    def backward(ctx, dw):\n-        t, f = ctx.saved_tensors\n-        dt = torch.zeros_like(t)\n-        df = torch.zeros_like(f)\n-        ext_module.ca_backward(dw, t, f, dt, df)\n-        return dt, df\n-\n-\n-class CAMapFunction(torch.autograd.Function):\n-\n-    @staticmethod\n-    def symbolic(g, weight, v):\n-        return g.op('MMCVCAMap', weight, v)\n-\n-    @staticmethod\n-    def forward(ctx, weight, v):\n-        out = torch.zeros_like(v)\n-        ext_module.ca_map_forward(weight, v, out)\n-\n-        ctx.save_for_backward(weight, v)\n-\n-        return out\n-\n-    @staticmethod\n-    @once_differentiable\n-    def backward(ctx, dout):\n-        weight, v = ctx.saved_tensors\n-        dw = torch.zeros_like(weight)\n-        dv = torch.zeros_like(v)\n-        ext_module.ca_map_backward(dout, weight, v, dw, dv)\n-\n-        return dw, dv\n-\n-\n-ca_weight = CAWeightFunction.apply\n-ca_map = CAMapFunction.apply\n+    The diagonal are all \"-inf\". This is for avoiding calculating the\n+    overlapped element in the Criss-Cross twice.\n+    \"\"\"\n+    return torch.diag(torch.tensor(float('-inf')).to(device).repeat(n), 0)\n \n \n class CrissCrossAttention(nn.Module):\n-    \"\"\"Criss-Cross Attention Module.\"\"\"\n+    \"\"\"Criss-Cross Attention Module.\n+\nComment: Suggest to also update arguments in the docstring.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mmcv/ops/cc_attention.py",
    "pr_number": 1186,
    "repo": "mmcv",
    "owner": "open-mmlab",
    "comment_id": 668458022,
    "comment_created_at": "2021-07-13T06:16:20Z"
  },
  {
    "code": "@@ -42,8 +42,14 @@ supports both libraries. Support for ``psycopg2`` is likely to be deprecated and removed at some point in the future. + .. admonition:: psycopg3 introduces breaking changes over psycopg2 + + Before upgrading to psycopg3 please be aware of any changes you may need to + make to raw queries as documented in: `Differences from psycopg2`_.",
    "comment": "we've avoided the pyscopg3 terminology as the package is actually called psycopg",
    "line_number": 48,
    "enriched": "File: docs/releases/4.2.txt\nCode: @@ -42,8 +42,14 @@ supports both libraries.\n Support for ``psycopg2`` is likely to be deprecated and removed at some point\n in the future.\n \n+  .. admonition:: psycopg3 introduces breaking changes over psycopg2\n+\n+     Before upgrading to psycopg3 please be aware of any changes you may need to\n+     make to raw queries as documented in: `Differences from psycopg2`_.\nComment: We've avoided the `pyscopg3` terminology as the package is actually called `psycopg`\r\n\r\n```suggestion\r\n.. admonition:: ``psycopg >= 3`` introduces breaking changes over ``psycopg2``\r\n\r\n   Before upgrading please be aware of any changes you may need to make to account for `differences from psycopg2`_\r\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "docs/releases/4.2.txt",
    "pr_number": 16722,
    "repo": "django",
    "owner": "django",
    "comment_id": 1157150422,
    "comment_created_at": "2023-04-04T12:09:30Z"
  },
  {
    "code": "@@ -90,6 +91,49 @@ return self.proto.body +@dataclass(init=False) +class Markdown(Element): + proto: MarkdownProto + + type: str + root: ElementTree = field(repr=False) + is_caption: bool + allow_html: bool + key: None = None + + def __init__(self, proto: MarkdownProto, root: ElementTree): + self.proto = proto + self.is_caption = proto.is_caption + self.allow_html = proto.allow_html + self.root = root + self.type = \"markdown\" + + @property + def value(self) -> str: + return self.proto.body + + +@dataclass(init=False) +class Caption(Markdown): + def __init__(self, proto: MarkdownProto, root: ElementTree): + super().__init__(proto, root) + self.type = \"caption\"",
    "comment": "## overwriting attribute in super-class or sub-class\n\nassignment overwrites attribute type, which was previously defined in superclass [element](1).\nassignment overwrites attribute type, which was previously defined in superclass [markdown](2).\n\n[show more details](https://github.com/streamlit/streamlit/security/code-scanning/5714)",
    "line_number": 120,
    "enriched": "File: lib/streamlit/testing/element_tree.py\nCode: @@ -90,6 +91,49 @@\n         return self.proto.body\n \n \n+@dataclass(init=False)\n+class Markdown(Element):\n+    proto: MarkdownProto\n+\n+    type: str\n+    root: ElementTree = field(repr=False)\n+    is_caption: bool\n+    allow_html: bool\n+    key: None = None\n+\n+    def __init__(self, proto: MarkdownProto, root: ElementTree):\n+        self.proto = proto\n+        self.is_caption = proto.is_caption\n+        self.allow_html = proto.allow_html\n+        self.root = root\n+        self.type = \"markdown\"\n+\n+    @property\n+    def value(self) -> str:\n+        return self.proto.body\n+\n+\n+@dataclass(init=False)\n+class Caption(Markdown):\n+    def __init__(self, proto: MarkdownProto, root: ElementTree):\n+        super().__init__(proto, root)\n+        self.type = \"caption\"\nComment: ## Overwriting attribute in super-class or sub-class\n\nAssignment overwrites attribute type, which was previously defined in superclass [Element](1).\nAssignment overwrites attribute type, which was previously defined in superclass [Markdown](2).\n\n[Show more details](https://github.com/streamlit/streamlit/security/code-scanning/5714)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "lib/streamlit/testing/element_tree.py",
    "pr_number": 5923,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 1066989821,
    "comment_created_at": "2023-01-11T13:25:49Z"
  },
  {
    "code": "@@ -99,48 +99,49 @@ }) -class CNNBlogsIE(InfoExtractor): - _VALID_URL = r'https?://[^\\.]+\\.blogs\\.cnn\\.com/.+' - _TEST = { - 'url': 'http://reliablesources.blogs.cnn.com/2014/02/09/criminalizing-journalism/', - 'md5': '3e56f97b0b6ffb4b79f4ea0749551084', +class CNNArticleIE(InfoExtractor): + _VALID_URL = r'''(?x)https?://(?:(?P<sub_domain>edition|www|money)\\.)?cnn\\.com/(?:\\d{4}/\\d{2}/\\d{2}/\\w+/)?(?:video/(?:data/.+?|\\?)/)?videos?/? + (?P<path>.+?/?(?P<title>[^/]+?)(?:\\.?(?:[a-z0-9\\-]+)|(?=&)))?''' + + _TESTS = [{ + 'url': \"https://www.cnn.com/2024/05/31/sport/video/jadon-sancho-borussia-dortmund-champions-league-exclusive-spt-intl\", 'info_dict': { - 'id': 'bestoftv/2014/02/09/criminalizing-journalism.cnn', + 'id': 'jadon-sancho-borussia-dortmund-champions-league-exclusive-spt-intl-1553374-1920x1080_8000k', 'ext': 'mp4', - 'title': 'Criminalizing journalism?', - 'description': 'Glenn Greenwald responds to comments made this week on Capitol Hill that journalists could be criminal accessories.', - 'upload_date': '20140209', - }, - 'expected_warnings': ['Failed to download m3u8 information'], - 'add_ie': ['CNN'], - } - - def _real_extract(self, url): - webpage = self._download_webpage(url, url_basename(url)) - cnn_url = self._html_search_regex(r'data-url=\"(.+?)\"', webpage, 'cnn url') - return self.url_result(cnn_url, CNNIE.ie_key()) - - -class CNNArticleIE(InfoExtractor): - _VALID_URL = r'https?://(?:(?:edition|www)\\.)?cnn\\.com/(?!videos?/)' - _TEST = { - 'url': 'http://www.cnn.com/2014/12/21/politics/obama-north-koreas-hack-not-war-but-cyber-vandalism/', - 'md5': '689034c2a3d9c6dc4aa72d65a81efd01', + 'direct': True, + 'title': 'jadon-sancho-borussia-dortmund-champions-league-exclusive-spt-intl-1553374-1920x1080_8000k', + 'timestamp': 1717148749.0, + 'upload_date': '20240531', + } + }, { + 'url': \"https://edition.cnn.com/2024/06/11/politics/video/inmates-vote-jail-nevada-murray-dnt-ac360-digvid\", 'info_dict': { - 'id': 'bestoftv/2014/12/21/ip-north-korea-obama.cnn', + 'id': 'inmates-vote-jail-nevada-murray-dnt-ac360-digvid-1563291-1920x1080_8000k', 'ext': 'mp4', - 'title': 'Obama: Cyberattack not an act of war', - 'description': 'md5:0a802a40d2376f60e6b04c8d5bcebc4b', - 'upload_date': '20141221', - }, - 'expected_warnings': ['Failed to download m3u8 information'], - 'add_ie': ['CNN'], - } + 'direct': True, + 'title': 'inmates-vote-jail-nevada-murray-dnt-ac360-digvid-1563291-1920x1080_8000k', + 'timestamp': 1718158370.0, + 'upload_date': '20240612', + } + }, { + 'url': \"https://www.cnn.com/2024/06/11/style/video/king-charles-portrait-vandalized-activists-foster-intl-digvid\", + 'info_dict': { + 'id': 'king-charles-portrait-vandalized-activists-foster-intl-digvid-1562674-1920x1080_8000k', + 'ext': 'mp4', + 'direct': True, + 'title': 'king-charles-portrait-vandalized-activists-foster-intl-digvid-1562674-1920x1080_8000k', + 'timestamp': 1718116155.0, + 'upload_date': '20240611', + } + }] def _real_extract(self, url): webpage = self._download_webpage(url, url_basename(url)) - cnn_url = self._html_search_regex(r\"video:\\s*'([^']+)'\", webpage, 'cnn url') - return self.url_result('http://cnn.com/video/?/video/' + cnn_url, CNNIE.ie_key()) + cnn_url = self._search_regex(r'\"@type\":\"VideoObject\",\"contentUrl\":\"(.*?)\"', webpage, 'content URL') + if (\"vod-media-aka.warnermediacdn.com\" in cnn_url):",
    "comment": "## incomplete url substring sanitization\n\nthe string [vod-media-aka.warnermediacdn.com](1) may be at an arbitrary position in the sanitized url.\n\n[show more details](https://github.com/yt-dlp/yt-dlp/security/code-scanning/90)",
    "line_number": 141,
    "enriched": "File: yt_dlp/extractor/cnn.py\nCode: @@ -99,48 +99,49 @@\n             })\n \n \n-class CNNBlogsIE(InfoExtractor):\n-    _VALID_URL = r'https?://[^\\.]+\\.blogs\\.cnn\\.com/.+'\n-    _TEST = {\n-        'url': 'http://reliablesources.blogs.cnn.com/2014/02/09/criminalizing-journalism/',\n-        'md5': '3e56f97b0b6ffb4b79f4ea0749551084',\n+class CNNArticleIE(InfoExtractor):\n+    _VALID_URL = r'''(?x)https?://(?:(?P<sub_domain>edition|www|money)\\.)?cnn\\.com/(?:\\d{4}/\\d{2}/\\d{2}/\\w+/)?(?:video/(?:data/.+?|\\?)/)?videos?/?\n+        (?P<path>.+?/?(?P<title>[^/]+?)(?:\\.?(?:[a-z0-9\\-]+)|(?=&)))?'''\n+\n+    _TESTS = [{\n+        'url': \"https://www.cnn.com/2024/05/31/sport/video/jadon-sancho-borussia-dortmund-champions-league-exclusive-spt-intl\",\n         'info_dict': {\n-            'id': 'bestoftv/2014/02/09/criminalizing-journalism.cnn',\n+            'id': 'jadon-sancho-borussia-dortmund-champions-league-exclusive-spt-intl-1553374-1920x1080_8000k',\n             'ext': 'mp4',\n-            'title': 'Criminalizing journalism?',\n-            'description': 'Glenn Greenwald responds to comments made this week on Capitol Hill that journalists could be criminal accessories.',\n-            'upload_date': '20140209',\n-        },\n-        'expected_warnings': ['Failed to download m3u8 information'],\n-        'add_ie': ['CNN'],\n-    }\n-\n-    def _real_extract(self, url):\n-        webpage = self._download_webpage(url, url_basename(url))\n-        cnn_url = self._html_search_regex(r'data-url=\"(.+?)\"', webpage, 'cnn url')\n-        return self.url_result(cnn_url, CNNIE.ie_key())\n-\n-\n-class CNNArticleIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:(?:edition|www)\\.)?cnn\\.com/(?!videos?/)'\n-    _TEST = {\n-        'url': 'http://www.cnn.com/2014/12/21/politics/obama-north-koreas-hack-not-war-but-cyber-vandalism/',\n-        'md5': '689034c2a3d9c6dc4aa72d65a81efd01',\n+            'direct': True,\n+            'title': 'jadon-sancho-borussia-dortmund-champions-league-exclusive-spt-intl-1553374-1920x1080_8000k',\n+            'timestamp': 1717148749.0,\n+            'upload_date': '20240531',\n+        }\n+    }, {\n+        'url': \"https://edition.cnn.com/2024/06/11/politics/video/inmates-vote-jail-nevada-murray-dnt-ac360-digvid\",\n         'info_dict': {\n-            'id': 'bestoftv/2014/12/21/ip-north-korea-obama.cnn',\n+            'id': 'inmates-vote-jail-nevada-murray-dnt-ac360-digvid-1563291-1920x1080_8000k',\n             'ext': 'mp4',\n-            'title': 'Obama: Cyberattack not an act of war',\n-            'description': 'md5:0a802a40d2376f60e6b04c8d5bcebc4b',\n-            'upload_date': '20141221',\n-        },\n-        'expected_warnings': ['Failed to download m3u8 information'],\n-        'add_ie': ['CNN'],\n-    }\n+            'direct': True,\n+            'title': 'inmates-vote-jail-nevada-murray-dnt-ac360-digvid-1563291-1920x1080_8000k',\n+            'timestamp': 1718158370.0,\n+            'upload_date': '20240612',\n+        }\n+    }, {\n+        'url': \"https://www.cnn.com/2024/06/11/style/video/king-charles-portrait-vandalized-activists-foster-intl-digvid\",\n+        'info_dict': {\n+            'id': 'king-charles-portrait-vandalized-activists-foster-intl-digvid-1562674-1920x1080_8000k',\n+            'ext': 'mp4',\n+            'direct': True,\n+            'title': 'king-charles-portrait-vandalized-activists-foster-intl-digvid-1562674-1920x1080_8000k',\n+            'timestamp': 1718116155.0,\n+            'upload_date': '20240611',\n+        }\n+    }]\n \n     def _real_extract(self, url):\n         webpage = self._download_webpage(url, url_basename(url))\n-        cnn_url = self._html_search_regex(r\"video:\\s*'([^']+)'\", webpage, 'cnn url')\n-        return self.url_result('http://cnn.com/video/?/video/' + cnn_url, CNNIE.ie_key())\n+        cnn_url = self._search_regex(r'\"@type\":\"VideoObject\",\"contentUrl\":\"(.*?)\"', webpage, 'content URL')\n+        if (\"vod-media-aka.warnermediacdn.com\" in cnn_url):\nComment: ## Incomplete URL substring sanitization\n\nThe string [vod-media-aka.warnermediacdn.com](1) may be at an arbitrary position in the sanitized URL.\n\n[Show more details](https://github.com/yt-dlp/yt-dlp/security/code-scanning/90)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "yt_dlp/extractor/cnn.py",
    "pr_number": 10185,
    "repo": "yt-dlp",
    "owner": "yt-dlp",
    "comment_id": 1640101619,
    "comment_created_at": "2024-06-14T16:43:09Z"
  },
  {
    "code": "@@ -336,17 +364,22 @@ function ButtonGroup(props: Readonly<Props>): ReactElement { selectionVisualization, clickMode, value, - style + style, + containerWidth ) // TODO: Update to match React best practices // eslint-disable-next-line @eslint-react/no-array-index-key return <Element key={`${option.content}-${index}`} /> }), - [clickMode, options, selectionVisualization, style, value] + [clickMode, options, selectionVisualization, style, value, containerWidth] ) return ( - <div className=\"stButtonGroup\" data-testid=\"stButtonGroup\"> + <div + className=\"stButtonGroup\" + data-testid=\"stButtonGroup\" + style={{ width: containerWidth ? \"100%\" : \"auto\" }}",
    "comment": "**suggestion (non-blocking):** prefer to keep styles defined in styled-components",
    "line_number": 381,
    "enriched": "File: frontend/lib/src/components/widgets/ButtonGroup/ButtonGroup.tsx\nCode: @@ -336,17 +364,22 @@ function ButtonGroup(props: Readonly<Props>): ReactElement {\n           selectionVisualization,\n           clickMode,\n           value,\n-          style\n+          style,\n+          containerWidth\n         )\n         // TODO: Update to match React best practices\n         // eslint-disable-next-line @eslint-react/no-array-index-key\n         return <Element key={`${option.content}-${index}`} />\n       }),\n-    [clickMode, options, selectionVisualization, style, value]\n+    [clickMode, options, selectionVisualization, style, value, containerWidth]\n   )\n \n   return (\n-    <div className=\"stButtonGroup\" data-testid=\"stButtonGroup\">\n+    <div\n+      className=\"stButtonGroup\"\n+      data-testid=\"stButtonGroup\"\n+      style={{ width: containerWidth ? \"100%\" : \"auto\" }}\nComment: **suggestion (non-blocking):** Prefer to keep styles defined in styled-components",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "frontend/lib/src/components/widgets/ButtonGroup/ButtonGroup.tsx",
    "pr_number": 11687,
    "repo": "streamlit",
    "owner": "streamlit",
    "comment_id": 2159293150,
    "comment_created_at": "2025-06-20T15:53:34Z"
  },
  {
    "code": "@@ -40,48 +48,105 @@ HAS_AIOSMTPD = False -class HeadersCheckMixin: +def message_from_bytes(s): + \"\"\"email.message_from_bytes() using modern email.policy.default.\"\"\" + return _message_from_bytes(s, policy=policy.default) + + +def message_from_binary_file(fp): + \"\"\"email.message_from_binary_file() using modern email.policy.default.\"\"\" + return _message_from_binary_file(fp, policy=policy.default) + + +class MailTestsMixin: def assertMessageHasHeaders(self, message, headers): \"\"\" Asserts that the `message` has all `headers`. - message: can be an instance of an email.Message subclass or a string + message: can be an instance of an email.Message subclass or bytes with the contents of an email message. headers: should be a set of (header-name, header-value) tuples. \"\"\" if isinstance(message, bytes): message = message_from_bytes(message) msg_headers = set(message.items()) - self.assertTrue( - headers.issubset(msg_headers), - msg=\"Message is missing \" - \"the following headers: %s\" % (headers - msg_headers), - ) + if not headers.issubset(msg_headers): + missing = \"\\n\".join(f\" {h}: {v}\" for h, v in headers - msg_headers) + actual = \"\\n\".join(f\" {h}: {v}\" for h, v in msg_headers) + raise self.failureException( + f\"Expected headers not found in message.\\n\" + f\"Missing headers:\\n{missing}\\n\" + f\"Actual headers:\\n{actual}\" + ) + def assertStartsWith(self, first, second): + if not first.startswith(second): + self.longMessage = True + self.assertEqual( + first[: len(second)], + second, + \"First string doesn't start with the second.\", + )",
    "comment": "do we need custom assertions for such checks? is it not enough to use\r\npython\r\nasserttrue(first.startswith(second), \"first string doesn't start with the second.\")\r\n\r\nor even:\r\npython\r\nasserttrue(first.startswith(second))",
    "line_number": 97,
    "enriched": "File: tests/mail/tests.py\nCode: @@ -40,48 +48,105 @@\n     HAS_AIOSMTPD = False\n \n \n-class HeadersCheckMixin:\n+def message_from_bytes(s):\n+    \"\"\"email.message_from_bytes() using modern email.policy.default.\"\"\"\n+    return _message_from_bytes(s, policy=policy.default)\n+\n+\n+def message_from_binary_file(fp):\n+    \"\"\"email.message_from_binary_file() using modern email.policy.default.\"\"\"\n+    return _message_from_binary_file(fp, policy=policy.default)\n+\n+\n+class MailTestsMixin:\n     def assertMessageHasHeaders(self, message, headers):\n         \"\"\"\n         Asserts that the `message` has all `headers`.\n \n-        message: can be an instance of an email.Message subclass or a string\n+        message: can be an instance of an email.Message subclass or bytes\n                  with the contents of an email message.\n         headers: should be a set of (header-name, header-value) tuples.\n         \"\"\"\n         if isinstance(message, bytes):\n             message = message_from_bytes(message)\n         msg_headers = set(message.items())\n-        self.assertTrue(\n-            headers.issubset(msg_headers),\n-            msg=\"Message is missing \"\n-            \"the following headers: %s\" % (headers - msg_headers),\n-        )\n+        if not headers.issubset(msg_headers):\n+            missing = \"\\n\".join(f\"  {h}: {v}\" for h, v in headers - msg_headers)\n+            actual = \"\\n\".join(f\"  {h}: {v}\" for h, v in msg_headers)\n+            raise self.failureException(\n+                f\"Expected headers not found in message.\\n\"\n+                f\"Missing headers:\\n{missing}\\n\"\n+                f\"Actual headers:\\n{actual}\"\n+            )\n \n+    def assertStartsWith(self, first, second):\n+        if not first.startswith(second):\n+            self.longMessage = True\n+            self.assertEqual(\n+                first[: len(second)],\n+                second,\n+                \"First string doesn't start with the second.\",\n+            )\nComment: Do we need custom assertions for such checks? Is it not enough to use\r\n```python\r\nassertTrue(first.startswith(second), \"First string doesn't start with the second.\")\r\n```\r\nor even:\r\n```python\r\nassertTrue(first.startswith(second))\r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/mail/tests.py",
    "pr_number": 18502,
    "repo": "django",
    "owner": "django",
    "comment_id": 1725024992,
    "comment_created_at": "2024-08-21T13:08:12Z"
  },
    {
    "code": "@@ -656,11 +656,12 @@ def parse_boundary_stream(stream, max_header_size): header_end = chunk.find(b\"\\r\\n\\r\\n\") def _parse_header(line): - main_value_pair, params = parse_header(line) + main_value_pair, params = parse_header_parameters(line.decode()) + params = {k: v.encode() for k, v in params.items()} try: name, value = main_value_pair.split(\":\", 1) except ValueError: - raise ValueError(\"Invalid header: %r\" % line) + raise ValueError(\"Invalid header: %r\" % line.decode())",
    "comment": "imo, we don't need to decode() here. we're currently raising an error with bytes and we can keep it that way. please add a test case for this message (in a separate commit).",
    "line_number": 664,
    "enriched": "File: django/http/multipartparser.py\nCode: @@ -656,11 +656,12 @@ def parse_boundary_stream(stream, max_header_size):\n     header_end = chunk.find(b\"\\r\\n\\r\\n\")\n \n     def _parse_header(line):\n-        main_value_pair, params = parse_header(line)\n+        main_value_pair, params = parse_header_parameters(line.decode())\n+        params = {k: v.encode() for k, v in params.items()}\n         try:\n             name, value = main_value_pair.split(\":\", 1)\n         except ValueError:\n-            raise ValueError(\"Invalid header: %r\" % line)\n+            raise ValueError(\"Invalid header: %r\" % line.decode())\nComment: IMO, we don't need to `decode()` here. We're currently raising an error with bytes and we can keep it that way. Please add a test case for this message (in a separate commit). ",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "django/http/multipartparser.py",
    "pr_number": 15797,
    "repo": "django",
    "owner": "django",
    "comment_id": 907255645,
    "comment_created_at": "2022-06-27T10:59:20Z"
  },
  {
    "code": "@@ -35,139 +28,123 @@ class RaiBaseIE(InfoExtractor): _GEO_BYPASS = False def _extract_relinker_info(self, relinker_url, video_id, audio_only=False): + def fix_cdata(s): + # remove \\r\\n\\t before and after <![CDATA[ ]]> to avoid + # polluted text with xpath_text + s = re.sub(r'(\\]\\]>)[\\r\\n\\t]+(</)', '\\\\1\\\\2', s) + return re.sub(r'(>)[\\r\\n\\t]+(<!\\[CDATA\\[)', '\\\\1\\\\2', s) + if not re.match(r'https?://', relinker_url): return {'formats': [{'url': relinker_url}]} - formats = [] - geoprotection = None - is_live = None - duration = None - - for platform in ('mon', 'flash', 'native'): - relinker = self._download_xml( - relinker_url, video_id, - note=f'Downloading XML metadata for platform {platform}', - transform_source=fix_xml_ampersands, - query={'output': 45, 'pl': platform}, - headers=self.geo_verification_headers()) - - if xpath_text(relinker, './license_url', default='{}') != '{}': - self.report_drm(video_id) - - if not geoprotection: - geoprotection = xpath_text( - relinker, './geoprotection', default=None) == 'Y' + # set User-Agent to generic 'Rai' to avoid quality filtering from + # the media server and get the maximum qualities available + relinker = self._download_xml( + relinker_url, video_id, note='Downloading XML metadata', + transform_source=fix_cdata, query={'output': 64}, + headers={**self.geo_verification_headers(), 'User-Agent': 'Rai'}) - if not is_live: - is_live = xpath_text( - relinker, './is_live', default=None) == 'Y' - if not duration: - duration = parse_duration(xpath_text( - relinker, './duration', default=None)) + if xpath_text(relinker, './license_url', default='{}') != '{}': + self.report_drm(video_id) - url_elem = find_xpath_attr(relinker, './url', 'type', 'content') - if url_elem is None: - continue + is_live = xpath_text(relinker, './is_live', default='N') == 'Y' + duration = parse_duration(xpath_text(relinker, './duration', default=None)) + media_url = xpath_text(relinker, './url[@type=\"content\"]', default=None) - media_url = url_elem.text + if not media_url: + self.raise_no_formats('The relinker returned no media url') + # geo flag is a bit unreliable and not properly set all the time + geoprotection = xpath_text(relinker, './geoprotection', default='N') == 'Y' + if '/video_no_available.mp4' in media_url: # This does not imply geo restriction (e.g. # http://www.raisport.rai.it/dl/raiSport/media/rassegna-stampa-04a9f4bd-b563-40cf-82a6-aad3529cb4a9.html) - if '/video_no_available.mp4' in media_url: - continue + self.report_warning('Video not available. Likely due to geo-restriction.')",
    "comment": "per discussion in #5672 it may be preferable to make this raise_geo_restricted instead",
    "line_number": 62,
    "enriched": "File: yt_dlp/extractor/rai.py\nCode: @@ -35,139 +28,123 @@ class RaiBaseIE(InfoExtractor):\n     _GEO_BYPASS = False\n \n     def _extract_relinker_info(self, relinker_url, video_id, audio_only=False):\n+        def fix_cdata(s):\n+            # remove \\r\\n\\t before and after <![CDATA[ ]]> to avoid\n+            # polluted text with xpath_text\n+            s = re.sub(r'(\\]\\]>)[\\r\\n\\t]+(</)', '\\\\1\\\\2', s)\n+            return re.sub(r'(>)[\\r\\n\\t]+(<!\\[CDATA\\[)', '\\\\1\\\\2', s)\n+\n         if not re.match(r'https?://', relinker_url):\n             return {'formats': [{'url': relinker_url}]}\n \n-        formats = []\n-        geoprotection = None\n-        is_live = None\n-        duration = None\n-\n-        for platform in ('mon', 'flash', 'native'):\n-            relinker = self._download_xml(\n-                relinker_url, video_id,\n-                note=f'Downloading XML metadata for platform {platform}',\n-                transform_source=fix_xml_ampersands,\n-                query={'output': 45, 'pl': platform},\n-                headers=self.geo_verification_headers())\n-\n-            if xpath_text(relinker, './license_url', default='{}') != '{}':\n-                self.report_drm(video_id)\n-\n-            if not geoprotection:\n-                geoprotection = xpath_text(\n-                    relinker, './geoprotection', default=None) == 'Y'\n+        # set User-Agent to generic 'Rai' to avoid quality filtering from\n+        # the media server and get the maximum qualities available\n+        relinker = self._download_xml(\n+            relinker_url, video_id, note='Downloading XML metadata',\n+            transform_source=fix_cdata, query={'output': 64},\n+            headers={**self.geo_verification_headers(), 'User-Agent': 'Rai'})\n \n-            if not is_live:\n-                is_live = xpath_text(\n-                    relinker, './is_live', default=None) == 'Y'\n-            if not duration:\n-                duration = parse_duration(xpath_text(\n-                    relinker, './duration', default=None))\n+        if xpath_text(relinker, './license_url', default='{}') != '{}':\n+            self.report_drm(video_id)\n \n-            url_elem = find_xpath_attr(relinker, './url', 'type', 'content')\n-            if url_elem is None:\n-                continue\n+        is_live = xpath_text(relinker, './is_live', default='N') == 'Y'\n+        duration = parse_duration(xpath_text(relinker, './duration', default=None))\n+        media_url = xpath_text(relinker, './url[@type=\"content\"]', default=None)\n \n-            media_url = url_elem.text\n+        if not media_url:\n+            self.raise_no_formats('The relinker returned no media url')\n \n+        # geo flag is a bit unreliable and not properly set all the time\n+        geoprotection = xpath_text(relinker, './geoprotection', default='N') == 'Y'\n+        if '/video_no_available.mp4' in media_url:\n             # This does not imply geo restriction (e.g.\n             # http://www.raisport.rai.it/dl/raiSport/media/rassegna-stampa-04a9f4bd-b563-40cf-82a6-aad3529cb4a9.html)\n-            if '/video_no_available.mp4' in media_url:\n-                continue\n+            self.report_warning('Video not available. Likely due to geo-restriction.')\nComment: per discussion in #5672 it may be preferable to make this `raise_geo_restricted` instead",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "yt_dlp/extractor/rai.py",
    "pr_number": 5940,
    "repo": "yt-dlp",
    "owner": "yt-dlp",
    "comment_id": 1070391730,
    "comment_created_at": "2023-01-14T19:23:26Z"
  },
  {
    "code": "@@ -217,20 +213,25 @@ def test_convert_dtypes_infer_objects(): convert_string=False, ) - assert np.shares_memory(get_array(ser), get_array(result)) + assert tm.shares_memory(get_array(ser), get_array(result)) result.iloc[0] = \"x\" tm.assert_series_equal(ser, ser_orig) -@pytest.mark.xfail(using_string_dtype() and HAS_PYARROW, reason=\"TODO(infer_string)\") -def test_convert_dtypes(): +def test_convert_dtypes(using_infer_string): df = DataFrame({\"a\": [\"a\", \"b\"], \"b\": [1, 2], \"c\": [1.5, 2.5], \"d\": [True, False]}) df_orig = df.copy() df2 = df.convert_dtypes() - assert np.shares_memory(get_array(df2, \"a\"), get_array(df, \"a\")) - assert np.shares_memory(get_array(df2, \"d\"), get_array(df, \"d\")) - assert np.shares_memory(get_array(df2, \"b\"), get_array(df, \"b\")) - assert np.shares_memory(get_array(df2, \"c\"), get_array(df, \"c\")) + if using_infer_string: + # TODO the default nullable string dtype still uses python storage + # this should be changed to pyarrow if installed + assert not tm.shares_memory(get_array(df2, \"a\"), get_array(df, \"a\"))",
    "comment": "why does this behavior change?",
    "line_number": 229,
    "enriched": "File: pandas/tests/copy_view/test_astype.py\nCode: @@ -217,20 +213,25 @@ def test_convert_dtypes_infer_objects():\n         convert_string=False,\n     )\n \n-    assert np.shares_memory(get_array(ser), get_array(result))\n+    assert tm.shares_memory(get_array(ser), get_array(result))\n     result.iloc[0] = \"x\"\n     tm.assert_series_equal(ser, ser_orig)\n \n \n-@pytest.mark.xfail(using_string_dtype() and HAS_PYARROW, reason=\"TODO(infer_string)\")\n-def test_convert_dtypes():\n+def test_convert_dtypes(using_infer_string):\n     df = DataFrame({\"a\": [\"a\", \"b\"], \"b\": [1, 2], \"c\": [1.5, 2.5], \"d\": [True, False]})\n     df_orig = df.copy()\n     df2 = df.convert_dtypes()\n \n-    assert np.shares_memory(get_array(df2, \"a\"), get_array(df, \"a\"))\n-    assert np.shares_memory(get_array(df2, \"d\"), get_array(df, \"d\"))\n-    assert np.shares_memory(get_array(df2, \"b\"), get_array(df, \"b\"))\n-    assert np.shares_memory(get_array(df2, \"c\"), get_array(df, \"c\"))\n+    if using_infer_string:\n+        # TODO the default nullable string dtype still uses python storage\n+        # this should be changed to pyarrow if installed\n+        assert not tm.shares_memory(get_array(df2, \"a\"), get_array(df, \"a\"))\nComment: why does this behavior change?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/tests/copy_view/test_astype.py",
    "pr_number": 60245,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1834677316,
    "comment_created_at": "2024-11-08T16:22:18Z"
  },
  {
    "code": "@@ -341,7 +341,7 @@ Inspired by [awesome-php](https://github.com/ziadoz/awesome-php). * [bqplot](https://github.com/bloomberg/bqplot) - Interactive Plotting Library for the Jupyter Notebook * [Dash](https://plot.ly/products/dash/) - Built on top of Flask, React and Plotly aimed at analytical web applications. * [awesome-dash](https://github.com/Acrotrend/awesome-dash) -* [ggplot](https://github.com/yhat/ggpy) - Same API as ggplot2 for R. +* [plotnine](https://github.com/has2k1/plotnine) - Same API as ggplot2 for R.",
    "comment": "hello @jarnorfb,\r\n\r\nit's probably worth to update the description as well.\r\n\r\n\r\n\r\nthis suggestion is based on plotnine's description on github and pypi.",
    "line_number": 344,
    "enriched": "File: README.md\nCode: @@ -341,7 +341,7 @@ Inspired by [awesome-php](https://github.com/ziadoz/awesome-php).\n * [bqplot](https://github.com/bloomberg/bqplot) - Interactive Plotting Library for the Jupyter Notebook\n * [Dash](https://plot.ly/products/dash/) - Built on top of Flask, React and Plotly aimed at analytical web applications.\n     * [awesome-dash](https://github.com/Acrotrend/awesome-dash)\n-* [ggplot](https://github.com/yhat/ggpy) - Same API as ggplot2 for R.\n+* [plotnine](https://github.com/has2k1/plotnine) - Same API as ggplot2 for R.\nComment: Hello @JarnoRFB,\r\n\r\nIt's probably worth to update the description as well.\r\n\r\n```suggestion\r\n* [plotnine](https://github.com/has2k1/plotnine) - A grammar of graphics for Python.\r\n```\r\n\r\nThis suggestion is based on plotnine's description on GitHub and PyPI.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "README.md",
    "pr_number": 1330,
    "repo": "awesome-python",
    "owner": "vinta",
    "comment_id": 311490281,
    "comment_created_at": "2019-08-07T10:53:53Z"
  },
  {
    "code": "@@ -49,22 +49,25 @@ export function getValue( return isLabeledValue(option) ? option.value : option; } +export function equalValues(a: V | LabeledValue, b: V | LabeledValue) { + const actualA = isObject(a) && 'value' in a ? a.value : a; + const actualB = isObject(b) && 'value' in b ? b.value : b; + // When comparing the values we use the equality + // operator to automatically convert different types + // eslint-disable-next-line eqeqeq + return actualA == actualB; +} + export function getOption( value: V, options?: V | LabeledValue | (V | LabeledValue)[], checkLabel = false, ): V | LabeledValue { const optionsArray = ensureIsArray(options); - // When comparing the values we use the equality - // operator to automatically convert different types return optionsArray.find( x => - // eslint-disable-next-line eqeqeq - x == value || - (isObject(x) && - // eslint-disable-next-line eqeqeq - (('value' in x && x.value == value) || - (checkLabel && 'label' in x && x.label === value))), + equalValues(x, value) || + (checkLabel && isObject(x) && 'label' in x && x.label === value),",
    "comment": "do you think it makes sense to extend equalvalues to accept \"key\" argument, so you could call equalvalues(x, checklabel, \"label\") here?",
    "line_number": 70,
    "enriched": "File: superset-frontend/src/components/Select/utils.tsx\nCode: @@ -49,22 +49,25 @@ export function getValue(\n   return isLabeledValue(option) ? option.value : option;\n }\n \n+export function equalValues(a: V | LabeledValue, b: V | LabeledValue) {\n+  const actualA = isObject(a) && 'value' in a ? a.value : a;\n+  const actualB = isObject(b) && 'value' in b ? b.value : b;\n+  // When comparing the values we use the equality\n+  // operator to automatically convert different types\n+  // eslint-disable-next-line eqeqeq\n+  return actualA == actualB;\n+}\n+\n export function getOption(\n   value: V,\n   options?: V | LabeledValue | (V | LabeledValue)[],\n   checkLabel = false,\n ): V | LabeledValue {\n   const optionsArray = ensureIsArray(options);\n-  // When comparing the values we use the equality\n-  // operator to automatically convert different types\n   return optionsArray.find(\n     x =>\n-      // eslint-disable-next-line eqeqeq\n-      x == value ||\n-      (isObject(x) &&\n-        // eslint-disable-next-line eqeqeq\n-        (('value' in x && x.value == value) ||\n-          (checkLabel && 'label' in x && x.label === value))),\n+      equalValues(x, value) ||\n+      (checkLabel && isObject(x) && 'label' in x && x.label === value),\nComment: Do you think it makes sense to extend `equalValues` to accept \"key\" argument, so you could call `equalValues(x, checkLabel, \"label\")` here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "superset-frontend/src/components/Select/utils.tsx",
    "pr_number": 27706,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1542927641,
    "comment_created_at": "2024-03-28T13:03:09Z"
  },
  {
    "code": "@@ -38,6 +38,7 @@ Bug fixes - Fixed bug in :meth:`DataFrame.stack` with ``future_stack=True`` and columns a non-:class:`MultiIndex` consisting of tuples (:issue:`54948`) - Fixed bug in :meth:`Series.dt.tz` with :class:`ArrowDtype` where a string was returned instead of a ``tzinfo`` object (:issue:`55003`) - Fixed bug in :meth:`Series.pct_change` and :meth:`DataFrame.pct_change` showing unnecessary ``FutureWarning`` (:issue:`54981`) +- Fixed bug where using the melt method would not preserve the datetime (:issue:`55254`)",
    "comment": "can you move this to 2.2.0.rst?",
    "line_number": 41,
    "enriched": "File: doc/source/whatsnew/v2.1.1.rst\nCode: @@ -38,6 +38,7 @@ Bug fixes\n - Fixed bug in :meth:`DataFrame.stack` with ``future_stack=True`` and columns a non-:class:`MultiIndex` consisting of tuples (:issue:`54948`)\n - Fixed bug in :meth:`Series.dt.tz` with :class:`ArrowDtype` where a string was returned instead of a ``tzinfo`` object (:issue:`55003`)\n - Fixed bug in :meth:`Series.pct_change` and :meth:`DataFrame.pct_change` showing unnecessary ``FutureWarning`` (:issue:`54981`)\n+- Fixed bug where using the melt method would not preserve the datetime (:issue:`55254`)\nComment: Can you move this to 2.2.0.rst?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "doc/source/whatsnew/v2.1.1.rst",
    "pr_number": 55270,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 1336245778,
    "comment_created_at": "2023-09-25T18:23:57Z"
  },
    {
    "code": "@@ -0,0 +1 @@\n+Add :func:`os.reload_environ` to `__all__`",
    "comment": "```suggestion\nAdd :func:`os.reload_environ` to ``os.__all__``.\n```\nYou need to use double backticks.",
    "line_number": 1,
    "enriched": "File: Misc/NEWS.d/next/Library/2025-10-29-16-12-41.gh-issue-None.qGj5Dl.rst\nCode: @@ -0,0 +1 @@\n+Add :func:`os.reload_environ` to `__all__`\nComment: ```suggestion\nAdd :func:`os.reload_environ` to ``os.__all__``.\n```\nYou need to use double backticks.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "Misc/NEWS.d/next/Library/2025-10-29-16-12-41.gh-issue-None.qGj5Dl.rst",
    "pr_number": 140763,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2474116054,
    "comment_created_at": "2025-10-29T16:14:39Z"
  },
  {
    "code": "@@ -204,11 +204,15 @@ def test_ForkingUnixStreamServer(self):\n                             socketserver.StreamRequestHandler,\n                             self.stream_examine)\n \n+    @unittest.skipIf(test.support.is_apple_mobile and test.support.on_github_actions,\n+                     \"Test fails regularly on iOS simulator on Github Actions - See #140702\")",
    "comment": "```suggestion\r\n                     \"gh-140702: Test fails regularly on iOS simulator on GitHub Actions\")\r\n```",
    "line_number": 208,
    "enriched": "File: Lib/test/test_socketserver.py\nCode: @@ -204,11 +204,15 @@ def test_ForkingUnixStreamServer(self):\n                             socketserver.StreamRequestHandler,\n                             self.stream_examine)\n \n+    @unittest.skipIf(test.support.is_apple_mobile and test.support.on_github_actions,\n+                     \"Test fails regularly on iOS simulator on Github Actions - See #140702\")\nComment: ```suggestion\r\n                     \"gh-140702: Test fails regularly on iOS simulator on GitHub Actions\")\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "Lib/test/test_socketserver.py",
    "pr_number": 140740,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2471965742,
    "comment_created_at": "2025-10-29T07:20:09Z"
  },
  {
    "code": "@@ -638,6 +638,17 @@ by setting ``color`` to ``False``::\n    ...                     help='an integer for the accumulator')\n    >>> parser.parse_args(['--help'])\n \n+Note that the environment variables take precedence over ``color=True``, but\n+``color=False`` takes precedence over the environment variables.  For\n+instance, if the environment variable ``NO_COLOR`` is set then there will be\n+no colored output even if the ``color`` parameter is explicitly set to\n+``True``.  If the environment variable ``FORCE_COLOR`` is set then colored\n+output will be depending on the ``color`` parameter.  If both are set then\n+``NO_COLOR`` takes precedence.\n+\n+See :ref:`controlling color <using-on-controlling-color>` for more information\n+on controlling colored output globally.",
    "comment": "We can remove this bit since it's actually listed above on line 630.",
    "line_number": 650,
    "enriched": "File: Doc/library/argparse.rst\nCode: @@ -638,6 +638,17 @@ by setting ``color`` to ``False``::\n    ...                     help='an integer for the accumulator')\n    >>> parser.parse_args(['--help'])\n \n+Note that the environment variables take precedence over ``color=True``, but\n+``color=False`` takes precedence over the environment variables.  For\n+instance, if the environment variable ``NO_COLOR`` is set then there will be\n+no colored output even if the ``color`` parameter is explicitly set to\n+``True``.  If the environment variable ``FORCE_COLOR`` is set then colored\n+output will be depending on the ``color`` parameter.  If both are set then\n+``NO_COLOR`` takes precedence.\n+\n+See :ref:`controlling color <using-on-controlling-color>` for more information\n+on controlling colored output globally.\nComment: We can remove this bit since it's actually listed above on line 630.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "Doc/library/argparse.rst",
    "pr_number": 140737,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2473784743,
    "comment_created_at": "2025-10-29T15:23:58Z"
  },
  {
    "code": "@@ -359,6 +359,7 @@\n     'papersize': 'a4paper',\n     # The font size ('10pt', '11pt' or '12pt').\n     'pointsize': '10pt',\n+    'maxlistdepth': 7,  # for layers in reference/executionmodel.rst#Python Runtime Model, see #gh-139588",
    "comment": "It is documented as being a string, but its use is compatible with being an int:\r\nhttps://github.com/sphinx-doc/sphinx/blob/e347e59ccc27a646c974526252e39a63870f6ea3/sphinx/writers/latex.py#L474-L475\r\n```\r\n        if self.elements['maxlistdepth']:\r\n            sphinxpkgoptions.append('maxlistdepth=%s' % self.elements['maxlistdepth'])\r\n```\r\nIt defaults to the empty string:\r\nhttps://github.com/sphinx-doc/sphinx/blob/e347e59ccc27a646c974526252e39a63870f6ea3/sphinx/builders/latex/constants.py#L80\r\n\r\n```\r\n    'maxlistdepth':    '',\r\n```\r\nPerhaps `'7'` would be better.  Or even `'8'`, it is not very costly on the LaTeX side, and having one extra deeply nested level which is never used will not hurt.  However, beware that upstream LaTeX kernel changes may break the Sphinx hack.  Nothing has changed in 30 years, but in the last few years many core LaTeX things are evolving, sadly they did not decide to change their names from LaTeX2e to LaTeX3.",
    "line_number": 362,
    "enriched": "File: Doc/conf.py\nCode: @@ -359,6 +359,7 @@\n     'papersize': 'a4paper',\n     # The font size ('10pt', '11pt' or '12pt').\n     'pointsize': '10pt',\n+    'maxlistdepth': 7,  # for layers in reference/executionmodel.rst#Python Runtime Model, see #gh-139588\nComment: It is documented as being a string, but its use is compatible with being an int:\r\nhttps://github.com/sphinx-doc/sphinx/blob/e347e59ccc27a646c974526252e39a63870f6ea3/sphinx/writers/latex.py#L474-L475\r\n```\r\n        if self.elements['maxlistdepth']:\r\n            sphinxpkgoptions.append('maxlistdepth=%s' % self.elements['maxlistdepth'])\r\n```\r\nIt defaults to the empty string:\r\nhttps://github.com/sphinx-doc/sphinx/blob/e347e59ccc27a646c974526252e39a63870f6ea3/sphinx/builders/latex/constants.py#L80\r\n\r\n```\r\n    'maxlistdepth':    '',\r\n```\r\nPerhaps `'7'` would be better.  Or even `'8'`, it is not very costly on the LaTeX side, and having one extra deeply nested level which is never used will not hurt.  However, beware that upstream LaTeX kernel changes may break the Sphinx hack.  Nothing has changed in 30 years, but in the last few years many core LaTeX things are evolving, sadly they did not decide to change their names from LaTeX2e to LaTeX3.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "Doc/conf.py",
    "pr_number": 140709,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2470362394,
    "comment_created_at": "2025-10-28T17:04:25Z"
  },
  {
    "code": "@@ -592,6 +592,23 @@ def test_RawIOBase_read(self):\n         self.assertEqual(rawio.read(2), None)\n         self.assertEqual(rawio.read(2), b\"\")\n \n+    def test_RawIOBase_read_bounds_checking(self):\n+        # Make sure a `.readinto` call which returns a value oustside\n+        # (0, len(buffer)) raises.\n+        class Misbehaved(io.RawIOBase):\n+            def __init__(self, readinto_return) -> None:\n+                self._readinto_return = readinto_return\n+            def readinto(self, b):\n+                return self._readinto_return\n+\n+        with self.assertRaises(ValueError) as cm:",
    "comment": "Maybe `self.assertRaisesRegex` approach will be better?",
    "line_number": 604,
    "enriched": "File: Lib/test/test_io/test_general.py\nCode: @@ -592,6 +592,23 @@ def test_RawIOBase_read(self):\n         self.assertEqual(rawio.read(2), None)\n         self.assertEqual(rawio.read(2), b\"\")\n \n+    def test_RawIOBase_read_bounds_checking(self):\n+        # Make sure a `.readinto` call which returns a value oustside\n+        # (0, len(buffer)) raises.\n+        class Misbehaved(io.RawIOBase):\n+            def __init__(self, readinto_return) -> None:\n+                self._readinto_return = readinto_return\n+            def readinto(self, b):\n+                return self._readinto_return\n+\n+        with self.assertRaises(ValueError) as cm:\nComment: Maybe `self.assertRaisesRegex` approach will be better?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "Lib/test/test_io/test_general.py",
    "pr_number": 140611,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2463738856,
    "comment_created_at": "2025-10-26T11:13:16Z"
  },
  {
    "code": "@@ -718,6 +718,24 @@ def test_expaterror(self):\n             self.assertEqual(e.code,\n                              errors.codes[errors.XML_ERROR_UNCLOSED_TOKEN])\n \n+class ElementDeclHandlerCleanUpLeakTest(unittest.TestCase):",
    "comment": "Can you change this class name to something like \"ElementDeclHandlerTest\" and put it under the `ChardataBufferTest` class? That way we group handler tests as much as possible and we can also add in the future more specific tests for the handler.",
    "line_number": 721,
    "enriched": "File: Lib/test/test_pyexpat.py\nCode: @@ -718,6 +718,24 @@ def test_expaterror(self):\n             self.assertEqual(e.code,\n                              errors.codes[errors.XML_ERROR_UNCLOSED_TOKEN])\n \n+class ElementDeclHandlerCleanUpLeakTest(unittest.TestCase):\nComment: Can you change this class name to something like \"ElementDeclHandlerTest\" and put it under the `ChardataBufferTest` class? That way we group handler tests as much as possible and we can also add in the future more specific tests for the handler.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "Lib/test/test_pyexpat.py",
    "pr_number": 140602,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2463668611,
    "comment_created_at": "2025-10-26T10:24:13Z"
  },
  {
    "code": "@@ -1601,6 +1601,21 @@ def __hash__(self):\n         with self.assertRaises(KeyError):\n             d.get(key2)\n \n+    def test_clear_at_lookup(self):\n+        d = {}\n+\n+        class X(object):\n+            def __hash__(self):\n+                return 1\n+            def __eq__(self, other):\n+                nonlocal d\n+                d.clear()\n+\n+        for _ in range(10):",
    "comment": "I believe the following code will be a bit clearer because it shows the failed second assignment:\r\n```\r\nd[X()] = None\r\nd[X()] = None\r\n```",
    "line_number": 1614,
    "enriched": "File: Lib/test/test_dict.py\nCode: @@ -1601,6 +1601,21 @@ def __hash__(self):\n         with self.assertRaises(KeyError):\n             d.get(key2)\n \n+    def test_clear_at_lookup(self):\n+        d = {}\n+\n+        class X(object):\n+            def __hash__(self):\n+                return 1\n+            def __eq__(self, other):\n+                nonlocal d\n+                d.clear()\n+\n+        for _ in range(10):\nComment: I believe the following code will be a bit clearer because it shows the failed second assignment:\r\n```\r\nd[X()] = None\r\nd[X()] = None\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "Lib/test/test_dict.py",
    "pr_number": 140558,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2461399700,
    "comment_created_at": "2025-10-24T17:35:00Z"
  },
  {
    "code": "@@ -271,12 +262,12 @@ _bz2_BZ2Compressor_compress_impl(BZ2Compressor *self, Py_buffer *data)\n {\n     PyObject *result = NULL;\n \n-    ACQUIRE_LOCK(self);",
    "comment": "How about simply changing the definition of `ACQUIRE_LOCK` and `RELEASE_LOCK`? This will reduce the diff a bit (we also do this kind of stuff in hashlib)",
    "line_number": 274,
    "enriched": "File: Modules/_bz2module.c\nCode: @@ -271,12 +262,12 @@ _bz2_BZ2Compressor_compress_impl(BZ2Compressor *self, Py_buffer *data)\n {\n     PyObject *result = NULL;\n \n-    ACQUIRE_LOCK(self);\nComment: How about simply changing the definition of `ACQUIRE_LOCK` and `RELEASE_LOCK`? This will reduce the diff a bit (we also do this kind of stuff in hashlib)",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "Modules/_bz2module.c",
    "pr_number": 140555,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2461267762,
    "comment_created_at": "2025-10-24T16:48:57Z"
  },
  {
    "code": "@@ -4079,30 +4079,43 @@ _asyncio_all_tasks_impl(PyObject *module, PyObject *loop)\n         return NULL;\n     }\n \n-    PyInterpreterState *interp = PyInterpreterState_Get();\n-    // Stop the world and traverse the per-thread linked list\n-    // of asyncio tasks for every thread, as well as the\n-    // interpreter's linked list, and add them to `tasks`.\n-    // The interpreter linked list is used for any lingering tasks\n-    // whose thread state has been deallocated while the task was\n-    // still alive. This can happen if a task is referenced by\n-    // a different thread, in which case the task is moved to\n-    // the interpreter's linked list from the thread's linked\n-    // list before deallocation. See PyThreadState_Clear.\n-    //\n-    // The stop-the-world pause is required so that no thread\n-    // modifies its linked list while being iterated here\n-    // in parallel. This design allows for lock-free\n-    // register_task/unregister_task for loops running in parallel\n-    // in different threads (the general case).\n-    _PyEval_StopTheWorld(interp);\n-    int ret = add_tasks_interp(interp, (PyListObject *)tasks);\n-    _PyEval_StartTheWorld(interp);\n-    if (ret < 0) {\n-        // call any escaping calls after starting the world to avoid any deadlocks.\n-        Py_DECREF(tasks);\n-        Py_DECREF(loop);\n-        return NULL;\n+    _PyThreadStateImpl *ts = (_PyThreadStateImpl *)_PyThreadState_GET();\n+    if (ts->asyncio_running_loop == loop) {\n+        // Fast path for the current running loop of current thread\n+        // no locking or stop the world pause is required\n+        struct llist_node *head = &ts->asyncio_tasks_head;\n+        if (add_tasks_llist(head, (PyListObject *)tasks) < 0) {\n+            Py_DECREF(tasks);\n+            Py_DECREF(loop);\n+            return NULL;\n+        }\n+    } else {\n+        // Slow path for loop running in different thread\n+        PyInterpreterState *interp = PyInterpreterState_Get();",
    "comment": "It'll be faster to access `ts->interp` rather than look up the thread-local again.",
    "line_number": 4094,
    "enriched": "File: Modules/_asynciomodule.c\nCode: @@ -4079,30 +4079,43 @@ _asyncio_all_tasks_impl(PyObject *module, PyObject *loop)\n         return NULL;\n     }\n \n-    PyInterpreterState *interp = PyInterpreterState_Get();\n-    // Stop the world and traverse the per-thread linked list\n-    // of asyncio tasks for every thread, as well as the\n-    // interpreter's linked list, and add them to `tasks`.\n-    // The interpreter linked list is used for any lingering tasks\n-    // whose thread state has been deallocated while the task was\n-    // still alive. This can happen if a task is referenced by\n-    // a different thread, in which case the task is moved to\n-    // the interpreter's linked list from the thread's linked\n-    // list before deallocation. See PyThreadState_Clear.\n-    //\n-    // The stop-the-world pause is required so that no thread\n-    // modifies its linked list while being iterated here\n-    // in parallel. This design allows for lock-free\n-    // register_task/unregister_task for loops running in parallel\n-    // in different threads (the general case).\n-    _PyEval_StopTheWorld(interp);\n-    int ret = add_tasks_interp(interp, (PyListObject *)tasks);\n-    _PyEval_StartTheWorld(interp);\n-    if (ret < 0) {\n-        // call any escaping calls after starting the world to avoid any deadlocks.\n-        Py_DECREF(tasks);\n-        Py_DECREF(loop);\n-        return NULL;\n+    _PyThreadStateImpl *ts = (_PyThreadStateImpl *)_PyThreadState_GET();\n+    if (ts->asyncio_running_loop == loop) {\n+        // Fast path for the current running loop of current thread\n+        // no locking or stop the world pause is required\n+        struct llist_node *head = &ts->asyncio_tasks_head;\n+        if (add_tasks_llist(head, (PyListObject *)tasks) < 0) {\n+            Py_DECREF(tasks);\n+            Py_DECREF(loop);\n+            return NULL;\n+        }\n+    } else {\n+        // Slow path for loop running in different thread\n+        PyInterpreterState *interp = PyInterpreterState_Get();\nComment: It'll be faster to access `ts->interp` rather than look up the thread-local again.",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "Modules/_asynciomodule.c",
    "pr_number": 140542,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2460418451,
    "comment_created_at": "2025-10-24T13:17:40Z"
  },
    {
    "code": "@@ -4,6 +4,7 @@\n import sys\n import threading\n \n+from importlib import import_module",
    "comment": "Nit, re: where the whitespace goes\n\n```suggestion\nfrom importlib import import_module\n\n```\n\nI see the existing sections as stdlib, test, and local rather than full imports, from imports, and local.",
    "line_number": 7,
    "enriched": "File: Lib/test/libregrtest/save_env.py\nCode: @@ -4,6 +4,7 @@\n import sys\n import threading\n \n+from importlib import import_module\nComment: Nit, re: where the whitespace goes\n\n```suggestion\nfrom importlib import import_module\n\n```\n\nI see the existing sections as stdlib, test, and local rather than full imports, from imports, and local.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "Lib/test/libregrtest/save_env.py",
    "pr_number": 140519,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2458518605,
    "comment_created_at": "2025-10-24T02:49:17Z"
  },
  {
    "code": "@@ -0,0 +1,73 @@\n+{\n+    \"image\": \"ghcr.io/python/wasicontainer:latest\",\n+    \"onCreateCommand\": [\n+        // Install common tooling.\n+        \"dnf\",\n+        \"install\",\n+        \"-y\",\n+        // For umask fix below.\n+        \"/usr/bin/setfacl\"\n+    ],\n+    \"updateContentCommand\": {\n+        // Using the shell for `nproc` usage.\n+        \"python\": \"python3 Tools/wasm/wasi build --quiet -- --with-pydebug -C\"",
    "comment": "For a second, I was confused as to why we didn't have `-j` here, but then discovered that we're doing it via script  ",
    "line_number": 13,
    "enriched": "File: .devcontainer/wasi/devcontainer.json\nCode: @@ -0,0 +1,73 @@\n+{\n+    \"image\": \"ghcr.io/python/wasicontainer:latest\",\n+    \"onCreateCommand\": [\n+        // Install common tooling.\n+        \"dnf\",\n+        \"install\",\n+        \"-y\",\n+        // For umask fix below.\n+        \"/usr/bin/setfacl\"\n+    ],\n+    \"updateContentCommand\": {\n+        // Using the shell for `nproc` usage.\n+        \"python\": \"python3 Tools/wasm/wasi build --quiet -- --with-pydebug -C\"\nComment: For a second, I was confused as to why we didn't have `-j` here, but then discovered that we're doing it via script  ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".devcontainer/wasi/devcontainer.json",
    "pr_number": 140473,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2453489573,
    "comment_created_at": "2025-10-22T22:25:53Z"
  },
  {
    "code": "@@ -0,0 +1,4 @@\n+Use a fused multiplyadd (fma) in math.log helper to compute the",
    "comment": "IMO this is an implementation detail that does not need to be documented.",
    "line_number": 1,
    "enriched": "File: Misc/NEWS.d/next/Core_and_Builtins/2025-10-22-23-26-37.gh-issue-140443.wT5i1A.rst\nCode: @@ -0,0 +1,4 @@\n+Use a fused multiplyadd (fma) in math.log helper to compute the\nComment: IMO this is an implementation detail that does not need to be documented.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "Misc/NEWS.d/next/Core_and_Builtins/2025-10-22-23-26-37.gh-issue-140443.wT5i1A.rst",
    "pr_number": 140469,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2452823184,
    "comment_created_at": "2025-10-22T17:31:59Z"
  },
  {
    "code": "@@ -612,16 +613,15 @@ are strings::\n    >>> parser.parse_args(['--action', 'sumn', 1, 2, 3])\n    tester.py: error: argument --action: invalid choice: 'sumn', maybe you meant 'sum'? (choose from 'sum', 'max')\n \n-If you're writing code that needs to be compatible with older Python versions\n-and want to opportunistically use ``suggest_on_error`` when it's available, you\n-can set it as an attribute after initializing the parser instead of using the\n-keyword argument::\n+If you prefer the old behavior without suggestions, you can disable this feature\n+by setting ``suggest_on_error`` to ``False``::",
    "comment": "```suggestion\r\nYou can disable suggestions by setting ``suggest_on_error`` to ``False``::\r\n```",
    "line_number": 617,
    "enriched": "File: Doc/library/argparse.rst\nCode: @@ -612,16 +613,15 @@ are strings::\n    >>> parser.parse_args(['--action', 'sumn', 1, 2, 3])\n    tester.py: error: argument --action: invalid choice: 'sumn', maybe you meant 'sum'? (choose from 'sum', 'max')\n \n-If you're writing code that needs to be compatible with older Python versions\n-and want to opportunistically use ``suggest_on_error`` when it's available, you\n-can set it as an attribute after initializing the parser instead of using the\n-keyword argument::\n+If you prefer the old behavior without suggestions, you can disable this feature\n+by setting ``suggest_on_error`` to ``False``::\nComment: ```suggestion\r\nYou can disable suggestions by setting ``suggest_on_error`` to ``False``::\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "Doc/library/argparse.rst",
    "pr_number": 140450,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2452335572,
    "comment_created_at": "2025-10-22T14:38:19Z"
  },
  {
    "code": "@@ -232,4 +232,4 @@ This Python distribution contains *no* GNU General Public License (GPL) code,\n so it may be used in proprietary projects.  There are interfaces to some GNU\n code but these are entirely optional.\n \n-All trademarks referenced herein are property of their respective holders.\n\\ No newline at end of file\n+All trademarks referenced herein are property of their respective holders.",
    "comment": "My editor added a trailing blank line here, seems to have been (accidentally?) removed in 29af6cee02fbe74d59c6d725a506fe60c77d37d6 (cc @hugovk).",
    "line_number": 235,
    "enriched": "File: README.rst\nCode: @@ -232,4 +232,4 @@ This Python distribution contains *no* GNU General Public License (GPL) code,\n so it may be used in proprietary projects.  There are interfaces to some GNU\n code but these are entirely optional.\n \n-All trademarks referenced herein are property of their respective holders.\n\\ No newline at end of file\n+All trademarks referenced herein are property of their respective holders.\nComment: My editor added a trailing blank line here, seems to have been (accidentally?) removed in 29af6cee02fbe74d59c6d725a506fe60c77d37d6 (cc @hugovk).",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "README.rst",
    "pr_number": 140416,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2447916830,
    "comment_created_at": "2025-10-21T12:06:46Z"
  },
  {
    "code": "@@ -68,7 +68,7 @@ jobs:\n           - true\n           - false\n         llvm:\n-          - 19\n+          - 20",
    "comment": "Really not major.. can we stick the version in a variable?",
    "line_number": 71,
    "enriched": "File: .github/workflows/jit.yml\nCode: @@ -68,7 +68,7 @@ jobs:\n           - true\n           - false\n         llvm:\n-          - 19\n+          - 20\nComment: Really not major.. can we stick the version in a variable?",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": ".github/workflows/jit.yml",
    "pr_number": 140329,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2443518315,
    "comment_created_at": "2025-10-19T22:13:49Z"
  },
   {
    "code": "@@ -784,8 +784,7 @@ def plural(n):\n \n     def total_seconds(self):\n         \"\"\"Total seconds in the duration.\"\"\"\n-        return ((self.days * 86400 + self.seconds) * 10**6 +\n-                self.microseconds) / 10**6\n+        return self.days * 86400 + self.seconds + self.microseconds / 1000000",
    "comment": "There are going to be rounding differences now, no?",
    "line_number": 787,
    "enriched": "File: Lib/_pydatetime.py\nCode: @@ -784,8 +784,7 @@ def plural(n):\n \n     def total_seconds(self):\n         \"\"\"Total seconds in the duration.\"\"\"\n-        return ((self.days * 86400 + self.seconds) * 10**6 +\n-                self.microseconds) / 10**6\n+        return self.days * 86400 + self.seconds + self.microseconds / 1000000\nComment: There are going to be rounding differences now, no?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "Lib/_pydatetime.py",
    "pr_number": 140305,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2442533816,
    "comment_created_at": "2025-10-18T17:57:01Z"
  },
  {
    "code": "@@ -1559,6 +1559,20 @@ def test_indirect_calls_with_gc_disabled(self):\n         finally:\n             gc.enable()\n \n+    # Ensure that setting *threshold0* to zero disables collection.\n+    @gc_threshold(0, 0, 0)",
    "comment": "It is better to set `gc_threshold(0)` to keep other values defaults.",
    "line_number": 1563,
    "enriched": "File: Lib/test/test_gc.py\nCode: @@ -1559,6 +1559,20 @@ def test_indirect_calls_with_gc_disabled(self):\n         finally:\n             gc.enable()\n \n+    # Ensure that setting *threshold0* to zero disables collection.\n+    @gc_threshold(0, 0, 0)\nComment: It is better to set `gc_threshold(0)` to keep other values defaults.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "Lib/test/test_gc.py",
    "pr_number": 140304,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2442506733,
    "comment_created_at": "2025-10-18T17:12:29Z"
  },
  {
    "code": "@@ -2505,6 +2507,7 @@ new_interpreter(PyThreadState **tstate_p,\n     if (tstate != NULL) {\n         Py_EndInterpreter(tstate);\n     } else if (interp != NULL) {\n+        PyConfig_Clear(&interp->config);",
    "comment": "Why not call this in `PyInterpreterState_Delete` itself?",
    "line_number": 2510,
    "enriched": "File: Python/pylifecycle.c\nCode: @@ -2505,6 +2507,7 @@ new_interpreter(PyThreadState **tstate_p,\n     if (tstate != NULL) {\n         Py_EndInterpreter(tstate);\n     } else if (interp != NULL) {\n+        PyConfig_Clear(&interp->config);\nComment: Why not call this in `PyInterpreterState_Delete` itself?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "Python/pylifecycle.c",
    "pr_number": 140303,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2442469160,
    "comment_created_at": "2025-10-18T15:37:51Z"
  },
    {
    "code": "@@ -1965,3 +1962,12 @@ def run(cmd, args):\n ''' % sys.argv[0])\n \n         raise\n+\n+\n+def __getattr__(name):\n+    if name == \"__version__\":\n+        from warnings import _deprecated\n+\n+        _deprecated(\"__version__\", remove=(3, 20))\n+        return \"2.50\"  # Do not change",
    "comment": "```suggestion\r\n        return \"2.60\"  # Do not change\r\n```",
    "line_number": 1972,
    "enriched": "File: Lib/imaplib.py\nCode: @@ -1965,3 +1962,12 @@ def run(cmd, args):\n ''' % sys.argv[0])\n \n         raise\n+\n+\n+def __getattr__(name):\n+    if name == \"__version__\":\n+        from warnings import _deprecated\n+\n+        _deprecated(\"__version__\", remove=(3, 20))\n+        return \"2.50\"  # Do not change\nComment: ```suggestion\r\n        return \"2.60\"  # Do not change\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "Lib/imaplib.py",
    "pr_number": 140299,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2442603920,
    "comment_created_at": "2025-10-18T21:14:26Z"
  },
  {
    "code": "@@ -1406,6 +1406,7 @@ class TestDumbTerminal(ReplTestCase):\n     def test_dumb_terminal_exits_cleanly(self):\n         env = os.environ.copy()\n         env.pop('PYTHON_BASIC_REPL', None)\n+        env.pop('PYTHONSTARTUP', None)",
    "comment": "I think it would be useful to include a comment here explaining why.",
    "line_number": 1409,
    "enriched": "File: Lib/test/test_pyrepl/test_pyrepl.py\nCode: @@ -1406,6 +1406,7 @@ class TestDumbTerminal(ReplTestCase):\n     def test_dumb_terminal_exits_cleanly(self):\n         env = os.environ.copy()\n         env.pop('PYTHON_BASIC_REPL', None)\n+        env.pop('PYTHONSTARTUP', None)\nComment: I think it would be useful to include a comment here explaining why.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "Lib/test/test_pyrepl/test_pyrepl.py",
    "pr_number": 140295,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2442424257,
    "comment_created_at": "2025-10-18T13:28:39Z"
  },
  {
    "code": "@@ -0,0 +1 @@\n+Fix memory leak in ``_gdbm.gdbm.clear`` by properly freeing memory allocated by ``gdbm_firstkey()``.",
    "comment": "```suggestion\r\nFix memory leak in the :meth:`clear` method of the :mod:`dbm.gnu` database.\r\n```\r\n`gdbm_firstkey()` and `_gdbm.gdbm` are implementation details not known to common Python user. The NEWS entry can be formulated in understandable terms.\r\n\r\nThe commit message can be more core developer oriented.",
    "line_number": 1,
    "enriched": "File: Misc/NEWS.d/next/Core_and_Builtins/2025-10-17-23-58-11.gh-issue-140272.lhY8uS.rst\nCode: @@ -0,0 +1 @@\n+Fix memory leak in ``_gdbm.gdbm.clear`` by properly freeing memory allocated by ``gdbm_firstkey()``.\nComment: ```suggestion\r\nFix memory leak in the :meth:`clear` method of the :mod:`dbm.gnu` database.\r\n```\r\n`gdbm_firstkey()` and `_gdbm.gdbm` are implementation details not known to common Python user. The NEWS entry can be formulated in understandable terms.\r\n\r\nThe commit message can be more core developer oriented.",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "Misc/NEWS.d/next/Core_and_Builtins/2025-10-17-23-58-11.gh-issue-140272.lhY8uS.rst",
    "pr_number": 140274,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2441851029,
    "comment_created_at": "2025-10-18T08:52:11Z"
  },
  {
    "code": "@@ -252,7 +252,16 @@ def testSyntaxErrorOffset(self):\n         check('[\\nfile\\nfor str(file)\\nin\\n[]\\n]', 3, 5)\n         check('[file for\\n str(file) in []]', 2, 2)\n         check(\"ages = {'Alice'=22, 'Bob'=23}\", 1, 9)\n-        check('match ...:\\n    case {**rest, \"key\": value}:\\n        ...', 2, 19)\n+        check(dedent(\"\"\"\\\n+          match ...:\n+            case {**rest1, \"after\": after}:\n+              ...\n+        \"\"\"), 2, 11)\n+        check(dedent(\"\"\"\\\n+          match ...:\n+            case {\"before\": before, **rest2, \"after\": after}:\n+              ...\n+        \"\"\"), 2, 29)",
    "comment": "I made it multi-line with dedent so it's easier to read--these tests were missing a case where some items were placed before the double-star pattern.",
    "line_number": 264,
    "enriched": "File: Lib/test/test_exceptions.py\nCode: @@ -252,7 +252,16 @@ def testSyntaxErrorOffset(self):\n         check('[\\nfile\\nfor str(file)\\nin\\n[]\\n]', 3, 5)\n         check('[file for\\n str(file) in []]', 2, 2)\n         check(\"ages = {'Alice'=22, 'Bob'=23}\", 1, 9)\n-        check('match ...:\\n    case {**rest, \"key\": value}:\\n        ...', 2, 19)\n+        check(dedent(\"\"\"\\\n+          match ...:\n+            case {**rest1, \"after\": after}:\n+              ...\n+        \"\"\"), 2, 11)\n+        check(dedent(\"\"\"\\\n+          match ...:\n+            case {\"before\": before, **rest2, \"after\": after}:\n+              ...\n+        \"\"\"), 2, 29)\nComment: I made it multi-line with dedent so it's easier to read--these tests were missing a case where some items were placed before the double-star pattern.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "Lib/test/test_exceptions.py",
    "pr_number": 140254,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2439850538,
    "comment_created_at": "2025-10-17T12:43:22Z"
  },
  {
    "code": "@@ -172,9 +172,12 @@ def remove_from(alist, to_remove):\n         \"folder\": \"build_tools/azure\",\n         \"platform\": \"linux-64\",\n         \"channels\": [\"conda-forge\"],\n-        \"conda_dependencies\": common_dependencies + [\"ccache\", \"polars\", \"pyarrow\"],\n+        \"conda_dependencies\": remove_from(common_dependencies, [\"pandas\"])\n+        + [\"ccache\", \"polars\", \"pyarrow\"],\n+        # TODO: for pandas > 1.5.0 move to conda_dependencies:",
    "comment": "Maybe something like this?\n```suggestion\n        # TODO: move pandas to conda_dependencies when pandas 2.0.0 is the minimum supported version\n```\nBasically pandas > 1.5.0 could mean pandas 1.5.1 and pandas 1.5.1 with python 3.11 is not on conda-forge",
    "line_number": 177,
    "enriched": "File: build_tools/update_environments_and_lock_files.py\nCode: @@ -172,9 +172,12 @@ def remove_from(alist, to_remove):\n         \"folder\": \"build_tools/azure\",\n         \"platform\": \"linux-64\",\n         \"channels\": [\"conda-forge\"],\n-        \"conda_dependencies\": common_dependencies + [\"ccache\", \"polars\", \"pyarrow\"],\n+        \"conda_dependencies\": remove_from(common_dependencies, [\"pandas\"])\n+        + [\"ccache\", \"polars\", \"pyarrow\"],\n+        # TODO: for pandas > 1.5.0 move to conda_dependencies:\nComment: Maybe something like this?\n```suggestion\n        # TODO: move pandas to conda_dependencies when pandas 2.0.0 is the minimum supported version\n```\nBasically pandas > 1.5.0 could mean pandas 1.5.1 and pandas 1.5.1 with python 3.11 is not on conda-forge",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "build_tools/update_environments_and_lock_files.py",
    "pr_number": 32530,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2445219968,
    "comment_created_at": "2025-10-20T14:39:00Z"
  },
  {
    "code": "@@ -1,21 +1,28 @@\n import argparse\n+import logging\n import os\n import subprocess\n-import warnings\n+\n+logger = logging.getLogger(__name__)\n \n \n def get_commit_message():\n     \"\"\"Retrieve the commit message.\"\"\"\n-    build_source_version_message = os.environ.get(\"BUILD_SOURCEVERSIONMESSAGE\")\n-    if build_source_version_message is None:\n-        # We are not on Azure: behaviour based on commit-message is not\n-        # supported for now.\n-        # TODO: this should be implemented at one point for GHA.\n-        warnings.warn(\n-            \"get_commit_message not supported outside Azure for now, \"\n-            \"returning empty commit message\"\n+\n+    if \"COMMIT_MESSAGE\" in os.environ or os.environ.get(\"GITHUB_ACTIONS\", False):",
    "comment": "I would do something like, wording improvements more than welcome.\n```py\nif \"BUILD_SOURCEVERSIONMESSAGE\" not is os.environ:\n    raise RuntimeError(\"This legacy script should only be used on Azure. On GitHub actions, use the 'COMMIT_MESSAGE' environment variable\")\n```\n\n",
    "line_number": 12,
    "enriched": "File: build_tools/azure/get_commit_message.py\nCode: @@ -1,21 +1,28 @@\n import argparse\n+import logging\n import os\n import subprocess\n-import warnings\n+\n+logger = logging.getLogger(__name__)\n \n \n def get_commit_message():\n     \"\"\"Retrieve the commit message.\"\"\"\n-    build_source_version_message = os.environ.get(\"BUILD_SOURCEVERSIONMESSAGE\")\n-    if build_source_version_message is None:\n-        # We are not on Azure: behaviour based on commit-message is not\n-        # supported for now.\n-        # TODO: this should be implemented at one point for GHA.\n-        warnings.warn(\n-            \"get_commit_message not supported outside Azure for now, \"\n-            \"returning empty commit message\"\n+\n+    if \"COMMIT_MESSAGE\" in os.environ or os.environ.get(\"GITHUB_ACTIONS\", False):\nComment: I would do something like, wording improvements more than welcome.\n```py\nif \"BUILD_SOURCEVERSIONMESSAGE\" not is os.environ:\n    raise RuntimeError(\"This legacy script should only be used on Azure. On GitHub actions, use the 'COMMIT_MESSAGE' environment variable\")\n```\n\n",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "build_tools/azure/get_commit_message.py",
    "pr_number": 32505,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2453959865,
    "comment_created_at": "2025-10-23T05:29:06Z"
  },
  {
    "code": "@@ -41,11 +42,28 @@ jobs:\n           pip install ninja meson scipy\n           python build_tools/check-meson-openmp-dependencies.py\n \n+  retrieve-commit-message:\n+    name: Retrieve the latest commit message\n+    runs-on: ubuntu-latest\n+    if: github.repository == 'scikit-learn/scikit-learn'\n+    outputs:\n+      message: ${{ steps.git-log.outputs.message }}\n+    steps:\n+    - uses: actions/checkout@v5\n+      with:\n+        ref: ${{ env.SHA }}\n+    - id: git-log\n+      shell: bash\n+      run: |\n+        set -eu\n+        message=$(git log --format=%B -n 1 \"${SHA}\")\n+        echo \"message=${message}\" >> \"${GITHUB_OUTPUT}\"",
    "comment": "I used a separate job to avoid re-running the same task in each run of the matrix `unit-test`.",
    "line_number": 60,
    "enriched": "File: .github/workflows/unit-tests.yml\nCode: @@ -41,11 +42,28 @@ jobs:\n           pip install ninja meson scipy\n           python build_tools/check-meson-openmp-dependencies.py\n \n+  retrieve-commit-message:\n+    name: Retrieve the latest commit message\n+    runs-on: ubuntu-latest\n+    if: github.repository == 'scikit-learn/scikit-learn'\n+    outputs:\n+      message: ${{ steps.git-log.outputs.message }}\n+    steps:\n+    - uses: actions/checkout@v5\n+      with:\n+        ref: ${{ env.SHA }}\n+    - id: git-log\n+      shell: bash\n+      run: |\n+        set -eu\n+        message=$(git log --format=%B -n 1 \"${SHA}\")\n+        echo \"message=${message}\" >> \"${GITHUB_OUTPUT}\"\nComment: I used a separate job to avoid re-running the same task in each run of the matrix `unit-test`.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": ".github/workflows/unit-tests.yml",
    "pr_number": 32501,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2431667600,
    "comment_created_at": "2025-10-15T08:46:07Z"
  },
  {
    "code": "@@ -2279,11 +2256,6 @@ def pairwise_distances_chunked(\n         \"Y\": [\"array-like\", \"sparse matrix\", None],\n         \"metric\": [StrOptions(set(_VALID_METRICS) | {\"precomputed\"}), callable],\n         \"n_jobs\": [Integral, None],\n-        \"force_all_finite\": [\n-            \"boolean\",\n-            StrOptions({\"allow-nan\"}),\n-            Hidden(StrOptions({\"deprecated\"})),\n-        ],\n         \"ensure_all_finite\": [\"boolean\", StrOptions({\"allow-nan\"}), Hidden(None)],",
    "comment": "we can remove None without deprecation. It was only added temporary during the deprecation cycle\n```suggestion\n        \"ensure_all_finite\": [\"boolean\", StrOptions({\"allow-nan\"})],\n```",
    "line_number": 2259,
    "enriched": "File: sklearn/metrics/pairwise.py\nCode: @@ -2279,11 +2256,6 @@ def pairwise_distances_chunked(\n         \"Y\": [\"array-like\", \"sparse matrix\", None],\n         \"metric\": [StrOptions(set(_VALID_METRICS) | {\"precomputed\"}), callable],\n         \"n_jobs\": [Integral, None],\n-        \"force_all_finite\": [\n-            \"boolean\",\n-            StrOptions({\"allow-nan\"}),\n-            Hidden(StrOptions({\"deprecated\"})),\n-        ],\n         \"ensure_all_finite\": [\"boolean\", StrOptions({\"allow-nan\"}), Hidden(None)],\nComment: we can remove None without deprecation. It was only added temporary during the deprecation cycle\n```suggestion\n        \"ensure_all_finite\": [\"boolean\", StrOptions({\"allow-nan\"})],\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "sklearn/metrics/pairwise.py",
    "pr_number": 32452,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2416554389,
    "comment_created_at": "2025-10-09T12:02:19Z"
  },
  {
    "code": "@@ -8,5 +8,9 @@ set -e\n source $HOME/.bashrc\n \n micromamba env create -f build_tools/circle/doc_environment.yml -n sklearn-dev --yes\n+# install ipykernel because it is handy to execute the Jupyter notebook VS Code\n+micromamba install ipykernel -n sklearn-dev --yes\n+# install pre-commit since it will avoid potential failures with the CI\n+micromamba install pre-commit -n sklearn-dev --yes",
    "comment": "Wouldn't it be faster to install the two packages in the same command?",
    "line_number": 14,
    "enriched": "File: .devcontainer/setup.sh\nCode: @@ -8,5 +8,9 @@ set -e\n source $HOME/.bashrc\n \n micromamba env create -f build_tools/circle/doc_environment.yml -n sklearn-dev --yes\n+# install ipykernel because it is handy to execute the Jupyter notebook VS Code\n+micromamba install ipykernel -n sklearn-dev --yes\n+# install pre-commit since it will avoid potential failures with the CI\n+micromamba install pre-commit -n sklearn-dev --yes\nComment: Wouldn't it be faster to install the two packages in the same command?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": ".devcontainer/setup.sh",
    "pr_number": 32342,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2398959597,
    "comment_created_at": "2025-10-02T14:02:14Z"
  },
  {
    "code": "@@ -1,6 +1,6 @@\n from cython cimport final\n \n-from ...utils._typedefs cimport intp_t, float64_t\n+from sklearn.utils._typedefs cimport intp_t, float64_t",
    "comment": "This change is unrelated and already implemented in your previous PR. Please revert it.",
    "line_number": 3,
    "enriched": "File: sklearn/metrics/_pairwise_distances_reduction/_base.pxd.tp\nCode: @@ -1,6 +1,6 @@\n from cython cimport final\n \n-from ...utils._typedefs cimport intp_t, float64_t\n+from sklearn.utils._typedefs cimport intp_t, float64_t\nComment: This change is unrelated and already implemented in your previous PR. Please revert it.",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "sklearn/metrics/_pairwise_distances_reduction/_base.pxd.tp",
    "pr_number": 32324,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2397984153,
    "comment_created_at": "2025-10-02T09:28:35Z"
  },
  {
    "code": "@@ -0,0 +1,3 @@\n+- `estimator_name` is deprecated in favour of `name` in\n+  :func:`metrics.PrecisionRecallDisplay` and will be removed in v1.10.\n+  By :user:`Lucy Liu <lucyleeow>`.",
    "comment": "```suggestion\n- The `estimator_name` parameter is deprecated in favour of `name` in\n  :class:`metrics.PrecisionRecallDisplay` and will be removed in 1.10.\n  By :user:`Lucy Liu <lucyleeow>`.\n```",
    "line_number": 3,
    "enriched": "File: doc/whats_new/upcoming_changes/sklearn.metrics/32310.api.rst\nCode: @@ -0,0 +1,3 @@\n+- `estimator_name` is deprecated in favour of `name` in\n+  :func:`metrics.PrecisionRecallDisplay` and will be removed in v1.10.\n+  By :user:`Lucy Liu <lucyleeow>`.\nComment: ```suggestion\n- The `estimator_name` parameter is deprecated in favour of `name` in\n  :class:`metrics.PrecisionRecallDisplay` and will be removed in 1.10.\n  By :user:`Lucy Liu <lucyleeow>`.\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "doc/whats_new/upcoming_changes/sklearn.metrics/32310.api.rst",
    "pr_number": 32310,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2394604871,
    "comment_created_at": "2025-10-01T13:32:31Z"
  },
  {
    "code": "@@ -1614,12 +1614,23 @@ def test_public_apply_sparse_trees(name, csr_container):\n \n \n def test_decision_path_hardcoded():\n+    # 1st example\n     X = iris.data\n     y = iris.target\n     est = DecisionTreeClassifier(random_state=0, max_depth=1).fit(X, y)\n     node_indicator = est.decision_path(X[:2]).toarray()\n     assert_array_equal(node_indicator, [[1, 1, 0], [1, 0, 1]])\n \n+    # 2nd example (toy dataset)\n+    # was failing before the fix in PR\n+    # https://github.com/scikit-learn/scikit-learn/pull/32280\n+    X = [0, np.nan, np.nan, 2, 3]\n+    y = [0, 0, 0, 1, 1]\n+    X = np.array(X).reshape(-1, 1)\n+    tree = DecisionTreeRegressor().fit(X, y)",
    "comment": "Nit: Just in case there is a stability RNG issue, can you set the `random_state` here?",
    "line_number": 1630,
    "enriched": "File: sklearn/tree/tests/test_tree.py\nCode: @@ -1614,12 +1614,23 @@ def test_public_apply_sparse_trees(name, csr_container):\n \n \n def test_decision_path_hardcoded():\n+    # 1st example\n     X = iris.data\n     y = iris.target\n     est = DecisionTreeClassifier(random_state=0, max_depth=1).fit(X, y)\n     node_indicator = est.decision_path(X[:2]).toarray()\n     assert_array_equal(node_indicator, [[1, 1, 0], [1, 0, 1]])\n \n+    # 2nd example (toy dataset)\n+    # was failing before the fix in PR\n+    # https://github.com/scikit-learn/scikit-learn/pull/32280\n+    X = [0, np.nan, np.nan, 2, 3]\n+    y = [0, 0, 0, 1, 1]\n+    X = np.array(X).reshape(-1, 1)\n+    tree = DecisionTreeRegressor().fit(X, y)\nComment: Nit: Just in case there is a stability RNG issue, can you set the `random_state` here?",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "sklearn/tree/tests/test_tree.py",
    "pr_number": 32280,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2391958299,
    "comment_created_at": "2025-09-30T15:07:13Z"
  },
  {
    "code": "@@ -1494,7 +1494,12 @@ def _get_class_level_metadata_request_values(cls, method: str):\n         # their parents.\n         substr = f\"__metadata_request__{method}\"\n         for base_class in reversed(inspect.getmro(cls)):\n-            for attr, value in vars(base_class).items():\n+            # Copy is needed with free-threaded context to avoid\n+            # RuntimeError: dictionary changed size during iteration.\n+            # copy.deepcopy applied on an instance of base_class adds\n+            # __slotnames__ attribute to base_class.\n+            base_class_items = vars(base_class).copy().items()\n+            for attr, value in base_class_items:",
    "comment": "where's this copy coming from? Where do we have the slot names (other than tags)?\r\n\r\nAlso, this only lowers the probability of encountering the issue I think, since you can still have an issue in the middle of `vars(base_class).copy()`?\r\n\r\nI'd like to understand where the issue actually comes from. `__slotnames__` seems to be just there:\r\n\r\n```py\r\n>>> class Test:\r\n...     pass\r\n...     \r\n>>> a = Test()\r\n>>> a.b = 10\r\n>>> import copy\r\n>>> dir(copy.deepcopy(a))\r\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slotnames__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', 'b']\r\n>>> dir(a)\r\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slotnames__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', 'b']\r\n```\r\n\r\nand not necessarily _added_?",
    "line_number": 1502,
    "enriched": "File: sklearn/utils/_metadata_requests.py\nCode: @@ -1494,7 +1494,12 @@ def _get_class_level_metadata_request_values(cls, method: str):\n         # their parents.\n         substr = f\"__metadata_request__{method}\"\n         for base_class in reversed(inspect.getmro(cls)):\n-            for attr, value in vars(base_class).items():\n+            # Copy is needed with free-threaded context to avoid\n+            # RuntimeError: dictionary changed size during iteration.\n+            # copy.deepcopy applied on an instance of base_class adds\n+            # __slotnames__ attribute to base_class.\n+            base_class_items = vars(base_class).copy().items()\n+            for attr, value in base_class_items:\nComment: where's this copy coming from? Where do we have the slot names (other than tags)?\r\n\r\nAlso, this only lowers the probability of encountering the issue I think, since you can still have an issue in the middle of `vars(base_class).copy()`?\r\n\r\nI'd like to understand where the issue actually comes from. `__slotnames__` seems to be just there:\r\n\r\n```py\r\n>>> class Test:\r\n...     pass\r\n...     \r\n>>> a = Test()\r\n>>> a.b = 10\r\n>>> import copy\r\n>>> dir(copy.deepcopy(a))\r\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slotnames__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', 'b']\r\n>>> dir(a)\r\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slotnames__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', 'b']\r\n```\r\n\r\nand not necessarily _added_?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "sklearn/utils/_metadata_requests.py",
    "pr_number": 32264,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2378606247,
    "comment_created_at": "2025-09-25T10:31:09Z"
  },
  {
    "code": "@@ -487,19 +480,14 @@ class AdaBoostClassifier(\n     refer to :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_twoclass.py`.\n     \"\"\"\n \n-    # TODO(1.8): remove \"algorithm\" entry\n-    _parameter_constraints: dict = {\n-        **BaseWeightBoosting._parameter_constraints,\n-        \"algorithm\": [StrOptions({\"SAMME\"}), Hidden(StrOptions({\"deprecated\"}))],\n-    }\n+    _parameter_constraints: dict = {**BaseWeightBoosting._parameter_constraints}",
    "comment": "Then we can simply remove this, right?",
    "line_number": 483,
    "enriched": "File: sklearn/ensemble/_weight_boosting.py\nCode: @@ -487,19 +480,14 @@ class AdaBoostClassifier(\n     refer to :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_twoclass.py`.\n     \"\"\"\n \n-    # TODO(1.8): remove \"algorithm\" entry\n-    _parameter_constraints: dict = {\n-        **BaseWeightBoosting._parameter_constraints,\n-        \"algorithm\": [StrOptions({\"SAMME\"}), Hidden(StrOptions({\"deprecated\"}))],\n-    }\n+    _parameter_constraints: dict = {**BaseWeightBoosting._parameter_constraints}\nComment: Then we can simply remove this, right?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "sklearn/ensemble/_weight_boosting.py",
    "pr_number": 32262,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2375401906,
    "comment_created_at": "2025-09-24T11:05:53Z"
  },
  {
    "code": "@@ -969,6 +969,13 @@ best split is found through an exhaustive search of the feature values of\n either all input features or a random subset of size ``max_features``.\n (See the :ref:`parameter tuning guidelines <random_forest_parameters>` for more details.)\n \n+In scikit-learn, the underlying decision trees used in\n+:class:`RandomForestClassifier` and :class:`RandomForestRegressor`\n+always use the ``splitter=\"best\"`` strategy. This means that all possible",
    "comment": "Not sure if this is really clear. May sound like `splitter` is a parameter in random forest estimators.",
    "line_number": 974,
    "enriched": "File: doc/modules/ensemble.rst\nCode: @@ -969,6 +969,13 @@ best split is found through an exhaustive search of the feature values of\n either all input features or a random subset of size ``max_features``.\n (See the :ref:`parameter tuning guidelines <random_forest_parameters>` for more details.)\n \n+In scikit-learn, the underlying decision trees used in\n+:class:`RandomForestClassifier` and :class:`RandomForestRegressor`\n+always use the ``splitter=\"best\"`` strategy. This means that all possible\nComment: Not sure if this is really clear. May sound like `splitter` is a parameter in random forest estimators.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "doc/modules/ensemble.rst",
    "pr_number": 32256,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2372409837,
    "comment_created_at": "2025-09-23T13:49:00Z"
  },
  {
    "code": "@@ -13,3 +13,17 @@ all priceless contributions.\n \n We abide by the principles of openness, respect, and consideration of others of\n the Python Software Foundation: https://www.python.org/psf/codeofconduct/\n+\n+# Low Quality and AI Generated Contributions Policy\n+\n+Due to the burden put on maintainers, users submitting multiple low quality pull\n+requests, or AI generated comments, reviews, issues, or pull requests, where the\n+user does not show a good understanding of what they are posting, might be banned\n+from the organisation.\n+\n+For more context, you can check out this blog post on [\n+The Cost of AI in Open Source Maintenance\n+](https://adrin.info/the-cost-of-ai-in-open-source-maintenance.html).\n+\n+If this happens to you and you believe it's been a mistake, you can reach us on\n+`coc@scikit-learn.org`.",
    "comment": "@ogrisel do we have a `coc@scikit-learn.org`? ",
    "line_number": 29,
    "enriched": "File: CODE_OF_CONDUCT.md\nCode: @@ -13,3 +13,17 @@ all priceless contributions.\n \n We abide by the principles of openness, respect, and consideration of others of\n the Python Software Foundation: https://www.python.org/psf/codeofconduct/\n+\n+# Low Quality and AI Generated Contributions Policy\n+\n+Due to the burden put on maintainers, users submitting multiple low quality pull\n+requests, or AI generated comments, reviews, issues, or pull requests, where the\n+user does not show a good understanding of what they are posting, might be banned\n+from the organisation.\n+\n+For more context, you can check out this blog post on [\n+The Cost of AI in Open Source Maintenance\n+](https://adrin.info/the-cost-of-ai-in-open-source-maintenance.html).\n+\n+If this happens to you and you believe it's been a mistake, you can reach us on\n+`coc@scikit-learn.org`.\nComment: @ogrisel do we have a `coc@scikit-learn.org`? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "CODE_OF_CONDUCT.md",
    "pr_number": 32254,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2371418760,
    "comment_created_at": "2025-09-23T07:25:55Z"
  },
  {
    "code": "@@ -1063,13 +1067,16 @@ def precision_recall_curve(\n             \"No positive class found in y_true, \"\n             \"recall is set to one for all thresholds.\"\n         )\n-        recall = np.ones_like(tps)\n+        recall = xp.full(tps.shape, 1.0)\n     else:\n         recall = tps / tps[-1]\n \n     # reverse the outputs so recall is decreasing\n-    sl = slice(None, None, -1)\n-    return np.hstack((precision[sl], 1)), np.hstack((recall[sl], 0)), thresholds[sl]\n+    return (\n+        xp.concat((xp.flip(precision), xp.asarray([1.0], device=device))),\n+        xp.concat((xp.flip(recall), xp.asarray([0.0], device=device))),",
    "comment": "Are precision and recall guaranteed to be 1d?",
    "line_number": 1077,
    "enriched": "File: sklearn/metrics/_ranking.py\nCode: @@ -1063,13 +1067,16 @@ def precision_recall_curve(\n             \"No positive class found in y_true, \"\n             \"recall is set to one for all thresholds.\"\n         )\n-        recall = np.ones_like(tps)\n+        recall = xp.full(tps.shape, 1.0)\n     else:\n         recall = tps / tps[-1]\n \n     # reverse the outputs so recall is decreasing\n-    sl = slice(None, None, -1)\n-    return np.hstack((precision[sl], 1)), np.hstack((recall[sl], 0)), thresholds[sl]\n+    return (\n+        xp.concat((xp.flip(precision), xp.asarray([1.0], device=device))),\n+        xp.concat((xp.flip(recall), xp.asarray([0.0], device=device))),\nComment: Are precision and recall guaranteed to be 1d?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "sklearn/metrics/_ranking.py",
    "pr_number": 32249,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2390522556,
    "comment_created_at": "2025-09-30T09:23:12Z"
  },
  {
    "code": "@@ -356,7 +356,7 @@ def benchmark(clf, custom_name=False):\n # Notice that the most important hyperparameters values were tuned using a grid\n # search procedure not shown in this notebook for the sake of simplicity. See\n # the example script\n-# :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`  # noqa: E501",
    "comment": "I think lint is failing due to removal of this",
    "line_number": 359,
    "enriched": "File: examples/text/plot_document_classification_20newsgroups.py\nCode: @@ -356,7 +356,7 @@ def benchmark(clf, custom_name=False):\n # Notice that the most important hyperparameters values were tuned using a grid\n # search procedure not shown in this notebook for the sake of simplicity. See\n # the example script\n-# :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`  # noqa: E501\nComment: I think lint is failing due to removal of this",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "examples/text/plot_document_classification_20newsgroups.py",
    "pr_number": 32241,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2366715332,
    "comment_created_at": "2025-09-22T04:54:52Z"
  },
  {
    "code": "@@ -2185,49 +2185,46 @@ of 0.0.\n     -0.552\n \n \n-|details-start|\n-**D2 Brier score**\n-|details-split|\n+.. dropdown:: **D2 Brier score**",
    "comment": "```suggestion\r\n.. dropdown:: D2 Brier score\r\n```",
    "line_number": 2188,
    "enriched": "File: doc/modules/model_evaluation.rst\nCode: @@ -2185,49 +2185,46 @@ of 0.0.\n     -0.552\n \n \n-|details-start|\n-**D2 Brier score**\n-|details-split|\n+.. dropdown:: **D2 Brier score**\nComment: ```suggestion\r\n.. dropdown:: D2 Brier score\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "doc/modules/model_evaluation.rst",
    "pr_number": 32226,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2366189950,
    "comment_created_at": "2025-09-21T11:37:35Z"
  },
    {
    "code": "@@ -233,12 +237,19 @@ Cross-Validation.\n Lasso\n =====\n \n+\n+\n+",
    "comment": "I don't think we need this extra spacing\r\n```suggestion\r\n```",
    "line_number": 242,
    "enriched": "File: doc/modules/linear_model.rst\nCode: @@ -233,12 +237,19 @@ Cross-Validation.\n Lasso\n =====\n \n+\n+\n+\nComment: I don't think we need this extra spacing\r\n```suggestion\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "doc/modules/linear_model.rst",
    "pr_number": 32196,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2352910835,
    "comment_created_at": "2025-09-16T15:35:41Z"
  },
  {
    "code": "@@ -148,18 +147,11 @@ class CalibratedClassifierCV(ClassifierMixin, MetaEstimatorMixin, BaseEstimator)\n         .. versionchanged:: 0.22\n             ``cv`` default value if None changed from 3-fold to 5-fold.\n \n-        .. versionchanged:: 1.6\n-            `\"prefit\"` is deprecated. Use :class:`~sklearn.frozen.FrozenEstimator`\n-            instead.\n-\n     n_jobs : int, default=None\n         Number of jobs to run in parallel.\n         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n         ``-1`` means using all processors.\n \n-        Base estimator clones are fitted in parallel across cross-validation\n-        iterations. Therefore parallelism happens only when `cv != \"prefit\"`.",
    "comment": "I'd keep the first sentence since it explains what `n_jobs` is used for",
    "line_number": 161,
    "enriched": "File: sklearn/calibration.py\nCode: @@ -148,18 +147,11 @@ class CalibratedClassifierCV(ClassifierMixin, MetaEstimatorMixin, BaseEstimator)\n         .. versionchanged:: 0.22\n             ``cv`` default value if None changed from 3-fold to 5-fold.\n \n-        .. versionchanged:: 1.6\n-            `\"prefit\"` is deprecated. Use :class:`~sklearn.frozen.FrozenEstimator`\n-            instead.\n-\n     n_jobs : int, default=None\n         Number of jobs to run in parallel.\n         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n         ``-1`` means using all processors.\n \n-        Base estimator clones are fitted in parallel across cross-validation\n-        iterations. Therefore parallelism happens only when `cv != \"prefit\"`.\nComment: I'd keep the first sentence since it explains what `n_jobs` is used for",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "sklearn/calibration.py",
    "pr_number": 32157,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2341011491,
    "comment_created_at": "2025-09-11T13:57:45Z"
  },
  {
    "code": "@@ -79,6 +81,10 @@ def _murmurhash3_bytes_array_s32(\n     return np.asarray(out)\n \n \n+@deprecated(",
    "comment": "We have a convention to quickly search what needs to be cleaned-up before the releases\r\n```suggestion\r\n# TODO(1.10): remove\r\n@deprecated(\r\n```",
    "line_number": 84,
    "enriched": "File: sklearn/utils/murmurhash.pyx\nCode: @@ -79,6 +81,10 @@ def _murmurhash3_bytes_array_s32(\n     return np.asarray(out)\n \n \n+@deprecated(\nComment: We have a convention to quickly search what needs to be cleaned-up before the releases\r\n```suggestion\r\n# TODO(1.10): remove\r\n@deprecated(\r\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "sklearn/utils/murmurhash.pyx",
    "pr_number": 32103,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2321823399,
    "comment_created_at": "2025-09-04T11:57:06Z"
  },
  {
    "code": "@@ -72,11 +72,9 @@ jobs:\n           CIBW_PLATFORM: pyodide\n           SKLEARN_SKIP_OPENMP_TEST: \"true\"\n           SKLEARN_SKIP_NETWORK_TESTS: 1\n-          # Temporary work-around to avoid joblib 1.5.0 until there is a joblib\n-          # release with https://github.com/joblib/joblib/pull/1721\n-          CIBW_TEST_REQUIRES: \"pytest pandas joblib!=1.5.0\"",
    "comment": "While I am at it looking at Pyodide CI, I cleaned up this, this is not needed since joblib 1.5.1\r\n",
    "line_number": 77,
    "enriched": "File: .github/workflows/emscripten.yml\nCode: @@ -72,11 +72,9 @@ jobs:\n           CIBW_PLATFORM: pyodide\n           SKLEARN_SKIP_OPENMP_TEST: \"true\"\n           SKLEARN_SKIP_NETWORK_TESTS: 1\n-          # Temporary work-around to avoid joblib 1.5.0 until there is a joblib\n-          # release with https://github.com/joblib/joblib/pull/1721\n-          CIBW_TEST_REQUIRES: \"pytest pandas joblib!=1.5.0\"\nComment: While I am at it looking at Pyodide CI, I cleaned up this, this is not needed since joblib 1.5.1\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": ".github/workflows/emscripten.yml",
    "pr_number": 32089,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2318181217,
    "comment_created_at": "2025-09-03T08:23:20Z"
  },
   {
    "code": "@@ -22,7 +22,8 @@ This file needs to be added to the right folder like `sklearn.linear_model` or\n `sklearn.tree` depending on which part of scikit-learn your PR changes. There\n are also a few folders for some topics like `array-api`, `metadata-routing` or `security`.\n \n-In almost all cases, your fragment should be formatted as a bullet point.\n+In almost all cases, your fragment should be formatted as a **single** bullet point.\n+Note the aggregation software cannot handle more than one bullet point per entry.",
    "comment": "Not sure that this second line is necessary. Maybe just adding 'single' above is adequate?",
    "line_number": 26,
    "enriched": "File: doc/whats_new/upcoming_changes/README.md\nCode: @@ -22,7 +22,8 @@ This file needs to be added to the right folder like `sklearn.linear_model` or\n `sklearn.tree` depending on which part of scikit-learn your PR changes. There\n are also a few folders for some topics like `array-api`, `metadata-routing` or `security`.\n \n-In almost all cases, your fragment should be formatted as a bullet point.\n+In almost all cases, your fragment should be formatted as a **single** bullet point.\n+Note the aggregation software cannot handle more than one bullet point per entry.\nComment: Not sure that this second line is necessary. Maybe just adding 'single' above is adequate?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "doc/whats_new/upcoming_changes/README.md",
    "pr_number": 32085,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2317565206,
    "comment_created_at": "2025-09-03T01:51:37Z"
  },
  {
    "code": "@@ -1493,6 +1493,15 @@ class labels (multi-output problem).\n     RandomForestClassifier(...)\n     >>> print(clf.predict([[0, 0, 0, 0]]))\n     [1]\n+    Notes\n+    -----\n+    The trees in the forest use the same split strategy as\n+    :class:`~sklearn.tree.DecisionTreeClassifier` and\n+    :class:`~sklearn.tree.DecisionTreeRegressor`.",
    "comment": "This part was already addressed in https://github.com/scikit-learn/scikit-learn/pull/27746, also no need to mention `DecisionTreeRegressor` in the `RandomForestClassifier` docstring.",
    "line_number": 1500,
    "enriched": "File: sklearn/ensemble/_forest.py\nCode: @@ -1493,6 +1493,15 @@ class labels (multi-output problem).\n     RandomForestClassifier(...)\n     >>> print(clf.predict([[0, 0, 0, 0]]))\n     [1]\n+    Notes\n+    -----\n+    The trees in the forest use the same split strategy as\n+    :class:`~sklearn.tree.DecisionTreeClassifier` and\n+    :class:`~sklearn.tree.DecisionTreeRegressor`.\nComment: This part was already addressed in https://github.com/scikit-learn/scikit-learn/pull/27746, also no need to mention `DecisionTreeRegressor` in the `RandomForestClassifier` docstring.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "sklearn/ensemble/_forest.py",
    "pr_number": 32071,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2348491041,
    "comment_created_at": "2025-09-15T10:03:33Z"
  },
  {
    "code": "@@ -1027,18 +1009,10 @@ def transform(self, X):\n         n_splines = self.bsplines_[0].c.shape[1]\n         degree = self.degree\n \n-        # TODO: Remove this condition, once scipy 1.10 is the minimum version.\n-        #       Only scipy >= 1.10 supports design_matrix(.., extrapolate=..).\n-        #       The default (implicit in scipy < 1.10) is extrapolate=False.\n-        scipy_1_10 = sp_version >= parse_version(\"1.10.0\")\n         # Note: self.bsplines_[0].extrapolate is True for extrapolation in\n         # [\"periodic\", \"continue\"]\n-        if scipy_1_10:\n-            use_sparse = self.sparse_output\n-            kwargs_extrapolate = {\"extrapolate\": self.bsplines_[0].extrapolate}\n-        else:\n-            use_sparse = self.sparse_output and not self.bsplines_[0].extrapolate\n-            kwargs_extrapolate = dict()\n+        use_sparse = self.sparse_output",
    "comment": "I would use `self.sparse_output` everywhere. I guess `use_sparse` was defined only to cover more easily scipy < 1.10 and scipy >= 1.10",
    "line_number": 1014,
    "enriched": "File: sklearn/preprocessing/_polynomial.py\nCode: @@ -1027,18 +1009,10 @@ def transform(self, X):\n         n_splines = self.bsplines_[0].c.shape[1]\n         degree = self.degree\n \n-        # TODO: Remove this condition, once scipy 1.10 is the minimum version.\n-        #       Only scipy >= 1.10 supports design_matrix(.., extrapolate=..).\n-        #       The default (implicit in scipy < 1.10) is extrapolate=False.\n-        scipy_1_10 = sp_version >= parse_version(\"1.10.0\")\n         # Note: self.bsplines_[0].extrapolate is True for extrapolation in\n         # [\"periodic\", \"continue\"]\n-        if scipy_1_10:\n-            use_sparse = self.sparse_output\n-            kwargs_extrapolate = {\"extrapolate\": self.bsplines_[0].extrapolate}\n-        else:\n-            use_sparse = self.sparse_output and not self.bsplines_[0].extrapolate\n-            kwargs_extrapolate = dict()\n+        use_sparse = self.sparse_output\nComment: I would use `self.sparse_output` everywhere. I guess `use_sparse` was defined only to cover more easily scipy < 1.10 and scipy >= 1.10",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "sklearn/preprocessing/_polynomial.py",
    "pr_number": 32615,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2480348303,
    "comment_created_at": "2025-10-31T07:09:23Z"
  },
    {
    "code": "@@ -162,6 +165,17 @@ Reference Steps\n     - In the release branch, change the version number `__version__` in\n       `sklearn/__init__.py` to `{{ version_full }}`.\n \n+    {% if key == \"rc\" %}\n+    - Still in the release branch, set or update the upper bound on the build\n+      dependencies in the `[build-system]` section of `pyproject.toml`. The goal is to\n+      prevent future backward incompatible releases of the dependencies to break the\n+      build in the maintenance branch.\n+      \n+      The upper bounds should match the latest already-released minor versions of the\n+      dependencies and should allow future micro (bug-fix) versions. For instance, if\n+      numpy 2.2.5 is the most rencent version, its upper bound should be set <2.3.0.",
    "comment": "```suggestion\r\n      numpy 2.2.5 is the most recent version, its upper bound should be set to <2.3.0.\r\n```",
    "line_number": 176,
    "enriched": "File: doc/developers/maintainer.rst.template\nCode: @@ -162,6 +165,17 @@ Reference Steps\n     - In the release branch, change the version number `__version__` in\n       `sklearn/__init__.py` to `{{ version_full }}`.\n \n+    {% if key == \"rc\" %}\n+    - Still in the release branch, set or update the upper bound on the build\n+      dependencies in the `[build-system]` section of `pyproject.toml`. The goal is to\n+      prevent future backward incompatible releases of the dependencies to break the\n+      build in the maintenance branch.\n+      \n+      The upper bounds should match the latest already-released minor versions of the\n+      dependencies and should allow future micro (bug-fix) versions. For instance, if\n+      numpy 2.2.5 is the most rencent version, its upper bound should be set <2.3.0.\nComment: ```suggestion\r\n      numpy 2.2.5 is the most recent version, its upper bound should be set to <2.3.0.\r\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "doc/developers/maintainer.rst.template",
    "pr_number": 31345,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2083712232,
    "comment_created_at": "2025-05-12T02:08:25Z"
  },
  {
    "code": "@@ -0,0 +1,204 @@\n+# ===================================================================================\n+# Project: ChatSkLearn\n+# File: app/main.py\n+# Description: This file Orchasterates the main ChatSkLearn Application.\n+# Author: LALAN KUMAR\n+# Created: [25-04-2025]\n+# Updated: [01-05-2025]\n+# LAST MODIFIED BY: LALAN KUMAR [https://github.com/kumar8074]\n+# Version: 1.0.0\n+# ===================================================================================\n+\n+from flask import Flask, render_template, request, jsonify, session\n+import os\n+import sys\n+from uuid import uuid4\n+import asyncio\n+from langchain_core.messages import HumanMessage\n+from langsmith import Client\n+from langchain.callbacks.tracers import LangChainTracer\n+\n+# Add root path\n+current_file_path = os.path.abspath(__file__)\n+project_root = os.path.abspath(os.path.join(current_file_path, \"../..\"))\n+if project_root not in sys.path:\n+    sys.path.append(project_root)\n+\n+# Import the router graph and settings\n+from app.graphs.states import AgentState\n+from app.graphs.router import create_router_graph\n+from config import settings\n+\n+from langchain.callbacks.base import AsyncCallbackHandler\n+\n+class StepStreamer(AsyncCallbackHandler):\n+    \"\"\"Collect every token the LLM emits and stash it in `self.tokens`.\"\"\"\n+    def __init__(self):\n+        super().__init__()\n+        self.tokens: list[str] = []\n+\n+    async def on_llm_new_token(self, token: str, **kwargs):\n+        # called for every new token\n+        self.tokens.append(token)\n+\n+\n+\n+app = Flask(__name__)\n+app.secret_key = os.environ.get('SECRET_KEY', str(uuid4()))\n+\n+# Create the router graph\n+router_graph = create_router_graph()\n+\n+langsmith_client = Client(api_key=os.environ.get(\"LANGSMITH_API_KEY\"))\n+tracer = LangChainTracer(client=langsmith_client, project_name=\"SkLearnAssistantProject\")\n+\n+# Define models that are available and their corresponding LLM providers\n+MODELS = {\n+    \"gpt-4o\": \"OpenAI GPT-4o\",\n+    \"claude-3-sonnet\": \"Claude 3 Sonnet\",\n+    \"gemini-2.0-flash\": \"Gemini 2.0 flash\",\n+    \"command\": \"Cohere Command\"\n+}\n+\n+# Map frontend model selections to LLM providers\n+MODEL_TO_PROVIDER = {\n+    \"gpt-4o\": settings.LLM_PROVIDER_OPENAI,\n+    \"claude-3-sonnet\": settings.LLM_PROVIDER_ANTHROPIC,\n+    \"gemini-2.0-flash\": settings.LLM_PROVIDER_GEMINI,\n+    \"command\": settings.LLM_PROVIDER_COHERE\n+}\n+\n+# Sample suggestions\n+SUGGESTIONS = [\n+    \"How do I use RandomForestClassifier in scikit-learn?\",\n+    \"How to apply PCA to images?\",\n+    \"How can I tune hyperparameters in scikit-learn?\",\n+    \"Explain how to handle imbalanced datasets in scikit-learn.\"\n+]\n+\n+# In-memory chat history storage\n+# In a production app, you'd use a database instead\n+chat_history = {}\n+\n+\n+@app.route('/')\n+def index():\n+    # Set default model if none is selected\n+    if 'selected_model' not in session:\n+        session['selected_model'] = 'gemini-2.0-flash'\n+    \n+    # Generate a unique session ID if none exists\n+    if 'session_id' not in session:\n+        session['session_id'] = str(uuid4())\n+        chat_history[session['session_id']] = []\n+    \n+    return render_template('index.html', \n+                          models=MODELS, \n+                          selected_model=session['selected_model'],\n+                          selected_model_name=MODELS[session['selected_model']],\n+                          suggestions=SUGGESTIONS,\n+                          chat_history=chat_history.get(session['session_id'], []))\n+\n+@app.route('/select_model', methods=['POST'])\n+def select_model():\n+    model_id = request.form.get('model')\n+    if model_id in MODELS:\n+        session['selected_model'] = model_id\n+        \n+        # Update the LLM provider based on the selected model\n+        llm_provider = MODEL_TO_PROVIDER[model_id]\n+        os.environ[\"LLM_PROVIDER\"] = llm_provider\n+        os.environ[\"EMBEDDING_PROVIDER\"] = llm_provider\n+        \n+        # Update settings\n+        settings.LLM_PROVIDER = llm_provider\n+        settings.EMBEDDING_PROVIDER = llm_provider\n+        \n+        return jsonify({\"status\": \"success\", \"model\": MODELS[model_id]})\n+    return jsonify({\"status\": \"error\", \"message\": \"Invalid model selection\"})\n+\n+@app.route('/send_message', methods=['POST'])\n+def send_message():\n+    # Support both JSON and form data\n+    if request.is_json:\n+        message = request.json.get('message', '').strip()\n+    else:\n+        message = request.form.get('message', '').strip()\n+    model_id = session.get('selected_model', 'gemini-2.0-flash')\n+    session_id = session.get('session_id')\n+\n+    if not message:\n+        return jsonify({\"status\": \"error\", \"message\": \"Message cannot be empty\"})\n+\n+    streamer = StepStreamer()\n+\n+    # Prepare config with thread_id for persistence\n+    config = {\n+        \"configurable\": {\"thread_id\": session_id},\n+        \"callbacks\":[tracer, streamer],\n+        \"streaming\": True\n+    }\n+\n+    # Try to get current state from graph\n+    try:\n+        latest_state = router_graph.get_state(config)\n+        current_messages = latest_state.values.get(\"messages\", [])\n+    except Exception:\n+        current_messages = []\n+\n+    # Append new user message\n+    current_messages.append(HumanMessage(content=message))\n+\n+    # Prepare input state\n+    state = AgentState(messages=current_messages)\n+\n+    # Set LLM provider environment variables\n+    llm_provider = MODEL_TO_PROVIDER[model_id]\n+    os.environ[\"LLM_PROVIDER\"] = llm_provider\n+    os.environ[\"EMBEDDING_PROVIDER\"] = llm_provider\n+    settings.LLM_PROVIDER = llm_provider\n+    settings.EMBEDDING_PROVIDER = llm_provider\n+\n+    # Run the graph asynchronously\n+    loop = asyncio.new_event_loop()\n+    asyncio.set_event_loop(loop)\n+    try:\n+        result = loop.run_until_complete(router_graph.ainvoke(state, config))\n+        full_streamed_text=\"\".join(streamer.tokens)\n+        #print(\"STREAMED:\" ,full_streamed_text)\n+        assistant_messages = result.get(\"messages\", [])\n+        if assistant_messages:\n+            current_messages.extend(assistant_messages)\n+        # Update chat history for UI (optional)\n+        chat_history.setdefault(session_id, []).append(message)\n+        if assistant_messages:\n+            chat_history[session_id].append(assistant_messages[-1].content)\n+        response_content = assistant_messages[-1].content if assistant_messages else \"\"\n+        return jsonify({\n+            \"status\": \"success\",\n+            \"message\": message,\n+            \"response\": response_content,\n+            \"streamed_text\": full_streamed_text,\n+            \"model\": MODELS[model_id]\n+        })\n+    finally:\n+        loop.close()\n+\n+@app.route('/get_suggestions')\n+def get_suggestions():\n+    return jsonify({\"suggestions\": SUGGESTIONS})\n+\n+@app.route('/get_chat_history')\n+def get_chat_history():\n+    session_id = session.get('session_id')\n+    return jsonify({\"history\": chat_history.get(session_id, [])})\n+\n+@app.route('/clear_chat_history', methods=['POST'])\n+def clear_chat_history():\n+    session_id = session.get('session_id')\n+    if session_id in chat_history:\n+        chat_history[session_id] = []\n+    return jsonify({\"status\": \"success\"})\n+\n+if __name__ == '__main__':\n+    app.run(host=\"0.0.0.0\", port=8000,debug=True)",
    "comment": "## Flask app is run in debug mode\n\nA Flask app appears to be run in debug mode. This may allow an attacker to run arbitrary code through the debugger.\n\n[Show more details](https://github.com/scikit-learn/scikit-learn/security/code-scanning/63)",
    "line_number": 204,
    "enriched": "File: ChatSkLearn/app/main.py\nCode: @@ -0,0 +1,204 @@\n+# ===================================================================================\n+# Project: ChatSkLearn\n+# File: app/main.py\n+# Description: This file Orchasterates the main ChatSkLearn Application.\n+# Author: LALAN KUMAR\n+# Created: [25-04-2025]\n+# Updated: [01-05-2025]\n+# LAST MODIFIED BY: LALAN KUMAR [https://github.com/kumar8074]\n+# Version: 1.0.0\n+# ===================================================================================\n+\n+from flask import Flask, render_template, request, jsonify, session\n+import os\n+import sys\n+from uuid import uuid4\n+import asyncio\n+from langchain_core.messages import HumanMessage\n+from langsmith import Client\n+from langchain.callbacks.tracers import LangChainTracer\n+\n+# Add root path\n+current_file_path = os.path.abspath(__file__)\n+project_root = os.path.abspath(os.path.join(current_file_path, \"../..\"))\n+if project_root not in sys.path:\n+    sys.path.append(project_root)\n+\n+# Import the router graph and settings\n+from app.graphs.states import AgentState\n+from app.graphs.router import create_router_graph\n+from config import settings\n+\n+from langchain.callbacks.base import AsyncCallbackHandler\n+\n+class StepStreamer(AsyncCallbackHandler):\n+    \"\"\"Collect every token the LLM emits and stash it in `self.tokens`.\"\"\"\n+    def __init__(self):\n+        super().__init__()\n+        self.tokens: list[str] = []\n+\n+    async def on_llm_new_token(self, token: str, **kwargs):\n+        # called for every new token\n+        self.tokens.append(token)\n+\n+\n+\n+app = Flask(__name__)\n+app.secret_key = os.environ.get('SECRET_KEY', str(uuid4()))\n+\n+# Create the router graph\n+router_graph = create_router_graph()\n+\n+langsmith_client = Client(api_key=os.environ.get(\"LANGSMITH_API_KEY\"))\n+tracer = LangChainTracer(client=langsmith_client, project_name=\"SkLearnAssistantProject\")\n+\n+# Define models that are available and their corresponding LLM providers\n+MODELS = {\n+    \"gpt-4o\": \"OpenAI GPT-4o\",\n+    \"claude-3-sonnet\": \"Claude 3 Sonnet\",\n+    \"gemini-2.0-flash\": \"Gemini 2.0 flash\",\n+    \"command\": \"Cohere Command\"\n+}\n+\n+# Map frontend model selections to LLM providers\n+MODEL_TO_PROVIDER = {\n+    \"gpt-4o\": settings.LLM_PROVIDER_OPENAI,\n+    \"claude-3-sonnet\": settings.LLM_PROVIDER_ANTHROPIC,\n+    \"gemini-2.0-flash\": settings.LLM_PROVIDER_GEMINI,\n+    \"command\": settings.LLM_PROVIDER_COHERE\n+}\n+\n+# Sample suggestions\n+SUGGESTIONS = [\n+    \"How do I use RandomForestClassifier in scikit-learn?\",\n+    \"How to apply PCA to images?\",\n+    \"How can I tune hyperparameters in scikit-learn?\",\n+    \"Explain how to handle imbalanced datasets in scikit-learn.\"\n+]\n+\n+# In-memory chat history storage\n+# In a production app, you'd use a database instead\n+chat_history = {}\n+\n+\n+@app.route('/')\n+def index():\n+    # Set default model if none is selected\n+    if 'selected_model' not in session:\n+        session['selected_model'] = 'gemini-2.0-flash'\n+    \n+    # Generate a unique session ID if none exists\n+    if 'session_id' not in session:\n+        session['session_id'] = str(uuid4())\n+        chat_history[session['session_id']] = []\n+    \n+    return render_template('index.html', \n+                          models=MODELS, \n+                          selected_model=session['selected_model'],\n+                          selected_model_name=MODELS[session['selected_model']],\n+                          suggestions=SUGGESTIONS,\n+                          chat_history=chat_history.get(session['session_id'], []))\n+\n+@app.route('/select_model', methods=['POST'])\n+def select_model():\n+    model_id = request.form.get('model')\n+    if model_id in MODELS:\n+        session['selected_model'] = model_id\n+        \n+        # Update the LLM provider based on the selected model\n+        llm_provider = MODEL_TO_PROVIDER[model_id]\n+        os.environ[\"LLM_PROVIDER\"] = llm_provider\n+        os.environ[\"EMBEDDING_PROVIDER\"] = llm_provider\n+        \n+        # Update settings\n+        settings.LLM_PROVIDER = llm_provider\n+        settings.EMBEDDING_PROVIDER = llm_provider\n+        \n+        return jsonify({\"status\": \"success\", \"model\": MODELS[model_id]})\n+    return jsonify({\"status\": \"error\", \"message\": \"Invalid model selection\"})\n+\n+@app.route('/send_message', methods=['POST'])\n+def send_message():\n+    # Support both JSON and form data\n+    if request.is_json:\n+        message = request.json.get('message', '').strip()\n+    else:\n+        message = request.form.get('message', '').strip()\n+    model_id = session.get('selected_model', 'gemini-2.0-flash')\n+    session_id = session.get('session_id')\n+\n+    if not message:\n+        return jsonify({\"status\": \"error\", \"message\": \"Message cannot be empty\"})\n+\n+    streamer = StepStreamer()\n+\n+    # Prepare config with thread_id for persistence\n+    config = {\n+        \"configurable\": {\"thread_id\": session_id},\n+        \"callbacks\":[tracer, streamer],\n+        \"streaming\": True\n+    }\n+\n+    # Try to get current state from graph\n+    try:\n+        latest_state = router_graph.get_state(config)\n+        current_messages = latest_state.values.get(\"messages\", [])\n+    except Exception:\n+        current_messages = []\n+\n+    # Append new user message\n+    current_messages.append(HumanMessage(content=message))\n+\n+    # Prepare input state\n+    state = AgentState(messages=current_messages)\n+\n+    # Set LLM provider environment variables\n+    llm_provider = MODEL_TO_PROVIDER[model_id]\n+    os.environ[\"LLM_PROVIDER\"] = llm_provider\n+    os.environ[\"EMBEDDING_PROVIDER\"] = llm_provider\n+    settings.LLM_PROVIDER = llm_provider\n+    settings.EMBEDDING_PROVIDER = llm_provider\n+\n+    # Run the graph asynchronously\n+    loop = asyncio.new_event_loop()\n+    asyncio.set_event_loop(loop)\n+    try:\n+        result = loop.run_until_complete(router_graph.ainvoke(state, config))\n+        full_streamed_text=\"\".join(streamer.tokens)\n+        #print(\"STREAMED:\" ,full_streamed_text)\n+        assistant_messages = result.get(\"messages\", [])\n+        if assistant_messages:\n+            current_messages.extend(assistant_messages)\n+        # Update chat history for UI (optional)\n+        chat_history.setdefault(session_id, []).append(message)\n+        if assistant_messages:\n+            chat_history[session_id].append(assistant_messages[-1].content)\n+        response_content = assistant_messages[-1].content if assistant_messages else \"\"\n+        return jsonify({\n+            \"status\": \"success\",\n+            \"message\": message,\n+            \"response\": response_content,\n+            \"streamed_text\": full_streamed_text,\n+            \"model\": MODELS[model_id]\n+        })\n+    finally:\n+        loop.close()\n+\n+@app.route('/get_suggestions')\n+def get_suggestions():\n+    return jsonify({\"suggestions\": SUGGESTIONS})\n+\n+@app.route('/get_chat_history')\n+def get_chat_history():\n+    session_id = session.get('session_id')\n+    return jsonify({\"history\": chat_history.get(session_id, [])})\n+\n+@app.route('/clear_chat_history', methods=['POST'])\n+def clear_chat_history():\n+    session_id = session.get('session_id')\n+    if session_id in chat_history:\n+        chat_history[session_id] = []\n+    return jsonify({\"status\": \"success\"})\n+\n+if __name__ == '__main__':\n+    app.run(host=\"0.0.0.0\", port=8000,debug=True)\nComment: ## Flask app is run in debug mode\n\nA Flask app appears to be run in debug mode. This may allow an attacker to run arbitrary code through the debugger.\n\n[Show more details](https://github.com/scikit-learn/scikit-learn/security/code-scanning/63)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "ChatSkLearn/app/main.py",
    "pr_number": 31417,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2104343659,
    "comment_created_at": "2025-05-23T10:56:30Z"
  },
  {
    "code": "@@ -837,13 +837,15 @@ prior over all :math:`\\lambda_i` is chosen to be the same gamma distribution\n given by the hyperparameters :math:`\\lambda_1` and :math:`\\lambda_2`.\n \n ARD is also known in the literature as *Sparse Bayesian Learning* and *Relevance\n-Vector Machine* [3]_ [4]_. For a worked-out comparison between ARD and `Bayesian\n-Ridge Regression`_, see the example below.\n+Vector Machine* [3]_ [4]_.\n \n-.. rubric:: Examples\n+See :ref:`sphx_glr_auto_examples_linear_model_plot_ard.py` for a worked-out comparison between ARD and `Bayesian Ridge Regression`_.\n+\n+See :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py` for a comparison between various methods - Lasso, ARD and ElasticNet - on correlated data.\n \n-* :ref:`sphx_glr_auto_examples_linear_model_plot_ard.py`\n+See :ref:`sphx_glr_auto_examples_linear_model_plot_ard.py` for a worked-out comparison between ARD and `Bayesian Ridge Regression`_.\n \n+See :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py` for a comparison between various methods - Lasso, ARD and ElasticNet - on correlated data.\n ",
    "comment": "```suggestion\r\n```\r\n\r\nNow these two are linked twice. So, we should remove the duplicates. (The line after should also be removed, to avoid double blank lines again, but I cannot comment there.)",
    "line_number": 849,
    "enriched": "File: doc/modules/linear_model.rst\nCode: @@ -837,13 +837,15 @@ prior over all :math:`\\lambda_i` is chosen to be the same gamma distribution\n given by the hyperparameters :math:`\\lambda_1` and :math:`\\lambda_2`.\n \n ARD is also known in the literature as *Sparse Bayesian Learning* and *Relevance\n-Vector Machine* [3]_ [4]_. For a worked-out comparison between ARD and `Bayesian\n-Ridge Regression`_, see the example below.\n+Vector Machine* [3]_ [4]_.\n \n-.. rubric:: Examples\n+See :ref:`sphx_glr_auto_examples_linear_model_plot_ard.py` for a worked-out comparison between ARD and `Bayesian Ridge Regression`_.\n+\n+See :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py` for a comparison between various methods - Lasso, ARD and ElasticNet - on correlated data.\n \n-* :ref:`sphx_glr_auto_examples_linear_model_plot_ard.py`\n+See :ref:`sphx_glr_auto_examples_linear_model_plot_ard.py` for a worked-out comparison between ARD and `Bayesian Ridge Regression`_.\n \n+See :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py` for a comparison between various methods - Lasso, ARD and ElasticNet - on correlated data.\n \nComment: ```suggestion\r\n```\r\n\r\nNow these two are linked twice. So, we should remove the duplicates. (The line after should also be removed, to avoid double blank lines again, but I cannot comment there.)",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "doc/modules/linear_model.rst",
    "pr_number": 31425,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2109302596,
    "comment_created_at": "2025-05-27T14:07:47Z"
  },
  {
    "code": "@@ -633,6 +633,10 @@ class MinCovDet(EmpiricalCovariance):\n     location_ : ndarray of shape (n_features,)\n         Estimated robust location.\n \n+        For an example of comparing raw robust estimates with\n+        the true location and covariance, refer to\n+        :ref:`sphx_glr_auto_examples_covariance_plot_robust_vs_empirical_covariance.py`",
    "comment": "```suggestion\r\n        :ref:`sphx_glr_auto_examples_covariance_plot_robust_vs_empirical_covariance.py`.\r\n```\r\nJust a little nit.",
    "line_number": 638,
    "enriched": "File: sklearn/covariance/_robust_covariance.py\nCode: @@ -633,6 +633,10 @@ class MinCovDet(EmpiricalCovariance):\n     location_ : ndarray of shape (n_features,)\n         Estimated robust location.\n \n+        For an example of comparing raw robust estimates with\n+        the true location and covariance, refer to\n+        :ref:`sphx_glr_auto_examples_covariance_plot_robust_vs_empirical_covariance.py`\nComment: ```suggestion\r\n        :ref:`sphx_glr_auto_examples_covariance_plot_robust_vs_empirical_covariance.py`.\r\n```\r\nJust a little nit.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "sklearn/covariance/_robust_covariance.py",
    "pr_number": 31511,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2150180288,
    "comment_created_at": "2025-06-16T14:39:20Z"
  },
  {
    "code": "@@ -1162,7 +1162,12 @@ def cross_val_predict(\n     >>> y = diabetes.target[:150]\n     >>> lasso = linear_model.Lasso()\n     >>> y_pred = cross_val_predict(lasso, X, y, cv=3)\n+\n+    For a detailed example of using ``cross_val_predict`` to visualize\n+    prediction errors, please see\n+    :ref:`this example <sphx_glr_auto_examples_model_selection_plot_cv_predict.py>`.",
    "comment": "We don't need \"this example\" inside `:ref:` here.",
    "line_number": 1168,
    "enriched": "File: sklearn/model_selection/_validation.py\nCode: @@ -1162,7 +1162,12 @@ def cross_val_predict(\n     >>> y = diabetes.target[:150]\n     >>> lasso = linear_model.Lasso()\n     >>> y_pred = cross_val_predict(lasso, X, y, cv=3)\n+\n+    For a detailed example of using ``cross_val_predict`` to visualize\n+    prediction errors, please see\n+    :ref:`this example <sphx_glr_auto_examples_model_selection_plot_cv_predict.py>`.\nComment: We don't need \"this example\" inside `:ref:` here.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "sklearn/model_selection/_validation.py",
    "pr_number": 31504,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2146208389,
    "comment_created_at": "2025-06-13T22:06:10Z"
  },
  {
    "code": "@@ -1000,3 +1001,46 @@ def test_get_params_html():\n \n     assert est._get_params_html() == {\"l1\": 0, \"empty\": \"test\"}\n     assert est._get_params_html().non_default == (\"empty\",)\n+\n+\n+# non regression test see github issue #31525\n+def test_get_params_html_ridgecv():\n+    est = RidgeCV(np.logspace(-3, 3, num=10))\n+    assert_allclose(\n+        est._get_params_html()[\"alphas\"],\n+        np.array(\n+            [\n+                1.00000000e-03,\n+                4.64158883e-03,\n+                2.15443469e-02,\n+                1.00000000e-01,\n+                4.64158883e-01,\n+                2.15443469e00,\n+                1.00000000e01,\n+                4.64158883e01,\n+                2.15443469e02,\n+                1.00000000e03,\n+            ]\n+        ),\n+    )\n+\n+\n+@pytest.mark.parametrize(\n+    \"initial_value, instance_value, non_default_length\",\n+    [\n+        (np.array([np.float32(2)]), [2], 0),\n+        ([2.0], [np.int32(2)], 0),\n+        (True, 1, 0),\n+        ([1, 2], [3], 1),\n+        (np.array([1]), 1, 1),\n+    ],\n+)\n+def test_get_params_html_types(initial_value, instance_value, non_default_length):\n+    class MyEstimator(BaseEstimator):\n+        def __init__(self, initial_value=initial_value):\n+            self.initial_value = initial_value\n+\n+    est = MyEstimator(initial_value=instance_value)\n+    # when initial_value and instance_value are different, it means\n+    # that instance_value is a non default value\n+    assert len(est._get_params_html().non_default) == non_default_length",
    "comment": "Let's make this test a bit more generic to cover more cases:\r\n\r\n```python\r\ndef make_estimator_with_param(default_value):\r\n    class DynamicEstimator(BaseEstimator):\r\n        def __init__(self, param=default_value):\r\n            self.param = param\r\n\r\n    return DynamicEstimator\r\n\r\n\r\n@pytest.mark.parametrize(\r\n    \"default_value, test_value\",\r\n    [\r\n        ((), (1,)),\r\n        ((), [1]),\r\n        ((), np.array([1])),\r\n        ((1, 2), (3, 4)),\r\n        ((1, 2), [3, 4]),\r\n        ((1, 2), np.array([3, 4])),\r\n        (None, 1),\r\n        (None, []),\r\n        (None, lambda x: x),\r\n        (np.nan, 1.0),\r\n        (np.nan, np.array([np.nan])),\r\n        (\"abc\", \"def\"),\r\n        (\"abc\", [\"abc\"]),\r\n        (True, False),\r\n        (1, 2),\r\n        (1, [1]),\r\n        (1, np.array([1])),\r\n        (1.0, 2.0),\r\n        (1.0, [1.0]),\r\n        (1.0, np.array([1.0])),\r\n    ],\r\n)\r\ndef test_param_is_non_default(default_value, test_value):\r\n    \"\"\"Check that we detect non-default parameters with various types.\r\n\r\n    Non-regression test for:\r\n    https://github.com/scikit-learn/scikit-learn/issues/31525\r\n    \"\"\"\r\n    estimator = make_estimator_with_param(default_value)(param=test_value)\r\n    non_default = estimator._get_params_html().non_default\r\n    assert (\r\n        \"param\" in non_default\r\n    ), f\"param should be non-default for {default_value!r} vs {test_value!r}\"\r\n\r\n\r\n@pytest.mark.parametrize(\r\n    \"default_value, test_value\",\r\n    [\r\n        (None, None),\r\n        ((), ()),\r\n        ((), []),\r\n        ((), np.array([])),\r\n        ((1, 2, 3), (1, 2, 3)),\r\n        ((1, 2, 3), [1, 2, 3]),\r\n        ((1, 2, 3), np.array([1, 2, 3])),\r\n        (np.nan, np.nan),\r\n        (\"abc\", \"abc\"),\r\n        (True, True),\r\n        (1, 1),\r\n        (1.0, 1.0),\r\n    ],\r\n)\r\ndef test_param_is_default(default_value, test_value):\r\n    \"\"\"Check that we detect the default parameters and values in an array-like will\r\n    be reported as default as well.\r\n\r\n    Non-regression test for:\r\n    https://github.com/scikit-learn/scikit-learn/issues/31525\r\n    \"\"\"\r\n    estimator = make_estimator_with_param(default_value)(param=test_value)\r\n    non_default = estimator._get_params_html().non_default\r\n    assert (\r\n        \"param\" not in non_default\r\n    ), f\"param should not be non-default for {default_value!r} vs {test_value!r}\"\r\n```\r\n\r\nThose tests where indeed failing before in various manner when we had array and scalar or empty arrays.",
    "line_number": 1046,
    "enriched": "File: sklearn/tests/test_base.py\nCode: @@ -1000,3 +1001,46 @@ def test_get_params_html():\n \n     assert est._get_params_html() == {\"l1\": 0, \"empty\": \"test\"}\n     assert est._get_params_html().non_default == (\"empty\",)\n+\n+\n+# non regression test see github issue #31525\n+def test_get_params_html_ridgecv():\n+    est = RidgeCV(np.logspace(-3, 3, num=10))\n+    assert_allclose(\n+        est._get_params_html()[\"alphas\"],\n+        np.array(\n+            [\n+                1.00000000e-03,\n+                4.64158883e-03,\n+                2.15443469e-02,\n+                1.00000000e-01,\n+                4.64158883e-01,\n+                2.15443469e00,\n+                1.00000000e01,\n+                4.64158883e01,\n+                2.15443469e02,\n+                1.00000000e03,\n+            ]\n+        ),\n+    )\n+\n+\n+@pytest.mark.parametrize(\n+    \"initial_value, instance_value, non_default_length\",\n+    [\n+        (np.array([np.float32(2)]), [2], 0),\n+        ([2.0], [np.int32(2)], 0),\n+        (True, 1, 0),\n+        ([1, 2], [3], 1),\n+        (np.array([1]), 1, 1),\n+    ],\n+)\n+def test_get_params_html_types(initial_value, instance_value, non_default_length):\n+    class MyEstimator(BaseEstimator):\n+        def __init__(self, initial_value=initial_value):\n+            self.initial_value = initial_value\n+\n+    est = MyEstimator(initial_value=instance_value)\n+    # when initial_value and instance_value are different, it means\n+    # that instance_value is a non default value\n+    assert len(est._get_params_html().non_default) == non_default_length\nComment: Let's make this test a bit more generic to cover more cases:\r\n\r\n```python\r\ndef make_estimator_with_param(default_value):\r\n    class DynamicEstimator(BaseEstimator):\r\n        def __init__(self, param=default_value):\r\n            self.param = param\r\n\r\n    return DynamicEstimator\r\n\r\n\r\n@pytest.mark.parametrize(\r\n    \"default_value, test_value\",\r\n    [\r\n        ((), (1,)),\r\n        ((), [1]),\r\n        ((), np.array([1])),\r\n        ((1, 2), (3, 4)),\r\n        ((1, 2), [3, 4]),\r\n        ((1, 2), np.array([3, 4])),\r\n        (None, 1),\r\n        (None, []),\r\n        (None, lambda x: x),\r\n        (np.nan, 1.0),\r\n        (np.nan, np.array([np.nan])),\r\n        (\"abc\", \"def\"),\r\n        (\"abc\", [\"abc\"]),\r\n        (True, False),\r\n        (1, 2),\r\n        (1, [1]),\r\n        (1, np.array([1])),\r\n        (1.0, 2.0),\r\n        (1.0, [1.0]),\r\n        (1.0, np.array([1.0])),\r\n    ],\r\n)\r\ndef test_param_is_non_default(default_value, test_value):\r\n    \"\"\"Check that we detect non-default parameters with various types.\r\n\r\n    Non-regression test for:\r\n    https://github.com/scikit-learn/scikit-learn/issues/31525\r\n    \"\"\"\r\n    estimator = make_estimator_with_param(default_value)(param=test_value)\r\n    non_default = estimator._get_params_html().non_default\r\n    assert (\r\n        \"param\" in non_default\r\n    ), f\"param should be non-default for {default_value!r} vs {test_value!r}\"\r\n\r\n\r\n@pytest.mark.parametrize(\r\n    \"default_value, test_value\",\r\n    [\r\n        (None, None),\r\n        ((), ()),\r\n        ((), []),\r\n        ((), np.array([])),\r\n        ((1, 2, 3), (1, 2, 3)),\r\n        ((1, 2, 3), [1, 2, 3]),\r\n        ((1, 2, 3), np.array([1, 2, 3])),\r\n        (np.nan, np.nan),\r\n        (\"abc\", \"abc\"),\r\n        (True, True),\r\n        (1, 1),\r\n        (1.0, 1.0),\r\n    ],\r\n)\r\ndef test_param_is_default(default_value, test_value):\r\n    \"\"\"Check that we detect the default parameters and values in an array-like will\r\n    be reported as default as well.\r\n\r\n    Non-regression test for:\r\n    https://github.com/scikit-learn/scikit-learn/issues/31525\r\n    \"\"\"\r\n    estimator = make_estimator_with_param(default_value)(param=test_value)\r\n    non_default = estimator._get_params_html().non_default\r\n    assert (\r\n        \"param\" not in non_default\r\n    ), f\"param should not be non-default for {default_value!r} vs {test_value!r}\"\r\n```\r\n\r\nThose tests where indeed failing before in various manner when we had array and scalar or empty arrays.",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "sklearn/tests/test_base.py",
    "pr_number": 31528,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2145110082,
    "comment_created_at": "2025-06-13T13:33:28Z"
  },
  {
    "code": "@@ -23,6 +76,31 @@ def _read_params(name, value, non_default_params):\n     return {\"param_type\": param_type, \"param_name\": name, \"param_value\": cleaned_value}\n ",
    "comment": "It seems that `r.maxstring`is not working. \r\nNoticed while working on adding methods table.",
    "line_number": 77,
    "enriched": "File: sklearn/utils/_repr_html/params.py\nCode: @@ -23,6 +76,31 @@ def _read_params(name, value, non_default_params):\n     return {\"param_type\": param_type, \"param_name\": name, \"param_value\": cleaned_value}\n \nComment: It seems that `r.maxstring`is not working. \r\nNoticed while working on adding methods table.",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "sklearn/utils/_repr_html/params.py",
    "pr_number": 31564,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2189174044,
    "comment_created_at": "2025-07-07T07:17:04Z"
  },
  {
    "code": "@@ -392,3 +392,15 @@ def _in_unstable_openblas_configuration():\n             # See discussions in https://github.com/numpy/numpy/issues/19411\n             return True  # pragma: no cover\n     return False\n+\n+\n+# Remove when minimum pyarrow version is 17.0.0",
    "comment": "```suggestion\r\n# TODO(pyarrow): Remove when minimum pyarrow version is 17.0.0\r\n```",
    "line_number": 397,
    "enriched": "File: sklearn/utils/fixes.py\nCode: @@ -392,3 +392,15 @@ def _in_unstable_openblas_configuration():\n             # See discussions in https://github.com/numpy/numpy/issues/19411\n             return True  # pragma: no cover\n     return False\n+\n+\n+# Remove when minimum pyarrow version is 17.0.0\nComment: ```suggestion\r\n# TODO(pyarrow): Remove when minimum pyarrow version is 17.0.0\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "sklearn/utils/fixes.py",
    "pr_number": 31605,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2163239916,
    "comment_created_at": "2025-06-24T08:04:17Z"
  },
  {
    "code": "@@ -215,16 +210,18 @@ def remove_from(alist, to_remove):\n         \"folder\": \"build_tools/azure\",\n         \"platform\": \"linux-64\",\n         \"channels\": [\"defaults\"],\n-        \"conda_dependencies\": [\"python\", \"ccache\"],\n-        \"pip_dependencies\": (\n-            remove_from(common_dependencies, [\"python\", \"blas\", \"pip\"])\n+        \"conda_dependencies\": (",
    "comment": "So this build's goal (note the `pip` in `pylatest_pip_openblas_pandas`) is to install everything with `pip`, and you don't want to move things to `conda` dependencies here.",
    "line_number": 213,
    "enriched": "File: build_tools/update_environments_and_lock_files.py\nCode: @@ -215,16 +210,18 @@ def remove_from(alist, to_remove):\n         \"folder\": \"build_tools/azure\",\n         \"platform\": \"linux-64\",\n         \"channels\": [\"defaults\"],\n-        \"conda_dependencies\": [\"python\", \"ccache\"],\n-        \"pip_dependencies\": (\n-            remove_from(common_dependencies, [\"python\", \"blas\", \"pip\"])\n+        \"conda_dependencies\": (\nComment: So this build's goal (note the `pip` in `pylatest_pip_openblas_pandas`) is to install everything with `pip`, and you don't want to move things to `conda` dependencies here.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "build_tools/update_environments_and_lock_files.py",
    "pr_number": 31623,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2166312605,
    "comment_created_at": "2025-06-25T09:52:16Z"
  },
  {
    "code": "@@ -297,7 +297,7 @@ def remove_from(alist, to_remove):\n         ],\n         \"package_constraints\": {\n             \"python\": \"3.10\",\n-            \"blas\": \"[build=mkl]\",\n+            \"blas\": \"[build=openblas]\",",
    "comment": "we should rename the mkl to openblas but I wanted to test without to have to change anything else.",
    "line_number": 300,
    "enriched": "File: build_tools/update_environments_and_lock_files.py\nCode: @@ -297,7 +297,7 @@ def remove_from(alist, to_remove):\n         ],\n         \"package_constraints\": {\n             \"python\": \"3.10\",\n-            \"blas\": \"[build=mkl]\",\n+            \"blas\": \"[build=openblas]\",\nComment: we should rename the mkl to openblas but I wanted to test without to have to change anything else.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "build_tools/update_environments_and_lock_files.py",
    "pr_number": 31641,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2166267516,
    "comment_created_at": "2025-06-25T09:30:56Z"
  },
  {
    "code": "@@ -735,13 +722,6 @@ def make_scorer(\n r2_scorer = make_scorer(r2_score)\n neg_max_error_scorer = make_scorer(max_error, greater_is_better=False)\n max_error_scorer = make_scorer(max_error, greater_is_better=False)",
    "comment": "I think we can remove the defined `max_error_scorer` (from currently line 724) as well.\r\nIt is not not used anywhere anymore.",
    "line_number": 724,
    "enriched": "File: sklearn/metrics/_scorer.py\nCode: @@ -735,13 +722,6 @@ def make_scorer(\n r2_scorer = make_scorer(r2_score)\n neg_max_error_scorer = make_scorer(max_error, greater_is_better=False)\n max_error_scorer = make_scorer(max_error, greater_is_better=False)\nComment: I think we can remove the defined `max_error_scorer` (from currently line 724) as well.\r\nIt is not not used anywhere anymore.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "sklearn/metrics/_scorer.py",
    "pr_number": 31753,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2204766364,
    "comment_created_at": "2025-07-14T12:23:19Z"
  },
  {
    "code": "@@ -278,19 +294,36 @@ def from_predictions(\n         >>> X_train, X_test, y_train, y_test = train_test_split(\n         ...     X, y, test_size=0.4, random_state=0)\n         >>> clf = SVC(random_state=0).fit(X_train, y_train)\n-        >>> y_pred = clf.decision_function(X_test)\n+        >>> y_score = clf.decision_function(X_test)\n         >>> DetCurveDisplay.from_predictions(\n-        ...    y_test, y_pred)\n+        ...    y_test, y_score)\n         <...>\n         >>> plt.show()\n         \"\"\"\n+        # TODO(1.9): remove after the end of the deprecation period of `y_pred`\n+        if y_score is not None and not (\n+            isinstance(y_pred, str) and y_pred == \"deprecated\"\n+        ):\n+            raise ValueError(\n+                \"`y_pred` and `y_score` cannot be both specified. Please use `y_score`\"\n+                \" only as `y_pred` is deprecated in 1.7 and will be removed in 1.9.\"\n+            )\n+        if not (isinstance(y_pred, str) and y_pred == \"deprecated\"):\n+            warnings.warn(\n+                (\n+                    \"y_pred is deprecated in 1.7 and will be removed in 1.9. \"\n+                    \"Please use `y_score` instead.\"\n+                ),\n+                FutureWarning,\n+            )\n+            y_score = y_pred",
    "comment": "Let's refactor this out to `sklearn/utils/_plotting.py` and it can be used for precision recall, ROC and DET displays",
    "line_number": 319,
    "enriched": "File: sklearn/metrics/_plot/det_curve.py\nCode: @@ -278,19 +294,36 @@ def from_predictions(\n         >>> X_train, X_test, y_train, y_test = train_test_split(\n         ...     X, y, test_size=0.4, random_state=0)\n         >>> clf = SVC(random_state=0).fit(X_train, y_train)\n-        >>> y_pred = clf.decision_function(X_test)\n+        >>> y_score = clf.decision_function(X_test)\n         >>> DetCurveDisplay.from_predictions(\n-        ...    y_test, y_pred)\n+        ...    y_test, y_score)\n         <...>\n         >>> plt.show()\n         \"\"\"\n+        # TODO(1.9): remove after the end of the deprecation period of `y_pred`\n+        if y_score is not None and not (\n+            isinstance(y_pred, str) and y_pred == \"deprecated\"\n+        ):\n+            raise ValueError(\n+                \"`y_pred` and `y_score` cannot be both specified. Please use `y_score`\"\n+                \" only as `y_pred` is deprecated in 1.7 and will be removed in 1.9.\"\n+            )\n+        if not (isinstance(y_pred, str) and y_pred == \"deprecated\"):\n+            warnings.warn(\n+                (\n+                    \"y_pred is deprecated in 1.7 and will be removed in 1.9. \"\n+                    \"Please use `y_score` instead.\"\n+                ),\n+                FutureWarning,\n+            )\n+            y_score = y_pred\nComment: Let's refactor this out to `sklearn/utils/_plotting.py` and it can be used for precision recall, ROC and DET displays",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "sklearn/metrics/_plot/det_curve.py",
    "pr_number": 31764,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2212207569,
    "comment_created_at": "2025-07-17T04:07:19Z"
  },
  {
    "code": "@@ -61,6 +61,8 @@ jobs:\n       - OPENBLAS_NUM_THREADS: 2\n       - CONDA_ENV_NAME: testenv\n       - LOCK_FILE: build_tools/circle/doc_linux-64_conda.lock\n+      # Disable sphinx parallelism to avoid EOFError or job stalling in CircleCI\n+      - SPHINX_NUMJOBS: 1",
    "comment": "it'd be nice to have it run sequencially on a local machine too though.",
    "line_number": 65,
    "enriched": "File: .circleci/config.yml\nCode: @@ -61,6 +61,8 @@ jobs:\n       - OPENBLAS_NUM_THREADS: 2\n       - CONDA_ENV_NAME: testenv\n       - LOCK_FILE: build_tools/circle/doc_linux-64_conda.lock\n+      # Disable sphinx parallelism to avoid EOFError or job stalling in CircleCI\n+      - SPHINX_NUMJOBS: 1\nComment: it'd be nice to have it run sequencially on a local machine too though.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": ".circleci/config.yml",
    "pr_number": 25832,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1133741586,
    "comment_created_at": "2023-03-13T10:43:43Z"
  },
  {
    "code": "@@ -1,106 +1,149 @@\n-# -*- coding: utf-8 -*-\n \"\"\"\n-=========================================================\n+=============================\n Importance of Feature Scaling\n-=========================================================\n-\n-Feature scaling through standardization (or Z-score normalization)\n-can be an important preprocessing step for many machine learning\n-algorithms. Standardization involves rescaling the features such\n-that they have the properties of a standard normal distribution\n-with a mean of zero and a standard deviation of one.\n-\n-While many algorithms (such as SVM, K-nearest neighbors, and logistic\n-regression) require features to be normalized, intuitively we can\n-think of Principle Component Analysis (PCA) as being a prime example\n-of when normalization is important. In PCA we are interested in the\n-components that maximize the variance. If one component (e.g. human\n-height) varies less than another (e.g. weight) because of their\n-respective scales (meters vs. kilos), PCA might determine that the\n-direction of maximal variance more closely corresponds with the\n-'weight' axis, if those features are not scaled. As a change in\n-height of one meter can be considered much more important than the\n-change in weight of one kilogram, this is clearly incorrect.\n-\n-To illustrate this, :class:`PCA <sklearn.decomposition.PCA>`\n-is performed comparing the use of data with\n-:class:`StandardScaler <sklearn.preprocessing.StandardScaler>` applied,\n-to unscaled data. The results are visualized and a clear difference noted.\n-The 1st principal component in the unscaled set can be seen. It can be seen\n-that feature #13 dominates the direction, being a whole two orders of\n-magnitude above the other features. This is contrasted when observing\n-the principal component for the scaled version of the data. In the scaled\n-version, the orders of magnitude are roughly the same across all the features.\n-\n-The dataset used is the Wine Dataset available at UCI. This dataset\n-has continuous features that are heterogeneous in scale due to differing\n-properties that they measure (i.e. alcohol content and malic acid).\n-\n-The transformed data is then used to train a naive Bayes classifier, and a\n-clear difference in prediction accuracies is observed wherein the dataset\n-which is scaled before PCA vastly outperforms the unscaled version.\n+=============================\n \n-\"\"\"\n-import matplotlib.pyplot as plt\n+Feature scaling through standardization (also called Z-score normalization) can\n+be an important preprocessing step for many machine learning algorithms. It\n+involves rescaling each feature such that it has a standard deviation equals to\n+1 and a mean equals to 0.\n \n-from sklearn.model_selection import train_test_split\n-from sklearn.preprocessing import StandardScaler\n-from sklearn.decomposition import PCA\n-from sklearn.naive_bayes import GaussianNB\n-from sklearn.metrics import accuracy_score\n-from sklearn.datasets import load_wine\n-from sklearn.pipeline import make_pipeline\n+Many algorithms require features to be normalized, either to ease the",
    "comment": "It would be nice to know which ones are not affected (tree based models).",
    "line_number": 11,
    "enriched": "File: examples/preprocessing/plot_scaling_importance.py\nCode: @@ -1,106 +1,149 @@\n-# -*- coding: utf-8 -*-\n \"\"\"\n-=========================================================\n+=============================\n Importance of Feature Scaling\n-=========================================================\n-\n-Feature scaling through standardization (or Z-score normalization)\n-can be an important preprocessing step for many machine learning\n-algorithms. Standardization involves rescaling the features such\n-that they have the properties of a standard normal distribution\n-with a mean of zero and a standard deviation of one.\n-\n-While many algorithms (such as SVM, K-nearest neighbors, and logistic\n-regression) require features to be normalized, intuitively we can\n-think of Principle Component Analysis (PCA) as being a prime example\n-of when normalization is important. In PCA we are interested in the\n-components that maximize the variance. If one component (e.g. human\n-height) varies less than another (e.g. weight) because of their\n-respective scales (meters vs. kilos), PCA might determine that the\n-direction of maximal variance more closely corresponds with the\n-'weight' axis, if those features are not scaled. As a change in\n-height of one meter can be considered much more important than the\n-change in weight of one kilogram, this is clearly incorrect.\n-\n-To illustrate this, :class:`PCA <sklearn.decomposition.PCA>`\n-is performed comparing the use of data with\n-:class:`StandardScaler <sklearn.preprocessing.StandardScaler>` applied,\n-to unscaled data. The results are visualized and a clear difference noted.\n-The 1st principal component in the unscaled set can be seen. It can be seen\n-that feature #13 dominates the direction, being a whole two orders of\n-magnitude above the other features. This is contrasted when observing\n-the principal component for the scaled version of the data. In the scaled\n-version, the orders of magnitude are roughly the same across all the features.\n-\n-The dataset used is the Wine Dataset available at UCI. This dataset\n-has continuous features that are heterogeneous in scale due to differing\n-properties that they measure (i.e. alcohol content and malic acid).\n-\n-The transformed data is then used to train a naive Bayes classifier, and a\n-clear difference in prediction accuracies is observed wherein the dataset\n-which is scaled before PCA vastly outperforms the unscaled version.\n+=============================\n \n-\"\"\"\n-import matplotlib.pyplot as plt\n+Feature scaling through standardization (also called Z-score normalization) can\n+be an important preprocessing step for many machine learning algorithms. It\n+involves rescaling each feature such that it has a standard deviation equals to\n+1 and a mean equals to 0.\n \n-from sklearn.model_selection import train_test_split\n-from sklearn.preprocessing import StandardScaler\n-from sklearn.decomposition import PCA\n-from sklearn.naive_bayes import GaussianNB\n-from sklearn.metrics import accuracy_score\n-from sklearn.datasets import load_wine\n-from sklearn.pipeline import make_pipeline\n+Many algorithms require features to be normalized, either to ease the\nComment: It would be nice to know which ones are not affected (tree based models).",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "examples/preprocessing/plot_scaling_importance.py",
    "pr_number": 25012,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1043833444,
    "comment_created_at": "2022-12-08T21:15:34Z"
  },
    {
    "code": "@@ -959,8 +959,10 @@ def __init__(self):\n         detect_arch = (\n             (\"cc_on_x64\",      \".*(x|x86_|amd)64.*\", \"\"),\n             (\"cc_on_x86\",      \".*(win32|x86|i386|i686).*\", \"\"),\n-            (\"cc_on_ppc64le\",  \".*(powerpc|ppc)64(el|le).*\", \"\"),\n-            (\"cc_on_ppc64\",    \".*(powerpc|ppc)64.*\", \"\"),\n+            (\"cc_on_ppc64le\",  \".*(powerpc|ppc).*\", \"defined(__LP64__) && \"\n+                                          \"defined(__LITTLE_ENDIAN__)\"),\n+            (\"cc_on_ppc64\",    \".*(powerpc|ppc).*\", \"defined(__LP64__) && \"\n+                                          \"defined(__BIG_ENDIAN__)\"),",
    "comment": "Are these macros also good for gcc?",
    "line_number": 967,
    "enriched": "File: numpy/distutils/ccompiler_opt.py\nCode: @@ -959,8 +959,10 @@ def __init__(self):\n         detect_arch = (\n             (\"cc_on_x64\",      \".*(x|x86_|amd)64.*\", \"\"),\n             (\"cc_on_x86\",      \".*(win32|x86|i386|i686).*\", \"\"),\n-            (\"cc_on_ppc64le\",  \".*(powerpc|ppc)64(el|le).*\", \"\"),\n-            (\"cc_on_ppc64\",    \".*(powerpc|ppc)64.*\", \"\"),\n+            (\"cc_on_ppc64le\",  \".*(powerpc|ppc).*\", \"defined(__LP64__) && \"\n+                                          \"defined(__LITTLE_ENDIAN__)\"),\n+            (\"cc_on_ppc64\",    \".*(powerpc|ppc).*\", \"defined(__LP64__) && \"\n+                                          \"defined(__BIG_ENDIAN__)\"),\nComment: Are these macros also good for gcc?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "numpy/distutils/ccompiler_opt.py",
    "pr_number": 22777,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 1046189684,
    "comment_created_at": "2022-12-12T17:58:18Z"
  },
  {
    "code": "@@ -482,28 +482,20 @@ Importing the API\n \n .. c:macro:: PY_UFUNC_UNIQUE_SYMBOL\n \n+.. c:macro:: PY_UFUNC_ATTRIBUTE\n+\n .. c:macro:: NO_IMPORT_UFUNC\n \n .. c:function:: void import_ufunc(void)\n \n-    These are the constants and functions for accessing the ufunc\n+    These are the macros and functions for accessing the ufunc\n     C-API from extension modules in precisely the same way as the\n     array C-API can be accessed. The ``import_ufunc`` () function must\n     always be called (in the initialization subroutine of the\n-    extension module). If your extension module is in one file then\n-    that is all that is required. The other two constants are useful\n-    if your extension module makes use of multiple files. In that\n-    case, define :c:data:`PY_UFUNC_UNIQUE_SYMBOL` to something unique to\n-    your code and then in source files that do not contain the module\n-    initialization function but still need access to the UFUNC API,\n-    define :c:data:`PY_UFUNC_UNIQUE_SYMBOL` to the same name used previously\n-    and also define :c:data:`NO_IMPORT_UFUNC`.\n-\n-    The C-API is actually an array of function pointers. This array is\n-    created (and pointed to by a global variable) by import_ufunc. The\n-    global variable is either statically defined or allowed to be seen\n-    by other files depending on the state of\n-    :c:data:`PY_UFUNC_UNIQUE_SYMBOL` and :c:data:`NO_IMPORT_UFUNC`.\n+    extension module).\n+\n+    To understand how these macros work together, please refer to\n+    :c:func:`import_array`.",
    "comment": "Good idea to just point there, thanks.",
    "line_number": 498,
    "enriched": "File: doc/source/reference/c-api/ufunc.rst\nCode: @@ -482,28 +482,20 @@ Importing the API\n \n .. c:macro:: PY_UFUNC_UNIQUE_SYMBOL\n \n+.. c:macro:: PY_UFUNC_ATTRIBUTE\n+\n .. c:macro:: NO_IMPORT_UFUNC\n \n .. c:function:: void import_ufunc(void)\n \n-    These are the constants and functions for accessing the ufunc\n+    These are the macros and functions for accessing the ufunc\n     C-API from extension modules in precisely the same way as the\n     array C-API can be accessed. The ``import_ufunc`` () function must\n     always be called (in the initialization subroutine of the\n-    extension module). If your extension module is in one file then\n-    that is all that is required. The other two constants are useful\n-    if your extension module makes use of multiple files. In that\n-    case, define :c:data:`PY_UFUNC_UNIQUE_SYMBOL` to something unique to\n-    your code and then in source files that do not contain the module\n-    initialization function but still need access to the UFUNC API,\n-    define :c:data:`PY_UFUNC_UNIQUE_SYMBOL` to the same name used previously\n-    and also define :c:data:`NO_IMPORT_UFUNC`.\n-\n-    The C-API is actually an array of function pointers. This array is\n-    created (and pointed to by a global variable) by import_ufunc. The\n-    global variable is either statically defined or allowed to be seen\n-    by other files depending on the state of\n-    :c:data:`PY_UFUNC_UNIQUE_SYMBOL` and :c:data:`NO_IMPORT_UFUNC`.\n+    extension module).\n+\n+    To understand how these macros work together, please refer to\n+    :c:func:`import_array`.\nComment: Good idea to just point there, thanks.",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "doc/source/reference/c-api/ufunc.rst",
    "pr_number": 22774,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 1053274604,
    "comment_created_at": "2022-12-20T12:43:37Z"
  },
  {
    "code": "@@ -0,0 +1,447 @@\n+# -*- coding: utf-8 -*-\n+\"\"\"\n+=======================================================\n+Demo of DBSCAN, HDBSCAN, OPTICS clustering algorithms\n+=======================================================\n+.. currentmodule:: sklearn\n+\n+In this demo we will take a look at DBSCAN, HDBSCAN, and OPTICs clustering\n+algorithms. We will run each algorithm on multiclass datasets of varying\n+densities, and note the performance changes as we adjust important\n+hyperparameters. We will also show how OPTICS and HDBSCAN\n+can be viewed as generalizations of DBSCAN, and efficiently extract DBSCAN\n+clusterings from the results of running these algorithms.\n+\n+We start by defining helper functions to visualize a dataset and the resulting\n+cluster labels after running a clustering algorithm, if applicable. We will\n+use this function throughout this document.\n+\"\"\"\n+\n+# Authors: The scikit-learn developers\n+# SPDX-License-Identifier: BSD-3-Clause\n+\n+# %%\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\n+def cluster_colours(labels):",
    "comment": "a little docstring for these functions to explain what they try to do would be nice",
    "line_number": 28,
    "enriched": "File: examples/cluster/plot_dbscan_hdbscan_optics.py\nCode: @@ -0,0 +1,447 @@\n+# -*- coding: utf-8 -*-\n+\"\"\"\n+=======================================================\n+Demo of DBSCAN, HDBSCAN, OPTICS clustering algorithms\n+=======================================================\n+.. currentmodule:: sklearn\n+\n+In this demo we will take a look at DBSCAN, HDBSCAN, and OPTICs clustering\n+algorithms. We will run each algorithm on multiclass datasets of varying\n+densities, and note the performance changes as we adjust important\n+hyperparameters. We will also show how OPTICS and HDBSCAN\n+can be viewed as generalizations of DBSCAN, and efficiently extract DBSCAN\n+clusterings from the results of running these algorithms.\n+\n+We start by defining helper functions to visualize a dataset and the resulting\n+cluster labels after running a clustering algorithm, if applicable. We will\n+use this function throughout this document.\n+\"\"\"\n+\n+# Authors: The scikit-learn developers\n+# SPDX-License-Identifier: BSD-3-Clause\n+\n+# %%\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\n+def cluster_colours(labels):\nComment: a little docstring for these functions to explain what they try to do would be nice",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "examples/cluster/plot_dbscan_hdbscan_optics.py",
    "pr_number": 31102,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2068766699,
    "comment_created_at": "2025-04-30T14:19:01Z"
  },
  {
    "code": "@@ -29,7 +29,6 @@\n from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\n from ..utils.validation import _num_samples, check_non_negative, validate_data\n \n-# mypy error: Module 'sklearn.manifold' has no attribute '_utils'",
    "comment": "mypy didn't complain about `_utils` but did complain about `_barnes_hut_tsne` :woman_shrugging: ",
    "line_number": 32,
    "enriched": "File: sklearn/manifold/_t_sne.py\nCode: @@ -29,7 +29,6 @@\n from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\n from ..utils.validation import _num_samples, check_non_negative, validate_data\n \n-# mypy error: Module 'sklearn.manifold' has no attribute '_utils'\nComment: mypy didn't complain about `_utils` but did complain about `_barnes_hut_tsne` :woman_shrugging: ",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "sklearn/manifold/_t_sne.py",
    "pr_number": 30671,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1923093194,
    "comment_created_at": "2025-01-21T05:31:46Z"
  },
  {
    "code": "@@ -1291,8 +1291,11 @@ def size(obj: ArrayLike, axis: SupportsIndex | None = None) -> int: ...\n def diff(a, /, n=..., axis=..., prepend=..., append=...): ...\n def where(condition, x=..., y=...): ...\n def choose(indices, choices, out=..., mode=...): ...\n+# NOTE: np.ma.round_ is deprecated. Use np.round instead.",
    "comment": "This comment seems redundant, given the deprecation message below\r\n```suggestion\r\n```",
    "line_number": 1294,
    "enriched": "File: numpy/ma/core.pyi\nCode: @@ -1291,8 +1291,11 @@ def size(obj: ArrayLike, axis: SupportsIndex | None = None) -> int: ...\n def diff(a, /, n=..., axis=..., prepend=..., append=...): ...\n def where(condition, x=..., y=...): ...\n def choose(indices, choices, out=..., mode=...): ...\n+# NOTE: np.ma.round_ is deprecated. Use np.round instead.\nComment: This comment seems redundant, given the deprecation message below\r\n```suggestion\r\n```",
    "subcategory": "false positive",
    "category": "false positive",
    "file_path": "numpy/ma/core.pyi",
    "pr_number": 28917,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2078721515,
    "comment_created_at": "2025-05-08T00:51:13Z"
  },
    {
    "code": "@@ -121,49 +120,59 @@ def _smacof_single(\n                 \"init matrix should be of shape (%d, %d)\" % (n_samples, n_components)\n             )\n         X = init\n+    dis = euclidean_distances(X)\n+\n+    # Out of bounds condition cannot happen because we are transforming\n+    # the training set here, but does sometimes get triggered in\n+    # practice due to machine precision issues. Hence \"clip\".\n+    ir = IsotonicRegression(out_of_bounds=\"clip\")\n \n     old_stress = None\n-    ir = IsotonicRegression()\n     for it in range(max_iter):\n         # Compute distance and monotonic regression\n-        dis = euclidean_distances(X)\n-\n         if metric:\n             disparities = dissimilarities\n         else:\n             dis_flat = dis.ravel()\n             # dissimilarities with 0 are considered as missing values\n             dis_flat_w = dis_flat[sim_flat != 0]\n \n-            # Compute the disparities using a monotonic regression\n-            disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)\n-            disparities = dis_flat.copy()\n+            # Compute the disparities using isotonic regression.\n+            # For the first SMACOF iteration, use scaled original dissimilarities.\n+            if it < 1:\n+                disparities_flat = sim_flat_w\n+            else:\n+                disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)\n+            disparities = np.zeros_like(dis_flat)\n             disparities[sim_flat != 0] = disparities_flat\n             disparities = disparities.reshape((n_samples, n_samples))\n             disparities *= np.sqrt(\n                 (n_samples * (n_samples - 1) / 2) / (disparities**2).sum()\n             )\n+            disparities = disparities + disparities.T",
    "comment": "I think the normalization of the `disparities` to `n_samples * (n_samples - 1) / 2`  should happen after fixing the lower triangular part.",
    "line_number": 152,
    "enriched": "File: sklearn/manifold/_mds.py\nCode: @@ -121,49 +120,59 @@ def _smacof_single(\n                 \"init matrix should be of shape (%d, %d)\" % (n_samples, n_components)\n             )\n         X = init\n+    dis = euclidean_distances(X)\n+\n+    # Out of bounds condition cannot happen because we are transforming\n+    # the training set here, but does sometimes get triggered in\n+    # practice due to machine precision issues. Hence \"clip\".\n+    ir = IsotonicRegression(out_of_bounds=\"clip\")\n \n     old_stress = None\n-    ir = IsotonicRegression()\n     for it in range(max_iter):\n         # Compute distance and monotonic regression\n-        dis = euclidean_distances(X)\n-\n         if metric:\n             disparities = dissimilarities\n         else:\n             dis_flat = dis.ravel()\n             # dissimilarities with 0 are considered as missing values\n             dis_flat_w = dis_flat[sim_flat != 0]\n \n-            # Compute the disparities using a monotonic regression\n-            disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)\n-            disparities = dis_flat.copy()\n+            # Compute the disparities using isotonic regression.\n+            # For the first SMACOF iteration, use scaled original dissimilarities.\n+            if it < 1:\n+                disparities_flat = sim_flat_w\n+            else:\n+                disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)\n+            disparities = np.zeros_like(dis_flat)\n             disparities[sim_flat != 0] = disparities_flat\n             disparities = disparities.reshape((n_samples, n_samples))\n             disparities *= np.sqrt(\n                 (n_samples * (n_samples - 1) / 2) / (disparities**2).sum()\n             )\n+            disparities = disparities + disparities.T\nComment: I think the normalization of the `disparities` to `n_samples * (n_samples - 1) / 2`  should happen after fixing the lower triangular part.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "sklearn/manifold/_mds.py",
    "pr_number": 30514,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2007946353,
    "comment_created_at": "2025-03-21T16:38:14Z"
  },
  {
    "code": "@@ -14,9 +14,15 @@\n \n \n def has_source_openmp_flags(target_source):\n+    \"\"\"Check if a source in the target has OpenMP flags\"\"\"",
    "comment": "Given these are one-line implementations, I think it's okay to not have a docstring.",
    "line_number": 17,
    "enriched": "File: build_tools/check-meson-openmp-dependencies.py\nCode: @@ -14,9 +14,15 @@\n \n \n def has_source_openmp_flags(target_source):\n+    \"\"\"Check if a source in the target has OpenMP flags\"\"\"\nComment: Given these are one-line implementations, I think it's okay to not have a docstring.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "build_tools/check-meson-openmp-dependencies.py",
    "pr_number": 30423,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1874480701,
    "comment_created_at": "2024-12-07T14:00:35Z"
  },
  {
    "code": "@@ -1,5 +1,11 @@\n \"\"\"\n-Logistic Regression\n+Logistic Regression(aka logit,MaxEnt) classifier.\n+\n+For a detailed example of how to use logistic reression,see:\n+:ref:`sphx_glr_auto_examples_linear_model_plot_logistic.py`\n+\n+For more information, refer to the user guide:\n+https://scikit-learn.org/regressionstable/modules/linear_model.html#logistic-",
    "comment": "Please revert all of these - they are completely redundant as I mentioned in the previous comment.",
    "line_number": 8,
    "enriched": "File: sklearn/linear_model/_logistic.py\nCode: @@ -1,5 +1,11 @@\n \"\"\"\n-Logistic Regression\n+Logistic Regression(aka logit,MaxEnt) classifier.\n+\n+For a detailed example of how to use logistic reression,see:\n+:ref:`sphx_glr_auto_examples_linear_model_plot_logistic.py`\n+\n+For more information, refer to the user guide:\n+https://scikit-learn.org/regressionstable/modules/linear_model.html#logistic-\nComment: Please revert all of these - they are completely redundant as I mentioned in the previous comment.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "sklearn/linear_model/_logistic.py",
    "pr_number": 29859,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1764361545,
    "comment_created_at": "2024-09-18T04:05:54Z"
  },
  {
    "code": "@@ -2322,6 +2362,46 @@ def _is_multitask(self):\n     def _more_tags(self):\n         return {\"multioutput\": False}\n \n+    def fit(self, X, y, sample_weight=None, **params):\n+        \"\"\"Fit ElasticNet model with coordinate descent.\n+\n+        Fit is on grid of alphas and best alpha estimated by cross-validation.\n+\n+        Parameters\n+        ----------\n+        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n+            Training data. Pass directly as Fortran-contiguous data\n+            to avoid unnecessary memory duplication. If y is mono-output,\n+            X can be sparse. Note that large sparse matrices and arrays\n+            requiring `int64` indices are not accepted.\n+\n+        y : array-like of shape (n_samples,)\n+            Target values.\n+\n+        sample_weight : float or array-like of shape (n_samples,), \\\n+                default=None\n+            Sample weights used for fitting and evaluation of the weighted\n+            mean squared error of each cv-fold. Note that the cross validated\n+            MSE that is finally used to find the best model is the unweighted\n+            mean over the (weighted) MSEs of each test fold.\n+\n+        **params : dict, default=None\n+            Parameters to be passed to the CV splitter.\n+\n+            .. versionadded:: 1.4\n+                Only available if `enable_metadata_routing=True`,\n+                which can be set by using\n+                ``sklearn.set_config(enable_metadata_routing=True)``.\n+                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n+                more details.\n+\n+        Returns\n+        -------\n+        self : object\n+            Returns an instance of fitted model.\n+        \"\"\"\n+        super().fit(X, y, sample_weight=sample_weight, **params)",
    "comment": "```suggestion\r\n        return super().fit(X, y, sample_weight=sample_weight, **params)\r\n```",
    "line_number": 2403,
    "enriched": "File: sklearn/linear_model/_coordinate_descent.py\nCode: @@ -2322,6 +2362,46 @@ def _is_multitask(self):\n     def _more_tags(self):\n         return {\"multioutput\": False}\n \n+    def fit(self, X, y, sample_weight=None, **params):\n+        \"\"\"Fit ElasticNet model with coordinate descent.\n+\n+        Fit is on grid of alphas and best alpha estimated by cross-validation.\n+\n+        Parameters\n+        ----------\n+        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n+            Training data. Pass directly as Fortran-contiguous data\n+            to avoid unnecessary memory duplication. If y is mono-output,\n+            X can be sparse. Note that large sparse matrices and arrays\n+            requiring `int64` indices are not accepted.\n+\n+        y : array-like of shape (n_samples,)\n+            Target values.\n+\n+        sample_weight : float or array-like of shape (n_samples,), \\\n+                default=None\n+            Sample weights used for fitting and evaluation of the weighted\n+            mean squared error of each cv-fold. Note that the cross validated\n+            MSE that is finally used to find the best model is the unweighted\n+            mean over the (weighted) MSEs of each test fold.\n+\n+        **params : dict, default=None\n+            Parameters to be passed to the CV splitter.\n+\n+            .. versionadded:: 1.4\n+                Only available if `enable_metadata_routing=True`,\n+                which can be set by using\n+                ``sklearn.set_config(enable_metadata_routing=True)``.\n+                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n+                more details.\n+\n+        Returns\n+        -------\n+        self : object\n+            Returns an instance of fitted model.\n+        \"\"\"\n+        super().fit(X, y, sample_weight=sample_weight, **params)\nComment: ```suggestion\r\n        return super().fit(X, y, sample_weight=sample_weight, **params)\r\n```",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "sklearn/linear_model/_coordinate_descent.py",
    "pr_number": 29708,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1731028599,
    "comment_created_at": "2024-08-26T10:13:29Z"
  },
  {
    "code": "@@ -11,6 +11,8 @@ being bumped:\n +------------+-----------------+---------------+\n | kiwisolver |       1.0.1     |      1.3.1    |\n +------------+-----------------+---------------+\n+|   numpy    |       1.20.0    |      1.21.0   |",
    "comment": "```suggestion\r\n|   NumPy    |       1.20.0    |      1.21.0   |\r\n```",
    "line_number": 14,
    "enriched": "File: doc/api/next_api_changes/development/24919-KS.rst\nCode: @@ -11,6 +11,8 @@ being bumped:\n +------------+-----------------+---------------+\n | kiwisolver |       1.0.1     |      1.3.1    |\n +------------+-----------------+---------------+\n+|   numpy    |       1.20.0    |      1.21.0   |\nComment: ```suggestion\r\n|   NumPy    |       1.20.0    |      1.21.0   |\r\n```",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "doc/api/next_api_changes/development/24919-KS.rst",
    "pr_number": 24992,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 1081864573,
    "comment_created_at": "2023-01-19T21:08:17Z"
  },
  {
    "code": "@@ -341,18 +341,34 @@ def _make_image(self, A, in_bbox, out_bbox, clip_bbox, magnification=1.0,\n         the given *clip_bbox* (also in pixel space), and magnified by the\n         *magnification* factor.\n \n-        *A* may be a greyscale image (M, N) with a dtype of `~numpy.float32`,\n-        `~numpy.float64`, `~numpy.float128`, `~numpy.uint16` or `~numpy.uint8`,\n-        or an (M, N, 4) RGBA image with a dtype of `~numpy.float32`,\n-        `~numpy.float64`, `~numpy.float128`, or `~numpy.uint8`.\n+        Parameters\n+        ----------\n+        A : ndarray\n+            This may be:",
    "comment": "```suggestion\r\n```\r\nI think this is implied by the next bit",
    "line_number": 347,
    "enriched": "File: lib/matplotlib/image.py\nCode: @@ -341,18 +341,34 @@ def _make_image(self, A, in_bbox, out_bbox, clip_bbox, magnification=1.0,\n         the given *clip_bbox* (also in pixel space), and magnified by the\n         *magnification* factor.\n \n-        *A* may be a greyscale image (M, N) with a dtype of `~numpy.float32`,\n-        `~numpy.float64`, `~numpy.float128`, `~numpy.uint16` or `~numpy.uint8`,\n-        or an (M, N, 4) RGBA image with a dtype of `~numpy.float32`,\n-        `~numpy.float64`, `~numpy.float128`, or `~numpy.uint8`.\n+        Parameters\n+        ----------\n+        A : ndarray\n+            This may be:\nComment: ```suggestion\r\n```\r\nI think this is implied by the next bit",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "lib/matplotlib/image.py",
    "pr_number": 28558,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 1676556880,
    "comment_created_at": "2024-07-12T23:29:51Z"
  },
    {
    "code": "@@ -155,14 +155,18 @@ reliability and consistency in documentation. They are not interchangeable.\n   |                  |                          |   rotational |              |\n   |                  |                          |   motion.\"   |              |\n   +------------------+--------------------------+--------------+--------------+\n-  | Explicit,        | Explicit approach of     | - Explicit   | - object     |\n-  | Object Oriented  | programming in           | - explicit   |   oriented   |\n-  | Programming (OOP)| Matplotlib.              | - OOP        | - OO-style   |\n+  | Axes interface   | Usage pattern in which   | - Axes       | - explicit   |\n+  |                  | one calls methods on     |   interface  |   interface  |\n+  |                  | Axes and Figure to       | - call       | - object     |",
    "comment": "```suggestion\r\n  |                  | Axes and Figure variables to       | - call       | - object     |\r\n```\r\nOr 'objects' instead of 'variables'?",
    "line_number": 160,
    "enriched": "File: doc/devel/style_guide.rst\nCode: @@ -155,14 +155,18 @@ reliability and consistency in documentation. They are not interchangeable.\n   |                  |                          |   rotational |              |\n   |                  |                          |   motion.\"   |              |\n   +------------------+--------------------------+--------------+--------------+\n-  | Explicit,        | Explicit approach of     | - Explicit   | - object     |\n-  | Object Oriented  | programming in           | - explicit   |   oriented   |\n-  | Programming (OOP)| Matplotlib.              | - OOP        | - OO-style   |\n+  | Axes interface   | Usage pattern in which   | - Axes       | - explicit   |\n+  |                  | one calls methods on     |   interface  |   interface  |\n+  |                  | Axes and Figure to       | - call       | - object     |\nComment: ```suggestion\r\n  |                  | Axes and Figure variables to       | - call       | - object     |\r\n```\r\nOr 'objects' instead of 'variables'?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "doc/devel/style_guide.rst",
    "pr_number": 28025,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 1554974876,
    "comment_created_at": "2024-04-07T13:21:03Z"
  },
    {
    "code": "@@ -163,8 +163,25 @@ for section markup characters, i.e.:\n \n This may not yet be applied consistently in existing docs.\n \n+Table formatting\n+^^^^^^^^^^^^^^^^\n+Given the size of the table and length of each entry, use:\n+\n++-------------+-------------------------------+--------------------+\n+|             | small table                   | large table        |\n++-------------+-------------------------------+--------------------+\n+| short entry | `simple or grid table <sg>`__ | `grid table <sg>`_ |",
    "comment": "I think there's one underscore too much:\r\n```suggestion\r\n| short entry | `simple or grid table <sg>`_  | `grid table <sg>`_ |\r\n```",
    "line_number": 173,
    "enriched": "File: doc/devel/document.rst\nCode: @@ -163,8 +163,25 @@ for section markup characters, i.e.:\n \n This may not yet be applied consistently in existing docs.\n \n+Table formatting\n+^^^^^^^^^^^^^^^^\n+Given the size of the table and length of each entry, use:\n+\n++-------------+-------------------------------+--------------------+\n+|             | small table                   | large table        |\n++-------------+-------------------------------+--------------------+\n+| short entry | `simple or grid table <sg>`__ | `grid table <sg>`_ |\nComment: I think there's one underscore too much:\r\n```suggestion\r\n| short entry | `simple or grid table <sg>`_  | `grid table <sg>`_ |\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "doc/devel/document.rst",
    "pr_number": 26754,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 1348597935,
    "comment_created_at": "2023-10-06T11:26:47Z"
  },
  {
    "code": "@@ -1,6 +1,7 @@\n .. _3D_plots:\n \n-3D\n---\n+3D and volumetric data\n+----------------------\n \n-3D plots using the `mpl_toolkits.mplot3d` library.\n+Plotting for three dimensional ``(x,y,z)`` and volumetric ``V(x,y,z)`` data",
    "comment": "```suggestion\nPlotting for three-dimensional ``(x,y,z)`` and volumetric ``V(x,y,z)`` data\n```\n",
    "line_number": 6,
    "enriched": "File: galleries/plot_types/3D/README.rst\nCode: @@ -1,6 +1,7 @@\n .. _3D_plots:\n \n-3D\n---\n+3D and volumetric data\n+----------------------\n \n-3D plots using the `mpl_toolkits.mplot3d` library.\n+Plotting for three dimensional ``(x,y,z)`` and volumetric ``V(x,y,z)`` data\nComment: ```suggestion\nPlotting for three-dimensional ``(x,y,z)`` and volumetric ``V(x,y,z)`` data\n```\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "galleries/plot_types/3D/README.rst",
    "pr_number": 26328,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 1264912000,
    "comment_created_at": "2023-07-17T06:12:41Z"
  },
  {
    "code": "@@ -1885,12 +1885,16 @@ def __call__(self, value, clip=None):\n                 result = np.ma.array(np.clip(result.filled(vmax), vmin, vmax),\n                                      mask=mask)\n             resdat = result.data\n-            resdat -= vmin\n-            resdat[resdat < 0] = 0\n-            np.power(resdat, gamma, resdat)\n-            resdat /= (vmax - vmin) ** gamma\n+            # These values are masked later, but set to zero now to prevent\n+            # invalid value warnings when taking the power\n+            under_mask = resdat < vmin\n+            resdat[under_mask] = vmin\n \n-            result = np.ma.array(resdat, mask=result.mask, copy=False)\n+            resdat = (resdat - vmin) / (vmax - vmin)\n+            resdat = resdat ** gamma\n+\n+            mask = result.mask | under_mask\n+            result = np.ma.array(resdat, mask=mask, copy=False)",
    "comment": "I think this should be \r\n```python\r\ngood = resdat >= vmin\r\nresdat = (resdat - vmin) / (vmax - vmin)\r\nresdat[good] = resdat[good]**gamma\r\n```\r\nThis will make the negative part of the scale linear, but it's better than setting it to be masked.  ",
    "line_number": 1897,
    "enriched": "File: lib/matplotlib/colors.py\nCode: @@ -1885,12 +1885,16 @@ def __call__(self, value, clip=None):\n                 result = np.ma.array(np.clip(result.filled(vmax), vmin, vmax),\n                                      mask=mask)\n             resdat = result.data\n-            resdat -= vmin\n-            resdat[resdat < 0] = 0\n-            np.power(resdat, gamma, resdat)\n-            resdat /= (vmax - vmin) ** gamma\n+            # These values are masked later, but set to zero now to prevent\n+            # invalid value warnings when taking the power\n+            under_mask = resdat < vmin\n+            resdat[under_mask] = vmin\n \n-            result = np.ma.array(resdat, mask=result.mask, copy=False)\n+            resdat = (resdat - vmin) / (vmax - vmin)\n+            resdat = resdat ** gamma\n+\n+            mask = result.mask | under_mask\n+            result = np.ma.array(resdat, mask=mask, copy=False)\nComment: I think this should be \r\n```python\r\ngood = resdat >= vmin\r\nresdat = (resdat - vmin) / (vmax - vmin)\r\nresdat[good] = resdat[good]**gamma\r\n```\r\nThis will make the negative part of the scale linear, but it's better than setting it to be masked.  ",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "lib/matplotlib/colors.py",
    "pr_number": 25256,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 1111276448,
    "comment_created_at": "2023-02-19T16:45:56Z"
  },
  {
    "code": "@@ -36,16 +36,15 @@ Contributor incubator\n ---------------------\n \n The incubator is our non-public communication channel for new contributors. It\n-is a private gitter room moderated by core Matplotlib developers where you can\n+is a private gitter_ room moderated by core Matplotlib developers where you can",
    "comment": "We should probably put (chat) in parenthesis ",
    "line_number": 39,
    "enriched": "File: doc/devel/contributing.rst\nCode: @@ -36,16 +36,15 @@ Contributor incubator\n ---------------------\n \n The incubator is our non-public communication channel for new contributors. It\n-is a private gitter room moderated by core Matplotlib developers where you can\n+is a private gitter_ room moderated by core Matplotlib developers where you can\nComment: We should probably put (chat) in parenthesis ",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "doc/devel/contributing.rst",
    "pr_number": 25214,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 1106008105,
    "comment_created_at": "2023-02-14T15:47:22Z"
  },
  {
    "code": "@@ -2498,7 +2498,7 @@ def tick_values(self, vmin, vmax):\n         #\n         # \"simple\" mode is when the range falls entirely within (-t,\n         # t) -- it should just display (vmin, 0, vmax)",
    "comment": "```suggestion\r\n        # \"simple\" mode is when the range falls entirely within [-t, t]\r\n        #  -- it should just display (vmin, 0, vmax)\r\n```\r\nSquare brackets to [denote inclusive interval](https://en.wikipedia.org/wiki/Interval_(mathematics)#Notations_for_intervals).  Also I think it's slightly more readable to have the interval on one line (there is plenty of space).",
    "line_number": 2500,
    "enriched": "File: lib/matplotlib/ticker.py\nCode: @@ -2498,7 +2498,7 @@ def tick_values(self, vmin, vmax):\n         #\n         # \"simple\" mode is when the range falls entirely within (-t,\n         # t) -- it should just display (vmin, 0, vmax)\nComment: ```suggestion\r\n        # \"simple\" mode is when the range falls entirely within [-t, t]\r\n        #  -- it should just display (vmin, 0, vmax)\r\n```\r\nSquare brackets to [denote inclusive interval](https://en.wikipedia.org/wiki/Interval_(mathematics)#Notations_for_intervals).  Also I think it's slightly more readable to have the interval on one line (there is plenty of space).",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "lib/matplotlib/ticker.py",
    "pr_number": 25970,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 1208615002,
    "comment_created_at": "2023-05-28T18:03:52Z"
  },
    {
    "code": "@@ -1564,8 +1564,10 @@ def subfigures(self, nrows=1, ncols=1, squeeze=True,\n         wspace, hspace : float, default: None\n             The amount of width/height reserved for space between subfigures,\n             expressed as a fraction of the average subfigure width/height.\n-            If not given, the values will be inferred from a figure or\n-            rcParams when necessary.\n+            If not given, the values will be inferred from rcParams if using\n+            constrained layout (see\n+            :class:`~matplotlib.layout_engine.ConstrainedLayoutEngine`), or\n+            zero if not using a layout engine.",
    "comment": "We usually don't need `:class:`; I think this should work:\r\n```suggestion\r\n            constrained layout (see `~.ConstrainedLayoutEngine`), or zero if \r\n            not using a layout engine.\r\n```",
    "line_number": 1570,
    "enriched": "File: lib/matplotlib/figure.py\nCode: @@ -1564,8 +1564,10 @@ def subfigures(self, nrows=1, ncols=1, squeeze=True,\n         wspace, hspace : float, default: None\n             The amount of width/height reserved for space between subfigures,\n             expressed as a fraction of the average subfigure width/height.\n-            If not given, the values will be inferred from a figure or\n-            rcParams when necessary.\n+            If not given, the values will be inferred from rcParams if using\n+            constrained layout (see\n+            :class:`~matplotlib.layout_engine.ConstrainedLayoutEngine`), or\n+            zero if not using a layout engine.\nComment: We usually don't need `:class:`; I think this should work:\r\n```suggestion\r\n            constrained layout (see `~.ConstrainedLayoutEngine`), or zero if \r\n            not using a layout engine.\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "lib/matplotlib/figure.py",
    "pr_number": 25960,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 1238232812,
    "comment_created_at": "2023-06-22T09:04:35Z"
  },
  {
    "code": "@@ -3,7 +3,8 @@\n \"\"\"\n \n import codecs\n-import datetime\n+from datetime import timezone\n+from datetime import datetime",
    "comment": "You should either do like this and then change to `datetime.fromtimestamp` below or only have the previous import and change to `datetime.timezone.utc` below. Same for `backend_svg.py`.",
    "line_number": 7,
    "enriched": "File: lib/matplotlib/backends/backend_ps.py\nCode: @@ -3,7 +3,8 @@\n \"\"\"\n \n import codecs\n-import datetime\n+from datetime import timezone\n+from datetime import datetime\nComment: You should either do like this and then change to `datetime.fromtimestamp` below or only have the previous import and change to `datetime.timezone.utc` below. Same for `backend_svg.py`.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "lib/matplotlib/backends/backend_ps.py",
    "pr_number": 25918,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 1199315224,
    "comment_created_at": "2023-05-19T19:47:34Z"
  },
    {
    "code": "@@ -0,0 +1,294 @@\n+\"\"\"\n+Implements a ``.. figure-mpl::`` directive that is like\n+``.. figure::`` except it allows ``srcset=`` to be passed to the image tag, hence\n+allowing responsive resolution images.\n+\n+There is no particular reason this could not be used standalone, but is meant\n+to be used with :doc:`/api/sphinxext_plot_directive_api`.\n+\n+Note that the directory organization is a bit different than ``.. figure::``.\n+See the *FigureMpl* documentation below.\n+\n+\"\"\"\n+from docutils import nodes\n+\n+from docutils.parsers.rst import directives\n+from docutils.parsers.rst.directives.images import Figure, Image\n+\n+import os\n+from os.path import relpath\n+from pathlib import PurePosixPath, Path\n+import shutil\n+\n+from sphinx.errors import ExtensionError\n+\n+import matplotlib\n+\n+\n+class figmplnode(nodes.General, nodes.Element):\n+    pass\n+\n+\n+class FigureMpl(Figure):\n+    \"\"\"\n+    Implements a directive to allow an optional hidpi image.  Meant to be\n+    used with the *plot_srcset* configuration option in conf.py,\n+    and gets set in the TEMPLATE of plot_directive.py\n+\n+    e.g.::\n+\n+        .. figure-mpl:: plot_directive/some_plots-1.png\n+            :alt: bar\n+            :srcset: plot_directive/some_plots-1.png,\n+                     plot_directive/some_plots-1.2x.png 2.00x\n+            :class: plot-directive\n+\n+    The resulting html (at ``some_plots.html``) is::\n+\n+        <img src=\"sphx_glr_bar_001_hidpi.png\"\n+            srcset=\"_images/some_plot-1.png,\n+                    _images/some_plots-1.2x.png 2.00 x\",\n+            alt=\"bar\"\n+            class=\"plot_directive\" />\n+\n+    Note that the handling of subdirectories is different than that used by the sphinx\n+    figure directive::\n+\n+        .. figure-mpl:: plot_directive/nestedpage/index-1.png\n+            :alt: bar\n+            :srcset: plot_directive/nestedpage/index-1.png\n+                     plot_directive/nestedpage/index-1.2x.png 2.00x\n+            :class: plot_directive\n+\n+    The resulting html (at ``nestedpage/index.html``)::\n+\n+        <img src=\"../_images/nestedpage-index-1.png\"\n+            srcset=\"../_images/nestedpage-index-1.png,\n+                    ../_images/_images/nestedpage-index-1.2x.png 2x\",\n+            alt=\"bar\"\n+            class=\"sphx-glr-single-img\" />\n+\n+    where the subdirectory is included in the image name for uniqueness.\n+    \"\"\"\n+\n+    has_content = False\n+    required_arguments = 1\n+    optional_arguments = 2\n+    final_argument_whitespace = False\n+    option_spec = {\n+        'alt': directives.unchanged,\n+        'height': directives.length_or_unitless,\n+        'width': directives.length_or_percentage_or_unitless,\n+        'scale': directives.nonnegative_int,\n+        'align': Image.align,\n+        'class': directives.class_option,\n+        'caption': directives.unchanged,\n+        'srcset': directives.unchanged,\n+    }\n+\n+    def run(self):\n+\n+        image_node = figmplnode()\n+\n+        imagenm = self.arguments[0]",
    "comment": "Sorry, this is probably because I don't know much about the docbuild process but where is `self.arguments` defined?",
    "line_number": 96,
    "enriched": "File: lib/matplotlib/sphinxext/figmpl_directive.py\nCode: @@ -0,0 +1,294 @@\n+\"\"\"\n+Implements a ``.. figure-mpl::`` directive that is like\n+``.. figure::`` except it allows ``srcset=`` to be passed to the image tag, hence\n+allowing responsive resolution images.\n+\n+There is no particular reason this could not be used standalone, but is meant\n+to be used with :doc:`/api/sphinxext_plot_directive_api`.\n+\n+Note that the directory organization is a bit different than ``.. figure::``.\n+See the *FigureMpl* documentation below.\n+\n+\"\"\"\n+from docutils import nodes\n+\n+from docutils.parsers.rst import directives\n+from docutils.parsers.rst.directives.images import Figure, Image\n+\n+import os\n+from os.path import relpath\n+from pathlib import PurePosixPath, Path\n+import shutil\n+\n+from sphinx.errors import ExtensionError\n+\n+import matplotlib\n+\n+\n+class figmplnode(nodes.General, nodes.Element):\n+    pass\n+\n+\n+class FigureMpl(Figure):\n+    \"\"\"\n+    Implements a directive to allow an optional hidpi image.  Meant to be\n+    used with the *plot_srcset* configuration option in conf.py,\n+    and gets set in the TEMPLATE of plot_directive.py\n+\n+    e.g.::\n+\n+        .. figure-mpl:: plot_directive/some_plots-1.png\n+            :alt: bar\n+            :srcset: plot_directive/some_plots-1.png,\n+                     plot_directive/some_plots-1.2x.png 2.00x\n+            :class: plot-directive\n+\n+    The resulting html (at ``some_plots.html``) is::\n+\n+        <img src=\"sphx_glr_bar_001_hidpi.png\"\n+            srcset=\"_images/some_plot-1.png,\n+                    _images/some_plots-1.2x.png 2.00 x\",\n+            alt=\"bar\"\n+            class=\"plot_directive\" />\n+\n+    Note that the handling of subdirectories is different than that used by the sphinx\n+    figure directive::\n+\n+        .. figure-mpl:: plot_directive/nestedpage/index-1.png\n+            :alt: bar\n+            :srcset: plot_directive/nestedpage/index-1.png\n+                     plot_directive/nestedpage/index-1.2x.png 2.00x\n+            :class: plot_directive\n+\n+    The resulting html (at ``nestedpage/index.html``)::\n+\n+        <img src=\"../_images/nestedpage-index-1.png\"\n+            srcset=\"../_images/nestedpage-index-1.png,\n+                    ../_images/_images/nestedpage-index-1.2x.png 2x\",\n+            alt=\"bar\"\n+            class=\"sphx-glr-single-img\" />\n+\n+    where the subdirectory is included in the image name for uniqueness.\n+    \"\"\"\n+\n+    has_content = False\n+    required_arguments = 1\n+    optional_arguments = 2\n+    final_argument_whitespace = False\n+    option_spec = {\n+        'alt': directives.unchanged,\n+        'height': directives.length_or_unitless,\n+        'width': directives.length_or_percentage_or_unitless,\n+        'scale': directives.nonnegative_int,\n+        'align': Image.align,\n+        'class': directives.class_option,\n+        'caption': directives.unchanged,\n+        'srcset': directives.unchanged,\n+    }\n+\n+    def run(self):\n+\n+        image_node = figmplnode()\n+\n+        imagenm = self.arguments[0]\nComment: Sorry, this is probably because I don't know much about the docbuild process but where is `self.arguments` defined?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/matplotlib/sphinxext/figmpl_directive.py",
    "pr_number": 25515,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 1162197474,
    "comment_created_at": "2023-04-11T00:52:23Z"
  },
  {
    "code": "@@ -0,0 +1,7 @@\n+The ``canvas`` and ``background`` attributes of ``MultiCursor``",
    "comment": "Update file name.",
    "line_number": 1,
    "enriched": "File: doc/api/next_api_changes/deprecations/XXXXX-AL.rst\nCode: @@ -0,0 +1,7 @@\n+The ``canvas`` and ``background`` attributes of ``MultiCursor``\nComment: Update file name.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "doc/api/next_api_changes/deprecations/XXXXX-AL.rst",
    "pr_number": 23348,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 907767601,
    "comment_created_at": "2022-06-27T20:24:06Z"
  },
  {
    "code": "@@ -28,6 +28,8 @@\n     Affine2D, BboxBase, Bbox, BboxTransform, BboxTransformTo,\n     IdentityTransform, TransformedBbox)\n \n+import matplotlib.image as mimage",
    "comment": "This is a very confusing import in this file... this file IS `matplotlib.image`\r\n\r\nYou should be able to remove the `mimage.` below, as `AxesImage` is defined in this file and therefore exists in the namespace already.",
    "line_number": 31,
    "enriched": "File: lib/matplotlib/image.py\nCode: @@ -28,6 +28,8 @@\n     Affine2D, BboxBase, Bbox, BboxTransform, BboxTransformTo,\n     IdentityTransform, TransformedBbox)\n \n+import matplotlib.image as mimage\nComment: This is a very confusing import in this file... this file IS `matplotlib.image`\r\n\r\nYou should be able to remove the `mimage.` below, as `AxesImage` is defined in this file and therefore exists in the namespace already.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "lib/matplotlib/image.py",
    "pr_number": 29203,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 1870269428,
    "comment_created_at": "2024-12-04T20:57:18Z"
  },
    {
    "code": "@@ -912,6 +912,8 @@ http --offline --print=B pie.dev/post \\\n ]\n ```\n \n+Sending other JSON types (`null`, `true`/`false`, numbers or strings) as the top level object is not possible.",
    "comment": "```suggestion\r\nSending scalar JSON types (a single `null`, `true`, `false`,  string or number) as the top-level object is impossible using the key/value syntax. But you can still pass it via [`--raw='<value>'`](#raw-request-body).\r\n```",
    "line_number": 915,
    "enriched": "File: docs/README.md\nCode: @@ -912,6 +912,8 @@ http --offline --print=B pie.dev/post \\\n ]\n ```\n \n+Sending other JSON types (`null`, `true`/`false`, numbers or strings) as the top level object is not possible.\nComment: ```suggestion\r\nSending scalar JSON types (a single `null`, `true`, `false`,  string or number) as the top-level object is impossible using the key/value syntax. But you can still pass it via [`--raw='<value>'`](#raw-request-body).\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/README.md",
    "pr_number": 1496,
    "repo": "httpie",
    "owner": "httpie",
    "comment_id": 1202196657,
    "comment_created_at": "2023-05-23T12:11:24Z"
  },
  {
    "code": "@@ -2,96 +2,54 @@\n Support\n =======\n \n-There are several ways to get in touch with the developers.\n+Connect with scikit-learn developers through various channels for assistance, feedback, or contributions.\n \n+Mailing Lists\n+=============\n \n-.. _mailing_lists:",
    "comment": "Thanks for the PR! Please do not remove this or any of the sphinx directives of the form `.. _directive:` as this is breaking the doc building.",
    "line_number": 8,
    "enriched": "File: doc/support.rst\nCode: @@ -2,96 +2,54 @@\n Support\n =======\n \n-There are several ways to get in touch with the developers.\n+Connect with scikit-learn developers through various channels for assistance, feedback, or contributions.\n \n+Mailing Lists\n+=============\n \n-.. _mailing_lists:\nComment: Thanks for the PR! Please do not remove this or any of the sphinx directives of the form `.. _directive:` as this is breaking the doc building.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "doc/support.rst",
    "pr_number": 28237,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1464602294,
    "comment_created_at": "2024-01-24T09:27:03Z"
  },
  {
    "code": "@@ -117,6 +116,7 @@ class GPTConfig:\n     block_size: int = 1024\n     vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n     n_layer: int = 12\n+    head_size: None | int = None",
    "comment": "It's not working in Python <3.10 - `TypeError: unsupported operand type(s) for |: 'NoneType' and 'type'`. In Python 3.8 I had to use the Union to make it working:\r\n```python\r\nfrom typing import Union\r\n\r\nhead_size: Union[None, int] = None\r\n```\r\nNevertheless I don't think we should create such dependency and moreover we should avoid limiting to use Python 3.10+",
    "line_number": 119,
    "enriched": "File: model.py\nCode: @@ -117,6 +116,7 @@ class GPTConfig:\n     block_size: int = 1024\n     vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n     n_layer: int = 12\n+    head_size: None | int = None\nComment: It's not working in Python <3.10 - `TypeError: unsupported operand type(s) for |: 'NoneType' and 'type'`. In Python 3.8 I had to use the Union to make it working:\r\n```python\r\nfrom typing import Union\r\n\r\nhead_size: Union[None, int] = None\r\n```\r\nNevertheless I don't think we should create such dependency and moreover we should avoid limiting to use Python 3.10+",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "model.py",
    "pr_number": 152,
    "repo": "nanoGPT",
    "owner": "karpathy",
    "comment_id": 1117015348,
    "comment_created_at": "2023-02-24T13:38:15Z"
  },
  {
    "code": "@@ -71,8 +72,9 @@ def pydantic_v1_patch():\n                     del sys.modules[k]\n \n \n-with pydantic_v1_patch():\n-    import sqlmodel as sqlmodel\n+if find_spec(\"pydantic\"):\n+    with pydantic_v1_patch():\n+        import sqlmodel as sqlmodel\n \n \n def sqlmodel_field_has_primary_key(field: Any) -> bool:",
    "comment": "syntax: Typo in docstring: 'priamary' should be 'primary'\n\n```suggestion\n\"\"\"Determines if a field is a primary key.\n\n    Args:\n        field: a rx.model field\n\n    Returns:\n        If field is a primary key (Bool)\n    \"\"\"\n```",
    "line_number": 80,
    "enriched": "File: reflex/utils/compat.py\nCode: @@ -71,8 +72,9 @@ def pydantic_v1_patch():\n                     del sys.modules[k]\n \n \n-with pydantic_v1_patch():\n-    import sqlmodel as sqlmodel\n+if find_spec(\"pydantic\"):\n+    with pydantic_v1_patch():\n+        import sqlmodel as sqlmodel\n \n \n def sqlmodel_field_has_primary_key(field: Any) -> bool:\nComment: syntax: Typo in docstring: 'priamary' should be 'primary'\n\n```suggestion\n\"\"\"Determines if a field is a primary key.\n\n    Args:\n        field: a rx.model field\n\n    Returns:\n        If field is a primary key (Bool)\n    \"\"\"\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "reflex/utils/compat.py",
    "pr_number": 5438,
    "repo": "reflex",
    "owner": "reflex-dev",
    "comment_id": 2141355243,
    "comment_created_at": "2025-06-12T01:36:59Z"
  },
    {
    "code": "@@ -136,7 +136,7 @@\n # The gitignore file.\n GITIGNORE_FILE = \".gitignore\"\n # Files to gitignore.\n-DEFAULT_GITIGNORE = {WEB_DIR, DB_NAME}\n+DEFAULT_GITIGNORE = {WEB_DIR, DB_NAME, \"__pycache__/\", \"*.py[cod]\"}",
    "comment": "we should probably add these to the .gitignore file instead",
    "line_number": 139,
    "enriched": "File: pynecone/constants.py\nCode: @@ -136,7 +136,7 @@\n # The gitignore file.\n GITIGNORE_FILE = \".gitignore\"\n # Files to gitignore.\n-DEFAULT_GITIGNORE = {WEB_DIR, DB_NAME}\n+DEFAULT_GITIGNORE = {WEB_DIR, DB_NAME, \"__pycache__/\", \"*.py[cod]\"}\nComment: we should probably add these to the .gitignore file instead",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "pynecone/constants.py",
    "pr_number": 718,
    "repo": "reflex",
    "owner": "reflex-dev",
    "comment_id": 1146432435,
    "comment_created_at": "2023-03-23T16:08:00Z"
  },
  {
    "code": "@@ -119,6 +119,8 @@ Join our growing community!\n - [Whale](http://whale.im)\n - [Windsor.ai](https://www.windsor.ai/) [@octaviancorlade]\n - [Zeta](https://www.zeta.tech/) [@shaikidris]\n+- [YTL Communication](https://itsmanalytics.yes.my/)",
    "comment": "you should definitely upgrade Superset ;)",
    "line_number": 122,
    "enriched": "File: RESOURCES/INTHEWILD.md\nCode: @@ -119,6 +119,8 @@ Join our growing community!\n - [Whale](http://whale.im)\n - [Windsor.ai](https://www.windsor.ai/) [@octaviancorlade]\n - [Zeta](https://www.zeta.tech/) [@shaikidris]\n+- [YTL Communication](https://itsmanalytics.yes.my/)\nComment: you should definitely upgrade Superset ;)",
    "subcategory": "support issues",
    "category": "refactoring",
    "file_path": "RESOURCES/INTHEWILD.md",
    "pr_number": 26809,
    "repo": "superset",
    "owner": "apache",
    "comment_id": 1467571129,
    "comment_created_at": "2024-01-26T11:55:29Z"
  },
    {
    "code": "@@ -38,10 +40,102 @@ def submit(self, fn: Callable[P, T], /, *args: P.args, **kwargs: P.kwargs) -> Fu\n         return fut\n \n \n+class TimeoutThreadPoolExecutor(ThreadPoolExecutor):\n+    def __init__(\n+        self,\n+        max_workers: int = None,\n+        timeout: Optional[float] = None,\n+        thread_name_prefix: str = \"\",\n+        initializer: AnyFun = None,\n+        initargs: Tuple[Any, ...] = (),\n+    ):\n+        \"\"\"Initializes a new ThreadPoolExecutor instance.\n+\n+        Args:\n+            max_workers: The maximum number of threads that can be used to\n+                execute the given calls.\n+            timeout: Waits for pool threads to complete, None to wait indefinite\n+            thread_name_prefix: An optional name prefix to give our threads.\n+            initializer: A callable used to initialize worker threads.\n+            initargs: A tuple of arguments to pass to the initializer.\n+        \"\"\"\n+        super().__init__(\n+            max_workers,\n+            thread_name_prefix=thread_name_prefix,\n+            initializer=initializer,\n+            initargs=initargs,\n+        )\n+        self.timeout = timeout\n+        self._is_alive: bool = None  # set on shutdown\n+\n+    def shutdown(self, wait: bool = True, *, cancel_futures: bool = False) -> None:\n+        if not wait or self.timeout is None:\n+            # behave exactly like the std-lib version\n+            return super().shutdown(wait=wait, cancel_futures=cancel_futures)\n+\n+        # initiate shutdown but return immediately\n+        super().shutdown(wait=False, cancel_futures=cancel_futures)\n+\n+        deadline = time.monotonic() + self.timeout\n+        for t in list(self._threads):\n+            remaining = deadline - time.monotonic()\n+            if remaining <= 0:\n+                break\n+            t.join(remaining)\n+\n+        alive = [t for t in self._threads if t.is_alive()]\n+        if alive:\n+            logger.error(\n+                f\"Shutdown of dlt thread pool {self._thread_name_prefix} could not be completed in\"\n+                f\" {self.timeout} seconds. All the tasks in the pool are completed, nevertheless\"\n+                \" some threads didn't join. This may happen, for example, if thread local storage \"\n+                \" is used with object with __del__ method that locks thread when garbage\"\n+                \" collecting or if os.fork() is used. NOTE: if you use database clients or other\"\n+                \" Python libraries in resource or custom destinations, they can do that without\"\n+                \" you knowing. WARNING! This process will still try to join those threads on exit\"\n+                \" and most probably will lock indefinitely.\"\n+            )\n+            logger.info(\n+                f\"Shutdown of dlt thread pool {self._thread_name_prefix} - \"\n+                f\"{len(alive)} thread(s) still alive: {', '.join(t.name for t in alive)}.\"\n+            )\n+        self._is_alive = len(alive) > 0\n+\n+\n+def get_default_start_method(method_: str) -> str:\n+    \"\"\"Sets method to `spawn` is running in one of orchestrator tasks.\n+\n+    Called when explicit start method is not set on `PoolRunnerConfiguration`\n+    \"\"\"\n+    if method_ == \"fork\":\n+        from dlt.common.runtime.exec_info import (\n+            is_running_in_airflow_task,\n+            is_running_in_dagster_task,\n+            is_running_in_prefect_flow,\n+        )\n+\n+        for m_ in [\n+            is_running_in_airflow_task,\n+            is_running_in_dagster_task,\n+            is_running_in_prefect_flow,\n+        ]:\n+            if m_():\n+                logger.info(\n+                    f\"Switching pool start method to `spawn` because `{m_.__name__}` is True\"",
    "comment": "praise: nice way of using the functions name. very concise. doesnt look like a human wrote it to me, but actually very nice.",
    "line_number": 124,
    "enriched": "File: dlt/common/runners/pool_runner.py\nCode: @@ -38,10 +40,102 @@ def submit(self, fn: Callable[P, T], /, *args: P.args, **kwargs: P.kwargs) -> Fu\n         return fut\n \n \n+class TimeoutThreadPoolExecutor(ThreadPoolExecutor):\n+    def __init__(\n+        self,\n+        max_workers: int = None,\n+        timeout: Optional[float] = None,\n+        thread_name_prefix: str = \"\",\n+        initializer: AnyFun = None,\n+        initargs: Tuple[Any, ...] = (),\n+    ):\n+        \"\"\"Initializes a new ThreadPoolExecutor instance.\n+\n+        Args:\n+            max_workers: The maximum number of threads that can be used to\n+                execute the given calls.\n+            timeout: Waits for pool threads to complete, None to wait indefinite\n+            thread_name_prefix: An optional name prefix to give our threads.\n+            initializer: A callable used to initialize worker threads.\n+            initargs: A tuple of arguments to pass to the initializer.\n+        \"\"\"\n+        super().__init__(\n+            max_workers,\n+            thread_name_prefix=thread_name_prefix,\n+            initializer=initializer,\n+            initargs=initargs,\n+        )\n+        self.timeout = timeout\n+        self._is_alive: bool = None  # set on shutdown\n+\n+    def shutdown(self, wait: bool = True, *, cancel_futures: bool = False) -> None:\n+        if not wait or self.timeout is None:\n+            # behave exactly like the std-lib version\n+            return super().shutdown(wait=wait, cancel_futures=cancel_futures)\n+\n+        # initiate shutdown but return immediately\n+        super().shutdown(wait=False, cancel_futures=cancel_futures)\n+\n+        deadline = time.monotonic() + self.timeout\n+        for t in list(self._threads):\n+            remaining = deadline - time.monotonic()\n+            if remaining <= 0:\n+                break\n+            t.join(remaining)\n+\n+        alive = [t for t in self._threads if t.is_alive()]\n+        if alive:\n+            logger.error(\n+                f\"Shutdown of dlt thread pool {self._thread_name_prefix} could not be completed in\"\n+                f\" {self.timeout} seconds. All the tasks in the pool are completed, nevertheless\"\n+                \" some threads didn't join. This may happen, for example, if thread local storage \"\n+                \" is used with object with __del__ method that locks thread when garbage\"\n+                \" collecting or if os.fork() is used. NOTE: if you use database clients or other\"\n+                \" Python libraries in resource or custom destinations, they can do that without\"\n+                \" you knowing. WARNING! This process will still try to join those threads on exit\"\n+                \" and most probably will lock indefinitely.\"\n+            )\n+            logger.info(\n+                f\"Shutdown of dlt thread pool {self._thread_name_prefix} - \"\n+                f\"{len(alive)} thread(s) still alive: {', '.join(t.name for t in alive)}.\"\n+            )\n+        self._is_alive = len(alive) > 0\n+\n+\n+def get_default_start_method(method_: str) -> str:\n+    \"\"\"Sets method to `spawn` is running in one of orchestrator tasks.\n+\n+    Called when explicit start method is not set on `PoolRunnerConfiguration`\n+    \"\"\"\n+    if method_ == \"fork\":\n+        from dlt.common.runtime.exec_info import (\n+            is_running_in_airflow_task,\n+            is_running_in_dagster_task,\n+            is_running_in_prefect_flow,\n+        )\n+\n+        for m_ in [\n+            is_running_in_airflow_task,\n+            is_running_in_dagster_task,\n+            is_running_in_prefect_flow,\n+        ]:\n+            if m_():\n+                logger.info(\n+                    f\"Switching pool start method to `spawn` because `{m_.__name__}` is True\"\nComment: praise: nice way of using the functions name. very concise. doesnt look like a human wrote it to me, but actually very nice.",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "dlt/common/runners/pool_runner.py",
    "pr_number": 2818,
    "repo": "dlt",
    "owner": "dlt-hub",
    "comment_id": 2177560667,
    "comment_created_at": "2025-07-01T13:07:16Z"
  },
  {
    "code": "@@ -60,6 +60,7 @@ classifiers = [\n   \"Programming Language :: Python :: 3.11\",\n   \"Programming Language :: Python :: 3.12\",\n   \"Programming Language :: Python :: 3.13\",\n+  \"Programming Language :: Python :: 3.14\",",
    "comment": "Happy to take this out until we have what we think is all parsing support ...",
    "line_number": 63,
    "enriched": "File: pyproject.toml\nCode: @@ -60,6 +60,7 @@ classifiers = [\n   \"Programming Language :: Python :: 3.11\",\n   \"Programming Language :: Python :: 3.12\",\n   \"Programming Language :: Python :: 3.13\",\n+  \"Programming Language :: Python :: 3.14\",\nComment: Happy to take this out until we have what we think is all parsing support ...",
    "subcategory": "support issue",
    "category": "functional",
    "file_path": "pyproject.toml",
    "pr_number": 4804,
    "repo": "black",
    "owner": "psf",
    "comment_id": 2450246117,
    "comment_created_at": "2025-10-22T02:33:01Z"
  },
  {
    "code": "@@ -1851,8 +1874,76 @@ def get_value(self, key: str) -> Any:\n         Returns:\n             The value of the field.\n         \"\"\"\n+        from reflex.components.component import Component\n+\n+        def make_component(component: Component) -> str:",
    "comment": "this needs to move out to its own module i think",
    "line_number": 1879,
    "enriched": "File: reflex/state.py\nCode: @@ -1851,8 +1874,76 @@ def get_value(self, key: str) -> Any:\n         Returns:\n             The value of the field.\n         \"\"\"\n+        from reflex.components.component import Component\n+\n+        def make_component(component: Component) -> str:\nComment: this needs to move out to its own module i think",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "reflex/state.py",
    "pr_number": 3732,
    "repo": "reflex",
    "owner": "reflex-dev",
    "comment_id": 1761647081,
    "comment_created_at": "2024-09-16T18:22:52Z"
  },

  {
    "code": "@@ -502,6 +542,19 @@ def create_logs(\n         model_id = self.pool._auditlog_model_cache[res_model]\n         auditlog_rule = self.env[\"auditlog.rule\"].search([(\"model_id\", \"=\", model_id)])\n         fields_to_exclude = auditlog_rule.fields_to_exclude_ids.mapped(\"name\")\n+\n+        vals = {\n+            \"model_id\": model_id,\n+            \"method\": method,\n+            \"user_id\": uid,\n+            \"http_request_id\": http_request_model.current_http_request(),\n+            \"http_session_id\": http_session_model.current_http_session(),\n+        }",
    "comment": "Is there a reason you did not eliminate the redefinition of `vals` in line 561 as in https://github.com/OCA/server-tools/pull/2768/files#diff-312eab5bf54da620ee733a3b9c034685fef15546d8602d851a92eda2227a80b1L527-R582?",
    "line_number": 552,
    "enriched": "File: auditlog/models/rule.py\nCode: @@ -502,6 +542,19 @@ def create_logs(\n         model_id = self.pool._auditlog_model_cache[res_model]\n         auditlog_rule = self.env[\"auditlog.rule\"].search([(\"model_id\", \"=\", model_id)])\n         fields_to_exclude = auditlog_rule.fields_to_exclude_ids.mapped(\"name\")\n+\n+        vals = {\n+            \"model_id\": model_id,\n+            \"method\": method,\n+            \"user_id\": uid,\n+            \"http_request_id\": http_request_model.current_http_request(),\n+            \"http_session_id\": http_session_model.current_http_session(),\n+        }\nComment: Is there a reason you did not eliminate the redefinition of `vals` in line 561 as in https://github.com/OCA/server-tools/pull/2768/files#diff-312eab5bf54da620ee733a3b9c034685fef15546d8602d851a92eda2227a80b1L527-R582?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "auditlog/models/rule.py",
    "pr_number": 3397,
    "repo": "server-tools",
    "owner": "OCA",
    "comment_id": 2387693592,
    "comment_created_at": "2025-09-29T12:03:12Z"
  },
  {
    "code": "@@ -271,11 +272,11 @@ def test_LogDelete(self):\n         )\n \n \n-class TestAuditlogFull(TransactionCase, AuditlogCommon):\n+class TestAuditlogFull(AuditLogRuleCommon, AuditlogCommon):\n     def setUp(self):\n         super().setUp()",
    "comment": "If you move this to setUpClass, you can remove the class' now redundant tearDown.",
    "line_number": 277,
    "enriched": "File: auditlog/tests/test_auditlog.py\nCode: @@ -271,11 +272,11 @@ def test_LogDelete(self):\n         )\n \n \n-class TestAuditlogFull(TransactionCase, AuditlogCommon):\n+class TestAuditlogFull(AuditLogRuleCommon, AuditlogCommon):\n     def setUp(self):\n         super().setUp()\nComment: If you move this to setUpClass, you can remove the class' now redundant tearDown.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "auditlog/tests/test_auditlog.py",
    "pr_number": 3389,
    "repo": "server-tools",
    "owner": "OCA",
    "comment_id": 2376610648,
    "comment_created_at": "2025-09-24T17:57:30Z"
  },
  {
    "code": "@@ -3,7 +3,7 @@\n \n {\n     \"name\": \"Audit Log\",\n-    \"version\": \"16.0.3.0.0\",\n+    \"version\": \"16.0.3.0.1\",",
    "comment": "Please dont update the version manually. Ocabot will do this during merge.",
    "line_number": 6,
    "enriched": "File: auditlog/__manifest__.py\nCode: @@ -3,7 +3,7 @@\n \n {\n     \"name\": \"Audit Log\",\n-    \"version\": \"16.0.3.0.0\",\n+    \"version\": \"16.0.3.0.1\",\nComment: Please dont update the version manually. Ocabot will do this during merge.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "auditlog/__manifest__.py",
    "pr_number": 3386,
    "repo": "server-tools",
    "owner": "OCA",
    "comment_id": 2375118560,
    "comment_created_at": "2025-09-24T09:08:57Z"
  },
  {
    "code": "@@ -0,0 +1,131 @@\n+# Copyright 2016 Therp BV <https://therp.nl>\n+# Copyright 2018 Tecnativa - Sergio Teruel\n+# License LGPL-3.0 or later (https://www.gnu.org/licenses/lgpl.html).\n+from lxml import etree\n+\n+from odoo import api, models, tools\n+from odoo.tools.safe_eval import safe_eval\n+\n+\n+class UnquoteObject(str):\n+    def __getattr__(self, name):\n+        return UnquoteObject(\"{}.{}\".format(self, name))\n+\n+    def __repr__(self):\n+        return self\n+\n+    def __call__(self, *args, **kwargs):\n+        return UnquoteObject(\n+            \"%s(%s)\"\n+            % (\n+                self,\n+                \",\".join(\n+                    [\n+                        UnquoteObject(a if not isinstance(a, str) else \"'%s'\" % a)\n+                        for a in args\n+                    ]\n+                    + [\"{}={}\".format(UnquoteObject(k), v) for (k, v) in kwargs.items()]\n+                ),\n+            )\n+        )\n+\n+\n+class UnquoteEvalObjectContext(tools.misc.UnquoteEvalContext):\n+    def __missing__(self, key):\n+        return UnquoteObject(key)\n+\n+\n+class IrUiView(models.Model):\n+    _inherit = \"ir.ui.view\"\n+\n+    @api.model\n+    def apply_inheritance_specs(self, source, specs_tree, inherit_id):\n+        for specs, handled_by in self._iter_inheritance_specs(specs_tree):\n+            source = handled_by(source, specs, inherit_id)\n+        return source\n+\n+    @api.model\n+    def _iter_inheritance_specs(self, spec):\n+        if spec.tag == \"data\":\n+            for child in spec:\n+                for node, handler in self._iter_inheritance_specs(child):\n+                    yield node, handler\n+            return\n+        if spec.get(\"position\") == \"attributes\":\n+            if all(not c.get(\"operation\") for c in spec):\n+                yield spec, self._get_inheritance_handler(spec)\n+                return\n+            for child in spec:\n+                node = etree.Element(spec.tag, **spec.attrib)\n+                node.insert(0, child)\n+                yield node, self._get_inheritance_handler_attributes(child)\n+            return\n+        yield spec, self._get_inheritance_handler(spec)\n+\n+    @api.model\n+    def _get_inheritance_handler(self, node):\n+        handler = super(IrUiView, self).apply_inheritance_specs",
    "comment": "```suggestion\r\n        handler = super().apply_inheritance_specs\r\n```",
    "line_number": 67,
    "enriched": "File: base_view_inheritance_extension/models/ir_ui_view.py\nCode: @@ -0,0 +1,131 @@\n+# Copyright 2016 Therp BV <https://therp.nl>\n+# Copyright 2018 Tecnativa - Sergio Teruel\n+# License LGPL-3.0 or later (https://www.gnu.org/licenses/lgpl.html).\n+from lxml import etree\n+\n+from odoo import api, models, tools\n+from odoo.tools.safe_eval import safe_eval\n+\n+\n+class UnquoteObject(str):\n+    def __getattr__(self, name):\n+        return UnquoteObject(\"{}.{}\".format(self, name))\n+\n+    def __repr__(self):\n+        return self\n+\n+    def __call__(self, *args, **kwargs):\n+        return UnquoteObject(\n+            \"%s(%s)\"\n+            % (\n+                self,\n+                \",\".join(\n+                    [\n+                        UnquoteObject(a if not isinstance(a, str) else \"'%s'\" % a)\n+                        for a in args\n+                    ]\n+                    + [\"{}={}\".format(UnquoteObject(k), v) for (k, v) in kwargs.items()]\n+                ),\n+            )\n+        )\n+\n+\n+class UnquoteEvalObjectContext(tools.misc.UnquoteEvalContext):\n+    def __missing__(self, key):\n+        return UnquoteObject(key)\n+\n+\n+class IrUiView(models.Model):\n+    _inherit = \"ir.ui.view\"\n+\n+    @api.model\n+    def apply_inheritance_specs(self, source, specs_tree, inherit_id):\n+        for specs, handled_by in self._iter_inheritance_specs(specs_tree):\n+            source = handled_by(source, specs, inherit_id)\n+        return source\n+\n+    @api.model\n+    def _iter_inheritance_specs(self, spec):\n+        if spec.tag == \"data\":\n+            for child in spec:\n+                for node, handler in self._iter_inheritance_specs(child):\n+                    yield node, handler\n+            return\n+        if spec.get(\"position\") == \"attributes\":\n+            if all(not c.get(\"operation\") for c in spec):\n+                yield spec, self._get_inheritance_handler(spec)\n+                return\n+            for child in spec:\n+                node = etree.Element(spec.tag, **spec.attrib)\n+                node.insert(0, child)\n+                yield node, self._get_inheritance_handler_attributes(child)\n+            return\n+        yield spec, self._get_inheritance_handler(spec)\n+\n+    @api.model\n+    def _get_inheritance_handler(self, node):\n+        handler = super(IrUiView, self).apply_inheritance_specs\nComment: ```suggestion\r\n        handler = super().apply_inheritance_specs\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "base_view_inheritance_extension/models/ir_ui_view.py",
    "pr_number": 1779,
    "repo": "server-tools",
    "owner": "OCA",
    "comment_id": 409529706,
    "comment_created_at": "2020-04-16T12:50:53Z"
  },
    {
    "code": "@@ -187,22 +193,52 @@ export const sidePanelSdkDoctorLogic = kea<sidePanelSdkDoctorLogicType>([\n             },\n         ],\n \n-        outdatedSdkCount: [\n+        needsUpdatingCount: [\n             (s) => [s.sdkVersionsMap],\n             (sdkVersionsMap: AugmentedTeamSdkVersionsInfo): number => {\n-                return Object.values(sdkVersionsMap).filter((sdk) => sdk.isOutdated).length\n+                return Object.values(sdkVersionsMap).filter((sdk) => sdk.needsUpdating).length\n+            },\n+        ],\n+\n+        needsAttention: [\n+            (s) => [s.sdkVersionsMap, s.needsUpdatingCount, s.snoozedUntil],\n+            (\n+                sdkVersionsMap: AugmentedTeamSdkVersionsInfo,\n+                needsUpdatingCount: number,\n+                snoozedUntil: string | null\n+            ): boolean => {\n+                // If snoozed, we don't need attention to this\n+                if (snoozedUntil !== null) {\n+                    return false\n+                }\n+\n+                // Let's call their attention if at least half of their SDKs are outdated\n+                // It's unlikely for people to have more than 3 SDKs, but let's be safe\n+                // and handle it very generically.\n+                //\n+                // | Outdated SDKs \\ Total SDKs |  1  |  2  |  3  |  4  |  5  |\n+                // |----------------------------|-----|-----|-----|-----|-----|\n+                // |                0           |  NO |  NO |  NO |  NO |  NO |\n+                // |                1           | YES | YES |  NO |  NO |  NO |\n+                // |                2           |     | YES | YES | YES |  NO |\n+                // |                3           |     |     | YES | YES | YES |\n+                // |                4           |     |     |     | YES | YES |\n+                // |                5           |     |     |     |     | YES |\n+                const teamSdkCount = Object.values(sdkVersionsMap).length\n+                return needsUpdatingCount >= Math.ceil(teamSdkCount / 2)",
    "comment": "**logic:** Edge case: If there are 0 SDKs (`teamSdkCount === 0`), `Math.ceil(0 / 2)` is 0, so `needsUpdatingCount >= 0` will always be true. Check `teamSdkCount > 0` first.\n\n```suggestion\n                const teamSdkCount = Object.values(sdkVersionsMap).length\n                return teamSdkCount > 0 && needsUpdatingCount >= Math.ceil(teamSdkCount / 2)\n```\n\n<details><summary>Prompt To Fix With AI</summary>\n\n`````markdown\nThis is a comment left during a code review.\nPath: frontend/src/layout/navigation-3000/sidepanel/panels/sidePanelSdkDoctorLogic.tsx\nLine: 227:228\n\nComment:\n**logic:** Edge case: If there are 0 SDKs (`teamSdkCount === 0`), `Math.ceil(0 / 2)` is 0, so `needsUpdatingCount >= 0` will always be true. Check `teamSdkCount > 0` first.\n\n```suggestion\n                const teamSdkCount = Object.values(sdkVersionsMap).length\n                return teamSdkCount > 0 && needsUpdatingCount >= Math.ceil(teamSdkCount / 2)\n```\n\nHow can I resolve this? If you propose a fix, please make it concise.\n`````\n</details>",
    "line_number": 228,
    "enriched": "File: frontend/src/layout/navigation-3000/sidepanel/panels/sidePanelSdkDoctorLogic.tsx\nCode: @@ -187,22 +193,52 @@ export const sidePanelSdkDoctorLogic = kea<sidePanelSdkDoctorLogicType>([\n             },\n         ],\n \n-        outdatedSdkCount: [\n+        needsUpdatingCount: [\n             (s) => [s.sdkVersionsMap],\n             (sdkVersionsMap: AugmentedTeamSdkVersionsInfo): number => {\n-                return Object.values(sdkVersionsMap).filter((sdk) => sdk.isOutdated).length\n+                return Object.values(sdkVersionsMap).filter((sdk) => sdk.needsUpdating).length\n+            },\n+        ],\n+\n+        needsAttention: [\n+            (s) => [s.sdkVersionsMap, s.needsUpdatingCount, s.snoozedUntil],\n+            (\n+                sdkVersionsMap: AugmentedTeamSdkVersionsInfo,\n+                needsUpdatingCount: number,\n+                snoozedUntil: string | null\n+            ): boolean => {\n+                // If snoozed, we don't need attention to this\n+                if (snoozedUntil !== null) {\n+                    return false\n+                }\n+\n+                // Let's call their attention if at least half of their SDKs are outdated\n+                // It's unlikely for people to have more than 3 SDKs, but let's be safe\n+                // and handle it very generically.\n+                //\n+                // | Outdated SDKs \\ Total SDKs |  1  |  2  |  3  |  4  |  5  |\n+                // |----------------------------|-----|-----|-----|-----|-----|\n+                // |                0           |  NO |  NO |  NO |  NO |  NO |\n+                // |                1           | YES | YES |  NO |  NO |  NO |\n+                // |                2           |     | YES | YES | YES |  NO |\n+                // |                3           |     |     | YES | YES | YES |\n+                // |                4           |     |     |     | YES | YES |\n+                // |                5           |     |     |     |     | YES |\n+                const teamSdkCount = Object.values(sdkVersionsMap).length\n+                return needsUpdatingCount >= Math.ceil(teamSdkCount / 2)\nComment: **logic:** Edge case: If there are 0 SDKs (`teamSdkCount === 0`), `Math.ceil(0 / 2)` is 0, so `needsUpdatingCount >= 0` will always be true. Check `teamSdkCount > 0` first.\n\n```suggestion\n                const teamSdkCount = Object.values(sdkVersionsMap).length\n                return teamSdkCount > 0 && needsUpdatingCount >= Math.ceil(teamSdkCount / 2)\n```\n\n<details><summary>Prompt To Fix With AI</summary>\n\n`````markdown\nThis is a comment left during a code review.\nPath: frontend/src/layout/navigation-3000/sidepanel/panels/sidePanelSdkDoctorLogic.tsx\nLine: 227:228\n\nComment:\n**logic:** Edge case: If there are 0 SDKs (`teamSdkCount === 0`), `Math.ceil(0 / 2)` is 0, so `needsUpdatingCount >= 0` will always be true. Check `teamSdkCount > 0` first.\n\n```suggestion\n                const teamSdkCount = Object.values(sdkVersionsMap).length\n                return teamSdkCount > 0 && needsUpdatingCount >= Math.ceil(teamSdkCount / 2)\n```\n\nHow can I resolve this? If you propose a fix, please make it concise.\n`````\n</details>",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "frontend/src/layout/navigation-3000/sidepanel/panels/sidePanelSdkDoctorLogic.tsx",
    "pr_number": 40069,
    "repo": "posthog",
    "owner": "PostHog",
    "comment_id": 2449709018,
    "comment_created_at": "2025-10-21T21:08:53Z"
  },
  {
    "code": "@@ -1,23 +1,179 @@\n+from posthog.hogql.ast import ArrayType, BooleanType, FloatType, IntegerType, StringType, TupleType\n+\n from ..core import HogQLFunctionMeta\n+from ..typegen import generate_json_path_signatures\n \n # json\n JSON_FUNCTIONS: dict[str, HogQLFunctionMeta] = {\n-    \"isValidJSON\": HogQLFunctionMeta(\"isValidJSON\", 1, 1),\n-    \"JSONHas\": HogQLFunctionMeta(\"JSONHas\", 1, None),\n-    \"JSONLength\": HogQLFunctionMeta(\"JSONLength\", 1, None),\n-    \"JSONArrayLength\": HogQLFunctionMeta(\"JSONArrayLength\", 1, None),\n-    \"JSONType\": HogQLFunctionMeta(\"JSONType\", 1, None),\n-    \"JSONExtract\": HogQLFunctionMeta(\"JSONExtract\", 2, None),\n-    \"JSONExtractUInt\": HogQLFunctionMeta(\"JSONExtractUInt\", 1, None),\n-    \"JSONExtractInt\": HogQLFunctionMeta(\"JSONExtractInt\", 1, None),\n-    \"JSONExtractFloat\": HogQLFunctionMeta(\"JSONExtractFloat\", 1, None),\n-    \"JSONExtractBool\": HogQLFunctionMeta(\"JSONExtractBool\", 1, None),\n-    \"JSONExtractString\": HogQLFunctionMeta(\"JSONExtractString\", 1, None),\n-    \"JSONExtractKey\": HogQLFunctionMeta(\"JSONExtractKey\", 1, None),\n-    \"JSONExtractKeys\": HogQLFunctionMeta(\"JSONExtractKeys\", 1, None),\n-    \"JSONExtractRaw\": HogQLFunctionMeta(\"JSONExtractRaw\", 1, None),\n-    \"JSONExtractArrayRaw\": HogQLFunctionMeta(\"JSONExtractArrayRaw\", 1, None),\n-    \"JSONExtractKeysAndValues\": HogQLFunctionMeta(\"JSONExtractKeysAndValues\", 1, 3),\n-    \"JSONExtractKeysAndValuesRaw\": HogQLFunctionMeta(\"JSONExtractKeysAndValuesRaw\", 1, None),\n-    \"JSON_VALUE\": HogQLFunctionMeta(\"JSON_VALUE\", 2, None),\n+    \"isValidJSON\": HogQLFunctionMeta(\"isValidJSON\", 1, 1, signatures=[((StringType(),), IntegerType())]),\n+    \"JSONHas\": HogQLFunctionMeta(\n+        \"JSONHas\",\n+        2,\n+        6,\n+        signatures=generate_json_path_signatures(\n+            fixed_types=[StringType()],  # JSON parameter\n+            return_type=IntegerType(),  # Returns 1 or 0\n+            min_paths=1,  # Requires at least 1 path\n+            max_paths=5,  # Up to 5 path levels\n+        ),\n+    ),\n+    \"JSONLength\": HogQLFunctionMeta(\n+        \"JSONLength\",\n+        1,\n+        6,\n+        signatures=generate_json_path_signatures(\n+            fixed_types=[StringType()],  # JSON parameter\n+            return_type=IntegerType(),  # Returns length as integer\n+            min_paths=0,  # Can work without paths (root length)\n+            max_paths=5,  # Up to 5 path levels\n+        ),\n+    ),\n+    \"JSONArrayLength\": HogQLFunctionMeta(\n+        \"JSONArrayLength\",\n+        1,\n+        6,\n+        signatures=generate_json_path_signatures(\n+            fixed_types=[StringType()],  # JSON parameter\n+            return_type=IntegerType(),  # Returns array length as integer\n+            min_paths=0,  # Can work without paths (root array)\n+            max_paths=5,  # Up to 5 path levels\n+        ),\n+    ),\n+    \"JSONType\": HogQLFunctionMeta(\n+        \"JSONType\",\n+        1,\n+        6,\n+        signatures=generate_json_path_signatures(\n+            fixed_types=[StringType()],  # JSON parameter\n+            return_type=StringType(),  # Returns type name as string\n+            min_paths=0,  # Can work without paths (root type)\n+            max_paths=5,  # Up to 5 path levels\n+        ),\n+    ),\n+    \"JSONExtract\": HogQLFunctionMeta(\n+        \"JSONExtract\",\n+        2,\n+        7,\n+        signatures=generate_json_path_signatures(\n+            fixed_types=[StringType()],  # JSON parameter\n+            suffix_types=[StringType()],  # ClickHouse data type as string\n+            return_type=StringType(),  # Returns type name as string\n+            min_paths=0,  # Can work without paths (root type)\n+            max_paths=5,  # Up to 5 path levels\n+        ),\n+    ),\n+    \"JSONExtractUInt\": HogQLFunctionMeta(\n+        \"JSONExtractUInt\",\n+        2,\n+        6,\n+        signatures=generate_json_path_signatures(\n+            fixed_types=[StringType()],  # JSON parameter\n+            return_type=IntegerType(),  # Returns unsigned integer\n+            min_paths=1,  # Requires at least 1 path\n+            max_paths=5,  # Up to 4 path levels\n+        ),",
    "comment": "style: comment mismatch in `JSONExtractUInt`: the comment says \"Up to 4 path levels\" but max_paths=5 allows up to 5 levels\n\n```suggestion\n            min_paths=1,  # Requires at least 1 path\n            max_paths=5,  # Up to 5 path levels\n```",
    "line_number": 74,
    "enriched": "File: posthog/hogql/functions/clickhouse/json.py\nCode: @@ -1,23 +1,179 @@\n+from posthog.hogql.ast import ArrayType, BooleanType, FloatType, IntegerType, StringType, TupleType\n+\n from ..core import HogQLFunctionMeta\n+from ..typegen import generate_json_path_signatures\n \n # json\n JSON_FUNCTIONS: dict[str, HogQLFunctionMeta] = {\n-    \"isValidJSON\": HogQLFunctionMeta(\"isValidJSON\", 1, 1),\n-    \"JSONHas\": HogQLFunctionMeta(\"JSONHas\", 1, None),\n-    \"JSONLength\": HogQLFunctionMeta(\"JSONLength\", 1, None),\n-    \"JSONArrayLength\": HogQLFunctionMeta(\"JSONArrayLength\", 1, None),\n-    \"JSONType\": HogQLFunctionMeta(\"JSONType\", 1, None),\n-    \"JSONExtract\": HogQLFunctionMeta(\"JSONExtract\", 2, None),\n-    \"JSONExtractUInt\": HogQLFunctionMeta(\"JSONExtractUInt\", 1, None),\n-    \"JSONExtractInt\": HogQLFunctionMeta(\"JSONExtractInt\", 1, None),\n-    \"JSONExtractFloat\": HogQLFunctionMeta(\"JSONExtractFloat\", 1, None),\n-    \"JSONExtractBool\": HogQLFunctionMeta(\"JSONExtractBool\", 1, None),\n-    \"JSONExtractString\": HogQLFunctionMeta(\"JSONExtractString\", 1, None),\n-    \"JSONExtractKey\": HogQLFunctionMeta(\"JSONExtractKey\", 1, None),\n-    \"JSONExtractKeys\": HogQLFunctionMeta(\"JSONExtractKeys\", 1, None),\n-    \"JSONExtractRaw\": HogQLFunctionMeta(\"JSONExtractRaw\", 1, None),\n-    \"JSONExtractArrayRaw\": HogQLFunctionMeta(\"JSONExtractArrayRaw\", 1, None),\n-    \"JSONExtractKeysAndValues\": HogQLFunctionMeta(\"JSONExtractKeysAndValues\", 1, 3),\n-    \"JSONExtractKeysAndValuesRaw\": HogQLFunctionMeta(\"JSONExtractKeysAndValuesRaw\", 1, None),\n-    \"JSON_VALUE\": HogQLFunctionMeta(\"JSON_VALUE\", 2, None),\n+    \"isValidJSON\": HogQLFunctionMeta(\"isValidJSON\", 1, 1, signatures=[((StringType(),), IntegerType())]),\n+    \"JSONHas\": HogQLFunctionMeta(\n+        \"JSONHas\",\n+        2,\n+        6,\n+        signatures=generate_json_path_signatures(\n+            fixed_types=[StringType()],  # JSON parameter\n+            return_type=IntegerType(),  # Returns 1 or 0\n+            min_paths=1,  # Requires at least 1 path\n+            max_paths=5,  # Up to 5 path levels\n+        ),\n+    ),\n+    \"JSONLength\": HogQLFunctionMeta(\n+        \"JSONLength\",\n+        1,\n+        6,\n+        signatures=generate_json_path_signatures(\n+            fixed_types=[StringType()],  # JSON parameter\n+            return_type=IntegerType(),  # Returns length as integer\n+            min_paths=0,  # Can work without paths (root length)\n+            max_paths=5,  # Up to 5 path levels\n+        ),\n+    ),\n+    \"JSONArrayLength\": HogQLFunctionMeta(\n+        \"JSONArrayLength\",\n+        1,\n+        6,\n+        signatures=generate_json_path_signatures(\n+            fixed_types=[StringType()],  # JSON parameter\n+            return_type=IntegerType(),  # Returns array length as integer\n+            min_paths=0,  # Can work without paths (root array)\n+            max_paths=5,  # Up to 5 path levels\n+        ),\n+    ),\n+    \"JSONType\": HogQLFunctionMeta(\n+        \"JSONType\",\n+        1,\n+        6,\n+        signatures=generate_json_path_signatures(\n+            fixed_types=[StringType()],  # JSON parameter\n+            return_type=StringType(),  # Returns type name as string\n+            min_paths=0,  # Can work without paths (root type)\n+            max_paths=5,  # Up to 5 path levels\n+        ),\n+    ),\n+    \"JSONExtract\": HogQLFunctionMeta(\n+        \"JSONExtract\",\n+        2,\n+        7,\n+        signatures=generate_json_path_signatures(\n+            fixed_types=[StringType()],  # JSON parameter\n+            suffix_types=[StringType()],  # ClickHouse data type as string\n+            return_type=StringType(),  # Returns type name as string\n+            min_paths=0,  # Can work without paths (root type)\n+            max_paths=5,  # Up to 5 path levels\n+        ),\n+    ),\n+    \"JSONExtractUInt\": HogQLFunctionMeta(\n+        \"JSONExtractUInt\",\n+        2,\n+        6,\n+        signatures=generate_json_path_signatures(\n+            fixed_types=[StringType()],  # JSON parameter\n+            return_type=IntegerType(),  # Returns unsigned integer\n+            min_paths=1,  # Requires at least 1 path\n+            max_paths=5,  # Up to 4 path levels\n+        ),\nComment: style: comment mismatch in `JSONExtractUInt`: the comment says \"Up to 4 path levels\" but max_paths=5 allows up to 5 levels\n\n```suggestion\n            min_paths=1,  # Requires at least 1 path\n            max_paths=5,  # Up to 5 path levels\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "posthog/hogql/functions/clickhouse/json.py",
    "pr_number": 38619,
    "repo": "posthog",
    "owner": "PostHog",
    "comment_id": 2377260744,
    "comment_created_at": "2025-09-24T23:03:02Z"
  },
  {
    "code": "@@ -80,41 +83,53 @@ export function Settings({\n                   </OptionButton>\n               ),\n           }))\n-        : levels.map((level) => ({\n-              key: level,\n-              content: (\n-                  <OptionButton\n-                      to={urls.settings(level)}\n-                      handleLocally={handleLocally}\n-                      active={selectedLevel === level && !selectedSectionId}\n-                      onClick={() => selectLevel(level)}\n-                  >\n-                      <span className=\"text-secondary\">{SettingLevelNames[level]}</span>\n-                  </OptionButton>\n-              ),\n-              items: sections\n-                  .filter((x) => x.level === level)\n-                  .map((section) => ({\n-                      key: section.id,\n-                      content: (\n-                          <OptionButton\n-                              to={section.to ?? urls.settings(section.id)}\n-                              handleLocally={handleLocally}\n-                              active={selectedSectionId === section.id}\n-                              isLink={!!section.to}\n-                              onClick={() => {\n-                                  if (section.to) {\n-                                      router.actions.push(section.to)\n-                                  } else {\n-                                      selectSection(section.id, level)\n-                                  }\n-                              }}\n-                          >\n-                              {section.title}\n-                          </OptionButton>\n-                      ),\n-                  })),\n-          }))\n+        : filteredLevels.map((level) => {\n+              const levelSections = filteredSections.filter((x) => x.level === level)\n+              const isCollapsed = collapsedLevels[level]\n+              const hasItems = levelSections.length > 0\n+\n+              return {\n+                  key: level,\n+                  content: (\n+                      <OptionButton\n+                          handleLocally={handleLocally}\n+                          active={selectedLevel === level && !selectedSectionId}\n+                          onClick={() => {\n+                              if (hasItems) {\n+                                  toggleLevelCollapse(level)\n+                              } else {\n+                                  selectLevel(level)\n+                              }\n+                          }}\n+                          sideIcon={hasItems ? isCollapsed ? <IconChevronRight /> : <IconChevronDown /> : undefined}",
    "comment": "style: The ternary operator nesting here could be cleaner - consider extracting the icon logic into a variable",
    "line_number": 104,
    "enriched": "File: frontend/src/scenes/settings/Settings.tsx\nCode: @@ -80,41 +83,53 @@ export function Settings({\n                   </OptionButton>\n               ),\n           }))\n-        : levels.map((level) => ({\n-              key: level,\n-              content: (\n-                  <OptionButton\n-                      to={urls.settings(level)}\n-                      handleLocally={handleLocally}\n-                      active={selectedLevel === level && !selectedSectionId}\n-                      onClick={() => selectLevel(level)}\n-                  >\n-                      <span className=\"text-secondary\">{SettingLevelNames[level]}</span>\n-                  </OptionButton>\n-              ),\n-              items: sections\n-                  .filter((x) => x.level === level)\n-                  .map((section) => ({\n-                      key: section.id,\n-                      content: (\n-                          <OptionButton\n-                              to={section.to ?? urls.settings(section.id)}\n-                              handleLocally={handleLocally}\n-                              active={selectedSectionId === section.id}\n-                              isLink={!!section.to}\n-                              onClick={() => {\n-                                  if (section.to) {\n-                                      router.actions.push(section.to)\n-                                  } else {\n-                                      selectSection(section.id, level)\n-                                  }\n-                              }}\n-                          >\n-                              {section.title}\n-                          </OptionButton>\n-                      ),\n-                  })),\n-          }))\n+        : filteredLevels.map((level) => {\n+              const levelSections = filteredSections.filter((x) => x.level === level)\n+              const isCollapsed = collapsedLevels[level]\n+              const hasItems = levelSections.length > 0\n+\n+              return {\n+                  key: level,\n+                  content: (\n+                      <OptionButton\n+                          handleLocally={handleLocally}\n+                          active={selectedLevel === level && !selectedSectionId}\n+                          onClick={() => {\n+                              if (hasItems) {\n+                                  toggleLevelCollapse(level)\n+                              } else {\n+                                  selectLevel(level)\n+                              }\n+                          }}\n+                          sideIcon={hasItems ? isCollapsed ? <IconChevronRight /> : <IconChevronDown /> : undefined}\nComment: style: The ternary operator nesting here could be cleaner - consider extracting the icon logic into a variable",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "frontend/src/scenes/settings/Settings.tsx",
    "pr_number": 37622,
    "repo": "posthog",
    "owner": "PostHog",
    "comment_id": 2322382851,
    "comment_created_at": "2025-09-04T14:24:52Z"
  },
  {
    "code": "@@ -47,6 +47,8 @@ services:\n                     @webhooks {\n                         path /public/webhooks\n                         path /public/webhooks/*\n+                        path /public/m/",
    "comment": "Will put all messaging stuff under /m/ so its a little obscur",
    "line_number": 50,
    "enriched": "File: docker-compose.dev-minimal.yml\nCode: @@ -47,6 +47,8 @@ services:\n                     @webhooks {\n                         path /public/webhooks\n                         path /public/webhooks/*\n+                        path /public/m/\nComment: Will put all messaging stuff under /m/ so its a little obscur",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "docker-compose.dev-minimal.yml",
    "pr_number": 36783,
    "repo": "posthog",
    "owner": "PostHog",
    "comment_id": 2282864371,
    "comment_created_at": "2025-08-18T16:17:40Z"
  },
    {
    "code": "@@ -0,0 +1,82 @@\n+import { actions, kea, listeners, path, reducers } from 'kea'\n+import { loaders } from 'kea-loaders'\n+import api, { ApiMethodOptions, PaginatedResponse } from 'lib/api'\n+\n+import { ExternalDataSource } from '~/types'\n+\n+import type { externalDataSourcesLogicType } from './externalDataSourcesLogicType'\n+\n+export const externalDataSourcesLogic = kea<externalDataSourcesLogicType>([\n+    path(['scenes', 'data-warehouse', 'externalDataSourcesLogic']),\n+    actions({\n+        abortAnyRunningQuery: true,\n+        deleteSource: (source: ExternalDataSource) => ({ source }),\n+        reloadSource: (source: ExternalDataSource) => ({ source }),\n+        reloadSourceSuccess: (source: ExternalDataSource) => ({ source }),\n+        reloadSourceFailure: (source: ExternalDataSource, error: any) => ({ source, error }),\n+    }),\n+    loaders(({ cache, actions, values }) => ({\n+        dataWarehouseSources: [\n+            null as PaginatedResponse<ExternalDataSource> | null,\n+            {\n+                loadSources: async (_, breakpoint) => {\n+                    await breakpoint(300)\n+                    actions.abortAnyRunningQuery()\n+\n+                    cache.abortController = new AbortController()\n+                    const methodOptions: ApiMethodOptions = {\n+                        signal: cache.abortController.signal,\n+                    }\n+\n+                    const res = await api.externalDataSources.list(methodOptions)\n+                    breakpoint()\n+\n+                    cache.abortController = null\n+\n+                    return res\n+                },\n+                updateSource: async (source: ExternalDataSource) => {\n+                    const updatedSource = await api.externalDataSources.update(source.id, source)\n+                    return {\n+                        ...values.dataWarehouseSources,\n+                        results:\n+                            values.dataWarehouseSources?.results.map((s) =>\n+                                s.id === updatedSource.id ? updatedSource : s\n+                            ) || [],\n+                    } as PaginatedResponse<ExternalDataSource>\n+                },\n+            },\n+        ],\n+    })),\n+    reducers(() => ({\n+        dataWarehouseSourcesLoading: [\n+            false as boolean,\n+            {\n+                loadSources: () => true,\n+                loadSourcesFailure: () => false,\n+                loadSourcesSuccess: () => false,\n+            },\n+        ],\n+    })),\n+    listeners(({ cache, actions }) => ({\n+        abortAnyRunningQuery: () => {\n+            if (cache.abortController) {\n+                cache.abortController.abort()\n+                cache.abortController = null\n+            }\n+        },\n+        deleteSource: async ({ source }) => {\n+            await api.externalDataSources.delete(source.id)\n+            actions.loadSources(null)\n+        },\n+        reloadSource: async ({ source }) => {\n+            try {\n+                await api.externalDataSources.reload(source.id)\n+                actions.loadSources(null)\n+                actions.reloadSourceSuccess(source)\n+            } catch (e: any) {\n+                actions.reloadSourceFailure(source, e)\n+            }\n+        },",
    "comment": "logic: Missing error handling for deleteSource operation. Consider adding try/catch block similar to reloadSource to handle potential API failures gracefully.",
    "line_number": 80,
    "enriched": "File: frontend/src/scenes/data-warehouse/externalDataSourcesLogic.ts\nCode: @@ -0,0 +1,82 @@\n+import { actions, kea, listeners, path, reducers } from 'kea'\n+import { loaders } from 'kea-loaders'\n+import api, { ApiMethodOptions, PaginatedResponse } from 'lib/api'\n+\n+import { ExternalDataSource } from '~/types'\n+\n+import type { externalDataSourcesLogicType } from './externalDataSourcesLogicType'\n+\n+export const externalDataSourcesLogic = kea<externalDataSourcesLogicType>([\n+    path(['scenes', 'data-warehouse', 'externalDataSourcesLogic']),\n+    actions({\n+        abortAnyRunningQuery: true,\n+        deleteSource: (source: ExternalDataSource) => ({ source }),\n+        reloadSource: (source: ExternalDataSource) => ({ source }),\n+        reloadSourceSuccess: (source: ExternalDataSource) => ({ source }),\n+        reloadSourceFailure: (source: ExternalDataSource, error: any) => ({ source, error }),\n+    }),\n+    loaders(({ cache, actions, values }) => ({\n+        dataWarehouseSources: [\n+            null as PaginatedResponse<ExternalDataSource> | null,\n+            {\n+                loadSources: async (_, breakpoint) => {\n+                    await breakpoint(300)\n+                    actions.abortAnyRunningQuery()\n+\n+                    cache.abortController = new AbortController()\n+                    const methodOptions: ApiMethodOptions = {\n+                        signal: cache.abortController.signal,\n+                    }\n+\n+                    const res = await api.externalDataSources.list(methodOptions)\n+                    breakpoint()\n+\n+                    cache.abortController = null\n+\n+                    return res\n+                },\n+                updateSource: async (source: ExternalDataSource) => {\n+                    const updatedSource = await api.externalDataSources.update(source.id, source)\n+                    return {\n+                        ...values.dataWarehouseSources,\n+                        results:\n+                            values.dataWarehouseSources?.results.map((s) =>\n+                                s.id === updatedSource.id ? updatedSource : s\n+                            ) || [],\n+                    } as PaginatedResponse<ExternalDataSource>\n+                },\n+            },\n+        ],\n+    })),\n+    reducers(() => ({\n+        dataWarehouseSourcesLoading: [\n+            false as boolean,\n+            {\n+                loadSources: () => true,\n+                loadSourcesFailure: () => false,\n+                loadSourcesSuccess: () => false,\n+            },\n+        ],\n+    })),\n+    listeners(({ cache, actions }) => ({\n+        abortAnyRunningQuery: () => {\n+            if (cache.abortController) {\n+                cache.abortController.abort()\n+                cache.abortController = null\n+            }\n+        },\n+        deleteSource: async ({ source }) => {\n+            await api.externalDataSources.delete(source.id)\n+            actions.loadSources(null)\n+        },\n+        reloadSource: async ({ source }) => {\n+            try {\n+                await api.externalDataSources.reload(source.id)\n+                actions.loadSources(null)\n+                actions.reloadSourceSuccess(source)\n+            } catch (e: any) {\n+                actions.reloadSourceFailure(source, e)\n+            }\n+        },\nComment: logic: Missing error handling for deleteSource operation. Consider adding try/catch block similar to reloadSource to handle potential API failures gracefully.",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "frontend/src/scenes/data-warehouse/externalDataSourcesLogic.ts",
    "pr_number": 36002,
    "repo": "posthog",
    "owner": "PostHog",
    "comment_id": 2250275040,
    "comment_created_at": "2025-08-04T02:23:02Z"
  }
]