[
  {
    "code": "@@ -20,7 +20,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        postgis-version: [latest, \"17-3.5-alpine\", \"17-master\"]\n+        postgis-version: [latest, \"17-3.6-alpine\", \"17-master\"]",
    "comment": "this is a good change! i think we should make this:\n\n",
    "line_number": 23,
    "enriched": "File: .github/workflows/postgis.yml\nCode: @@ -20,7 +20,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        postgis-version: [latest, \"17-3.5-alpine\", \"17-master\"]\n+        postgis-version: [latest, \"17-3.6-alpine\", \"17-master\"]\nComment: This is a good change! I think we should make this:\n\n```suggestion\n        postgis-version: [\"latest\", \"18-3.6-alpine\u2060\", \"17-3.6-alpine\", \"17-master\"]\n```",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": ".github/workflows/postgis.yml",
    "pr_number": 19928,
    "repo": "django",
    "owner": "django",
    "comment_id": 2411811987,
    "comment_created_at": "2025-10-07T20:24:26Z"
  },
  {
    "code": "@@ -29,8 +29,9 @@ def builtin_template_path(name):\n \n def set_language(request):\n     \"\"\"\n-    Redirect to a given URL while setting the chosen language in the session",
    "comment": "elsewhere (other than the other place in this pr), this is just \"the language cookie\", so i think this is sufficient here:\n",
    "line_number": 32,
    "enriched": "File: django/views/i18n.py\nCode: @@ -29,8 +29,9 @@ def builtin_template_path(name):\n \n def set_language(request):\n     \"\"\"\n-    Redirect to a given URL while setting the chosen language in the session\nComment: Elsewhere (other than the other place in this PR), this is just \"the language cookie\", so I think this is sufficient here:\n```suggestion\n    Redirect to a given URL while setting the chosen language in the language cookie.\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "django/views/i18n.py",
    "pr_number": 19917,
    "repo": "django",
    "owner": "django",
    "comment_id": 2402243330,
    "comment_created_at": "2025-10-03T14:50:01Z"
  },
  {
    "code": "@@ -1302,8 +1302,8 @@ materialized view.\n     real column. If ``False``, the column acts as a virtual column and does\n     not occupy database storage space.\n \n-    PostgreSQL only supports persisted columns. Oracle only supports virtual\n-    columns.\n+    PostgreSQL supports persisted and virtual columns.\n+    Oracle only supports virtual columns.",
    "comment": "the comment should mention that virtual columns are only supported on 18+ on postgres.",
    "line_number": 1306,
    "enriched": "File: docs/ref/models/fields.txt\nCode: @@ -1302,8 +1302,8 @@ materialized view.\n     real column. If ``False``, the column acts as a virtual column and does\n     not occupy database storage space.\n \n-    PostgreSQL only supports persisted columns. Oracle only supports virtual\n-    columns.\n+    PostgreSQL supports persisted and virtual columns.\n+    Oracle only supports virtual columns.\nComment: The comment should mention that virtual columns are only supported on 18+ on Postgres.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "docs/ref/models/fields.txt",
    "pr_number": 19907,
    "repo": "django",
    "owner": "django",
    "comment_id": 2384293267,
    "comment_created_at": "2025-09-27T19:38:57Z"
  },
  {
    "code": "@@ -2445,8 +2445,8 @@ This has a number of caveats though:\n         Entry.objects.bulk_create(batch, batch_size)\n \n The ``batch_size`` parameter controls how many objects are created in a single\n-query. The default is to create all objects in one batch, except for SQLite\n-where the default is such that at most 999 variables per query are used.\n+query. The default is to create as many objects in one batch as the database\n+will allow. (SQLite and Oracle limit the number of parameters in a query.)",
    "comment": "i think you could combine punctuation:\n\n\n\ni'm not sure what the \"rules\" are though.",
    "line_number": 2449,
    "enriched": "File: docs/ref/models/querysets.txt\nCode: @@ -2445,8 +2445,8 @@ This has a number of caveats though:\n         Entry.objects.bulk_create(batch, batch_size)\n \n The ``batch_size`` parameter controls how many objects are created in a single\n-query. The default is to create all objects in one batch, except for SQLite\n-where the default is such that at most 999 variables per query are used.\n+query. The default is to create as many objects in one batch as the database\n+will allow. (SQLite and Oracle limit the number of parameters in a query.)\nComment: I think you could combine punctuation:\n\n```suggestion\nwill allow (SQLite and Oracle limit the number of parameters in a query).\n```\n\nI'm not sure what the \"rules\" are though.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "docs/ref/models/querysets.txt",
    "pr_number": 19903,
    "repo": "django",
    "owner": "django",
    "comment_id": 2381267951,
    "comment_created_at": "2025-09-26T08:00:11Z"
  },
  {
    "code": "@@ -0,0 +1,65 @@\n+export const recommended = { ",
    "comment": "it would be nice for this to stay aligned with our version of eslint\r\ni believe this is coming from here: https://github.com/eslint/eslint/blob/main/packages/js/src/configs/eslint-recommended.js\r\nwe should have at least some comments in the file about where this comes from and ideally a script/commands on how to update the file. maybe a comment by the eslint version in package.json that this file should be updated",
    "line_number": 1,
    "enriched": "File: eslint.recommended.mjs\nCode: @@ -0,0 +1,65 @@\n+export const recommended = { \nComment: It would be nice for this to stay aligned with our version of eslint\r\nI believe this is coming from here: https://github.com/eslint/eslint/blob/main/packages/js/src/configs/eslint-recommended.js\r\nWe should have at least some comments in the file about where this comes from and ideally a script/commands on how to update the file. Maybe a comment by the eslint version in package.json that this file should be updated",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "eslint.recommended.mjs",
    "pr_number": 19899,
    "repo": "django",
    "owner": "django",
    "comment_id": 2375147454,
    "comment_created_at": "2025-09-24T09:20:38Z"
  },
  {
    "code": "@@ -36,7 +36,7 @@ jobs:\n       - name: Build docs\n         run: |\n           cd docs\n-          sphinx-build -b spelling -n -q -W -d _build/doctrees -D language=en_US -j auto . _build/spelling\n+          sphinx-build -b spelling -n -q -W -d _build/doctrees -D language=en_US -j auto -W . _build/spelling",
    "comment": ":thinking: but -w is already there",
    "line_number": 39,
    "enriched": "File: .github/workflows/docs.yml\nCode: @@ -36,7 +36,7 @@ jobs:\n       - name: Build docs\n         run: |\n           cd docs\n-          sphinx-build -b spelling -n -q -W -d _build/doctrees -D language=en_US -j auto . _build/spelling\n+          sphinx-build -b spelling -n -q -W -d _build/doctrees -D language=en_US -j auto -W . _build/spelling\nComment: :thinking: but `-W` is already there ",
    "subcategory": "false positive",
    "category": "false positive",
    "file_path": ".github/workflows/docs.yml",
    "pr_number": 19893,
    "repo": "django",
    "owner": "django",
    "comment_id": 2370356415,
    "comment_created_at": "2025-09-22T21:34:04Z"
  },
  {
    "code": "@@ -412,6 +420,15 @@ For example, if we had a ``Book`` model with a ``ManyToManyField`` linking to\n             ),\n         ]\n \n+.. note::\n+   When switching from an implicit ``ManyToManyField`` to a custom through\n+   model, Django does not automatically preserve the unique index on the\n+   foreign keys. To maintain the same behavior, add a ``UniqueConstraint``\n+   in the migration. This ensures consistent behavior across database\n+   backends. It helps avoid subtle bugs that may arise from missing\n+   uniqueness enforcement. The older ``unique_together`` option is\n+   deprecated and should be avoided.\n+",
    "comment": "i don't think we need this\r\nin the manytomany.through docs it does say (https://docs.djangoproject.com/en/5.2/ref/models/fields/#django.db.models.manytomanyfield.through):\r\n\r\n> there is a unique constraint on the two foreign keys.\r\n\r\nwe are just making sure what is created here represents the auto created through model",
    "line_number": 431,
    "enriched": "File: docs/howto/writing-migrations.txt\nCode: @@ -412,6 +420,15 @@ For example, if we had a ``Book`` model with a ``ManyToManyField`` linking to\n             ),\n         ]\n \n+.. note::\n+   When switching from an implicit ``ManyToManyField`` to a custom through\n+   model, Django does not automatically preserve the unique index on the\n+   foreign keys. To maintain the same behavior, add a ``UniqueConstraint``\n+   in the migration. This ensures consistent behavior across database\n+   backends. It helps avoid subtle bugs that may arise from missing\n+   uniqueness enforcement. The older ``unique_together`` option is\n+   deprecated and should be avoided.\n+\nComment: I don't think we need this\r\nIn the ManyToMany.through docs it does say (https://docs.djangoproject.com/en/5.2/ref/models/fields/#django.db.models.ManyToManyField.through):\r\n\r\n> There is a unique constraint on the two foreign keys.\r\n\r\nWe are just making sure what is created here represents the auto created through model\r\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/howto/writing-migrations.txt",
    "pr_number": 19891,
    "repo": "django",
    "owner": "django",
    "comment_id": 2378877743,
    "comment_created_at": "2025-09-25T12:11:45Z"
  },
  {
    "code": "@@ -635,6 +636,16 @@ def technical_404_response(request, exception):\n         ):\n             return default_urlconf(request)\n \n+    for inner_tried in itertools.chain(*tried or []):\n+        if isinstance(inner_tried, URLResolver):\n+            inner_tried.debug_key_val = TriedPatternDebugInfo(\n+                key=\"namespace\", val=inner_tried.namespace\n+            )\n+        else:\n+            inner_tried.debug_key_val = TriedPatternDebugInfo(\n+                key=\"name\", val=inner_tried.name\n+            )",
    "comment": "\r\n\r\ni wouldn't add a namedtuple unless we had to",
    "line_number": 647,
    "enriched": "File: django/views/debug.py\nCode: @@ -635,6 +636,16 @@ def technical_404_response(request, exception):\n         ):\n             return default_urlconf(request)\n \n+    for inner_tried in itertools.chain(*tried or []):\n+        if isinstance(inner_tried, URLResolver):\n+            inner_tried.debug_key_val = TriedPatternDebugInfo(\n+                key=\"namespace\", val=inner_tried.namespace\n+            )\n+        else:\n+            inner_tried.debug_key_val = TriedPatternDebugInfo(\n+                key=\"name\", val=inner_tried.name\n+            )\nComment: ```suggestion\r\n        if isinstance(inner_tried, URLResolver):\r\n            inner_tried.debug_key_val = {\r\n                \"key\": \"namespace\",\r\n                \"val\": inner_tried.namespace,\r\n            }\r\n        else:\r\n            inner_tried.debug_key_val = {\"key\": \"name\", \"val\": inner_tried.name}\r\n```\r\n\r\nI wouldn't add a `namedtuple` unless we had to",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "django/views/debug.py",
    "pr_number": 19880,
    "repo": "django",
    "owner": "django",
    "comment_id": 2375043322,
    "comment_created_at": "2025-09-24T08:49:31Z"
  },
  {
    "code": "@@ -131,18 +131,18 @@ The code below is equivalent to the code above::\n \n .. _field-checking:\n \n-Field, constraint, model, manager, template engine, and database checks\n------------------------------------------------------------------------\n+Field, constraint, model, manager, template engine, task, and database checks",
    "comment": "good catch! though i'm not sure we should keep this given we are removing all inbuilt checks for tasks :thinking:",
    "line_number": 134,
    "enriched": "File: docs/topics/checks.txt\nCode: @@ -131,18 +131,18 @@ The code below is equivalent to the code above::\n \n .. _field-checking:\n \n-Field, constraint, model, manager, template engine, and database checks\n------------------------------------------------------------------------\n+Field, constraint, model, manager, template engine, task, and database checks\nComment: Good catch! Though I'm not sure we should keep this given we are removing all inbuilt checks for tasks :thinking: ",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "docs/topics/checks.txt",
    "pr_number": 19876,
    "repo": "django",
    "owner": "django",
    "comment_id": 2355590967,
    "comment_created_at": "2025-09-17T13:47:35Z"
  },
  {
    "code": "@@ -1423,6 +1423,14 @@ def values_list(self, *fields, flat=False, named=False):\n                     \"field.\"\n                 )\n             elif not fields:\n+                # RemovedInDjango70Warning: When the deprecation ends, raise\n+                # TypeError instead.",
    "comment": "\r\ngiven other typeerrors here have a specific message, i think it makes sense to define one here",
    "line_number": 1427,
    "enriched": "File: django/db/models/query.py\nCode: @@ -1423,6 +1423,14 @@ def values_list(self, *fields, flat=False, named=False):\n                     \"field.\"\n                 )\n             elif not fields:\n+                # RemovedInDjango70Warning: When the deprecation ends, raise\n+                # TypeError instead.\nComment: ```suggestion\r\n                # RemovedInDjango70Warning: When the deprecation ends, replace with:\r\n                # raise TypeError(\r\n                #     \"'flat' is not valid when values_list is called with no fields.\"\r\n                # )\r\n```\r\nGiven other `TypeError`s here have a specific message, I think it makes sense to define one here",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "django/db/models/query.py",
    "pr_number": 19874,
    "repo": "django",
    "owner": "django",
    "comment_id": 2362490328,
    "comment_created_at": "2025-09-19T10:38:00Z"
  },
  {
    "code": "@@ -18,13 +18,13 @@ project.\n Python compatibility\n ====================\n \n-Django 6.0 supports Python 3.12 and 3.13. We **highly recommend** and only\n-officially support the latest release of each series.\n+Django 6.0 supports Python 3.12 and 3.13. We **highly recommend**, and only\n+officially support, the latest release of each series.",
    "comment": "if we prefer this change, we should do it also in https://github.com/django/django/pull/19834 (cc @jacobtylerwalls )",
    "line_number": 22,
    "enriched": "File: docs/releases/6.0.txt\nCode: @@ -18,13 +18,13 @@ project.\n Python compatibility\n ====================\n \n-Django 6.0 supports Python 3.12 and 3.13. We **highly recommend** and only\n-officially support the latest release of each series.\n+Django 6.0 supports Python 3.12 and 3.13. We **highly recommend**, and only\n+officially support, the latest release of each series.\nComment: If we prefer this change, we should do it also in https://github.com/django/django/pull/19834 (cc @jacobtylerwalls )",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "docs/releases/6.0.txt",
    "pr_number": 19872,
    "repo": "django",
    "owner": "django",
    "comment_id": 2355508632,
    "comment_created_at": "2025-09-17T13:21:00Z"
  },
  {
    "code": "@@ -1142,23 +1142,22 @@ def _save_table(\n                         ),\n                     )[\"_order__max\"]\n                 )\n-            fields = [\n+            insert_fields = [",
    "comment": "i changed the variable name as it made it much easier for me to reason about the origin of the f.generated and (pk_set or f is not meta.auto_field) check.",
    "line_number": 1145,
    "enriched": "File: django/db/models/base.py\nCode: @@ -1142,23 +1142,22 @@ def _save_table(\n                         ),\n                     )[\"_order__max\"]\n                 )\n-            fields = [\n+            insert_fields = [\nComment: I changed the variable name as it made it much easier for me to reason about the origin of the `f.generated and (pk_set or f is not meta.auto_field)` check.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "django/db/models/base.py",
    "pr_number": 19871,
    "repo": "django",
    "owner": "django",
    "comment_id": 2353771065,
    "comment_created_at": "2025-09-16T22:14:50Z"
  },
  {
    "code": "@@ -1153,7 +1153,8 @@ def _save_table(\n                     getattr(self, field.attname) if raw else field.pre_save(self, False)\n                 )\n                 if hasattr(value, \"resolve_expression\"):\n-                    returning_fields.append(field)\n+                    if field not in returning_fields:",
    "comment": "in tandem with #19868 we can collapse this when the else is gone.",
    "line_number": 1156,
    "enriched": "File: django/db/models/base.py\nCode: @@ -1153,7 +1153,8 @@ def _save_table(\n                     getattr(self, field.attname) if raw else field.pre_save(self, False)\n                 )\n                 if hasattr(value, \"resolve_expression\"):\n-                    returning_fields.append(field)\n+                    if field not in returning_fields:\nComment: In tandem with #19868 we can collapse this when the else is gone.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "django/db/models/base.py",
    "pr_number": 19869,
    "repo": "django",
    "owner": "django",
    "comment_id": 2352984002,
    "comment_created_at": "2025-09-16T16:03:26Z"
  },
  {
    "code": "@@ -1154,8 +1154,6 @@ def _save_table(\n                 )\n                 if hasattr(value, \"resolve_expression\"):\n                     returning_fields.append(field)\n-                elif field.db_returning:",
    "comment": "i think we might want to tweak this check to compare against pk_fields and pk_set as well.\r\n\r\nan alternative would be have the pre_save value assigned back to the instance in _assign_returned_values without transiting through the database.\r\n\r\ni can work on proposed adjusted later today if you'd like.",
    "line_number": 1157,
    "enriched": "File: django/db/models/base.py\nCode: @@ -1154,8 +1154,6 @@ def _save_table(\n                 )\n                 if hasattr(value, \"resolve_expression\"):\n                     returning_fields.append(field)\n-                elif field.db_returning:\nComment: I think we might want to tweak this check to compare against `pk_fields` and `pk_set` as well.\r\n\r\nAn alternative would be have the `pre_save` value assigned back to the instance in `_assign_returned_values` without transiting through the database.\r\n\r\nI can work on proposed adjusted later today if you'd like.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "django/db/models/base.py",
    "pr_number": 19868,
    "repo": "django",
    "owner": "django",
    "comment_id": 2352980310,
    "comment_created_at": "2025-09-16T16:01:49Z"
  },
  {
    "code": "@@ -513,7 +513,7 @@ def test_rfc2231_wrong_title(self):\n             (\n                 \"Content-Type: application/x-stuff; \"\n                 \"title*='This%20is%20%2A%2A%2Afun%2A%2A%2A\",\n-                \"'This is ***fun***\",\n+                \"'This%20is%20%2A%2A%2Afun%2A%2A%2A\",",
    "comment": "in the docstring we have \"but stdlib email still decodes (#35440).\", shouldn't we revert that also, if we are making this change?",
    "line_number": 515,
    "enriched": "File: tests/utils_tests/test_http.py\nCode: @@ -513,7 +513,7 @@ def test_rfc2231_wrong_title(self):\n             (\n                 \"Content-Type: application/x-stuff; \"\n                 \"title*='This%20is%20%2A%2A%2Afun%2A%2A%2A\",\n-                \"'This is ***fun***\",\n+                \"'This%20is%20%2A%2A%2Afun%2A%2A%2A\",\nComment: In the docstring we have \"But stdlib email still decodes (#35440).\", shouldn't we revert that also, if we are making this change?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/utils_tests/test_http.py",
    "pr_number": 19867,
    "repo": "django",
    "owner": "django",
    "comment_id": 2349799935,
    "comment_created_at": "2025-09-15T18:32:36Z"
  },
  {
    "code": "@@ -226,8 +226,8 @@ both appearing in the \"To:\"::\n         [\"john@example.com\", \"jane@example.com\"],\n     )\n \n-This sends a message to john@example.com and jane@example.com, with them both\n-receiving a separate email::\n+This sends a message to ``john@example.com`` and ``jane@example.com``, with\n+them both receiving a separate email::",
    "comment": "unrelated, but this sounds off to me. perhaps \"both of them\" or \"each of them\"",
    "line_number": 230,
    "enriched": "File: docs/topics/email.txt\nCode: @@ -226,8 +226,8 @@ both appearing in the \"To:\"::\n         [\"john@example.com\", \"jane@example.com\"],\n     )\n \n-This sends a message to john@example.com and jane@example.com, with them both\n-receiving a separate email::\n+This sends a message to ``john@example.com`` and ``jane@example.com``, with\n+them both receiving a separate email::\nComment: Unrelated, but this sounds off to me. Perhaps \"both of them\" or \"each of them\"",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/topics/email.txt",
    "pr_number": 19866,
    "repo": "django",
    "owner": "django",
    "comment_id": 2348730208,
    "comment_created_at": "2025-09-15T11:41:43Z"
  },
  {
    "code": "@@ -150,6 +150,7 @@ def gettext_noop(s):\n     (\"vi\", gettext_noop(\"Vietnamese\")),\n     (\"zh-hans\", gettext_noop(\"Simplified Chinese\")),\n     (\"zh-hant\", gettext_noop(\"Traditional Chinese\")),\n+    (\"ht\", gettext_noop(\"Haitian Creole\")),",
    "comment": "this list should be alphabetized.",
    "line_number": 153,
    "enriched": "File: django/conf/global_settings.py\nCode: @@ -150,6 +150,7 @@ def gettext_noop(s):\n     (\"vi\", gettext_noop(\"Vietnamese\")),\n     (\"zh-hans\", gettext_noop(\"Simplified Chinese\")),\n     (\"zh-hant\", gettext_noop(\"Traditional Chinese\")),\n+    (\"ht\", gettext_noop(\"Haitian Creole\")),\nComment: This list should be alphabetized.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "django/conf/global_settings.py",
    "pr_number": 19858,
    "repo": "django",
    "owner": "django",
    "comment_id": 2347986200,
    "comment_created_at": "2025-09-15T06:24:07Z"
  },
  {
    "code": "@@ -565,6 +565,11 @@ def run(self, result):\n         ]\n         test_results = pool.imap_unordered(self.run_subsuite.__func__, args)\n \n+        # Disable buffering on the local test result that will accumulate\n+        # remote suites results as each process will take care of its own\n+        # buffering and there's nothing to capture on the main process.\n+        result.buffer = False",
    "comment": "i'd move this up, since this could _potentially_ happen before you start running the multiprocessing pool\r\n",
    "line_number": 567,
    "enriched": "File: django/test/runner.py\nCode: @@ -565,6 +565,11 @@ def run(self, result):\n         ]\n         test_results = pool.imap_unordered(self.run_subsuite.__func__, args)\n \n+        # Disable buffering on the local test result that will accumulate\n+        # remote suites results as each process will take care of its own\n+        # buffering and there's nothing to capture on the main process.\n+        result.buffer = False\nComment: I'd move this up, since this could _potentially_ happen before you start running the multiprocessing pool\r\n```suggestion\r\n        # Disable buffering on the local test result that will accumulate\r\n        # remote suites results as each process will take care of its own\r\n        # buffering and there's nothing to capture on the main process.\r\n        result.buffer = False\r\n\r\n        test_results = pool.imap_unordered(self.run_subsuite.__func__, args)\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "django/test/runner.py",
    "pr_number": 19853,
    "repo": "django",
    "owner": "django",
    "comment_id": 2345252663,
    "comment_created_at": "2025-09-12T19:36:17Z"
  },
  {
    "code": "@@ -138,10 +139,21 @@ def on_template_render(self, sender, signal, template, context, **kwargs):\n         self.rendered_templates.append(template)\n         self.context.append(copy(context))\n \n+    @property\n+    def rendered_template_names(self):\n+        rendered_names = []\n+        for t in self.rendered_templates:\n+            if t.name is None:\n+                continue\n+            if isinstance(t, PartialTemplate):\n+                rendered_names.append(f\"{t.origin.template_name}#{t.name}\")\n+            rendered_names.append(t.name)",
    "comment": "factored this property out once we needed it in two places. also nice is that this \"qualified\" form now appears first in the output before the \"short\" form.",
    "line_number": 150,
    "enriched": "File: django/test/testcases.py\nCode: @@ -138,10 +139,21 @@ def on_template_render(self, sender, signal, template, context, **kwargs):\n         self.rendered_templates.append(template)\n         self.context.append(copy(context))\n \n+    @property\n+    def rendered_template_names(self):\n+        rendered_names = []\n+        for t in self.rendered_templates:\n+            if t.name is None:\n+                continue\n+            if isinstance(t, PartialTemplate):\n+                rendered_names.append(f\"{t.origin.template_name}#{t.name}\")\n+            rendered_names.append(t.name)\nComment: Factored this property out once we needed it in two places. Also nice is that this \"qualified\" form now appears first in the output before the \"short\" form.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "django/test/testcases.py",
    "pr_number": 19847,
    "repo": "django",
    "owner": "django",
    "comment_id": 2349300078,
    "comment_created_at": "2025-09-15T15:00:50Z"
  },
  {
    "code": "@@ -1414,11 +1414,14 @@ def values(self, *fields, **expressions):\n     def values_list(self, *fields, flat=False, named=False):\n         if flat and named:\n             raise TypeError(\"'flat' and 'named' can't be used together.\")\n-        if flat and len(fields) > 1:\n-            raise TypeError(\n-                \"'flat' is not valid when values_list is called with more than one \"\n-                \"field.\"\n-            )\n+        if flat:\n+            if len(fields) > 1:\n+                raise TypeError(\n+                    \"'flat' is not valid when values_list is called with more than one \"\n+                    \"field.\"\n+                )\n+            elif not fields:\n+                fields = [self.model._meta.concrete_fields[0].attname]",
    "comment": "i think this is always pk, but i wasn't 100% sure, so i copied here what's eventually used in query.set_values() when not passing any fields.",
    "line_number": 1426,
    "enriched": "File: django/db/models/query.py\nCode: @@ -1414,11 +1414,14 @@ def values(self, *fields, **expressions):\n     def values_list(self, *fields, flat=False, named=False):\n         if flat and named:\n             raise TypeError(\"'flat' and 'named' can't be used together.\")\n-        if flat and len(fields) > 1:\n-            raise TypeError(\n-                \"'flat' is not valid when values_list is called with more than one \"\n-                \"field.\"\n-            )\n+        if flat:\n+            if len(fields) > 1:\n+                raise TypeError(\n+                    \"'flat' is not valid when values_list is called with more than one \"\n+                    \"field.\"\n+                )\n+            elif not fields:\n+                fields = [self.model._meta.concrete_fields[0].attname]\nComment: I think this is always `pk`, but I wasn't 100% sure, so I copied here what's eventually used in `Query.set_values()` when not passing any fields.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "django/db/models/query.py",
    "pr_number": 19844,
    "repo": "django",
    "owner": "django",
    "comment_id": 2342048189,
    "comment_created_at": "2025-09-11T18:47:30Z"
  },
  {
    "code": "@@ -1184,24 +1185,77 @@ def in_bulk(self, id_list=None, *, field_name=\"pk\"):\n                 \"in_bulk()'s field_name must be a unique field but %r isn't.\"\n                 % field_name\n             )\n+\n+        qs = self\n+\n+        def get_obj(obj):\n+            return obj",
    "comment": "kind of wish python had an identity function to avoid the call but oh well.",
    "line_number": 1191,
    "enriched": "File: django/db/models/query.py\nCode: @@ -1184,24 +1185,77 @@ def in_bulk(self, id_list=None, *, field_name=\"pk\"):\n                 \"in_bulk()'s field_name must be a unique field but %r isn't.\"\n                 % field_name\n             )\n+\n+        qs = self\n+\n+        def get_obj(obj):\n+            return obj\nComment: Kind of wish Python had an identity function to avoid the call but oh well.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "django/db/models/query.py",
    "pr_number": 19842,
    "repo": "django",
    "owner": "django",
    "comment_id": 2341185778,
    "comment_created_at": "2025-09-11T14:34:06Z"
  },
  {
    "code": "@@ -49,6 +49,7 @@\n     padding: 8px;\n }\n \n+.tabular .selector-chosen-title label,",
    "comment": "this visual regression was introduced in a0f50c2a483678d31bd1ad6f08fd3a0b8399e27b\r\nwe might want to do something like:\r\ndiff\r\n--- a/django/contrib/admin/static/admin/css/widgets.css\r\n+++ b/django/contrib/admin/static/admin/css/widgets.css\r\n@@ -49,8 +49,7 @@\r\n     padding: 8px;\r\n }\r\n \r\n-.tabular .selector-chosen-title label,\r\n-.aligned .selector-chosen-title label {\r\n+.selector-chosen-title label {\r\n     color: var(--header-link-color);\r\n     width: 100%;\r\n }\r\n@@ -61,7 +60,7 @@\r\n     padding: 8px;\r\n }\r\n \r\n-.aligned .selector-available-title label {\r\n+.selector-available-title label {\r\n     width: 100%;\r\n }\r\n",
    "line_number": 52,
    "enriched": "File: django/contrib/admin/static/admin/css/widgets.css\nCode: @@ -49,6 +49,7 @@\n     padding: 8px;\n }\n \n+.tabular .selector-chosen-title label,\nComment: This visual regression was introduced in a0f50c2a483678d31bd1ad6f08fd3a0b8399e27b\r\nWe might want to do something like:\r\n```diff\r\n--- a/django/contrib/admin/static/admin/css/widgets.css\r\n+++ b/django/contrib/admin/static/admin/css/widgets.css\r\n@@ -49,8 +49,7 @@\r\n     padding: 8px;\r\n }\r\n \r\n-.tabular .selector-chosen-title label,\r\n-.aligned .selector-chosen-title label {\r\n+.selector-chosen-title label {\r\n     color: var(--header-link-color);\r\n     width: 100%;\r\n }\r\n@@ -61,7 +60,7 @@\r\n     padding: 8px;\r\n }\r\n \r\n-.aligned .selector-available-title label {\r\n+.selector-available-title label {\r\n     width: 100%;\r\n }\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "django/contrib/admin/static/admin/css/widgets.css",
    "pr_number": 19837,
    "repo": "django",
    "owner": "django",
    "comment_id": 2348625941,
    "comment_created_at": "2025-09-15T10:59:16Z"
  },
  {
    "code": "@@ -536,7 +536,20 @@ async def __aiter__(self):\n             )\n             # sync iterator. Consume via sync_to_async and yield via async\n             # generator.\n-            for part in await sync_to_async(list)(self.streaming_content):\n+            _iterator = self.streaming_content\n+            _sentinel = object()\n+\n+            def _next_wrapper():\n+                try:\n+                    return _iterator.__next__()\n+                except StopIteration:\n+                    return _sentinel\n+\n+            _next = sync_to_async(_next_wrapper, thread_sensitive=False)",
    "comment": "thread_sensitive=false. and if the iterator was using the orm?",
    "line_number": 548,
    "enriched": "File: django/http/response.py\nCode: @@ -536,7 +536,20 @@ async def __aiter__(self):\n             )\n             # sync iterator. Consume via sync_to_async and yield via async\n             # generator.\n-            for part in await sync_to_async(list)(self.streaming_content):\n+            _iterator = self.streaming_content\n+            _sentinel = object()\n+\n+            def _next_wrapper():\n+                try:\n+                    return _iterator.__next__()\n+                except StopIteration:\n+                    return _sentinel\n+\n+            _next = sync_to_async(_next_wrapper, thread_sensitive=False)\nComment: `thread_sensitive=False`. And if the iterator was using the ORM? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "django/http/response.py",
    "pr_number": 19835,
    "repo": "django",
    "owner": "django",
    "comment_id": 2339953150,
    "comment_created_at": "2025-09-11T10:10:13Z"
  },
  {
    "code": "@@ -0,0 +1,264 @@\n+============================================\n+Django 6.1 release notes - UNDER DEVELOPMENT\n+============================================\n+\n+*Expected August 2026*\n+\n+Welcome to Django 6.1!\n+\n+These release notes cover the :ref:`new features <whats-new-6.1>`, as well as\n+some :ref:`backwards incompatible changes <backwards-incompatible-6.1>` you'll\n+want to be aware of when upgrading from Django 6.0 or earlier. We've\n+:ref:`begun the deprecation process for some features\n+<deprecated-features-6.1>`.\n+\n+See the :doc:`/howto/upgrade-version` guide if you're updating an existing\n+project.\n+\n+Python compatibility\n+====================\n+\n+Django 6.1 supports Python 3.12, 3.13, and 3.14. We **highly recommend** and\n+only officially support the latest release of each series.\n+\n+.. _whats-new-6.1:\n+\n+What's new in Django 6.1\n+========================\n+\n+Minor features\n+--------------\n+\n+:mod:`django.contrib.admin`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.admindocs`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.auth`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.contenttypes`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.gis`\n+~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.messages`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.postgres`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.redirects`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.sessions`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.sitemaps`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.sites`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.staticfiles`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.syndication`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Asynchronous views\n+~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Cache\n+~~~~~\n+\n+* ...\n+\n+CSRF\n+~~~~\n+\n+* ...\n+\n+Decorators\n+~~~~~~~~~~\n+\n+* ...\n+\n+Email\n+~~~~~\n+\n+* ...\n+\n+Error Reporting\n+~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+File Storage\n+~~~~~~~~~~~~\n+\n+* ...\n+\n+File Uploads\n+~~~~~~~~~~~~\n+\n+* ...\n+\n+Forms\n+~~~~~\n+\n+* ...\n+\n+Generic Views\n+~~~~~~~~~~~~~\n+\n+* ...\n+\n+Internationalization\n+~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Logging\n+~~~~~~~\n+\n+* ...\n+\n+Management Commands\n+~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Migrations\n+~~~~~~~~~~\n+\n+* ...\n+\n+Models\n+~~~~~~\n+\n+* ...\n+\n+Pagination\n+~~~~~~~~~~\n+\n+* ...",
    "comment": "in previous stub release notes (e.g. 52ad2ebc365b6806e417d05854d8005a18ee63fa and cdaf333498f4c1792c666f96d73a3a2d97c9ff27), we've not had a section for pagination but i think we can keep in the general template",
    "line_number": 181,
    "enriched": "File: docs/releases/6.1.txt\nCode: @@ -0,0 +1,264 @@\n+============================================\n+Django 6.1 release notes - UNDER DEVELOPMENT\n+============================================\n+\n+*Expected August 2026*\n+\n+Welcome to Django 6.1!\n+\n+These release notes cover the :ref:`new features <whats-new-6.1>`, as well as\n+some :ref:`backwards incompatible changes <backwards-incompatible-6.1>` you'll\n+want to be aware of when upgrading from Django 6.0 or earlier. We've\n+:ref:`begun the deprecation process for some features\n+<deprecated-features-6.1>`.\n+\n+See the :doc:`/howto/upgrade-version` guide if you're updating an existing\n+project.\n+\n+Python compatibility\n+====================\n+\n+Django 6.1 supports Python 3.12, 3.13, and 3.14. We **highly recommend** and\n+only officially support the latest release of each series.\n+\n+.. _whats-new-6.1:\n+\n+What's new in Django 6.1\n+========================\n+\n+Minor features\n+--------------\n+\n+:mod:`django.contrib.admin`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.admindocs`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.auth`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.contenttypes`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.gis`\n+~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.messages`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.postgres`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.redirects`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.sessions`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.sitemaps`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.sites`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.staticfiles`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+:mod:`django.contrib.syndication`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Asynchronous views\n+~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Cache\n+~~~~~\n+\n+* ...\n+\n+CSRF\n+~~~~\n+\n+* ...\n+\n+Decorators\n+~~~~~~~~~~\n+\n+* ...\n+\n+Email\n+~~~~~\n+\n+* ...\n+\n+Error Reporting\n+~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+File Storage\n+~~~~~~~~~~~~\n+\n+* ...\n+\n+File Uploads\n+~~~~~~~~~~~~\n+\n+* ...\n+\n+Forms\n+~~~~~\n+\n+* ...\n+\n+Generic Views\n+~~~~~~~~~~~~~\n+\n+* ...\n+\n+Internationalization\n+~~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Logging\n+~~~~~~~\n+\n+* ...\n+\n+Management Commands\n+~~~~~~~~~~~~~~~~~~~\n+\n+* ...\n+\n+Migrations\n+~~~~~~~~~~\n+\n+* ...\n+\n+Models\n+~~~~~~\n+\n+* ...\n+\n+Pagination\n+~~~~~~~~~~\n+\n+* ...\nComment: In previous stub release notes (e.g. 52ad2ebc365b6806e417d05854d8005a18ee63fa and cdaf333498f4c1792c666f96d73a3a2d97c9ff27), we've not had a section for pagination but I think we can keep in the general template",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/releases/6.1.txt",
    "pr_number": 19834,
    "repo": "django",
    "owner": "django",
    "comment_id": 2348858940,
    "comment_created_at": "2025-09-15T12:35:35Z"
  },
  {
    "code": "@@ -8,6 +8,8 @@\n \n import django\n \n+django_file_prefixes = (os.path.dirname(django.__file__),)",
    "comment": "what do you think about the proposal to use importlib.resources in ticket-30950?",
    "line_number": 11,
    "enriched": "File: django/utils/deprecation.py\nCode: @@ -8,6 +8,8 @@\n \n import django\n \n+django_file_prefixes = (os.path.dirname(django.__file__),)\nComment: What do you think about the proposal to use `importlib.resources` in ticket-30950?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "django/utils/deprecation.py",
    "pr_number": 19823,
    "repo": "django",
    "owner": "django",
    "comment_id": 2325304157,
    "comment_created_at": "2025-09-05T14:46:11Z"
  },
  {
    "code": "@@ -481,7 +481,7 @@ The class has the following methods:\n         message.attach(inline_image)\n         message.attach_alternative(f'\u2026 <img src=\"cid:${cid}\"> \u2026', \"text/html\")\n \n-    Python's :meth:`email.contentmanager.set_content` documentation describes\n+    Python's :func:`email.contentmanager.set_content` documentation describes",
    "comment": "set_content() and attach_alternative() are methods.",
    "line_number": 484,
    "enriched": "File: docs/topics/email.txt\nCode: @@ -481,7 +481,7 @@ The class has the following methods:\n         message.attach(inline_image)\n         message.attach_alternative(f'\u2026 <img src=\"cid:${cid}\"> \u2026', \"text/html\")\n \n-    Python's :meth:`email.contentmanager.set_content` documentation describes\n+    Python's :func:`email.contentmanager.set_content` documentation describes\nComment: `set_content()` and `attach_alternative()` are methods.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/topics/email.txt",
    "pr_number": 19817,
    "repo": "django",
    "owner": "django",
    "comment_id": 2338832727,
    "comment_created_at": "2025-09-11T06:27:42Z"
  },
  {
    "code": "@@ -323,185 +323,201 @@ email backend API :ref:`provides an alternative\n \n .. class:: EmailMessage\n \n-The :class:`~django.core.mail.EmailMessage` class is initialized with the\n-following parameters. All parameters are optional and can be set at any time\n-prior to calling the ``send()`` method.\n+    The :class:`!EmailMessage` class is initialized with the following\n+    parameters. All parameters are optional and can be set at any time prior\n+    to calling the :meth:`send` method.\n \n-The first four parameters can be passed as positional or keyword arguments,\n-but must be in the given order if positional arguments are used:\n+    The first four parameters can be passed as positional or keyword arguments,\n+    but must be in the given order if positional arguments are used:\n \n-* ``subject``: The subject line of the email.\n+    * ``subject``: The subject line of the email.\n \n-* ``body``: The body text. This should be a plain text message.\n+    * ``body``: The body text. This should be a plain text message.\n \n-* ``from_email``: The sender's address. Both ``fred@example.com`` and\n-  ``\"Fred\" <fred@example.com>`` forms are legal. If omitted, the\n-  :setting:`DEFAULT_FROM_EMAIL` setting is used.\n+    * ``from_email``: The sender's address. Both ``fred@example.com`` and\n+      ``\"Fred\" <fred@example.com>`` forms are legal. If omitted, the\n+      :setting:`DEFAULT_FROM_EMAIL` setting is used.\n \n-* ``to``: A list or tuple of recipient addresses.\n+    * ``to``: A list or tuple of recipient addresses.\n \n-The following parameters must be given as keyword arguments if used:\n+    The following parameters must be given as keyword arguments if used:\n \n-* ``cc``: A list or tuple of recipient addresses used in the \"Cc\" header\n-  when sending the email.\n+    * ``cc``: A list or tuple of recipient addresses used in the \"Cc\" header\n+      when sending the email.\n \n-* ``bcc``: A list or tuple of addresses used in the \"Bcc\" header when\n-  sending the email.\n+    * ``bcc``: A list or tuple of addresses used in the \"Bcc\" header when\n+      sending the email.\n \n-* ``reply_to``: A list or tuple of recipient addresses used in the \"Reply-To\"\n-  header when sending the email.\n+    * ``reply_to``: A list or tuple of recipient addresses used in the\n+      \"Reply-To\" header when sending the email.\n \n-* ``attachments``: A list of attachments to put on the message. Each can\n-  be an instance of :class:`~email.message.MIMEPart` or\n-  :class:`~django.core.mail.EmailAttachment`, or a tuple with attributes\n-  ``(filename, content, mimetype)``.\n+    * ``attachments``: A list of attachments to put on the message. Each can\n+      be an instance of :class:`~email.message.MIMEPart` or\n+      :class:`EmailAttachment`, or a tuple with attributes\n+      ``(filename, content, mimetype)``.\n \n-  .. versionchanged:: 5.2\n+      .. versionchanged:: 5.2\n \n-    Support for :class:`~django.core.mail.EmailAttachment` items of\n-    ``attachments`` was added.\n+          Support for :class:`EmailAttachment` items of ``attachments`` was\n+          added.\n \n-  .. versionchanged:: 6.0\n+      .. versionchanged:: 6.0\n \n-    Support for :class:`~email.message.MIMEPart` objects in the ``attachments``\n-    list was added.\n+          Support for :class:`~email.message.MIMEPart` objects in the\n+          ``attachments`` list was added.\n \n-  .. deprecated:: 6.0\n+      .. deprecated:: 6.0\n \n-    Support for Python's legacy :class:`~email.mime.base.MIMEBase` objects in\n-    ``attachments`` is deprecated. Use :class:`~email.message.MIMEPart`\n-    instead.\n+          Support for Python's legacy :class:`~email.mime.base.MIMEBase`\n+          objects in ``attachments`` is deprecated. Use\n+          :class:`~email.message.MIMEPart` instead.\n \n-* ``headers``: A dictionary of extra headers to put on the message. The\n-  keys are the header name, values are the header values. It's up to the\n-  caller to ensure header names and values are in the correct format for\n-  an email message. The corresponding attribute is ``extra_headers``.\n+    * ``headers``: A dictionary of extra headers to put on the message. The\n+      keys are the header name, values are the header values. It's up to the\n+      caller to ensure header names and values are in the correct format for\n+      an email message. The corresponding attribute is ``extra_headers``.\n \n-* ``connection``: An :ref:`email backend <topic-email-backends>` instance. Use\n-  this parameter if you are sending the ``EmailMessage`` via ``send()`` and you\n-  want to use the same connection for multiple messages. If omitted, a new\n-  connection is created when ``send()`` is called. This parameter is ignored\n-  when using :ref:`send_messages() <topics-sending-multiple-emails>`.\n+    * ``connection``: An :ref:`email backend <topic-email-backends>` instance.\n+      Use this parameter if you are sending the :class:`!EmailMessage` via\n+      :meth:`send` and you want to use the same connection for multiple\n+      messages. If omitted, a new connection is created when :meth:`send` is\n+      called. This parameter is ignored when using\n+      :ref:`send_messages() <topics-sending-multiple-emails>`.\n \n-.. deprecated:: 6.0\n+    .. deprecated:: 6.0\n \n-    Passing all except the first four parameters as positional arguments is\n-    deprecated.\n+        Passing all except the first four parameters as positional arguments is\n+        deprecated.\n \n-For example::\n+    For example::\n \n-    from django.core.mail import EmailMessage\n+        from django.core.mail import EmailMessage\n \n-    email = EmailMessage(\n-        subject=\"Hello\",\n-        body=\"Body goes here\",\n-        from_email=\"from@example.com\",\n-        to=[\"to1@example.com\", \"to2@example.com\"],\n-        bcc=[\"bcc@example.com\"],\n-        reply_to=[\"another@example.com\"],\n-        headers={\"Message-ID\": \"foo\"},\n-    )\n+        email = EmailMessage(\n+            subject=\"Hello\",\n+            body=\"Body goes here\",\n+            from_email=\"from@example.com\",\n+            to=[\"to1@example.com\", \"to2@example.com\"],\n+            bcc=[\"bcc@example.com\"],\n+            reply_to=[\"another@example.com\"],\n+            headers={\"Message-ID\": \"foo\"},\n+        )\n \n-The class has the following methods:\n-\n-* ``send(fail_silently=False)`` sends the message. If a connection was\n-  specified when the email was constructed, that connection will be used.\n-  Otherwise, an instance of the default backend will be instantiated and\n-  used. If the keyword argument ``fail_silently`` is ``True``, exceptions\n-  raised while sending the message will be quashed. An empty list of\n-  recipients will not raise an exception. It will return ``1`` if the message\n-  was sent successfully, otherwise ``0``.\n-\n-* ``message(policy=email.policy.default)`` constructs and returns a Python\n-  :class:`email.message.EmailMessage` object representing the message to be\n-  sent.\n-\n-  The keyword argument ``policy`` allows specifying the set of rules for\n-  updating and serializing the representation of the message. It must be an\n-  :mod:`email.policy.Policy <email.policy>` object. Defaults to\n-  :data:`email.policy.default`. In certain cases you may want to use\n-  :data:`~email.policy.SMTP`, :data:`~email.policy.SMTPUTF8` or a custom\n-  policy. For example, :class:`django.core.mail.backends.smtp.EmailBackend`\n-  uses the :data:`~email.policy.SMTP` policy to ensure ``\\r\\n`` line endings\n-  as required by the SMTP protocol.\n-\n-  If you ever need to extend Django's :class:`~django.core.mail.EmailMessage`\n-  class, you'll probably want to override this method to put the content you\n-  want into the Python EmailMessage object.\n-\n-  .. versionchanged:: 6.0\n-\n-      The ``policy`` keyword argument was added and the return type was updated\n-      to an instance of :class:`~email.message.EmailMessage`.\n-\n-* ``recipients()`` returns a list of all the recipients of the message,\n-  whether they're recorded in the ``to``, ``cc`` or ``bcc`` attributes. This\n-  is another method you might need to override when subclassing, because the\n-  SMTP server needs to be told the full list of recipients when the message\n-  is sent. If you add another way to specify recipients in your class, they\n-  need to be returned from this method as well.\n-\n-* ``attach()`` creates a new attachment and adds it to the message.\n-  There are two ways to call ``attach()``:\n-\n-  * You can pass it three arguments: ``filename``, ``content`` and\n-    ``mimetype``. ``filename`` is the name of the file attachment as it will\n-    appear in the email, ``content`` is the data that will be contained inside\n-    the attachment and ``mimetype`` is the optional MIME type for the\n-    attachment. If you omit ``mimetype``, the MIME content type will be guessed\n-    from the filename of the attachment.\n+    The class has the following methods:\n \n-    For example::\n+    .. method:: send(fail_silently=False)\n \n-       message.attach(\"design.png\", img_data, \"image/png\")\n-\n-    If you specify a ``mimetype`` of :mimetype:`message/rfc822`, ``content``\n-    can be a :class:`django.core.mail.EmailMessage` or Python's\n-    :class:`email.message.EmailMessage` or :class:`email.message.Message`.\n-\n-    For a ``mimetype`` starting with :mimetype:`text/`, content is expected to\n-    be a string. Binary data will be decoded using UTF-8, and if that fails,\n-    the MIME type will be changed to :mimetype:`application/octet-stream` and\n-    the data will be attached unchanged.\n-\n-  * Or for attachments requiring additional headers or parameters, you can pass\n-    ``attach()`` a single Python :class:`~email.message.MIMEPart` object.\n-    This will be attached directly to the resulting message. For example,\n-    to attach an inline image with a :mailheader:`Content-ID`::\n-\n-        cid = email.utils.make_msgid()\n-        inline_image = email.message.MIMEPart()\n-        inline_image.set_content(\n-            image_data_bytes,\n-            maintype=\"image\",\n-            subtype=\"png\",\n-            disposition=\"inline\",\n-            cid=f\"<{cid}>\",\n-        )\n-        message.attach(inline_image)\n-        message.attach_alternative(f'\u2026 <img src=\"cid:${cid}\"> \u2026', \"text/html\")\n+        Sends the message. If a connection was specified when the email was\n+        constructed, that connection will be used. Otherwise, an instance of\n+        the default backend will be instantiated and used. If the keyword\n+        argument ``fail_silently`` is ``True``, exceptions raised while sending\n+        the message will be quashed. An empty list of recipients will not raise\n+        an exception. It will return ``1`` if the message was sent\n+        successfully, otherwise ``0``.\n \n-    Python's :meth:`email.contentmanager.set_content` documentation describes\n-    the supported arguments for ``MIMEPart.set_content()``.\n+    .. method:: message(policy=email.policy.default)\n \n-    .. versionchanged:: 6.0\n+        Constructs and returns a Python :class:`email.message.EmailMessage`\n+        object representing the message to be sent.\n \n-        Support for :class:`~email.message.MIMEPart` attachments was added.\n+        The keyword argument ``policy`` allows specifying the set of rules for\n+        updating and serializing the representation of the message. It must be\n+        an :mod:`email.policy.Policy <email.policy>` object. Defaults to\n+        :data:`email.policy.default`. In certain cases you may want to use\n+        :data:`~email.policy.SMTP`, :data:`~email.policy.SMTPUTF8` or a custom\n+        policy. For example,\n+        :class:`django.core.mail.backends.smtp.EmailBackend` uses the\n+        :data:`~email.policy.SMTP` policy to ensure ``\\r\\n`` line endings as\n+        required by the SMTP protocol.\n \n-    .. deprecated:: 6.0\n+        If you ever need to extend Django's :class:`EmailMessage` class,\n+        you'll probably want to override this method to put the content you\n+        want into the Python EmailMessage object.\n+\n+        .. versionchanged:: 6.0\n+\n+            The ``policy`` keyword argument was added and the return type was\n+            updated to an instance of :class:`~email.message.EmailMessage`.\n+\n+    .. method:: recipients()\n+\n+        Returns a list of all the recipients of the message, whether they're\n+        recorded in the ``to``, ``cc`` or ``bcc`` attributes. This is another\n+        method you might need to override when subclassing, because the SMTP\n+        server needs to be told the full list of recipients when the message\n+        is sent. If you add another way to specify recipients in your class,\n+        they need to be returned from this method as well.\n+\n+    .. method:: attach(filename, content, mimetype)\n+                attach(mimepart)\n \n-        Support for :class:`email.mime.base.MIMEBase` attachments is\n-        deprecated. Use :class:`~email.message.MIMEPart` instead.\n+        Creates a new attachment and adds it to the message. There are two ways\n+        to call :meth:`!attach`:\n \n-* ``attach_file()`` creates a new attachment using a file from your\n-  filesystem. Call it with the path of the file to attach and, optionally,\n-  the MIME type to use for the attachment. If the MIME type is omitted, it\n-  will be guessed from the filename. You can use it like this::\n+        * You can pass it three arguments: ``filename``, ``content`` and\n+          ``mimetype``. ``filename`` is the name of the file attachment as it\n+          will appear in the email, ``content`` is the data that will be\n+          contained inside the attachment and ``mimetype`` is the optional MIME\n+          type for the attachment. If you omit ``mimetype``, the MIME content\n+          type will be guessed from the filename of the attachment.\n \n-    message.attach_file(\"/images/weather_map.png\")\n+          For example::\n \n-  For MIME types starting with :mimetype:`text/`, binary data is handled as in\n-  ``attach()``.\n+              message.attach(\"design.png\", img_data, \"image/png\")\n+\n+          If you specify a ``mimetype`` of :mimetype:`message/rfc822`,\n+          ``content`` can be a :class:`django.core.mail.EmailMessage` or\n+          Python's :class:`email.message.EmailMessage` or\n+          :class:`email.message.Message`.\n+\n+          For a ``mimetype`` starting with :mimetype:`text/`, content is\n+          expected to be a string. Binary data will be decoded using UTF-8,\n+          and if that fails, the MIME type will be changed to\n+          :mimetype:`application/octet-stream` and the data will be attached\n+          unchanged.\n+\n+        * Or for attachments requiring additional headers or parameters, you\n+          can pass :meth:`!attach` a single Python",
    "comment": "pre-existing issue, but this should probably be \"attach\", not \"pass attach\"? not a blocker here, happy to take a follow-up pr.",
    "line_number": 480,
    "enriched": "File: docs/topics/email.txt\nCode: @@ -323,185 +323,201 @@ email backend API :ref:`provides an alternative\n \n .. class:: EmailMessage\n \n-The :class:`~django.core.mail.EmailMessage` class is initialized with the\n-following parameters. All parameters are optional and can be set at any time\n-prior to calling the ``send()`` method.\n+    The :class:`!EmailMessage` class is initialized with the following\n+    parameters. All parameters are optional and can be set at any time prior\n+    to calling the :meth:`send` method.\n \n-The first four parameters can be passed as positional or keyword arguments,\n-but must be in the given order if positional arguments are used:\n+    The first four parameters can be passed as positional or keyword arguments,\n+    but must be in the given order if positional arguments are used:\n \n-* ``subject``: The subject line of the email.\n+    * ``subject``: The subject line of the email.\n \n-* ``body``: The body text. This should be a plain text message.\n+    * ``body``: The body text. This should be a plain text message.\n \n-* ``from_email``: The sender's address. Both ``fred@example.com`` and\n-  ``\"Fred\" <fred@example.com>`` forms are legal. If omitted, the\n-  :setting:`DEFAULT_FROM_EMAIL` setting is used.\n+    * ``from_email``: The sender's address. Both ``fred@example.com`` and\n+      ``\"Fred\" <fred@example.com>`` forms are legal. If omitted, the\n+      :setting:`DEFAULT_FROM_EMAIL` setting is used.\n \n-* ``to``: A list or tuple of recipient addresses.\n+    * ``to``: A list or tuple of recipient addresses.\n \n-The following parameters must be given as keyword arguments if used:\n+    The following parameters must be given as keyword arguments if used:\n \n-* ``cc``: A list or tuple of recipient addresses used in the \"Cc\" header\n-  when sending the email.\n+    * ``cc``: A list or tuple of recipient addresses used in the \"Cc\" header\n+      when sending the email.\n \n-* ``bcc``: A list or tuple of addresses used in the \"Bcc\" header when\n-  sending the email.\n+    * ``bcc``: A list or tuple of addresses used in the \"Bcc\" header when\n+      sending the email.\n \n-* ``reply_to``: A list or tuple of recipient addresses used in the \"Reply-To\"\n-  header when sending the email.\n+    * ``reply_to``: A list or tuple of recipient addresses used in the\n+      \"Reply-To\" header when sending the email.\n \n-* ``attachments``: A list of attachments to put on the message. Each can\n-  be an instance of :class:`~email.message.MIMEPart` or\n-  :class:`~django.core.mail.EmailAttachment`, or a tuple with attributes\n-  ``(filename, content, mimetype)``.\n+    * ``attachments``: A list of attachments to put on the message. Each can\n+      be an instance of :class:`~email.message.MIMEPart` or\n+      :class:`EmailAttachment`, or a tuple with attributes\n+      ``(filename, content, mimetype)``.\n \n-  .. versionchanged:: 5.2\n+      .. versionchanged:: 5.2\n \n-    Support for :class:`~django.core.mail.EmailAttachment` items of\n-    ``attachments`` was added.\n+          Support for :class:`EmailAttachment` items of ``attachments`` was\n+          added.\n \n-  .. versionchanged:: 6.0\n+      .. versionchanged:: 6.0\n \n-    Support for :class:`~email.message.MIMEPart` objects in the ``attachments``\n-    list was added.\n+          Support for :class:`~email.message.MIMEPart` objects in the\n+          ``attachments`` list was added.\n \n-  .. deprecated:: 6.0\n+      .. deprecated:: 6.0\n \n-    Support for Python's legacy :class:`~email.mime.base.MIMEBase` objects in\n-    ``attachments`` is deprecated. Use :class:`~email.message.MIMEPart`\n-    instead.\n+          Support for Python's legacy :class:`~email.mime.base.MIMEBase`\n+          objects in ``attachments`` is deprecated. Use\n+          :class:`~email.message.MIMEPart` instead.\n \n-* ``headers``: A dictionary of extra headers to put on the message. The\n-  keys are the header name, values are the header values. It's up to the\n-  caller to ensure header names and values are in the correct format for\n-  an email message. The corresponding attribute is ``extra_headers``.\n+    * ``headers``: A dictionary of extra headers to put on the message. The\n+      keys are the header name, values are the header values. It's up to the\n+      caller to ensure header names and values are in the correct format for\n+      an email message. The corresponding attribute is ``extra_headers``.\n \n-* ``connection``: An :ref:`email backend <topic-email-backends>` instance. Use\n-  this parameter if you are sending the ``EmailMessage`` via ``send()`` and you\n-  want to use the same connection for multiple messages. If omitted, a new\n-  connection is created when ``send()`` is called. This parameter is ignored\n-  when using :ref:`send_messages() <topics-sending-multiple-emails>`.\n+    * ``connection``: An :ref:`email backend <topic-email-backends>` instance.\n+      Use this parameter if you are sending the :class:`!EmailMessage` via\n+      :meth:`send` and you want to use the same connection for multiple\n+      messages. If omitted, a new connection is created when :meth:`send` is\n+      called. This parameter is ignored when using\n+      :ref:`send_messages() <topics-sending-multiple-emails>`.\n \n-.. deprecated:: 6.0\n+    .. deprecated:: 6.0\n \n-    Passing all except the first four parameters as positional arguments is\n-    deprecated.\n+        Passing all except the first four parameters as positional arguments is\n+        deprecated.\n \n-For example::\n+    For example::\n \n-    from django.core.mail import EmailMessage\n+        from django.core.mail import EmailMessage\n \n-    email = EmailMessage(\n-        subject=\"Hello\",\n-        body=\"Body goes here\",\n-        from_email=\"from@example.com\",\n-        to=[\"to1@example.com\", \"to2@example.com\"],\n-        bcc=[\"bcc@example.com\"],\n-        reply_to=[\"another@example.com\"],\n-        headers={\"Message-ID\": \"foo\"},\n-    )\n+        email = EmailMessage(\n+            subject=\"Hello\",\n+            body=\"Body goes here\",\n+            from_email=\"from@example.com\",\n+            to=[\"to1@example.com\", \"to2@example.com\"],\n+            bcc=[\"bcc@example.com\"],\n+            reply_to=[\"another@example.com\"],\n+            headers={\"Message-ID\": \"foo\"},\n+        )\n \n-The class has the following methods:\n-\n-* ``send(fail_silently=False)`` sends the message. If a connection was\n-  specified when the email was constructed, that connection will be used.\n-  Otherwise, an instance of the default backend will be instantiated and\n-  used. If the keyword argument ``fail_silently`` is ``True``, exceptions\n-  raised while sending the message will be quashed. An empty list of\n-  recipients will not raise an exception. It will return ``1`` if the message\n-  was sent successfully, otherwise ``0``.\n-\n-* ``message(policy=email.policy.default)`` constructs and returns a Python\n-  :class:`email.message.EmailMessage` object representing the message to be\n-  sent.\n-\n-  The keyword argument ``policy`` allows specifying the set of rules for\n-  updating and serializing the representation of the message. It must be an\n-  :mod:`email.policy.Policy <email.policy>` object. Defaults to\n-  :data:`email.policy.default`. In certain cases you may want to use\n-  :data:`~email.policy.SMTP`, :data:`~email.policy.SMTPUTF8` or a custom\n-  policy. For example, :class:`django.core.mail.backends.smtp.EmailBackend`\n-  uses the :data:`~email.policy.SMTP` policy to ensure ``\\r\\n`` line endings\n-  as required by the SMTP protocol.\n-\n-  If you ever need to extend Django's :class:`~django.core.mail.EmailMessage`\n-  class, you'll probably want to override this method to put the content you\n-  want into the Python EmailMessage object.\n-\n-  .. versionchanged:: 6.0\n-\n-      The ``policy`` keyword argument was added and the return type was updated\n-      to an instance of :class:`~email.message.EmailMessage`.\n-\n-* ``recipients()`` returns a list of all the recipients of the message,\n-  whether they're recorded in the ``to``, ``cc`` or ``bcc`` attributes. This\n-  is another method you might need to override when subclassing, because the\n-  SMTP server needs to be told the full list of recipients when the message\n-  is sent. If you add another way to specify recipients in your class, they\n-  need to be returned from this method as well.\n-\n-* ``attach()`` creates a new attachment and adds it to the message.\n-  There are two ways to call ``attach()``:\n-\n-  * You can pass it three arguments: ``filename``, ``content`` and\n-    ``mimetype``. ``filename`` is the name of the file attachment as it will\n-    appear in the email, ``content`` is the data that will be contained inside\n-    the attachment and ``mimetype`` is the optional MIME type for the\n-    attachment. If you omit ``mimetype``, the MIME content type will be guessed\n-    from the filename of the attachment.\n+    The class has the following methods:\n \n-    For example::\n+    .. method:: send(fail_silently=False)\n \n-       message.attach(\"design.png\", img_data, \"image/png\")\n-\n-    If you specify a ``mimetype`` of :mimetype:`message/rfc822`, ``content``\n-    can be a :class:`django.core.mail.EmailMessage` or Python's\n-    :class:`email.message.EmailMessage` or :class:`email.message.Message`.\n-\n-    For a ``mimetype`` starting with :mimetype:`text/`, content is expected to\n-    be a string. Binary data will be decoded using UTF-8, and if that fails,\n-    the MIME type will be changed to :mimetype:`application/octet-stream` and\n-    the data will be attached unchanged.\n-\n-  * Or for attachments requiring additional headers or parameters, you can pass\n-    ``attach()`` a single Python :class:`~email.message.MIMEPart` object.\n-    This will be attached directly to the resulting message. For example,\n-    to attach an inline image with a :mailheader:`Content-ID`::\n-\n-        cid = email.utils.make_msgid()\n-        inline_image = email.message.MIMEPart()\n-        inline_image.set_content(\n-            image_data_bytes,\n-            maintype=\"image\",\n-            subtype=\"png\",\n-            disposition=\"inline\",\n-            cid=f\"<{cid}>\",\n-        )\n-        message.attach(inline_image)\n-        message.attach_alternative(f'\u2026 <img src=\"cid:${cid}\"> \u2026', \"text/html\")\n+        Sends the message. If a connection was specified when the email was\n+        constructed, that connection will be used. Otherwise, an instance of\n+        the default backend will be instantiated and used. If the keyword\n+        argument ``fail_silently`` is ``True``, exceptions raised while sending\n+        the message will be quashed. An empty list of recipients will not raise\n+        an exception. It will return ``1`` if the message was sent\n+        successfully, otherwise ``0``.\n \n-    Python's :meth:`email.contentmanager.set_content` documentation describes\n-    the supported arguments for ``MIMEPart.set_content()``.\n+    .. method:: message(policy=email.policy.default)\n \n-    .. versionchanged:: 6.0\n+        Constructs and returns a Python :class:`email.message.EmailMessage`\n+        object representing the message to be sent.\n \n-        Support for :class:`~email.message.MIMEPart` attachments was added.\n+        The keyword argument ``policy`` allows specifying the set of rules for\n+        updating and serializing the representation of the message. It must be\n+        an :mod:`email.policy.Policy <email.policy>` object. Defaults to\n+        :data:`email.policy.default`. In certain cases you may want to use\n+        :data:`~email.policy.SMTP`, :data:`~email.policy.SMTPUTF8` or a custom\n+        policy. For example,\n+        :class:`django.core.mail.backends.smtp.EmailBackend` uses the\n+        :data:`~email.policy.SMTP` policy to ensure ``\\r\\n`` line endings as\n+        required by the SMTP protocol.\n \n-    .. deprecated:: 6.0\n+        If you ever need to extend Django's :class:`EmailMessage` class,\n+        you'll probably want to override this method to put the content you\n+        want into the Python EmailMessage object.\n+\n+        .. versionchanged:: 6.0\n+\n+            The ``policy`` keyword argument was added and the return type was\n+            updated to an instance of :class:`~email.message.EmailMessage`.\n+\n+    .. method:: recipients()\n+\n+        Returns a list of all the recipients of the message, whether they're\n+        recorded in the ``to``, ``cc`` or ``bcc`` attributes. This is another\n+        method you might need to override when subclassing, because the SMTP\n+        server needs to be told the full list of recipients when the message\n+        is sent. If you add another way to specify recipients in your class,\n+        they need to be returned from this method as well.\n+\n+    .. method:: attach(filename, content, mimetype)\n+                attach(mimepart)\n \n-        Support for :class:`email.mime.base.MIMEBase` attachments is\n-        deprecated. Use :class:`~email.message.MIMEPart` instead.\n+        Creates a new attachment and adds it to the message. There are two ways\n+        to call :meth:`!attach`:\n \n-* ``attach_file()`` creates a new attachment using a file from your\n-  filesystem. Call it with the path of the file to attach and, optionally,\n-  the MIME type to use for the attachment. If the MIME type is omitted, it\n-  will be guessed from the filename. You can use it like this::\n+        * You can pass it three arguments: ``filename``, ``content`` and\n+          ``mimetype``. ``filename`` is the name of the file attachment as it\n+          will appear in the email, ``content`` is the data that will be\n+          contained inside the attachment and ``mimetype`` is the optional MIME\n+          type for the attachment. If you omit ``mimetype``, the MIME content\n+          type will be guessed from the filename of the attachment.\n \n-    message.attach_file(\"/images/weather_map.png\")\n+          For example::\n \n-  For MIME types starting with :mimetype:`text/`, binary data is handled as in\n-  ``attach()``.\n+              message.attach(\"design.png\", img_data, \"image/png\")\n+\n+          If you specify a ``mimetype`` of :mimetype:`message/rfc822`,\n+          ``content`` can be a :class:`django.core.mail.EmailMessage` or\n+          Python's :class:`email.message.EmailMessage` or\n+          :class:`email.message.Message`.\n+\n+          For a ``mimetype`` starting with :mimetype:`text/`, content is\n+          expected to be a string. Binary data will be decoded using UTF-8,\n+          and if that fails, the MIME type will be changed to\n+          :mimetype:`application/octet-stream` and the data will be attached\n+          unchanged.\n+\n+        * Or for attachments requiring additional headers or parameters, you\n+          can pass :meth:`!attach` a single Python\nComment: Pre-existing issue, but this should probably be \"attach\", not \"pass attach\"? Not a blocker here, happy to take a follow-up PR.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/topics/email.txt",
    "pr_number": 19810,
    "repo": "django",
    "owner": "django",
    "comment_id": 2325910202,
    "comment_created_at": "2025-09-05T19:44:40Z"
  },
  {
    "code": "@@ -268,6 +268,13 @@ def trigger_reload(filename):\n \n def restart_with_reloader():\n     new_environ = {**os.environ, DJANGO_AUTORELOAD_ENV: \"true\"}\n+    orig = getattr(sys, \"orig_argv\", ())\n+    if any(\n+        (arg == \"-u\")\n+        or (arg.startswith(\"-\") and not arg.startswith((\"--\", \"-X\")) and \"u\" in arg[1:])",
    "comment": "\r\ni can remove this line without any test failures, can you add a test to cover this case?",
    "line_number": 274,
    "enriched": "File: django/utils/autoreload.py\nCode: @@ -268,6 +268,13 @@ def trigger_reload(filename):\n \n def restart_with_reloader():\n     new_environ = {**os.environ, DJANGO_AUTORELOAD_ENV: \"true\"}\n+    orig = getattr(sys, \"orig_argv\", ())\n+    if any(\n+        (arg == \"-u\")\n+        or (arg.startswith(\"-\") and not arg.startswith((\"--\", \"-X\")) and \"u\" in arg[1:])\nComment: ```suggestion\r\n```\r\nI can remove this line without any test failures, can you add a test to cover this case?",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "django/utils/autoreload.py",
    "pr_number": 19802,
    "repo": "django",
    "owner": "django",
    "comment_id": 2372081213,
    "comment_created_at": "2025-09-23T12:02:31Z"
  },
  {
    "code": "@@ -234,18 +234,32 @@ def test_name_with_import_error(self, modules_tmp_path):\n \n \n class TestStreaming:\n-    def test_streaming_with_context(self, app, client):",
    "comment": "why did you replace this test? it's still relevant as well.",
    "line_number": 237,
    "enriched": "File: tests/test_helpers.py\nCode: @@ -234,18 +234,32 @@ def test_name_with_import_error(self, modules_tmp_path):\n \n \n class TestStreaming:\n-    def test_streaming_with_context(self, app, client):\nComment: Why did you replace this test? It's still relevant as well.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/test_helpers.py",
    "pr_number": 5775,
    "repo": "flask",
    "owner": "pallets",
    "comment_id": 2210469867,
    "comment_created_at": "2025-07-16T13:37:47Z"
  },
  {
    "code": "@@ -9,6 +9,30 @@ will depend on each application's specific needs and threat model. Many hosting\n platforms may take care of certain types of problems without the need for the\n Flask application to handle them.\n \n+Host Header Injection and External URLs\n+---------------------------------------\n+\n+When generating external URLs using :func:`url_for` with the ``_external=True`` argument,\n+Flask constructs the URL using the requeust's ``Host`` header by default. If your application",
    "comment": "it always uses the host header, not only by default. that's why the config needs to be set.",
    "line_number": 16,
    "enriched": "File: docs/web-security.rst\nCode: @@ -9,6 +9,30 @@ will depend on each application's specific needs and threat model. Many hosting\n platforms may take care of certain types of problems without the need for the\n Flask application to handle them.\n \n+Host Header Injection and External URLs\n+---------------------------------------\n+\n+When generating external URLs using :func:`url_for` with the ``_external=True`` argument,\n+Flask constructs the URL using the requeust's ``Host`` header by default. If your application\nComment: It always uses the `Host` header, not only by default. That's why the config needs to be set.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "docs/web-security.rst",
    "pr_number": 5722,
    "repo": "flask",
    "owner": "pallets",
    "comment_id": 2283123541,
    "comment_created_at": "2025-08-18T18:18:14Z"
  },
  {
    "code": "@@ -71,14 +71,7 @@ export PYTORCH_BUILD_NUMBER=1\n \n # Set triton version as part of PYTORCH_EXTRA_INSTALL_REQUIREMENTS\n TRITON_VERSION=$(cat $PYTORCH_ROOT/.ci/docker/triton_version.txt)\n-\n-# Here PYTORCH_EXTRA_INSTALL_REQUIREMENTS is already set for the all the wheel builds hence append TRITON_CONSTRAINT\n-TRITON_CONSTRAINT=\"platform_system == 'Linux' and platform_machine == 'x86_64'\"\n-\n-# CUDA 12.9/13.0 builds have triton for Linux and Linux aarch64 binaries.\n-if [[ \"$DESIRED_CUDA\" == \"cu129\" ]] || [[ \"$DESIRED_CUDA\" == \"cu130\" ]]; then\n-  TRITON_CONSTRAINT=\"platform_system == 'Linux'\"\n-fi\n+TRITON_CONSTRAINT=\"platform_system == 'Linux'\"",
    "comment": "wouldn't that add triton constrain on cpu builds as well?",
    "line_number": 74,
    "enriched": "File: .circleci/scripts/binary_populate_env.sh\nCode: @@ -71,14 +71,7 @@ export PYTORCH_BUILD_NUMBER=1\n \n # Set triton version as part of PYTORCH_EXTRA_INSTALL_REQUIREMENTS\n TRITON_VERSION=$(cat $PYTORCH_ROOT/.ci/docker/triton_version.txt)\n-\n-# Here PYTORCH_EXTRA_INSTALL_REQUIREMENTS is already set for the all the wheel builds hence append TRITON_CONSTRAINT\n-TRITON_CONSTRAINT=\"platform_system == 'Linux' and platform_machine == 'x86_64'\"\n-\n-# CUDA 12.9/13.0 builds have triton for Linux and Linux aarch64 binaries.\n-if [[ \"$DESIRED_CUDA\" == \"cu129\" ]] || [[ \"$DESIRED_CUDA\" == \"cu130\" ]]; then\n-  TRITON_CONSTRAINT=\"platform_system == 'Linux'\"\n-fi\n+TRITON_CONSTRAINT=\"platform_system == 'Linux'\"\nComment: Wouldn't that add triton constrain on CPU builds as well?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".circleci/scripts/binary_populate_env.sh",
    "pr_number": 165013,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2415253484,
    "comment_created_at": "2025-10-08T23:52:43Z"
  },
  {
    "code": "@@ -281,11 +283,17 @@ def _get_slice_mesh_layout(self, device_mesh, mesh_dim_names) -> _MeshLayout:\n \n             layout_sliced = []\n             for name in mesh_dim_names:\n-                if name in device_mesh.mesh_dim_names:\n+                if name in not_none(device_mesh.mesh_dim_names):\n                     layout_sliced.append(\n-                        device_mesh._layout[device_mesh.mesh_dim_names.index(name)]\n+                        device_mesh._layout[\n+                            not_none(device_mesh.mesh_dim_names).index(name)\n+                        ]\n                     )\n                 elif name in flatten_name_to_root_layout:\n+                    warnings.warn(\n+                        \"Slicing a flattened dim from root mesh will be deprecated soon. \"\n+                        \"Users need to use and bookkeep the flattened mesh directly. \"",
    "comment": "the to use and bookkeep seems to be incorrect. are you trying to say users need to bookkeedp the flattened mesh directly.\"?",
    "line_number": 295,
    "enriched": "File: torch/distributed/device_mesh.py\nCode: @@ -281,11 +283,17 @@ def _get_slice_mesh_layout(self, device_mesh, mesh_dim_names) -> _MeshLayout:\n \n             layout_sliced = []\n             for name in mesh_dim_names:\n-                if name in device_mesh.mesh_dim_names:\n+                if name in not_none(device_mesh.mesh_dim_names):\n                     layout_sliced.append(\n-                        device_mesh._layout[device_mesh.mesh_dim_names.index(name)]\n+                        device_mesh._layout[\n+                            not_none(device_mesh.mesh_dim_names).index(name)\n+                        ]\n                     )\n                 elif name in flatten_name_to_root_layout:\n+                    warnings.warn(\n+                        \"Slicing a flattened dim from root mesh will be deprecated soon. \"\n+                        \"Users need to use and bookkeep the flattened mesh directly. \"\nComment: The `to use and bookkeep` seems to be incorrect. Are you trying to say `Users need to bookkeedp the flattened mesh directly.\"?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/distributed/device_mesh.py",
    "pr_number": 164993,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2415074977,
    "comment_created_at": "2025-10-08T21:28:08Z"
  },
  {
    "code": "@@ -4091,6 +4091,10 @@ such as `dist.all_reduce(tensor, async_op=True)`.\n             Stringified pickle work traces.\n             Default settings return everything - i.e. contains NCCL comm dumps and collective traces.\n       )\");\n+  module.def(\n+      \"_reset_fr_recording_nccl\",",
    "comment": "do we want to prefix the public api with _?",
    "line_number": 4095,
    "enriched": "File: torch/csrc/distributed/c10d/init.cpp\nCode: @@ -4091,6 +4091,10 @@ such as `dist.all_reduce(tensor, async_op=True)`.\n             Stringified pickle work traces.\n             Default settings return everything - i.e. contains NCCL comm dumps and collective traces.\n       )\");\n+  module.def(\n+      \"_reset_fr_recording_nccl\",\nComment: do we want to prefix the public api with _?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/csrc/distributed/c10d/init.cpp",
    "pr_number": 164988,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2414990929,
    "comment_created_at": "2025-10-08T20:42:33Z"
  },
  {
    "code": "@@ -290,7 +290,7 @@ def aot_stage2_inference(\n                 \"name\": \"torch._functorch.config\",\n                 \"encoding\": \"string\",\n             },\n-            payload_fn=lambda: torch._functorch.config.get_config_copy(),\n+            payload_fn=lambda: torch._functorch.config.get_serializable_config_copy(),",
    "comment": "what happened here?",
    "line_number": 293,
    "enriched": "File: torch/_functorch/_aot_autograd/graph_compile.py\nCode: @@ -290,7 +290,7 @@ def aot_stage2_inference(\n                 \"name\": \"torch._functorch.config\",\n                 \"encoding\": \"string\",\n             },\n-            payload_fn=lambda: torch._functorch.config.get_config_copy(),\n+            payload_fn=lambda: torch._functorch.config.get_serializable_config_copy(),\nComment: what happened here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/_functorch/_aot_autograd/graph_compile.py",
    "pr_number": 164981,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2414828827,
    "comment_created_at": "2025-10-08T19:31:58Z"
  },
  {
    "code": "@@ -121,7 +121,7 @@ def _snake_case(s: str) -> str:\n \n # starts with torch but does not start with torch._dynamo. or torch._inductor.\n _torch_but_not_dynamo = re.compile(\n-    r\"^torch(?:\\.(?!_dynamo\\.|_inductor\\.)[^.]+)*$\"\n+    r\"^torch(?:\\.(?!_dynamo\\.|_inductor\\.|_functorch\\.)[^.]+)*$\"",
    "comment": "what's the motivation?\r\ni thought for torch._dynamo /torch._inductor things, those shouldn't show up in the fx graph.\r\nit's actually desirable that torch._functorch things do show up in the fx graph.\r\n\r\nif you have a good enough motivation then feel free to ship it, i think the difference is just cosmetic (@guilhermeleobas can also confirm). it's just going to take you a while to get through all of the expecttests :p",
    "line_number": 124,
    "enriched": "File: torch/fx/graph.py\nCode: @@ -121,7 +121,7 @@ def _snake_case(s: str) -> str:\n \n # starts with torch but does not start with torch._dynamo. or torch._inductor.\n _torch_but_not_dynamo = re.compile(\n-    r\"^torch(?:\\.(?!_dynamo\\.|_inductor\\.)[^.]+)*$\"\n+    r\"^torch(?:\\.(?!_dynamo\\.|_inductor\\.|_functorch\\.)[^.]+)*$\"\nComment: What's the motivation?\r\nI thought for torch._dynamo /torch._inductor things, those shouldn't show up in the FX graph.\r\nIt's actually desirable that torch._functorch things do show up in the FX graph.\r\n\r\nIf you have a good enough motivation then feel free to ship it, I think the difference is just cosmetic (@guilhermeleobas can also confirm). It's just going to take you a while to get through all of the expecttests :P",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/fx/graph.py",
    "pr_number": 164900,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2414805928,
    "comment_created_at": "2025-10-08T19:20:54Z"
  },
  {
    "code": "@@ -604,6 +604,284 @@ def apply_same_precision(partial_model):\n \n         torch.distributed.destroy_process_group()\n \n+    @requires_accelerator_dist_backend([\"nccl\", \"xccl\"])\n+    @skip_if_lt_x_gpu(8)",
    "comment": "this test works with just 4 gpus",
    "line_number": 608,
    "enriched": "File: test/distributed/_composable/test_composability/test_pp_composability.py\nCode: @@ -604,6 +604,284 @@ def apply_same_precision(partial_model):\n \n         torch.distributed.destroy_process_group()\n \n+    @requires_accelerator_dist_backend([\"nccl\", \"xccl\"])\n+    @skip_if_lt_x_gpu(8)\nComment: this test works with just 4 GPUs",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "test/distributed/_composable/test_composability/test_pp_composability.py",
    "pr_number": 164890,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2414027431,
    "comment_created_at": "2025-10-08T14:17:34Z"
  },
  {
    "code": "@@ -370,11 +370,19 @@ def _load_global_deps() -> None:\n             \"cusparselt\": \"libcusparseLt.so.*[0-9]\",\n             \"cusolver\": \"libcusolver.so.*[0-9]\",\n             \"nccl\": \"libnccl.so.*[0-9]\",\n-            \"nvtx\": \"libnvToolsExt.so.*[0-9]\",",
    "comment": "hmm, do we have any binary that links against libnvtoolsext? i.e. may be easier solution is just to delete this line?",
    "line_number": 373,
    "enriched": "File: torch/__init__.py\nCode: @@ -370,11 +370,19 @@ def _load_global_deps() -> None:\n             \"cusparselt\": \"libcusparseLt.so.*[0-9]\",\n             \"cusolver\": \"libcusolver.so.*[0-9]\",\n             \"nccl\": \"libnccl.so.*[0-9]\",\n-            \"nvtx\": \"libnvToolsExt.so.*[0-9]\",\nComment: Hmm, do we have any binary that links against libnvToolsExt? I.e. may be easier solution is just to delete this line?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/__init__.py",
    "pr_number": 164870,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2411942967,
    "comment_created_at": "2025-10-07T21:12:21Z"
  },
  {
    "code": "@@ -1 +1 @@\n-e0dda9059d082537cee36be6c5e4fe3b18c880c0\n+deb42f2a8e48f5032b4a98ee781a15fa87a157cf",
    "comment": "intentional?",
    "line_number": 1,
    "enriched": "File: .ci/docker/ci_commit_pins/executorch.txt\nCode: @@ -1 +1 @@\n-e0dda9059d082537cee36be6c5e4fe3b18c880c0\n+deb42f2a8e48f5032b4a98ee781a15fa87a157cf\nComment: intentional?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".ci/docker/ci_commit_pins/executorch.txt",
    "pr_number": 164846,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2414074901,
    "comment_created_at": "2025-10-08T14:32:54Z"
  },
  {
    "code": "@@ -834,7 +834,7 @@ test_dynamo_benchmark() {\n     # TODO (huydhn): Just smoke test some sample models\n     if [[ \"${TEST_CONFIG}\" == *b200* ]]; then\n       if [[ \"${suite}\" == \"huggingface\" ]]; then\n-        export TORCHBENCH_ONLY_MODELS=\"DistillGPT2\"\n+        export TORCHBENCH_ONLY_MODELS=\"AlbertForMaskedLM\"",
    "comment": "distillgpt2 is deleted so run on albertformaskedlm instead.",
    "line_number": 837,
    "enriched": "File: .ci/pytorch/test.sh\nCode: @@ -834,7 +834,7 @@ test_dynamo_benchmark() {\n     # TODO (huydhn): Just smoke test some sample models\n     if [[ \"${TEST_CONFIG}\" == *b200* ]]; then\n       if [[ \"${suite}\" == \"huggingface\" ]]; then\n-        export TORCHBENCH_ONLY_MODELS=\"DistillGPT2\"\n+        export TORCHBENCH_ONLY_MODELS=\"AlbertForMaskedLM\"\nComment: `DistillGPT2` is deleted so run on `AlbertForMaskedLM` instead.",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": ".ci/pytorch/test.sh",
    "pr_number": 164815,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2409262833,
    "comment_created_at": "2025-10-07T03:45:30Z"
  },
  {
    "code": "@@ -2878,49 +2878,18 @@ def adapt_config_for_tiling(\n     )\n \n \n-class ReductionConfigKey:\n-    \"\"\"\n-    The part of reduction configs that affect determinism.\n-    \"\"\"\n-\n-    def __init__(self, config: Config):\n-        # persistent reduction does not have a RBLOCK, use -1 as a flag\n-        self.r0_block = config.kwargs.get(\"R0_BLOCK\", -1)\n-        self.r1_block = config.kwargs.get(\"R1_BLOCK\", -1)\n-        self.num_warps = config.num_warps\n-        self.num_ctas = config.num_ctas\n-\n-    def __hash__(self) -> int:\n-        return hash((self.r0_block, self.r1_block, self.num_warps, self.num_ctas))\n-\n-    def __eq__(self, other: object) -> bool:\n-        return (\n-            isinstance(other, ReductionConfigKey)\n-            and self.r0_block == other.r0_block\n-            and self.r1_block == other.r1_block\n-            and self.num_warps == other.num_warps\n-            and self.num_ctas == other.num_ctas\n-        )\n-\n-\n def filter_reduction_configs_for_determinism(\n     inductor_meta: dict[str, Any], configs: list[Config]\n ) -> list[Config]:\n     \"\"\"\n     Filter configs for reduction so the numerics can be deterministic.\n \n-    This function group configs by fields that affect determinism\n-    - rblock size\n-    - num warps\n-    - num ctas\n-    and return the most promising group based on heuristics.\n-\n     Heuristics:\n     - skip reduction configs with too small RBLOCK\n     - skip reduction configs with XBLOCK==1 if we are confident it will not perform well\n-    - pick the group with largest size: autotuning more configs may have more chance to give better perf\n-    - if there is a tie, pick the group with second largest RBLOCK\n-    - if there is still a tie, pick the group with second largest num_warps\n+    - if there is a tie, pick the config with second largest RBLOCK\n+    - if there is still a tie, pick the config with second largest num_warps",
    "comment": "why do we pick second largest in all of these cases? this for my understanding, everything else lgtm",
    "line_number": 2891,
    "enriched": "File: torch/_inductor/runtime/triton_heuristics.py\nCode: @@ -2878,49 +2878,18 @@ def adapt_config_for_tiling(\n     )\n \n \n-class ReductionConfigKey:\n-    \"\"\"\n-    The part of reduction configs that affect determinism.\n-    \"\"\"\n-\n-    def __init__(self, config: Config):\n-        # persistent reduction does not have a RBLOCK, use -1 as a flag\n-        self.r0_block = config.kwargs.get(\"R0_BLOCK\", -1)\n-        self.r1_block = config.kwargs.get(\"R1_BLOCK\", -1)\n-        self.num_warps = config.num_warps\n-        self.num_ctas = config.num_ctas\n-\n-    def __hash__(self) -> int:\n-        return hash((self.r0_block, self.r1_block, self.num_warps, self.num_ctas))\n-\n-    def __eq__(self, other: object) -> bool:\n-        return (\n-            isinstance(other, ReductionConfigKey)\n-            and self.r0_block == other.r0_block\n-            and self.r1_block == other.r1_block\n-            and self.num_warps == other.num_warps\n-            and self.num_ctas == other.num_ctas\n-        )\n-\n-\n def filter_reduction_configs_for_determinism(\n     inductor_meta: dict[str, Any], configs: list[Config]\n ) -> list[Config]:\n     \"\"\"\n     Filter configs for reduction so the numerics can be deterministic.\n \n-    This function group configs by fields that affect determinism\n-    - rblock size\n-    - num warps\n-    - num ctas\n-    and return the most promising group based on heuristics.\n-\n     Heuristics:\n     - skip reduction configs with too small RBLOCK\n     - skip reduction configs with XBLOCK==1 if we are confident it will not perform well\n-    - pick the group with largest size: autotuning more configs may have more chance to give better perf\n-    - if there is a tie, pick the group with second largest RBLOCK\n-    - if there is still a tie, pick the group with second largest num_warps\n+    - if there is a tie, pick the config with second largest RBLOCK\n+    - if there is still a tie, pick the config with second largest num_warps\nComment: Why do we pick second largest in all of these cases? this for my understanding, everything else lgtm",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/_inductor/runtime/triton_heuristics.py",
    "pr_number": 164801,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2409631648,
    "comment_created_at": "2025-10-07T07:15:47Z"
  },
  {
    "code": "@@ -459,6 +449,26 @@ def _is_backward_node_with_seq_nr(node):\n             node.meta[\"custom\"] = fwd_node.meta.get(\"custom\")\n \n \n+def copy_fwd_metadata_to_bw_nodes(fx_g):\n+    \"\"\"\n+    Input: `fx_g` which contains the joint fwd+bwd FX graph created by\n+    aot_autograd.\n+\n+    This function walks the graph and copies over metadata from forward nodes\n+    to backward nodes, using the `seq_nr` field as a one-to-many mapping\n+    from forward node to backward node. This metadata is useful for performance\n+    profiling and debugging.\n+    \"\"\"\n+\n+    # Copy the metadata recursively - useful for HOPs\n+    for node in fx_g.graph.nodes:\n+        if node.op == \"get_attr\":\n+            submod = getattr(fx_g, node.target)\n+            if isinstance(submod, torch.fx.GraphModule):\n+                copy_fwd_metadata_to_bw_nodes(submod)",
    "comment": "do we know for sure that the backward nodes and the forward nodes are always in the same subgraph? if so this would work. in the current implementation, the bw nodes would not be able to find the fw nodes in other subgraphs.",
    "line_number": 468,
    "enriched": "File: torch/_functorch/_aot_autograd/utils.py\nCode: @@ -459,6 +449,26 @@ def _is_backward_node_with_seq_nr(node):\n             node.meta[\"custom\"] = fwd_node.meta.get(\"custom\")\n \n \n+def copy_fwd_metadata_to_bw_nodes(fx_g):\n+    \"\"\"\n+    Input: `fx_g` which contains the joint fwd+bwd FX graph created by\n+    aot_autograd.\n+\n+    This function walks the graph and copies over metadata from forward nodes\n+    to backward nodes, using the `seq_nr` field as a one-to-many mapping\n+    from forward node to backward node. This metadata is useful for performance\n+    profiling and debugging.\n+    \"\"\"\n+\n+    # Copy the metadata recursively - useful for HOPs\n+    for node in fx_g.graph.nodes:\n+        if node.op == \"get_attr\":\n+            submod = getattr(fx_g, node.target)\n+            if isinstance(submod, torch.fx.GraphModule):\n+                copy_fwd_metadata_to_bw_nodes(submod)\nComment: do we know for sure that the backward nodes and the forward nodes are always in the same subgraph? If so this would work. In the current implementation, the bw nodes would not be able to find the fw nodes in other subgraphs.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/_functorch/_aot_autograd/utils.py",
    "pr_number": 164795,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2408969593,
    "comment_created_at": "2025-10-06T23:34:33Z"
  },
  {
    "code": "@@ -1,271 +1,221 @@\n+import itertools\n import logging\n-import operator\n-from typing import Any, Callable\n+from collections import defaultdict\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Union\n \n+import torch\n import torch.fx as fx\n-from torch._functorch.partitioners import _size_of, get_default_op_list\n+from torch.fx.experimental.symbolic_shapes import hint_int\n from torch.utils._ordered_set import OrderedSet\n+from torch.utils._pytree import tree_map_only\n \n \n log = logging.getLogger(__name__)\n \n \n+@dataclass(frozen=True)\n+class StorageKey:\n+    storage: torch.UntypedStorage\n+    device: torch.device\n+\n+    def __hash__(self) -> int:\n+        return self.storage._cdata\n+\n+    def __eq__(self, other: object) -> bool:\n+        if not isinstance(other, StorageKey):\n+            return False\n+        return (\n+            self.storage._cdata == other.storage._cdata and self.device == other.device\n+        )\n+\n+\n+class GraphAliasTracker:\n+    \"\"\"\n+    Tracks storage allocation and usage relationships in an FX graph.\n+\n+    Differentiates between:\n+    - Fresh allocations: nodes that allocate new storage (not views/aliases)\n+    - Uses: nodes that use a storage as input\n+    \"\"\"\n+\n+    def __init__(self, nodes: list[fx.Node]):\n+        # Map from node to the fresh storages it allocates (not views/aliases)\n+        self.node_to_fresh_allocations: dict[fx.Node, OrderedSet[StorageKey]] = {}\n+\n+        # Map from storage to the node that originally allocated it\n+        self.storage_to_allocator: dict[StorageKey, fx.Node] = {}\n+\n+        # Map from node to all storages it uses as inputs\n+        self.node_to_storage_uses: dict[fx.Node, OrderedSet[StorageKey]] = {}\n+\n+        # Map from storage to all nodes that use it\n+        self.storage_to_uses: dict[StorageKey, OrderedSet[fx.Node]] = defaultdict(\n+            OrderedSet\n+        )\n+\n+        # Map from storage to the last node that uses it\n+        self.storage_to_last_user: dict[StorageKey, fx.Node] = {}\n+\n+        # Map from node to storages that have their last use at that node\n+        self.node_to_storages_last_used: dict[fx.Node, OrderedSet[StorageKey]] = (\n+            defaultdict(OrderedSet)\n+        )\n+\n+        # Track all output storages for each node (for building usage graph)\n+        self.node_to_output_storages: dict[fx.Node, OrderedSet[StorageKey]] = {}\n+\n+        # First pass: build storage allocations and track uses\n+        for node in nodes:\n+            # Get output storages\n+            output_storages = self._get_output_storages(node)\n+            self.node_to_output_storages[node] = output_storages\n+\n+            # Track fresh allocations\n+            fresh_allocations: OrderedSet[StorageKey] = OrderedSet()\n+            for storage_key in output_storages:\n+                if storage_key not in self.storage_to_allocator:\n+                    self.storage_to_allocator[storage_key] = node\n+                    fresh_allocations.add(storage_key)\n+            self.node_to_fresh_allocations[node] = fresh_allocations\n+\n+            # Track input storage uses (safe because inputs were already processed)\n+            input_storages = self._get_input_storages(node)\n+            self.node_to_storage_uses[node] = input_storages\n+            for storage_key in input_storages:\n+                self.storage_to_uses[storage_key].add(node)\n+\n+        # Second pass: find last users (iterate in reverse)\n+        for node in reversed(nodes):\n+            input_storages = self.node_to_storage_uses[node]\n+            for storage_key in input_storages:\n+                if storage_key not in self.storage_to_last_user:\n+                    self.storage_to_last_user[storage_key] = node\n+                    self.node_to_storages_last_used[node].add(storage_key)\n+\n+    @staticmethod\n+    def _get_output_storages(node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"\n+        Get all storages from a node's outputs.\n+\n+        Uses pytree to handle arbitrary nested structures.\n+        \"\"\"\n+        val = node.meta.get(\"val\")\n+        if val is None:\n+            return OrderedSet()\n+\n+        storages: OrderedSet[StorageKey] = OrderedSet()\n+\n+        def collect_storage(tensor: torch._subclasses.FakeTensor) -> None:\n+            storages.add(StorageKey(tensor.untyped_storage(), tensor.device))\n+\n+        # Use tree_map_only to handle FakeTensors in nested structures\n+        tree_map_only(torch._subclasses.FakeTensor, collect_storage, val)\n+\n+        return storages\n+\n+    def _get_input_storages(self, node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"\n+        Get all storages from a node's inputs.\n+        \"\"\"\n+        input_storages: OrderedSet[StorageKey] = OrderedSet()\n+\n+        for input_node in node.all_input_nodes:\n+            input_storages.update(self.node_to_output_storages[input_node])\n+\n+        return input_storages\n+\n+    def get_fresh_allocations(self, node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"Get all fresh storage allocations by this node (not views/aliases).\"\"\"\n+        return self.node_to_fresh_allocations[node]\n+\n+    def get_storage_uses(self, node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"Get all storages that this node uses as inputs.\"\"\"\n+        return self.node_to_storage_uses[node]\n+\n+    def get_storages_last_used_at(\n+        self,\n+        node: fx.Node,\n+    ) -> OrderedSet[StorageKey]:\n+        \"\"\"\n+        Get storages whose last use is at this node.\n+\n+        Returns storages that are currently active and have their\n+        last use at this node.\n+        \"\"\"\n+        return self.node_to_storages_last_used[node]\n+\n+\n+def _size_of_default(num_bytes: Union[int, torch.SymInt]) -> int:\n+    return hint_int(num_bytes, fallback=torch._inductor.config.unbacked_symint_fallback)\n+\n+\n def build_memory_profile(\n     graph: fx.Graph,\n-    size_of: Callable[[fx.Node], int],\n     is_releasable: Callable[[fx.Node], bool],\n+    size_of: Optional[Callable[[Union[int, torch.SymInt]], int]] = None,\n ) -> list[int]:\n     \"\"\"\n     Function to estimate the memory profile of an input FX graph.\n \n     Args:\n     - graph (fx.Graph): The input FX graph for which the memory profile\n       is to be estimated.\n-    - size_of (Callable[[fx.Node], int]): A function that returns\n-      the size of a given node.\n     - is_releasable (Callable[[fx.Node], bool]): A function that\n       determines if a node's memory can be released (e.g. primal nodes\n       cannot be released).\n+    - size_of (Callable[[Union[int, torch.SymInt]], int]): A function that converts\n+      byte counts (possibly symbolic) to concrete integers.\n \n     Returns:\n     - List[int]: A list representing the memory profile over the execution\n       of the graph, where each entry corresponds to the memory usage at\n       a particular point in the execution.\n     \"\"\"\n \n+    size_of = size_of or _size_of_default\n     nodes = list(graph.nodes)\n-    op_types = get_default_op_list()\n-\n-    class AliasInfo:\n-        \"\"\"\n-        Class for storing and accessing alias information of a FX graph.\n-\n-        Attributes:\n-        - view_to_source: Maps view nodes to their source nodes\n-        - getitem_to_source: Maps getitem nodes to (source_node, key) tuples\n-        - source_to_getitems: Maps source nodes to dictionaries of\n-          {key: getitem_node, \"unclaimed\": None}\n-        - source_to_unclaimed_size: Maps source nodes to their storage size\n-          unclaimed by any getitem_nodes\n-        \"\"\"\n+    alias_info = GraphAliasTracker(nodes)\n \n-        def __init__(self, nodes: list[fx.Node]):\n-            \"\"\"\n-            Initialize the AliasInfo class with a list of FX graph nodes.\n-\n-            Args:\n-            - nodes (list[fx.Node]): A list of nodes from an FX graph,\n-              ordered in execution order.\n-\n-            The constructor analyzes the relationships between nodes in the FX graph\n-            to populate alias information. It identifies two types of alias nodes:\n-            getitem and view. For each view, it maps it to its source. For each\n-            getitem, it maps it to its source and key. It also populates mappings\n-            for source nodes to their getitems and calculates unclaimed storage sizes.\n-\n-            \"\"\"\n-            # For each view, we map it to its source.\n-            # Note that we treat getitems of a view (e.g. aten.split) as views.\n-            self.view_to_source: dict[fx.Node, fx.Node] = {}\n-\n-            # For each remaining getitem, we map it to its source and key.\n-            self.getitem_to_source: dict[fx.Node, tuple[fx.Node, Any]] = {}\n-\n-            # For each none-view source_node of getitems, we map it to a dictionary\n-            # in the form of {key: getitem_node, ..., \"unclaimed\": None}, where\n-            # \"unclaimed\" is a dummy key that represents all elements in the\n-            # source_node that is not claimed by any getitems.\n-            self.source_to_getitems: dict[fx.Node, dict[Any, fx.Node | None]] = {}\n-\n-            # For each none-view source_node of getitems with at least one unclaimed\n-            # elements, we map it to its unclaimed storage size.\n-            self.source_to_unclaimed_size: dict[fx.Node, int] = {}\n-\n-            for node in nodes:\n-                is_view = op_types.is_view(node)\n-                is_getitem = node.target is operator.getitem\n-                if not (is_view or is_getitem):\n-                    continue\n-                assert not (is_view and is_getitem)\n-                assert node.args and isinstance(node.args[0], fx.Node)\n-                source = node.args[0]\n-                if is_view:\n-                    assert not isinstance(source.meta[\"val\"], list | tuple | dict)\n-                    if source in self.view_to_source:\n-                        source = self.view_to_source[source]\n-                    self.view_to_source[node] = source\n-                if is_getitem:\n-                    assert isinstance(source.meta[\"val\"], list | tuple | dict)\n-                    # Source of getitem can be a view (e.g. aten.split).\n-                    if source in self.view_to_source:\n-                        if source in self.view_to_source:\n-                            source = self.view_to_source[source]\n-                        # In this case, the getitem node should be treated\n-                        # the same way as a regular view.\n-                        self.view_to_source[node] = source\n-                        continue\n-                    # Source of getitem cannot be a getitem.\n-                    assert source not in self.getitem_to_source\n-\n-                    # There must be a second argument that specifies the key.\n-                    assert len(node.args) >= 2\n-                    key = node.args[1]\n-                    self.getitem_to_source[node] = (source, key)\n-\n-                    # Populate source_to_getitems.\n-                    if source not in self.source_to_getitems:\n-                        self.source_to_getitems[source] = {\"unclaimed\": None}\n-                    assert key not in self.source_to_getitems[source]\n-                    self.source_to_getitems[source][key] = node  # type: ignore[index]\n-\n-            for source, getitem_map in self.source_to_getitems.items():\n-                unclaimed_source_size = size_of(source)\n-                for key, getitem_node in getitem_map.items():\n-                    if key != \"unclaimed\" and getitem_node is not None:\n-                        unclaimed_source_size -= size_of(getitem_node)\n-                assert unclaimed_source_size >= 0\n-                if unclaimed_source_size > 0:\n-                    self.source_to_unclaimed_size[source] = unclaimed_source_size\n-\n-        def is_view(self, node: fx.Node) -> bool:\n-            return node in self.view_to_source\n-\n-        def is_getitem(self, node: fx.Node) -> bool:\n-            return node in self.getitem_to_source\n-\n-        def get_source(self, node: fx.Node) -> fx.Node | tuple[fx.Node, Any]:\n-            if self.is_view(node):\n-                return self.view_to_source[node]\n-            if self.is_getitem(node):\n-                return self.getitem_to_source[node]\n-            return node\n-\n-        def is_source_of_getitems(self, node: fx.Node) -> bool:\n-            return node in self.source_to_getitems\n-\n-        def get_storage_keys(self, source_node: fx.Node) -> list[Any]:\n-            assert source_node in self.source_to_getitems\n-            return list(self.source_to_getitems[source_node].keys())\n-\n-        def get_unclaimed_storage_size(self, source_node: fx.Node) -> int:\n-            return self.source_to_unclaimed_size.get(source_node, 0)\n-\n-        def get_getitem_by_key(self, source: fx.Node, key: Any) -> fx.Node | None:\n-            assert source in self.source_to_getitems\n-            assert key in self.source_to_getitems[source]\n-            return self.source_to_getitems[source][key]\n-\n-    def _get_last_usage(\n-        nodes: list[fx.Node], alias_info: AliasInfo\n-    ) -> dict[fx.Node, list[tuple[fx.Node, Any]]]:\n-        \"\"\"\n-        Determine the last usage point of each storage. This information is used to\n-        identify when storages can be safely released.\n+    # Build memory profile\n+    current_memory = 0\n \n-        Args:\n-        - nodes (list[fx.Node]): A list of nodes from the FX graph, ordered\n-          in execution order.\n-        - alias_info (AliasInfo): An instance of AliasInfo containing aliasing\n-          relationships between nodes in the graph.\n+    for node in itertools.chain(\n+        graph.find_nodes(op=\"placeholder\"), graph.find_nodes(op=\"get_attr\")\n+    ):\n+        for storage_key in alias_info.get_fresh_allocations(node):\n+            if storage_key.device.type != \"cpu\":",
    "comment": "can we reuse device_filter instead of hardcode \"cpu\"?",
    "line_number": 187,
    "enriched": "File: torch/_inductor/fx_passes/memory_estimator.py\nCode: @@ -1,271 +1,221 @@\n+import itertools\n import logging\n-import operator\n-from typing import Any, Callable\n+from collections import defaultdict\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Union\n \n+import torch\n import torch.fx as fx\n-from torch._functorch.partitioners import _size_of, get_default_op_list\n+from torch.fx.experimental.symbolic_shapes import hint_int\n from torch.utils._ordered_set import OrderedSet\n+from torch.utils._pytree import tree_map_only\n \n \n log = logging.getLogger(__name__)\n \n \n+@dataclass(frozen=True)\n+class StorageKey:\n+    storage: torch.UntypedStorage\n+    device: torch.device\n+\n+    def __hash__(self) -> int:\n+        return self.storage._cdata\n+\n+    def __eq__(self, other: object) -> bool:\n+        if not isinstance(other, StorageKey):\n+            return False\n+        return (\n+            self.storage._cdata == other.storage._cdata and self.device == other.device\n+        )\n+\n+\n+class GraphAliasTracker:\n+    \"\"\"\n+    Tracks storage allocation and usage relationships in an FX graph.\n+\n+    Differentiates between:\n+    - Fresh allocations: nodes that allocate new storage (not views/aliases)\n+    - Uses: nodes that use a storage as input\n+    \"\"\"\n+\n+    def __init__(self, nodes: list[fx.Node]):\n+        # Map from node to the fresh storages it allocates (not views/aliases)\n+        self.node_to_fresh_allocations: dict[fx.Node, OrderedSet[StorageKey]] = {}\n+\n+        # Map from storage to the node that originally allocated it\n+        self.storage_to_allocator: dict[StorageKey, fx.Node] = {}\n+\n+        # Map from node to all storages it uses as inputs\n+        self.node_to_storage_uses: dict[fx.Node, OrderedSet[StorageKey]] = {}\n+\n+        # Map from storage to all nodes that use it\n+        self.storage_to_uses: dict[StorageKey, OrderedSet[fx.Node]] = defaultdict(\n+            OrderedSet\n+        )\n+\n+        # Map from storage to the last node that uses it\n+        self.storage_to_last_user: dict[StorageKey, fx.Node] = {}\n+\n+        # Map from node to storages that have their last use at that node\n+        self.node_to_storages_last_used: dict[fx.Node, OrderedSet[StorageKey]] = (\n+            defaultdict(OrderedSet)\n+        )\n+\n+        # Track all output storages for each node (for building usage graph)\n+        self.node_to_output_storages: dict[fx.Node, OrderedSet[StorageKey]] = {}\n+\n+        # First pass: build storage allocations and track uses\n+        for node in nodes:\n+            # Get output storages\n+            output_storages = self._get_output_storages(node)\n+            self.node_to_output_storages[node] = output_storages\n+\n+            # Track fresh allocations\n+            fresh_allocations: OrderedSet[StorageKey] = OrderedSet()\n+            for storage_key in output_storages:\n+                if storage_key not in self.storage_to_allocator:\n+                    self.storage_to_allocator[storage_key] = node\n+                    fresh_allocations.add(storage_key)\n+            self.node_to_fresh_allocations[node] = fresh_allocations\n+\n+            # Track input storage uses (safe because inputs were already processed)\n+            input_storages = self._get_input_storages(node)\n+            self.node_to_storage_uses[node] = input_storages\n+            for storage_key in input_storages:\n+                self.storage_to_uses[storage_key].add(node)\n+\n+        # Second pass: find last users (iterate in reverse)\n+        for node in reversed(nodes):\n+            input_storages = self.node_to_storage_uses[node]\n+            for storage_key in input_storages:\n+                if storage_key not in self.storage_to_last_user:\n+                    self.storage_to_last_user[storage_key] = node\n+                    self.node_to_storages_last_used[node].add(storage_key)\n+\n+    @staticmethod\n+    def _get_output_storages(node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"\n+        Get all storages from a node's outputs.\n+\n+        Uses pytree to handle arbitrary nested structures.\n+        \"\"\"\n+        val = node.meta.get(\"val\")\n+        if val is None:\n+            return OrderedSet()\n+\n+        storages: OrderedSet[StorageKey] = OrderedSet()\n+\n+        def collect_storage(tensor: torch._subclasses.FakeTensor) -> None:\n+            storages.add(StorageKey(tensor.untyped_storage(), tensor.device))\n+\n+        # Use tree_map_only to handle FakeTensors in nested structures\n+        tree_map_only(torch._subclasses.FakeTensor, collect_storage, val)\n+\n+        return storages\n+\n+    def _get_input_storages(self, node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"\n+        Get all storages from a node's inputs.\n+        \"\"\"\n+        input_storages: OrderedSet[StorageKey] = OrderedSet()\n+\n+        for input_node in node.all_input_nodes:\n+            input_storages.update(self.node_to_output_storages[input_node])\n+\n+        return input_storages\n+\n+    def get_fresh_allocations(self, node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"Get all fresh storage allocations by this node (not views/aliases).\"\"\"\n+        return self.node_to_fresh_allocations[node]\n+\n+    def get_storage_uses(self, node: fx.Node) -> OrderedSet[StorageKey]:\n+        \"\"\"Get all storages that this node uses as inputs.\"\"\"\n+        return self.node_to_storage_uses[node]\n+\n+    def get_storages_last_used_at(\n+        self,\n+        node: fx.Node,\n+    ) -> OrderedSet[StorageKey]:\n+        \"\"\"\n+        Get storages whose last use is at this node.\n+\n+        Returns storages that are currently active and have their\n+        last use at this node.\n+        \"\"\"\n+        return self.node_to_storages_last_used[node]\n+\n+\n+def _size_of_default(num_bytes: Union[int, torch.SymInt]) -> int:\n+    return hint_int(num_bytes, fallback=torch._inductor.config.unbacked_symint_fallback)\n+\n+\n def build_memory_profile(\n     graph: fx.Graph,\n-    size_of: Callable[[fx.Node], int],\n     is_releasable: Callable[[fx.Node], bool],\n+    size_of: Optional[Callable[[Union[int, torch.SymInt]], int]] = None,\n ) -> list[int]:\n     \"\"\"\n     Function to estimate the memory profile of an input FX graph.\n \n     Args:\n     - graph (fx.Graph): The input FX graph for which the memory profile\n       is to be estimated.\n-    - size_of (Callable[[fx.Node], int]): A function that returns\n-      the size of a given node.\n     - is_releasable (Callable[[fx.Node], bool]): A function that\n       determines if a node's memory can be released (e.g. primal nodes\n       cannot be released).\n+    - size_of (Callable[[Union[int, torch.SymInt]], int]): A function that converts\n+      byte counts (possibly symbolic) to concrete integers.\n \n     Returns:\n     - List[int]: A list representing the memory profile over the execution\n       of the graph, where each entry corresponds to the memory usage at\n       a particular point in the execution.\n     \"\"\"\n \n+    size_of = size_of or _size_of_default\n     nodes = list(graph.nodes)\n-    op_types = get_default_op_list()\n-\n-    class AliasInfo:\n-        \"\"\"\n-        Class for storing and accessing alias information of a FX graph.\n-\n-        Attributes:\n-        - view_to_source: Maps view nodes to their source nodes\n-        - getitem_to_source: Maps getitem nodes to (source_node, key) tuples\n-        - source_to_getitems: Maps source nodes to dictionaries of\n-          {key: getitem_node, \"unclaimed\": None}\n-        - source_to_unclaimed_size: Maps source nodes to their storage size\n-          unclaimed by any getitem_nodes\n-        \"\"\"\n+    alias_info = GraphAliasTracker(nodes)\n \n-        def __init__(self, nodes: list[fx.Node]):\n-            \"\"\"\n-            Initialize the AliasInfo class with a list of FX graph nodes.\n-\n-            Args:\n-            - nodes (list[fx.Node]): A list of nodes from an FX graph,\n-              ordered in execution order.\n-\n-            The constructor analyzes the relationships between nodes in the FX graph\n-            to populate alias information. It identifies two types of alias nodes:\n-            getitem and view. For each view, it maps it to its source. For each\n-            getitem, it maps it to its source and key. It also populates mappings\n-            for source nodes to their getitems and calculates unclaimed storage sizes.\n-\n-            \"\"\"\n-            # For each view, we map it to its source.\n-            # Note that we treat getitems of a view (e.g. aten.split) as views.\n-            self.view_to_source: dict[fx.Node, fx.Node] = {}\n-\n-            # For each remaining getitem, we map it to its source and key.\n-            self.getitem_to_source: dict[fx.Node, tuple[fx.Node, Any]] = {}\n-\n-            # For each none-view source_node of getitems, we map it to a dictionary\n-            # in the form of {key: getitem_node, ..., \"unclaimed\": None}, where\n-            # \"unclaimed\" is a dummy key that represents all elements in the\n-            # source_node that is not claimed by any getitems.\n-            self.source_to_getitems: dict[fx.Node, dict[Any, fx.Node | None]] = {}\n-\n-            # For each none-view source_node of getitems with at least one unclaimed\n-            # elements, we map it to its unclaimed storage size.\n-            self.source_to_unclaimed_size: dict[fx.Node, int] = {}\n-\n-            for node in nodes:\n-                is_view = op_types.is_view(node)\n-                is_getitem = node.target is operator.getitem\n-                if not (is_view or is_getitem):\n-                    continue\n-                assert not (is_view and is_getitem)\n-                assert node.args and isinstance(node.args[0], fx.Node)\n-                source = node.args[0]\n-                if is_view:\n-                    assert not isinstance(source.meta[\"val\"], list | tuple | dict)\n-                    if source in self.view_to_source:\n-                        source = self.view_to_source[source]\n-                    self.view_to_source[node] = source\n-                if is_getitem:\n-                    assert isinstance(source.meta[\"val\"], list | tuple | dict)\n-                    # Source of getitem can be a view (e.g. aten.split).\n-                    if source in self.view_to_source:\n-                        if source in self.view_to_source:\n-                            source = self.view_to_source[source]\n-                        # In this case, the getitem node should be treated\n-                        # the same way as a regular view.\n-                        self.view_to_source[node] = source\n-                        continue\n-                    # Source of getitem cannot be a getitem.\n-                    assert source not in self.getitem_to_source\n-\n-                    # There must be a second argument that specifies the key.\n-                    assert len(node.args) >= 2\n-                    key = node.args[1]\n-                    self.getitem_to_source[node] = (source, key)\n-\n-                    # Populate source_to_getitems.\n-                    if source not in self.source_to_getitems:\n-                        self.source_to_getitems[source] = {\"unclaimed\": None}\n-                    assert key not in self.source_to_getitems[source]\n-                    self.source_to_getitems[source][key] = node  # type: ignore[index]\n-\n-            for source, getitem_map in self.source_to_getitems.items():\n-                unclaimed_source_size = size_of(source)\n-                for key, getitem_node in getitem_map.items():\n-                    if key != \"unclaimed\" and getitem_node is not None:\n-                        unclaimed_source_size -= size_of(getitem_node)\n-                assert unclaimed_source_size >= 0\n-                if unclaimed_source_size > 0:\n-                    self.source_to_unclaimed_size[source] = unclaimed_source_size\n-\n-        def is_view(self, node: fx.Node) -> bool:\n-            return node in self.view_to_source\n-\n-        def is_getitem(self, node: fx.Node) -> bool:\n-            return node in self.getitem_to_source\n-\n-        def get_source(self, node: fx.Node) -> fx.Node | tuple[fx.Node, Any]:\n-            if self.is_view(node):\n-                return self.view_to_source[node]\n-            if self.is_getitem(node):\n-                return self.getitem_to_source[node]\n-            return node\n-\n-        def is_source_of_getitems(self, node: fx.Node) -> bool:\n-            return node in self.source_to_getitems\n-\n-        def get_storage_keys(self, source_node: fx.Node) -> list[Any]:\n-            assert source_node in self.source_to_getitems\n-            return list(self.source_to_getitems[source_node].keys())\n-\n-        def get_unclaimed_storage_size(self, source_node: fx.Node) -> int:\n-            return self.source_to_unclaimed_size.get(source_node, 0)\n-\n-        def get_getitem_by_key(self, source: fx.Node, key: Any) -> fx.Node | None:\n-            assert source in self.source_to_getitems\n-            assert key in self.source_to_getitems[source]\n-            return self.source_to_getitems[source][key]\n-\n-    def _get_last_usage(\n-        nodes: list[fx.Node], alias_info: AliasInfo\n-    ) -> dict[fx.Node, list[tuple[fx.Node, Any]]]:\n-        \"\"\"\n-        Determine the last usage point of each storage. This information is used to\n-        identify when storages can be safely released.\n+    # Build memory profile\n+    current_memory = 0\n \n-        Args:\n-        - nodes (list[fx.Node]): A list of nodes from the FX graph, ordered\n-          in execution order.\n-        - alias_info (AliasInfo): An instance of AliasInfo containing aliasing\n-          relationships between nodes in the graph.\n+    for node in itertools.chain(\n+        graph.find_nodes(op=\"placeholder\"), graph.find_nodes(op=\"get_attr\")\n+    ):\n+        for storage_key in alias_info.get_fresh_allocations(node):\n+            if storage_key.device.type != \"cpu\":\nComment: can we reuse `device_filter` instead of hardcode \"cpu\"?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/_inductor/fx_passes/memory_estimator.py",
    "pr_number": 164783,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2408778957,
    "comment_created_at": "2025-10-06T21:57:12Z"
  },
  {
    "code": "@@ -375,8 +379,159 @@ def apply_dp(partial_model):\n                     p.grad.full_tensor(), ref_p.grad, atol=5e-5, rtol=2e-2\n                 )\n \n+    @requires_nccl()\n+    @skip_if_lt_x_gpu(4)\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"Test requires 4+ GPUs\")\n+    @parametrize(\"dp_type\", [\"FSDP\", \"FSDP_MP\"])\n+    def test_pp_fsdp_unshard_reshard_runtime(self, dp_type):\n+        \"\"\"Test FSDP UNSHARD/RESHARD functionality using _PipelineScheduleRuntime with custom schedules.\"\"\"\n+        if TEST_WITH_ROCM:\n+            return\n+\n+        torch.get_device_module(device_type).set_device(self.device)\n+        mesh_shape = (self.world_size, 1)\n+        mesh_dim_names = (\"dp\", \"pp\")\n+        device_mesh = init_device_mesh(\n+            \"cuda\", mesh_shape=mesh_shape, mesh_dim_names=mesh_dim_names\n+        )\n+        pp_group = device_mesh[\"pp\"].get_group()\n+        dp_mesh = device_mesh[\"dp\"]\n+\n+        # fsdp_mixed-precision dtype\n+        mp_dtype = torch.bfloat16 if dp_type == \"FSDP_MP\" else torch.float32\n+        total_layers = 4\n+        dim = 10\n+        full_model = nn.ModuleList([MLPModule(dim) for _ in range(total_layers)])\n+\n+        def apply_dp(partial_model):\n+            mp_policy = MixedPrecisionPolicy(\n+                param_dtype=mp_dtype,\n+                reduce_dtype=torch.float32,\n+            )\n+            fsdp_config = {\"mesh\": dp_mesh, \"mp_policy\": mp_policy}\n+            for layer in partial_model.children():\n+                fully_shard(\n+                    layer,\n+                    **fsdp_config,\n+                    reshard_after_forward=False,\n+                )\n+            return fully_shard(partial_model, **fsdp_config)\n+\n+        # Build pipeline stages\n+        num_stages = pp_group.size()\n+        layers_per_stage = total_layers // num_stages\n+        stage_idx = pp_group.rank()\n+        offset = stage_idx * layers_per_stage\n+\n+        partial_model = nn.Sequential(\n+            *full_model[offset : (stage_idx + 1) * layers_per_stage]\n+        )\n+        partial_model.to(self.device)\n+        fsdp_model = apply_dp(partial_model)\n+        distributed_state = fully_shard.state(fsdp_model)\n+        distributed_state._lazy_init()\n+\n+        stage = PipelineStage(\n+            fsdp_model,\n+            stage_idx,\n+            num_stages,\n+            self.device,\n+            group=pp_group,\n+        )\n+\n+        # Helper function to check FSDP sharding state\n+        def check_fsdp_unsharded_state(module, expected_unsharded=False):\n+            \"\"\"Check if FSDP parameters are in expected sharding state.\"\"\"\n+            distributed_state = fully_shard.state(module)\n+            unsharded_count = 0\n+            total_fsdp_params = 0\n+\n+            for state in distributed_state._state_ctx.all_states:\n+                if state._fsdp_param_group:\n+                    group = state._fsdp_param_group\n+                    for fsdp_param in group.fsdp_params:\n+                        total_fsdp_params += 1\n+                        if fsdp_param.sharded_state == ShardedState.UNSHARDED:\n+                            unsharded_count += 1\n+\n+            if expected_unsharded:\n+                self.assertEqual(\n+                    unsharded_count,\n+                    total_fsdp_params,\n+                    f\"Expected all {total_fsdp_params} FSDP parameters to be unsharded, \"\n+                    f\"but only {unsharded_count} are unsharded\",\n+                )\n+            else:\n+                self.assertEqual(\n+                    unsharded_count,\n+                    0,\n+                    f\"Expected all FSDP parameters to be sharded, \"\n+                    f\"but {unsharded_count} out of {total_fsdp_params} are unsharded\",\n+                )\n+\n+            return total_fsdp_params > 0  # Return whether we found any FSDP parameters\n+\n+        # Test initial state - should be sharded\n+        has_fsdp = check_fsdp_unsharded_state(stage.submod, expected_unsharded=False)\n+\n+        if not has_fsdp:\n+            self.skipTest(\"No FSDP parameters found in the model\")\n+\n+        def create_schedule(computation_types, microbatch_index=None):\n+            schedule = {\n+                0: [\n+                    _Action(\n+                        stage_index=0,  # stage 0 (the only stage)\n+                        computation_type=comp_type,\n+                        microbatch_index=microbatch_index\n+                        if comp_type == _ComputationType.FORWARD\n+                        else None,\n+                    )\n+                    for comp_type in computation_types\n+                ]\n+            }\n+            return schedule\n+\n+        unshard_schedule = create_schedule(\n+            [\n+                _ComputationType.UNSHARD,\n+                _ComputationType.FORWARD,\n+            ],\n+            microbatch_index=0,\n+        )\n+        unshard_reshard_schedule = create_schedule(\n+            [\n+                _ComputationType.UNSHARD,\n+                _ComputationType.FORWARD,\n+                _ComputationType.RESHARD,\n+            ],\n+            microbatch_index=0,\n+        )\n+\n+        # Test 1: Run UNSHARD + RESHARD schedule\n+        runtime = _PipelineScheduleRuntime(\n+            [stage], n_microbatches=1, loss_fn=None, scale_grads=False\n+        )\n+        runtime.pipeline_order_with_comms = unshard_reshard_schedule\n+        dummy_input = torch.randn(1, dim, device=self.device, dtype=mp_dtype)\n+        runtime.step(dummy_input)\n+\n+        # Verify parameters are now sharded again\n+        check_fsdp_unsharded_state(stage.submod, expected_unsharded=False)\n+\n+        # Test 2: Run UNSHARD only schedule\n+        runtime.pipeline_order_with_comms = unshard_schedule\n+        runtime.step(dummy_input)\n+\n+        # Verify parameters are now unsharded\n+        check_fsdp_unsharded_state(stage.submod, expected_unsharded=True)\n+\n \n instantiate_parametrized_tests(ComposabilityTest)\n \n+import fbvscode\n+\n+\n+fbvscode.attach_debugger()",
    "comment": "remove?",
    "line_number": 535,
    "enriched": "File: test/distributed/test_composability.py\nCode: @@ -375,8 +379,159 @@ def apply_dp(partial_model):\n                     p.grad.full_tensor(), ref_p.grad, atol=5e-5, rtol=2e-2\n                 )\n \n+    @requires_nccl()\n+    @skip_if_lt_x_gpu(4)\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"Test requires 4+ GPUs\")\n+    @parametrize(\"dp_type\", [\"FSDP\", \"FSDP_MP\"])\n+    def test_pp_fsdp_unshard_reshard_runtime(self, dp_type):\n+        \"\"\"Test FSDP UNSHARD/RESHARD functionality using _PipelineScheduleRuntime with custom schedules.\"\"\"\n+        if TEST_WITH_ROCM:\n+            return\n+\n+        torch.get_device_module(device_type).set_device(self.device)\n+        mesh_shape = (self.world_size, 1)\n+        mesh_dim_names = (\"dp\", \"pp\")\n+        device_mesh = init_device_mesh(\n+            \"cuda\", mesh_shape=mesh_shape, mesh_dim_names=mesh_dim_names\n+        )\n+        pp_group = device_mesh[\"pp\"].get_group()\n+        dp_mesh = device_mesh[\"dp\"]\n+\n+        # fsdp_mixed-precision dtype\n+        mp_dtype = torch.bfloat16 if dp_type == \"FSDP_MP\" else torch.float32\n+        total_layers = 4\n+        dim = 10\n+        full_model = nn.ModuleList([MLPModule(dim) for _ in range(total_layers)])\n+\n+        def apply_dp(partial_model):\n+            mp_policy = MixedPrecisionPolicy(\n+                param_dtype=mp_dtype,\n+                reduce_dtype=torch.float32,\n+            )\n+            fsdp_config = {\"mesh\": dp_mesh, \"mp_policy\": mp_policy}\n+            for layer in partial_model.children():\n+                fully_shard(\n+                    layer,\n+                    **fsdp_config,\n+                    reshard_after_forward=False,\n+                )\n+            return fully_shard(partial_model, **fsdp_config)\n+\n+        # Build pipeline stages\n+        num_stages = pp_group.size()\n+        layers_per_stage = total_layers // num_stages\n+        stage_idx = pp_group.rank()\n+        offset = stage_idx * layers_per_stage\n+\n+        partial_model = nn.Sequential(\n+            *full_model[offset : (stage_idx + 1) * layers_per_stage]\n+        )\n+        partial_model.to(self.device)\n+        fsdp_model = apply_dp(partial_model)\n+        distributed_state = fully_shard.state(fsdp_model)\n+        distributed_state._lazy_init()\n+\n+        stage = PipelineStage(\n+            fsdp_model,\n+            stage_idx,\n+            num_stages,\n+            self.device,\n+            group=pp_group,\n+        )\n+\n+        # Helper function to check FSDP sharding state\n+        def check_fsdp_unsharded_state(module, expected_unsharded=False):\n+            \"\"\"Check if FSDP parameters are in expected sharding state.\"\"\"\n+            distributed_state = fully_shard.state(module)\n+            unsharded_count = 0\n+            total_fsdp_params = 0\n+\n+            for state in distributed_state._state_ctx.all_states:\n+                if state._fsdp_param_group:\n+                    group = state._fsdp_param_group\n+                    for fsdp_param in group.fsdp_params:\n+                        total_fsdp_params += 1\n+                        if fsdp_param.sharded_state == ShardedState.UNSHARDED:\n+                            unsharded_count += 1\n+\n+            if expected_unsharded:\n+                self.assertEqual(\n+                    unsharded_count,\n+                    total_fsdp_params,\n+                    f\"Expected all {total_fsdp_params} FSDP parameters to be unsharded, \"\n+                    f\"but only {unsharded_count} are unsharded\",\n+                )\n+            else:\n+                self.assertEqual(\n+                    unsharded_count,\n+                    0,\n+                    f\"Expected all FSDP parameters to be sharded, \"\n+                    f\"but {unsharded_count} out of {total_fsdp_params} are unsharded\",\n+                )\n+\n+            return total_fsdp_params > 0  # Return whether we found any FSDP parameters\n+\n+        # Test initial state - should be sharded\n+        has_fsdp = check_fsdp_unsharded_state(stage.submod, expected_unsharded=False)\n+\n+        if not has_fsdp:\n+            self.skipTest(\"No FSDP parameters found in the model\")\n+\n+        def create_schedule(computation_types, microbatch_index=None):\n+            schedule = {\n+                0: [\n+                    _Action(\n+                        stage_index=0,  # stage 0 (the only stage)\n+                        computation_type=comp_type,\n+                        microbatch_index=microbatch_index\n+                        if comp_type == _ComputationType.FORWARD\n+                        else None,\n+                    )\n+                    for comp_type in computation_types\n+                ]\n+            }\n+            return schedule\n+\n+        unshard_schedule = create_schedule(\n+            [\n+                _ComputationType.UNSHARD,\n+                _ComputationType.FORWARD,\n+            ],\n+            microbatch_index=0,\n+        )\n+        unshard_reshard_schedule = create_schedule(\n+            [\n+                _ComputationType.UNSHARD,\n+                _ComputationType.FORWARD,\n+                _ComputationType.RESHARD,\n+            ],\n+            microbatch_index=0,\n+        )\n+\n+        # Test 1: Run UNSHARD + RESHARD schedule\n+        runtime = _PipelineScheduleRuntime(\n+            [stage], n_microbatches=1, loss_fn=None, scale_grads=False\n+        )\n+        runtime.pipeline_order_with_comms = unshard_reshard_schedule\n+        dummy_input = torch.randn(1, dim, device=self.device, dtype=mp_dtype)\n+        runtime.step(dummy_input)\n+\n+        # Verify parameters are now sharded again\n+        check_fsdp_unsharded_state(stage.submod, expected_unsharded=False)\n+\n+        # Test 2: Run UNSHARD only schedule\n+        runtime.pipeline_order_with_comms = unshard_schedule\n+        runtime.step(dummy_input)\n+\n+        # Verify parameters are now unsharded\n+        check_fsdp_unsharded_state(stage.submod, expected_unsharded=True)\n+\n \n instantiate_parametrized_tests(ComposabilityTest)\n \n+import fbvscode\n+\n+\n+fbvscode.attach_debugger()\nComment: remove?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "test/distributed/test_composability.py",
    "pr_number": 164775,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2409390205,
    "comment_created_at": "2025-10-07T05:15:53Z"
  },
  {
    "code": "@@ -1224,48 +1224,84 @@ static PyObject* THPModule_allowTF32CuBLAS(\n \n static PyObject* THPModule_setAllowFP16ReductionCuBLAS(\n     PyObject* _unused,\n-    PyObject* arg) {\n+    PyObject* args) {\n   HANDLE_TH_ERRORS\n+  PyObject* allow_reduction_obj = nullptr;\n+  PyObject* allow_splitk_obj = Py_None;\n+  if (!PyArg_ParseTuple(args, \"O|O\", &allow_reduction_obj, &allow_splitk_obj)) {\n+    return nullptr;\n+  }\n   TORCH_CHECK(\n-      PyBool_Check(arg),\n-      \"set_allow_fp16_reduction_cublas expects a bool, \"\n+      PyBool_Check(allow_reduction_obj),\n+      \"set_allow_fp16_reduction_cublas expects a bool for allow_reduced_precision, \"\n       \"but got \",\n-      THPUtils_typename(arg));\n-  at::globalContext().setAllowFP16ReductionCuBLAS(arg == Py_True);\n+      THPUtils_typename(allow_reduction_obj));\n+  bool allow_reduction = allow_reduction_obj == Py_True;\n+  bool allow_splitk = true;\n+  if (allow_splitk_obj != Py_None) {\n+    TORCH_CHECK(\n+        PyBool_Check(allow_splitk_obj),\n+        \"set_allow_fp16_reduction_cublas expects a bool for allow_splitk, \"\n+        \"but got \",\n+        THPUtils_typename(allow_splitk_obj));\n+    allow_splitk = allow_splitk_obj == Py_True;\n+  }\n+  at::globalContext().setAllowFP16ReductionCuBLAS(\n+      allow_reduction, allow_splitk);\n   Py_RETURN_NONE;\n   END_HANDLE_TH_ERRORS\n }\n \n static PyObject* THPModule_allowFP16ReductionCuBLAS(\n     PyObject* _unused,\n     PyObject* noargs) {\n-  if (at::globalContext().allowFP16ReductionCuBLAS()) {\n-    Py_RETURN_TRUE;\n-  }\n-  Py_RETURN_FALSE;\n+  auto option = at::globalContext().allowFP16ReductionCuBLAS();\n+  bool allow_reduced_precision =\n+      option == at::CuBLASReductionOption::AllowReducedPrecisionWithSplitK;\n+  bool allow_splitk = option !=\n+      at::CuBLASReductionOption::DisallowReducedPrecisionDisallowSplitK;\n+  return Py_BuildValue(\"(pp)\", allow_reduced_precision, allow_splitk);",
    "comment": "maybe it wants something like py_buildvalue(\"(oo)\", allow_reduced_precision ? py_true : py_false, allow_splitk ? py_true : py_false);",
    "line_number": 1263,
    "enriched": "File: torch/csrc/Module.cpp\nCode: @@ -1224,48 +1224,84 @@ static PyObject* THPModule_allowTF32CuBLAS(\n \n static PyObject* THPModule_setAllowFP16ReductionCuBLAS(\n     PyObject* _unused,\n-    PyObject* arg) {\n+    PyObject* args) {\n   HANDLE_TH_ERRORS\n+  PyObject* allow_reduction_obj = nullptr;\n+  PyObject* allow_splitk_obj = Py_None;\n+  if (!PyArg_ParseTuple(args, \"O|O\", &allow_reduction_obj, &allow_splitk_obj)) {\n+    return nullptr;\n+  }\n   TORCH_CHECK(\n-      PyBool_Check(arg),\n-      \"set_allow_fp16_reduction_cublas expects a bool, \"\n+      PyBool_Check(allow_reduction_obj),\n+      \"set_allow_fp16_reduction_cublas expects a bool for allow_reduced_precision, \"\n       \"but got \",\n-      THPUtils_typename(arg));\n-  at::globalContext().setAllowFP16ReductionCuBLAS(arg == Py_True);\n+      THPUtils_typename(allow_reduction_obj));\n+  bool allow_reduction = allow_reduction_obj == Py_True;\n+  bool allow_splitk = true;\n+  if (allow_splitk_obj != Py_None) {\n+    TORCH_CHECK(\n+        PyBool_Check(allow_splitk_obj),\n+        \"set_allow_fp16_reduction_cublas expects a bool for allow_splitk, \"\n+        \"but got \",\n+        THPUtils_typename(allow_splitk_obj));\n+    allow_splitk = allow_splitk_obj == Py_True;\n+  }\n+  at::globalContext().setAllowFP16ReductionCuBLAS(\n+      allow_reduction, allow_splitk);\n   Py_RETURN_NONE;\n   END_HANDLE_TH_ERRORS\n }\n \n static PyObject* THPModule_allowFP16ReductionCuBLAS(\n     PyObject* _unused,\n     PyObject* noargs) {\n-  if (at::globalContext().allowFP16ReductionCuBLAS()) {\n-    Py_RETURN_TRUE;\n-  }\n-  Py_RETURN_FALSE;\n+  auto option = at::globalContext().allowFP16ReductionCuBLAS();\n+  bool allow_reduced_precision =\n+      option == at::CuBLASReductionOption::AllowReducedPrecisionWithSplitK;\n+  bool allow_splitk = option !=\n+      at::CuBLASReductionOption::DisallowReducedPrecisionDisallowSplitK;\n+  return Py_BuildValue(\"(pp)\", allow_reduced_precision, allow_splitk);\nComment: maybe it wants something like `Py_BuildValue(\"(OO)\", allow_reduced_precision ? Py_True : Py_False, allow_splitk ? Py_True : Py_False);`",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "torch/csrc/Module.cpp",
    "pr_number": 164766,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2411252705,
    "comment_created_at": "2025-10-07T16:46:06Z"
  },
  {
    "code": "@@ -978,6 +978,89 @@ void tile_reduce(\n   });\n }\n \n+/* Multi-tile Communication */\n+\n+void multi_root_tile_reduce(\n+    at::ArrayRef<at::Tensor> in_tiles,\n+    at::Tensor& out_tile,\n+    at::ArrayRef<int64_t> roots,\n+    std::string group_name,\n+    std::string reduce_op) {\n+  /* Perform multiple tile reductions concurrently, with each tile reduced to a separate root.\n+   Args:\n+     - `in_tiles` is a list of input tensors.\n+     - `out_tile` is the output tensor.\n+     - `roots` is a list of root ranks corresponding to each input tile, in the same order. A rank cannot be a root more than once.\n+     - `group_name` is the name of the group to use for the collective operation.\n+     - `reduce_op` is the reduction operation to perform. Currently only \"sum\" is supported.\n+   */\n+  TORCH_CHECK(reduce_op == \"sum\", \"tile_reduce: only sum is supported for now\");\n+  TORCH_CHECK(out_tile.dtype() == at::kFloat, \"Only float is supported\");\n+  TORCH_CHECK(out_tile.dim() == 2, \"Only 2D tensors are supported\");\n+  TORCH_CHECK(roots.size() == in_tiles.size(), \"Number of roots must match number of tiles\");\n+\n+  // Get device and stream\n+  auto device = out_tile.device();\n+  c10::cuda::CUDAGuard guard(device);\n+  auto stream = at::cuda::getCurrentCUDAStream();\n+\n+  // Rendezvous all tensors, and find the tile \"I\" need to reduce\n+  auto hdl = c10d::symmetric_memory::rendezvous(out_tile, group_name);\n+  int rank = hdl->get_rank();\n+  int world_size = hdl->get_world_size();\n+  int i = 0, my_tile_idx = 0, root = world_size;\n+  // Note: if there is no tile for the current rank, my_tile_idx will remain\n+  // initial value 0, and root will remain `world_size`. This is OK. In\n+  // `nvshmemx::tile_sum_reduce_block`, this rank would skip the reduction\n+  // operation, but would still participate in the barrier.\n+  for (auto& in_tile : in_tiles) {\n+    TORCH_CHECK(in_tile.dtype() == at::kFloat, \"Only float is supported\");\n+    c10d::symmetric_memory::rendezvous(in_tile, group_name);\n+    if (roots[i] == rank) {\n+      TORCH_CHECK(root == world_size, \"Each rank can only be a root once\");\n+      my_tile_idx = i;\n+      root = rank;\n+    }\n+    i++;\n+  }",
    "comment": "should we check that root != world_size here?",
    "line_number": 1025,
    "enriched": "File: torch/csrc/distributed/c10d/symm_mem/nvshmem_extension.cu\nCode: @@ -978,6 +978,89 @@ void tile_reduce(\n   });\n }\n \n+/* Multi-tile Communication */\n+\n+void multi_root_tile_reduce(\n+    at::ArrayRef<at::Tensor> in_tiles,\n+    at::Tensor& out_tile,\n+    at::ArrayRef<int64_t> roots,\n+    std::string group_name,\n+    std::string reduce_op) {\n+  /* Perform multiple tile reductions concurrently, with each tile reduced to a separate root.\n+   Args:\n+     - `in_tiles` is a list of input tensors.\n+     - `out_tile` is the output tensor.\n+     - `roots` is a list of root ranks corresponding to each input tile, in the same order. A rank cannot be a root more than once.\n+     - `group_name` is the name of the group to use for the collective operation.\n+     - `reduce_op` is the reduction operation to perform. Currently only \"sum\" is supported.\n+   */\n+  TORCH_CHECK(reduce_op == \"sum\", \"tile_reduce: only sum is supported for now\");\n+  TORCH_CHECK(out_tile.dtype() == at::kFloat, \"Only float is supported\");\n+  TORCH_CHECK(out_tile.dim() == 2, \"Only 2D tensors are supported\");\n+  TORCH_CHECK(roots.size() == in_tiles.size(), \"Number of roots must match number of tiles\");\n+\n+  // Get device and stream\n+  auto device = out_tile.device();\n+  c10::cuda::CUDAGuard guard(device);\n+  auto stream = at::cuda::getCurrentCUDAStream();\n+\n+  // Rendezvous all tensors, and find the tile \"I\" need to reduce\n+  auto hdl = c10d::symmetric_memory::rendezvous(out_tile, group_name);\n+  int rank = hdl->get_rank();\n+  int world_size = hdl->get_world_size();\n+  int i = 0, my_tile_idx = 0, root = world_size;\n+  // Note: if there is no tile for the current rank, my_tile_idx will remain\n+  // initial value 0, and root will remain `world_size`. This is OK. In\n+  // `nvshmemx::tile_sum_reduce_block`, this rank would skip the reduction\n+  // operation, but would still participate in the barrier.\n+  for (auto& in_tile : in_tiles) {\n+    TORCH_CHECK(in_tile.dtype() == at::kFloat, \"Only float is supported\");\n+    c10d::symmetric_memory::rendezvous(in_tile, group_name);\n+    if (roots[i] == rank) {\n+      TORCH_CHECK(root == world_size, \"Each rank can only be a root once\");\n+      my_tile_idx = i;\n+      root = rank;\n+    }\n+    i++;\n+  }\nComment: Should we check that `root != world_size` here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "torch/csrc/distributed/c10d/symm_mem/nvshmem_extension.cu",
    "pr_number": 164757,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2408093987,
    "comment_created_at": "2025-10-06T19:20:09Z"
  },
  {
    "code": "@@ -20,17 +21,28 @@ def normalize_graph(gm):\n     return normalize_gm(gm.print_readable(print_output=False))\n \n \n-@skipIf(not dist.is_available(), \"requires distributed\")\n-class TestFakeDistributed(DynamoTestCase):\n-    def setUp(self):\n-        # Use FakeProcessGroup to run tests on a single process\n-        dist.init_process_group(backend=\"fake\", rank=0, world_size=2)\n-        self.local_rank = 0\n-        self.world_size = 2\n+def with_fake_comms(func=None, local_rank=None, world_size=None):\n+    if func is None:\n+        return functools.partial(\n+            with_fake_comms, local_rank=local_rank, world_size=world_size\n+        )\n+\n+    @functools.wraps(func)\n+    def wrapper(self, *args, **kwargs):\n+        assert local_rank is not None\n+        assert world_size is not None\n+        dist.init_process_group(backend=\"fake\", rank=local_rank, world_size=world_size)\n+        try:\n+            return func(self, *args, **kwargs)\n+        finally:\n+            dist.destroy_process_group()\n \n-    def tearDown(self):\n-        dist.destroy_process_group()\n+    return wrapper\n \n+\n+@skipIf(not dist.is_available(), \"requires distributed\")\n+class TestFakeDistributed(DynamoTestCase):\n+    @with_fake_comms(local_rank=0, world_size=8)",
    "comment": "@bobrenjc93 if you change this world_size to something beyond 8 gpus",
    "line_number": 45,
    "enriched": "File: test/dynamo/test_fake_distributed.py\nCode: @@ -20,17 +21,28 @@ def normalize_graph(gm):\n     return normalize_gm(gm.print_readable(print_output=False))\n \n \n-@skipIf(not dist.is_available(), \"requires distributed\")\n-class TestFakeDistributed(DynamoTestCase):\n-    def setUp(self):\n-        # Use FakeProcessGroup to run tests on a single process\n-        dist.init_process_group(backend=\"fake\", rank=0, world_size=2)\n-        self.local_rank = 0\n-        self.world_size = 2\n+def with_fake_comms(func=None, local_rank=None, world_size=None):\n+    if func is None:\n+        return functools.partial(\n+            with_fake_comms, local_rank=local_rank, world_size=world_size\n+        )\n+\n+    @functools.wraps(func)\n+    def wrapper(self, *args, **kwargs):\n+        assert local_rank is not None\n+        assert world_size is not None\n+        dist.init_process_group(backend=\"fake\", rank=local_rank, world_size=world_size)\n+        try:\n+            return func(self, *args, **kwargs)\n+        finally:\n+            dist.destroy_process_group()\n \n-    def tearDown(self):\n-        dist.destroy_process_group()\n+    return wrapper\n \n+\n+@skipIf(not dist.is_available(), \"requires distributed\")\n+class TestFakeDistributed(DynamoTestCase):\n+    @with_fake_comms(local_rank=0, world_size=8)\nComment: @bobrenjc93 if you change this world_size to something beyond 8 gpus",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "test/dynamo/test_fake_distributed.py",
    "pr_number": 164754,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2408353338,
    "comment_created_at": "2025-10-06T20:18:03Z"
  },
  {
    "code": "@@ -8902,26 +8902,20 @@ def __init__(self) -> None:\n \n             def forward(self, start_pos: torch.Tensor):\n                 pos = start_pos.item()\n-                torch._check_is_size(pos)\n                 torch._check(pos >= 0)\n                 torch._check(pos <= 4)\n                 return self.freq[pos] * self.freq[pos]\n \n         ep = export(M(), (torch.tensor(1),))\n+        print(ep)",
    "comment": "debug cruft?",
    "line_number": 8910,
    "enriched": "File: test/export/test_export.py\nCode: @@ -8902,26 +8902,20 @@ def __init__(self) -> None:\n \n             def forward(self, start_pos: torch.Tensor):\n                 pos = start_pos.item()\n-                torch._check_is_size(pos)\n                 torch._check(pos >= 0)\n                 torch._check(pos <= 4)\n                 return self.freq[pos] * self.freq[pos]\n \n         ep = export(M(), (torch.tensor(1),))\n+        print(ep)\nComment: debug cruft?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "test/export/test_export.py",
    "pr_number": 164753,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2409064866,
    "comment_created_at": "2025-10-07T01:05:09Z"
  },
  {
    "code": "@@ -583,18 +578,7 @@ def _init_process_groups(\n \n                     # Respect dim group options specified via _MeshEnv.set_dim_group_options().",
    "comment": "nit: update this comment\n\nand check whether there are other ones?",
    "line_number": 579,
    "enriched": "File: torch/distributed/device_mesh.py\nCode: @@ -583,18 +578,7 @@ def _init_process_groups(\n \n                     # Respect dim group options specified via _MeshEnv.set_dim_group_options().\nComment: Nit: update this comment\n\nAnd check whether there are other ones?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "torch/distributed/device_mesh.py",
    "pr_number": 164750,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2409693269,
    "comment_created_at": "2025-10-07T07:40:27Z"
  },
  {
    "code": "@@ -30,6 +30,7 @@ class LinearReLU(nnqd.Linear):\n         torch.Size([128, 30])\n     \"\"\"\n \n+    # pyrefly: ignore  # bad-override\n     _FLOAT_MODULE = nni.LinearReLU",
    "comment": ",this is a legitimate error we should fix probably",
    "line_number": 34,
    "enriched": "File: torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\nCode: @@ -30,6 +30,7 @@ class LinearReLU(nnqd.Linear):\n         torch.Size([128, 30])\n     \"\"\"\n \n+    # pyrefly: ignore  # bad-override\n     _FLOAT_MODULE = nni.LinearReLU\nComment: ,This is a legitimate error we should fix probably",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py",
    "pr_number": 164748,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2407648019,
    "comment_created_at": "2025-10-06T17:41:14Z"
  },
  {
    "code": "@@ -1086,6 +1086,15 @@ def _is_make_fx_tracing():\n         return False\n \n \n+def _is_exporting():\n+    # More details at https://github.com/pytorch/pytorch/issues/164062\n+\n+    # The weird code is because of TorchScript not returning early on `and` conditions.\n+    if not torch.jit.is_scripting():",
    "comment": "maybe inline as not torch.jit.is_scripting() and torch.compiler.is_exporting()",
    "line_number": 1093,
    "enriched": "File: torch/nn/modules/activation.py\nCode: @@ -1086,6 +1086,15 @@ def _is_make_fx_tracing():\n         return False\n \n \n+def _is_exporting():\n+    # More details at https://github.com/pytorch/pytorch/issues/164062\n+\n+    # The weird code is because of TorchScript not returning early on `and` conditions.\n+    if not torch.jit.is_scripting():\nComment: maybe inline as `not torch.jit.is_scripting() and torch.compiler.is_exporting()`",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "torch/nn/modules/activation.py",
    "pr_number": 164721,
    "repo": "pytorch",
    "owner": "pytorch",
    "comment_id": 2407378818,
    "comment_created_at": "2025-10-06T16:40:17Z"
  },
  {
    "code": "@@ -74,13 +79,16 @@ jobs:\n           cache-dependency-glob: |\n             requirements**.txt\n             pyproject.toml\n+      - name: Preinstall Pydantic 2.12.0a1 for Python 3.14\n+        if: matrix.python-version == '3.14'\n+        run: uv pip install --pre \"pydantic==2.12.0a1\"",
    "comment": "this can be removed when a stable pydantic 2.12 release is available.",
    "line_number": 84,
    "enriched": "File: .github/workflows/test.yml\nCode: @@ -74,13 +79,16 @@ jobs:\n           cache-dependency-glob: |\n             requirements**.txt\n             pyproject.toml\n+      - name: Preinstall Pydantic 2.12.0a1 for Python 3.14\n+        if: matrix.python-version == '3.14'\n+        run: uv pip install --pre \"pydantic==2.12.0a1\"\nComment: This can be removed when a stable Pydantic 2.12 release is available.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": ".github/workflows/test.yml",
    "pr_number": 14110,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2376037999,
    "comment_created_at": "2025-09-24T14:39:21Z"
  },
  {
    "code": "@@ -10,7 +10,9 @@\n @app.exception_handler(RequestValidationError)\n async def validation_exception_handler(request: Request, exc: RequestValidationError):\n     return JSONResponse(\n-        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n+        # 422 = fastapi.status.HTTP_422_UNPROCESSABLE_CONTENT (RFC 9110,\n+        # Starlette >=0.48), previously HTTP_422_UNPROCESSABLE_ENTITY\n+        status_code=422,",
    "comment": "we have to be carefull here, this script is referenced in the docs [here](https://fastapi.tiangolo.com/tutorial/handling-errors/#use-the-requestvalidationerror-body), and by adding more lines, the wrong line will be highlighted in the docs (used to be l14 which is now l16).\r\n\r\nanyway, i don't think we need this comment here, or elsewhere where we're switching to 422. let's just edit to status_code=422 without the comments.",
    "line_number": 15,
    "enriched": "File: docs_src/handling_errors/tutorial005.py\nCode: @@ -10,7 +10,9 @@\n @app.exception_handler(RequestValidationError)\n async def validation_exception_handler(request: Request, exc: RequestValidationError):\n     return JSONResponse(\n-        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n+        # 422 = fastapi.status.HTTP_422_UNPROCESSABLE_CONTENT (RFC 9110,\n+        # Starlette >=0.48), previously HTTP_422_UNPROCESSABLE_ENTITY\n+        status_code=422,\nComment: We have to be carefull here, this script is referenced in the docs [here](https://fastapi.tiangolo.com/tutorial/handling-errors/#use-the-requestvalidationerror-body), and by adding more lines, the wrong line will be highlighted in the docs (used to be L14 which is now L16).\r\n\r\nAnyway, I don't think we need this comment here, or elsewhere where we're switching to 422. Let's just edit to `status_code=422` without the comments.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "docs_src/handling_errors/tutorial005.py",
    "pr_number": 14077,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2349101580,
    "comment_created_at": "2025-09-15T13:58:01Z"
  },
  {
    "code": "@@ -1,12 +1,52 @@\n-from typing import Any\n+from typing import Any, Optional\n+\n+from starlette.responses import FileResponse as StarletteFileResponse\n+from starlette.responses import HTMLResponse as StarletteHTMLResponse\n+from starlette.responses import JSONResponse as StarletteJSONResponse\n+from starlette.responses import PlainTextResponse as StarlettePlainTextResponse\n+from starlette.responses import RedirectResponse as StarletteRedirectResponse\n+from starlette.responses import Response as StarletteResponse\n+from starlette.responses import StreamingResponse as StarletteStreamingResponse\n+\n+\n+class Response(StarletteResponse):\n+    default_status_code: int = 200\n+\n+    def __init__(\n+        self,\n+        content: Any = None,\n+        *,",
    "comment": "looks like you are using [keyword-only arguments](https://peps.python.org/pep-3102/) feature in python,  it might be a breaking changes for people are currently not setting status_code as keyword argument. looking [starlette code ](https://github.com/kludex/starlette/blob/main/starlette/responses.py#l38), they also don't enforce it as keyword only argument. \r\n\r\ne.g. the response response(\"hello\", 404) is not going to work anymore.",
    "line_number": 18,
    "enriched": "File: fastapi/responses.py\nCode: @@ -1,12 +1,52 @@\n-from typing import Any\n+from typing import Any, Optional\n+\n+from starlette.responses import FileResponse as StarletteFileResponse\n+from starlette.responses import HTMLResponse as StarletteHTMLResponse\n+from starlette.responses import JSONResponse as StarletteJSONResponse\n+from starlette.responses import PlainTextResponse as StarlettePlainTextResponse\n+from starlette.responses import RedirectResponse as StarletteRedirectResponse\n+from starlette.responses import Response as StarletteResponse\n+from starlette.responses import StreamingResponse as StarletteStreamingResponse\n+\n+\n+class Response(StarletteResponse):\n+    default_status_code: int = 200\n+\n+    def __init__(\n+        self,\n+        content: Any = None,\n+        *,\nComment: Looks like you are using [Keyword-Only Arguments](https://peps.python.org/pep-3102/) feature in python,  It might be a breaking changes for people are currently not setting `status_code` as keyword argument. Looking [Starlette code ](https://github.com/Kludex/starlette/blob/main/starlette/responses.py#L38), they also don't enforce it as keyword only argument. \r\n\r\ne.g. the response `Response(\"Hello\", 404)` is not going to work anymore. ",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "fastapi/responses.py",
    "pr_number": 14075,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2370648463,
    "comment_created_at": "2025-09-22T23:38:57Z"
  },
  {
    "code": "@@ -19,13 +19,20 @@\n \n from fastapi.types import IncEx\n from pydantic import BaseModel\n-from pydantic.color import Color\n from pydantic.networks import AnyUrl, NameEmail\n from pydantic.types import SecretBytes, SecretStr\n+from pydantic_extra_types.color import Color  # \u2705 updated import",
    "comment": "you probably need to import this only when you need it for pydantic v2, as this will crash on pydantic v1.",
    "line_number": 24,
    "enriched": "File: fastapi/encoders.py\nCode: @@ -19,13 +19,20 @@\n \n from fastapi.types import IncEx\n from pydantic import BaseModel\n-from pydantic.color import Color\n from pydantic.networks import AnyUrl, NameEmail\n from pydantic.types import SecretBytes, SecretStr\n+from pydantic_extra_types.color import Color  # \u2705 updated import\nComment: You probably need to import this only when you need it for Pydantic v2, as this will crash on Pydantic v1.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "fastapi/encoders.py",
    "pr_number": 14060,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2343546755,
    "comment_created_at": "2025-09-12T09:09:30Z"
  },
  {
    "code": "@@ -44,6 +44,8 @@\n site_path = Path(\"site\").absolute()\n build_site_path = Path(\"site_build\").absolute()\n \n+header_with_permalink_pattern = re.compile(r\"^(#{1,6}) (.+?)(\\s*\\{\\s*#.*\\s*\\})\\s*$\")",
    "comment": "fancy regex! \ud83e\udd2f \ud83d\ude0e",
    "line_number": 47,
    "enriched": "File: scripts/docs.py\nCode: @@ -44,6 +44,8 @@\n site_path = Path(\"site\").absolute()\n build_site_path = Path(\"site_build\").absolute()\n \n+header_with_permalink_pattern = re.compile(r\"^(#{1,6}) (.+?)(\\s*\\{\\s*#.*\\s*\\})\\s*$\")\nComment: Fancy RegEx! \ud83e\udd2f \ud83d\ude0e ",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "scripts/docs.py",
    "pr_number": 14055,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2365598913,
    "comment_created_at": "2025-09-20T11:43:21Z"
  },
  {
    "code": "@@ -109,9 +110,24 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0a1 warns about this when building\n+                # TypeAdapters from field information that uses aliases.\n+                # The Pydantic team recommends ignoring this in this case:\n+                # https://github.com/fastapi/fastapi/pull/14036#issuecomment-3316045587",
    "comment": "suggestion\r\n                # pydantic >= 2.12.0 warns about field specific metadata that is unused\r\n                # (e.g. typeadapter(annotated[int, field(alias='b')])). in some cases, we\r\n                # end up building the type adapter from a model field annotation so we\r\n                # need to ignore the warning:",
    "line_number": 117,
    "enriched": "File: fastapi/_compat.py\nCode: @@ -109,9 +110,24 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0a1 warns about this when building\n+                # TypeAdapters from field information that uses aliases.\n+                # The Pydantic team recommends ignoring this in this case:\n+                # https://github.com/fastapi/fastapi/pull/14036#issuecomment-3316045587\nComment: ```suggestion\r\n                # Pydantic >= 2.12.0 warns about field specific metadata that is unused\r\n                # (e.g. `TypeAdapter(Annotated[int, Field(alias='b')])`). In some cases, we\r\n                # end up building the type adapter from a model field annotation so we\r\n                # need to ignore the warning:",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "fastapi/_compat.py",
    "pr_number": 14036,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2369192894,
    "comment_created_at": "2025-09-22T16:05:41Z"
  },
  {
    "code": "@@ -383,7 +383,7 @@ Starlette (\u0648 **FastAPI**) \u0628\u0631 \u067e\u0627\u06cc\u0647 <a href=\"https://anyio.readthedocs.io/e\n \n \u062a\u0648\u06cc \u0646\u0633\u062e\u0647\u200c\u0647\u0627\u06cc \u0642\u0628\u0644\u06cc \u067e\u0627\u06cc\u062a\u0648\u0646\u060c \u0645\u06cc\u200c\u062a\u0648\u0646\u0633\u062a\u06cc \u0627\u0632 \u0646\u062e\u200c\u0647\u0627 \u06cc\u0627 <a href=\"https://www.gevent.org/\" class=\"external-link\" target=\"_blank\">Gevent</a> \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u06a9\u0646\u06cc. \u0648\u0644\u06cc \u06a9\u062f \u062e\u06cc\u0644\u06cc \u067e\u06cc\u0686\u06cc\u062f\u0647\u200c\u062a\u0631 \u0645\u06cc\u200c\u0634\u0647 \u0628\u0631\u0627\u06cc \u0641\u0647\u0645\u06cc\u062f\u0646\u060c \u062f\u06cc\u0628\u0627\u06af \u06a9\u0631\u062f\u0646 \u0648 \u0641\u06a9\u0631 \u06a9\u0631\u062f\u0646 \u0628\u0647\u0634.\n \n-\u062a\u0648\u06cc \u0646\u0633\u062e\u0647\u200c\u0647\u0627\u06cc \u0642\u0628\u0644\u06cc NodeJS / \u062c\u0627\u0648\u0627\u0627\u0633\u06a9\u0631\u06cc\u067e\u062a \u0645\u0631\u0648\u0631\u06af\u0631\u060c \u0627\u0632 \"\u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627\" \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0645\u06cc\u200c\u06a9\u0631\u062f\u06cc. \u06a9\u0647 \u0645\u06cc\u200c\u0631\u0633\u06cc\u062f \u0628\u0647 <a href=\"http://callbackhell.com/\" class=\"external-link\" target=\"_blank\">\u062c\u0647\u0627\u0646 \u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627</a>.\n+\u062a\u0648\u06cc \u0646\u0633\u062e\u0647\u200c\u0647\u0627\u06cc \u0642\u0628\u0644\u06cc NodeJS / \u062c\u0627\u0648\u0627\u0627\u0633\u06a9\u0631\u06cc\u067e\u062a \u0645\u0631\u0648\u0631\u06af\u0631\u060c \u0627\u0632 \"\u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627\" \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0645\u06cc\u200c\u06a9\u0631\u062f\u06cc. \u06a9\u0647 \u0645\u06cc\u200c\u0631\u0633\u06cc\u062f \u0628\u0647 \"\u062c\u0647\u0627\u0646 \u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627\".",
    "comment": "this _looks_ incorrect when viewing the github diff (one of the quote marks seem to be placed incorrectly) but when viewing the actual doc page, it seems fine.",
    "line_number": 386,
    "enriched": "File: docs/fa/docs/async.md\nCode: @@ -383,7 +383,7 @@ Starlette (\u0648 **FastAPI**) \u0628\u0631 \u067e\u0627\u06cc\u0647 <a href=\"https://anyio.readthedocs.io/e\n \n \u062a\u0648\u06cc \u0646\u0633\u062e\u0647\u200c\u0647\u0627\u06cc \u0642\u0628\u0644\u06cc \u067e\u0627\u06cc\u062a\u0648\u0646\u060c \u0645\u06cc\u200c\u062a\u0648\u0646\u0633\u062a\u06cc \u0627\u0632 \u0646\u062e\u200c\u0647\u0627 \u06cc\u0627 <a href=\"https://www.gevent.org/\" class=\"external-link\" target=\"_blank\">Gevent</a> \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u06a9\u0646\u06cc. \u0648\u0644\u06cc \u06a9\u062f \u062e\u06cc\u0644\u06cc \u067e\u06cc\u0686\u06cc\u062f\u0647\u200c\u062a\u0631 \u0645\u06cc\u200c\u0634\u0647 \u0628\u0631\u0627\u06cc \u0641\u0647\u0645\u06cc\u062f\u0646\u060c \u062f\u06cc\u0628\u0627\u06af \u06a9\u0631\u062f\u0646 \u0648 \u0641\u06a9\u0631 \u06a9\u0631\u062f\u0646 \u0628\u0647\u0634.\n \n-\u062a\u0648\u06cc \u0646\u0633\u062e\u0647\u200c\u0647\u0627\u06cc \u0642\u0628\u0644\u06cc NodeJS / \u062c\u0627\u0648\u0627\u0627\u0633\u06a9\u0631\u06cc\u067e\u062a \u0645\u0631\u0648\u0631\u06af\u0631\u060c \u0627\u0632 \"\u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627\" \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0645\u06cc\u200c\u06a9\u0631\u062f\u06cc. \u06a9\u0647 \u0645\u06cc\u200c\u0631\u0633\u06cc\u062f \u0628\u0647 <a href=\"http://callbackhell.com/\" class=\"external-link\" target=\"_blank\">\u062c\u0647\u0627\u0646 \u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627</a>.\n+\u062a\u0648\u06cc \u0646\u0633\u062e\u0647\u200c\u0647\u0627\u06cc \u0642\u0628\u0644\u06cc NodeJS / \u062c\u0627\u0648\u0627\u0627\u0633\u06a9\u0631\u06cc\u067e\u062a \u0645\u0631\u0648\u0631\u06af\u0631\u060c \u0627\u0632 \"\u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627\" \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0645\u06cc\u200c\u06a9\u0631\u062f\u06cc. \u06a9\u0647 \u0645\u06cc\u200c\u0631\u0633\u06cc\u062f \u0628\u0647 \"\u062c\u0647\u0627\u0646 \u06a9\u0627\u0644\u200c\u0628\u06a9\u200c\u0647\u0627\".\nComment: This _looks_ incorrect when viewing the github diff (one of the quote marks seem to be placed incorrectly) but when viewing the actual doc page, it seems fine.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/fa/docs/async.md",
    "pr_number": 14006,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2297470814,
    "comment_created_at": "2025-08-25T08:34:47Z"
  },
  {
    "code": "@@ -106,7 +106,7 @@ See the similarities in `requests.get(...)` and `@app.get(...)`.\n \n ///\n \n-### <a href=\"https://swagger.io/\" class=\"external-link\" target=\"_blank\">Swagger</a> / <a href=\"https://github.com/OAI/OpenAPI-Specification/\" class=\"external-link\" target=\"_blank\">OpenAPI</a>\n+### <a href=\"https://swagger.io/\" class=\"external-link\" target=\"_blank\">Swagger</a> / <a href=\"https://github.com/OAI/OpenAPI-Specification/\" class=\"external-link\" target=\"_blank\">OpenAPI</a> { #swagger-openapi }",
    "comment": "this is an example of how the links inside headers are handled",
    "line_number": 109,
    "enriched": "File: docs/en/docs/alternatives.md\nCode: @@ -106,7 +106,7 @@ See the similarities in `requests.get(...)` and `@app.get(...)`.\n \n ///\n \n-### <a href=\"https://swagger.io/\" class=\"external-link\" target=\"_blank\">Swagger</a> / <a href=\"https://github.com/OAI/OpenAPI-Specification/\" class=\"external-link\" target=\"_blank\">OpenAPI</a>\n+### <a href=\"https://swagger.io/\" class=\"external-link\" target=\"_blank\">Swagger</a> / <a href=\"https://github.com/OAI/OpenAPI-Specification/\" class=\"external-link\" target=\"_blank\">OpenAPI</a> { #swagger-openapi }\nComment: This is an example of how the links inside headers are handled",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/en/docs/alternatives.md",
    "pr_number": 13993,
    "repo": "fastapi",
    "owner": "tiangolo",
    "comment_id": 2280735255,
    "comment_created_at": "2025-08-17T05:58:47Z"
  },
  {
    "code": "@@ -0,0 +1,146 @@\n+\"\"\"\n+Tests for the dont_encode_url parameter in Request class.\n+Related to issue #7028 and #833.\n+\"\"\"\n+\n+import pytest\n+\n+from scrapy.http import Request\n+\n+\n+class TestRequestDontEncodeUrl:\n+    \"\"\"Tests for Request.dont_encode_url parameter\"\"\"\n+\n+    def test_url_encoding_default_behavior(self):\n+        \"\"\"Test that URL encoding is enabled by default\"\"\"\n+        # This URL should pass through safe_url_string\n+        url = \"http://example.com/page\"\n+        req = Request(url)\n+        # By default, safe_url_string is called\n+        assert req.url == url\n+        assert req.dont_encode_url is False\n+\n+    def test_url_with_slashes_in_query_params_default(self):\n+        \"\"\"Test that slashes in query params are preserved by default with safe_url_string\"\"\"",
    "comment": "nice, so why is this change needed?",
    "line_number": 24,
    "enriched": "File: tests/test_request_dont_encode_url.py\nCode: @@ -0,0 +1,146 @@\n+\"\"\"\n+Tests for the dont_encode_url parameter in Request class.\n+Related to issue #7028 and #833.\n+\"\"\"\n+\n+import pytest\n+\n+from scrapy.http import Request\n+\n+\n+class TestRequestDontEncodeUrl:\n+    \"\"\"Tests for Request.dont_encode_url parameter\"\"\"\n+\n+    def test_url_encoding_default_behavior(self):\n+        \"\"\"Test that URL encoding is enabled by default\"\"\"\n+        # This URL should pass through safe_url_string\n+        url = \"http://example.com/page\"\n+        req = Request(url)\n+        # By default, safe_url_string is called\n+        assert req.url == url\n+        assert req.dont_encode_url is False\n+\n+    def test_url_with_slashes_in_query_params_default(self):\n+        \"\"\"Test that slashes in query params are preserved by default with safe_url_string\"\"\"\nComment: Nice, so why is this change needed?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/test_request_dont_encode_url.py",
    "pr_number": 7063,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2393602669,
    "comment_created_at": "2025-10-01T06:44:36Z"
  },
  {
    "code": "@@ -59,4 +59,17 @@ And follow the documentation_ to learn how to use it.\n \n If you wish to contribute, see Contributing_.\n \n+\n .. _Contributing: https://docs.scrapy.org/en/master/contributing.html\n+\n+Community & Support",
    "comment": "this duplicates https://docs.scrapy.org/en/latest/#getting-help",
    "line_number": 65,
    "enriched": "File: README.rst\nCode: @@ -59,4 +59,17 @@ And follow the documentation_ to learn how to use it.\n \n If you wish to contribute, see Contributing_.\n \n+\n .. _Contributing: https://docs.scrapy.org/en/master/contributing.html\n+\n+Community & Support\nComment: This duplicates https://docs.scrapy.org/en/latest/#getting-help",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "README.rst",
    "pr_number": 7053,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2360041187,
    "comment_created_at": "2025-09-18T15:56:59Z"
  },
  {
    "code": "@@ -135,6 +135,9 @@ branch = true\n include = [\"scrapy/*\"]\n omit = [\"tests/*\"]\n disable_warnings = [\"include-ignored\"]\n+patch = [",
    "comment": "https://pytest-cov.readthedocs.io/en/latest/subprocess-support.html",
    "line_number": 138,
    "enriched": "File: pyproject.toml\nCode: @@ -135,6 +135,9 @@ branch = true\n include = [\"scrapy/*\"]\n omit = [\"tests/*\"]\n disable_warnings = [\"include-ignored\"]\n+patch = [\nComment: https://pytest-cov.readthedocs.io/en/latest/subprocess-support.html",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "pyproject.toml",
    "pr_number": 7050,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2346882000,
    "comment_created_at": "2025-09-13T18:15:56Z"
  },
  {
    "code": "@@ -414,11 +414,11 @@ def test_crawl_multiple(self):\n             mockserver=self.mockserver,\n         )\n \n-        with LogCapture() as log:",
    "comment": "it looked liked logcapture() changes the root handler.",
    "line_number": 417,
    "enriched": "File: tests/test_crawl.py\nCode: @@ -414,11 +414,11 @@ def test_crawl_multiple(self):\n             mockserver=self.mockserver,\n         )\n \n-        with LogCapture() as log:\nComment: It looked liked `LogCapture()` changes the root handler.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "tests/test_crawl.py",
    "pr_number": 7046,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2331143909,
    "comment_created_at": "2025-09-08T19:13:50Z"
  },
  {
    "code": "@@ -28,9 +28,15 @@ def __init__(self, crawler: Crawler):\n         self._crawler: Crawler = crawler\n \n     def __getattribute__(self, name):\n+        cached_name = f\"_cached_{name}\"\n+        try:\n+            return super().__getattribute__(cached_name)",
    "comment": "til hasattr() \"is implemented by calling getattr(object, name) and seeing whether it raises an attributeerror or not\"",
    "line_number": 33,
    "enriched": "File: scrapy/statscollectors.py\nCode: @@ -28,9 +28,15 @@ def __init__(self, crawler: Crawler):\n         self._crawler: Crawler = crawler\n \n     def __getattribute__(self, name):\n+        cached_name = f\"_cached_{name}\"\n+        try:\n+            return super().__getattribute__(cached_name)\nComment: TIL `hasattr()` \"is implemented by calling `getattr(object, name)` and seeing whether it raises an `AttributeError` or not\"",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "scrapy/statscollectors.py",
    "pr_number": 7045,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2330033772,
    "comment_created_at": "2025-09-08T12:02:32Z"
  },
  {
    "code": "@@ -21,29 +22,34 @@\n \n \n class OffsiteMiddleware:\n+    crawler: Crawler\n+\n+    def __init__(self, stats: StatsCollector):\n+        self.stats = stats\n+        self.domains_seen: set[str] = set()\n+\n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         assert crawler.stats\n         o = cls(crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.request_scheduled, signal=signals.request_scheduled)\n+        o.crawler = crawler\n         return o\n \n-    def __init__(self, stats: StatsCollector):\n-        self.stats = stats\n-        self.domains_seen: set[str] = set()",
    "comment": "for the record, i kind of like the __init__ after the from_crawler, in order of execution :slightly_smiling_face:",
    "line_number": 34,
    "enriched": "File: scrapy/downloadermiddlewares/offsite.py\nCode: @@ -21,29 +22,34 @@\n \n \n class OffsiteMiddleware:\n+    crawler: Crawler\n+\n+    def __init__(self, stats: StatsCollector):\n+        self.stats = stats\n+        self.domains_seen: set[str] = set()\n+\n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         assert crawler.stats\n         o = cls(crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.request_scheduled, signal=signals.request_scheduled)\n+        o.crawler = crawler\n         return o\n \n-    def __init__(self, stats: StatsCollector):\n-        self.stats = stats\n-        self.domains_seen: set[str] = set()\nComment: For the record, I kind of like the `__init__` after the `from_crawler`, in order of execution :slightly_smiling_face:",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "scrapy/downloadermiddlewares/offsite.py",
    "pr_number": 7037,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2314576819,
    "comment_created_at": "2025-09-01T21:16:59Z"
  },
  {
    "code": "@@ -65,3 +66,68 @@ def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n         return deferToThread(func, *a, **kw)\n \n     return wrapped\n+\n+\n+@overload\n+def _warn_spider_arg(\n+    func: Callable[_P, Coroutine[Any, Any, _T]],\n+) -> Callable[_P, Coroutine[Any, Any, _T]]: ...\n+\n+\n+@overload\n+def _warn_spider_arg(\n+    func: Callable[_P, AsyncGenerator[_T]],\n+) -> Callable[_P, AsyncGenerator[_T]]: ...\n+\n+\n+@overload\n+def _warn_spider_arg(func: Callable[_P, _T]) -> Callable[_P, _T]: ...\n+\n+\n+def _warn_spider_arg(\n+    func: Callable[_P, _T],\n+) -> (\n+    Callable[_P, _T]\n+    | Callable[_P, Coroutine[Any, Any, _T]]\n+    | Callable[_P, AsyncGenerator[_T]]\n+):\n+    \"\"\"Decorator to warn if a (non-None) ``spider`` argument is passed to a function.\"\"\"\n+\n+    def check_args(*args: _P.args, **kwargs: _P.kwargs) -> None:\n+        bound = inspect.signature(func).bind(*args, **kwargs)\n+        bound.apply_defaults()\n+        if bound.arguments.get(\"spider\"):\n+            warnings.warn(\n+                f\"Passing a 'spider' argument to {func.__qualname__}() is deprecated and \"\n+                \"the argument will be removed in a future Scrapy version.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=3,\n+            )\n+\n+    if inspect.iscoroutinefunction(func):",
    "comment": "ok, this is not used/covered because the only async functions that are decorated are process_spider_output_async() which are async generator functions.",
    "line_number": 107,
    "enriched": "File: scrapy/utils/decorators.py\nCode: @@ -65,3 +66,68 @@ def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n         return deferToThread(func, *a, **kw)\n \n     return wrapped\n+\n+\n+@overload\n+def _warn_spider_arg(\n+    func: Callable[_P, Coroutine[Any, Any, _T]],\n+) -> Callable[_P, Coroutine[Any, Any, _T]]: ...\n+\n+\n+@overload\n+def _warn_spider_arg(\n+    func: Callable[_P, AsyncGenerator[_T]],\n+) -> Callable[_P, AsyncGenerator[_T]]: ...\n+\n+\n+@overload\n+def _warn_spider_arg(func: Callable[_P, _T]) -> Callable[_P, _T]: ...\n+\n+\n+def _warn_spider_arg(\n+    func: Callable[_P, _T],\n+) -> (\n+    Callable[_P, _T]\n+    | Callable[_P, Coroutine[Any, Any, _T]]\n+    | Callable[_P, AsyncGenerator[_T]]\n+):\n+    \"\"\"Decorator to warn if a (non-None) ``spider`` argument is passed to a function.\"\"\"\n+\n+    def check_args(*args: _P.args, **kwargs: _P.kwargs) -> None:\n+        bound = inspect.signature(func).bind(*args, **kwargs)\n+        bound.apply_defaults()\n+        if bound.arguments.get(\"spider\"):\n+            warnings.warn(\n+                f\"Passing a 'spider' argument to {func.__qualname__}() is deprecated and \"\n+                \"the argument will be removed in a future Scrapy version.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=3,\n+            )\n+\n+    if inspect.iscoroutinefunction(func):\nComment: OK, this is not used/covered because the only async functions that are decorated are `process_spider_output_async()` which are async generator functions.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "scrapy/utils/decorators.py",
    "pr_number": 7033,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2310684726,
    "comment_created_at": "2025-08-29T17:07:25Z"
  },
  {
    "code": "@@ -45,4 +55,38 @@ def process_item(self, item: Any, spider: Spider | None = None) -> Deferred[Any]\n         return deferred_from_coro(self.process_item_async(item))\n \n     async def process_item_async(self, item: Any) -> Any:\n-        return await self._process_chain(\"process_item\", item, self._spider)\n+        return await self._process_chain(\"process_item\", item, add_spider=True)\n+\n+    def _process_parallel(\n+        self, methodname: str, add_spider: bool = False",
    "comment": "what\u2019s the point of add_spider? it seems to be always true at the moment, and the method is private.",
    "line_number": 61,
    "enriched": "File: scrapy/pipelines/__init__.py\nCode: @@ -45,4 +55,38 @@ def process_item(self, item: Any, spider: Spider | None = None) -> Deferred[Any]\n         return deferred_from_coro(self.process_item_async(item))\n \n     async def process_item_async(self, item: Any) -> Any:\n-        return await self._process_chain(\"process_item\", item, self._spider)\n+        return await self._process_chain(\"process_item\", item, add_spider=True)\n+\n+    def _process_parallel(\n+        self, methodname: str, add_spider: bool = False\nComment: What\u2019s the point of `add_spider`? It seems to be always `True` at the moment, and the method is private.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "scrapy/pipelines/__init__.py",
    "pr_number": 7006,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2289042365,
    "comment_created_at": "2025-08-20T19:04:50Z"
  },
  {
    "code": "@@ -109,12 +110,38 @@ def __init__(self, crawler: Crawler) -> None:\n             crawler.settings[\"ITEM_PROCESSOR\"]\n         )\n         self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)\n-        self._itemproc_needs_spider: dict[str, bool] = {}\n-        for method in (\n+        itemproc_methods = [\n             \"open_spider\",\n             \"close_spider\",\n-            \"process_item\",\n+        ]\n+        if not hasattr(self.itemproc, \"process_item_async\"):\n+            warnings.warn(\n+                f\"{global_object_name(itemproc_cls)} doesn't define a process_item_async() method,\"\n+                f\" this is deprecated and the method will be required in the future Scrapy versions.\",\n+                ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+            itemproc_methods.append(\"process_item\")\n+            self._itemproc_has_process_async = False\n+        elif (\n+            issubclass(itemproc_cls, ItemPipelineManager)\n+            and method_is_overridden(itemproc_cls, ItemPipelineManager, \"process_item\")\n+            and not method_is_overridden(\n+                itemproc_cls, ItemPipelineManager, \"process_item_async\"\n+            )\n         ):\n+            warnings.warn(\n+                f\"{global_object_name(itemproc_cls)} overrides process_item() but doesn't override process_item_async().\"\n+                f\" This is deprecated. process_item() will be used, but in the future Scrapy versions process_item_async() will be used instead.\",",
    "comment": "\r\n\r\n(applies to similar strings as well)",
    "line_number": 135,
    "enriched": "File: scrapy/core/scraper.py\nCode: @@ -109,12 +110,38 @@ def __init__(self, crawler: Crawler) -> None:\n             crawler.settings[\"ITEM_PROCESSOR\"]\n         )\n         self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)\n-        self._itemproc_needs_spider: dict[str, bool] = {}\n-        for method in (\n+        itemproc_methods = [\n             \"open_spider\",\n             \"close_spider\",\n-            \"process_item\",\n+        ]\n+        if not hasattr(self.itemproc, \"process_item_async\"):\n+            warnings.warn(\n+                f\"{global_object_name(itemproc_cls)} doesn't define a process_item_async() method,\"\n+                f\" this is deprecated and the method will be required in the future Scrapy versions.\",\n+                ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+            itemproc_methods.append(\"process_item\")\n+            self._itemproc_has_process_async = False\n+        elif (\n+            issubclass(itemproc_cls, ItemPipelineManager)\n+            and method_is_overridden(itemproc_cls, ItemPipelineManager, \"process_item\")\n+            and not method_is_overridden(\n+                itemproc_cls, ItemPipelineManager, \"process_item_async\"\n+            )\n         ):\n+            warnings.warn(\n+                f\"{global_object_name(itemproc_cls)} overrides process_item() but doesn't override process_item_async().\"\n+                f\" This is deprecated. process_item() will be used, but in the future Scrapy versions process_item_async() will be used instead.\",\nComment: ```suggestion\r\n                f\" This is deprecated. process_item() will be used, but in future Scrapy versions process_item_async() will be used instead.\",\r\n```\r\n\r\n(applies to similar strings as well)",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "scrapy/core/scraper.py",
    "pr_number": 7005,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2269044848,
    "comment_created_at": "2025-08-12T08:06:32Z"
  },
  {
    "code": "@@ -84,7 +85,7 @@ def run(self, args: list[str], opts: Namespace) -> None:\n         crawler._apply_settings()\n         # The Shell class needs a persistent engine in the crawler\n         crawler.engine = crawler._create_engine()\n-        crawler.engine.start(_start_request_processing=False)\n+        deferred_from_coro(crawler.engine.start_async(_start_request_processing=False))",
    "comment": "deferred_from_coro() here and in some other places is used as a shortcut for \"if using asyncio, create a task for the coro\", as just calling the coro wouldn't schedule it; we could add a private helper function instead, or change all these places to keep the reference to the returned deferred so that the code doesn't look weird (and to add exception handling)",
    "line_number": 88,
    "enriched": "File: scrapy/commands/shell.py\nCode: @@ -84,7 +85,7 @@ def run(self, args: list[str], opts: Namespace) -> None:\n         crawler._apply_settings()\n         # The Shell class needs a persistent engine in the crawler\n         crawler.engine = crawler._create_engine()\n-        crawler.engine.start(_start_request_processing=False)\n+        deferred_from_coro(crawler.engine.start_async(_start_request_processing=False))\nComment: `deferred_from_coro()` here and in some other places is used as a shortcut for \"if using asyncio, create a task for the coro\", as just calling the coro wouldn't schedule it; we could add a private helper function instead, or change all these places to keep the reference to the returned deferred so that the code doesn't look weird (and to add exception handling)",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "scrapy/commands/shell.py",
    "pr_number": 6979,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2243709659,
    "comment_created_at": "2025-07-30T19:48:37Z"
  },
  {
    "code": "@@ -802,3 +804,20 @@ def from_crawler(cls, crawler):\n             assert len(w) == 0\n             assert pipe.store\n             assert pipe._from_crawler_called\n+\n+\n+@pytest.mark.parametrize(\"store\", [None, \"\"])\n+def test_files_pipeline_raises_notconfigured_when_files_store_invalid(\n+    monkeypatch, store\n+):\n+    settings = Settings()\n+\n+    if store is None:\n+        monkeypatch.delenv(\"IMAGES_STORE\", raising=False)\n+    else:\n+        monkeypatch.setenv(\"IMAGES_STORE\", store)",
    "comment": "why is this needed?",
    "line_number": 818,
    "enriched": "File: tests/test_pipeline_files.py\nCode: @@ -802,3 +804,20 @@ def from_crawler(cls, crawler):\n             assert len(w) == 0\n             assert pipe.store\n             assert pipe._from_crawler_called\n+\n+\n+@pytest.mark.parametrize(\"store\", [None, \"\"])\n+def test_files_pipeline_raises_notconfigured_when_files_store_invalid(\n+    monkeypatch, store\n+):\n+    settings = Settings()\n+\n+    if store is None:\n+        monkeypatch.delenv(\"IMAGES_STORE\", raising=False)\n+    else:\n+        monkeypatch.setenv(\"IMAGES_STORE\", store)\nComment: Why is this needed?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/test_pipeline_files.py",
    "pr_number": 6969,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2236173691,
    "comment_created_at": "2025-07-28T12:20:44Z"
  },
  {
    "code": "@@ -48,6 +55,12 @@ def _py_files(folder):\n     )\n \n \n+@pytest.fixture(scope=\"session\")\n+def mockserver() -> Generator[MockServer]:",
    "comment": "til you can skip the 2nd and 3rd param of generator :facepalm:",
    "line_number": 59,
    "enriched": "File: conftest.py\nCode: @@ -48,6 +55,12 @@ def _py_files(folder):\n     )\n \n \n+@pytest.fixture(scope=\"session\")\n+def mockserver() -> Generator[MockServer]:\nComment: TIL you can skip the 2nd and 3rd param of `Generator` :facepalm: ",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "conftest.py",
    "pr_number": 6960,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2230361719,
    "comment_created_at": "2025-07-25T07:22:47Z"
  },
  {
    "code": "@@ -91,12 +93,12 @@ def test_twistederrors(self):\n             ConnectError,\n             ConnectionDone,\n             ConnectionLost,\n-            ConnectionRefusedError,\n+            TxConnectionRefusedError,\n             defer.TimeoutError,\n             DNSLookupError,\n             ResponseFailed,\n             TCPTimedOutError,\n-            TimeoutError,\n+            TxTimeoutError,",
    "comment": "this checked builtin exceptions instead of twisted ones, the test was passing because the builtin exceptions are subclasses of oserror which is retried.",
    "line_number": 101,
    "enriched": "File: tests/test_downloadermiddleware_retry.py\nCode: @@ -91,12 +93,12 @@ def test_twistederrors(self):\n             ConnectError,\n             ConnectionDone,\n             ConnectionLost,\n-            ConnectionRefusedError,\n+            TxConnectionRefusedError,\n             defer.TimeoutError,\n             DNSLookupError,\n             ResponseFailed,\n             TCPTimedOutError,\n-            TimeoutError,\n+            TxTimeoutError,\nComment: This checked builtin exceptions instead of Twisted ones, the test was passing because the builtin exceptions are subclasses of OSError which is retried.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "tests/test_downloadermiddleware_retry.py",
    "pr_number": 6942,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2187298147,
    "comment_created_at": "2025-07-05T13:53:02Z"
  },
  {
    "code": "@@ -394,6 +390,9 @@ banned-module-level-imports = [\n     \"twisted.internet.reactor\",\n ]\n \n+[tool.ruff.lint.isort]\n+split-on-trailing-comma = false",
    "comment": "this forces short multiline imports to be folded back which i recently noticed to not be done by default. there is[ a similar option for all other comma-separated language things](https://docs.astral.sh/ruff/settings/#format_skip-magic-trailing-comma), but the diff without that one is much bigger and i think more controversial, i need to think on what do we want to do.",
    "line_number": 394,
    "enriched": "File: pyproject.toml\nCode: @@ -394,6 +390,9 @@ banned-module-level-imports = [\n     \"twisted.internet.reactor\",\n ]\n \n+[tool.ruff.lint.isort]\n+split-on-trailing-comma = false\nComment: This forces short multiline imports to be folded back which I recently noticed to not be done by default. There is[ a similar option for all other comma-separated language things](https://docs.astral.sh/ruff/settings/#format_skip-magic-trailing-comma), but the diff without that one is much bigger and I think more controversial, I need to think on what do we want to do.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "pyproject.toml",
    "pr_number": 6941,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2186943055,
    "comment_created_at": "2025-07-05T07:57:27Z"
  },
  {
    "code": "@@ -218,96 +214,98 @@ def setUp(self):\n         r.putChild(b\"payload\", PayloadResource())\n         r.putChild(b\"broken\", BrokenDownloadResource())\n         r.putChild(b\"encoding\", EncodingResource())\n-        self.site = server.Site(r, timeout=None)\n-        self.wrapper = WrappingFactory(self.site)\n-        self.port = self._listen(self.wrapper)\n-        self.portno = self.port.getHost().port\n+        site = server.Site(r, timeout=None)\n+        return WrappingFactory(site)\n \n-    @inlineCallbacks\n-    def tearDown(self):\n-        yield self.port.stopListening()\n-        shutil.rmtree(self.tmpname)\n+    @async_yield_fixture\n+    async def server_port(self, wrapper):\n+        port = self._listen(wrapper)\n+\n+        yield port.getHost().port\n+\n+        await port.stopListening()\n \n-    def getURL(self, path):\n-        return f\"http://127.0.0.1:{self.portno}/{path}\"\n+    @pytest.fixture\n+    def server_url(self, server_port):\n+        return f\"http://127.0.0.1:{server_port}/\"\n \n     @inlineCallbacks\n-    def testPayload(self):\n+    def testPayload(self, server_url):\n         s = \"0123456789\" * 10\n-        body = yield getPage(self.getURL(\"payload\"), body=s)\n+        body = yield getPage(server_url + \"payload\", body=s)\n         assert body == to_bytes(s)\n \n     @inlineCallbacks\n-    def testHostHeader(self):\n+    def testHostHeader(self, server_port, server_url):\n         # if we pass Host header explicitly, it should be used, otherwise\n         # it should extract from url\n-        body = yield getPage(self.getURL(\"host\"))\n-        assert body == to_bytes(f\"127.0.0.1:{self.portno}\")\n-        body = yield getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"})\n+        body = yield getPage(server_url + \"host\")\n+        assert body == to_bytes(f\"127.0.0.1:{server_port}\")\n+        body = yield getPage(server_url + \"host\", headers={\"Host\": \"www.example.com\"})\n         assert body == to_bytes(\"www.example.com\")\n \n     @inlineCallbacks\n-    def test_getPage(self):\n+    def test_getPage(self, server_url):\n         \"\"\"\n         L{client.getPage} returns a L{Deferred} which is called back with\n         the body of the response if the default method B{GET} is used.\n         \"\"\"\n-        body = yield getPage(self.getURL(\"file\"))\n+        body = yield getPage(server_url + \"file\")\n         assert body == b\"0123456789\"\n \n     @inlineCallbacks\n-    def test_getPageHead(self):\n+    def test_getPageHead(self, server_url):\n         \"\"\"\n         L{client.getPage} returns a L{Deferred} which is called back with\n         the empty string if the method is C{HEAD} and there is a successful\n         response code.\n         \"\"\"\n \n         def _getPage(method):\n-            return getPage(self.getURL(\"file\"), method=method)\n+            return getPage(server_url + \"file\", method=method)\n \n         body = yield _getPage(\"head\")\n         assert body == b\"\"\n         body = yield _getPage(\"HEAD\")\n         assert body == b\"\"\n \n     @inlineCallbacks\n-    def test_timeoutNotTriggering(self):\n+    def test_timeoutNotTriggering(self, server_port, server_url):\n         \"\"\"\n         When a non-zero timeout is passed to L{getPage} and the page is\n         retrieved before the timeout period elapses, the L{Deferred} is\n         called back with the contents of the page.\n         \"\"\"\n-        body = yield getPage(self.getURL(\"host\"), timeout=100)\n-        assert body == to_bytes(f\"127.0.0.1:{self.portno}\")\n+        body = yield getPage(server_url + \"host\", timeout=100)\n+        assert body == to_bytes(f\"127.0.0.1:{server_port}\")\n \n     @inlineCallbacks\n-    def test_timeoutTriggering(self):\n+    def test_timeoutTriggering(self, wrapper, server_url):\n         \"\"\"\n         When a non-zero timeout is passed to L{getPage} and that many\n         seconds elapse before the server responds to the request. the\n         L{Deferred} is errbacked with a L{error.TimeoutError}.\n         \"\"\"\n         with pytest.raises(defer.TimeoutError):\n-            yield getPage(self.getURL(\"wait\"), timeout=0.000001)\n+            yield getPage(server_url + \"wait\", timeout=0.000001)\n         # Clean up the server which is hanging around not doing\n         # anything.\n-        connected = list(self.wrapper.protocols.keys())\n+        connected = list(wrapper.protocols.keys())\n         # There might be nothing here if the server managed to already see\n         # that the connection was lost.\n         if connected:\n             connected[0].transport.loseConnection()\n \n     @inlineCallbacks",
    "comment": "this test uses yield getpage(...) but is not decorated with @inlinecallbacks, so the deferred won\u2019t be awaited. add @inlinecallbacks above the method or convert it to async def and use await.\n",
    "line_number": 299,
    "enriched": "File: tests/test_webclient.py\nCode: @@ -218,96 +214,98 @@ def setUp(self):\n         r.putChild(b\"payload\", PayloadResource())\n         r.putChild(b\"broken\", BrokenDownloadResource())\n         r.putChild(b\"encoding\", EncodingResource())\n-        self.site = server.Site(r, timeout=None)\n-        self.wrapper = WrappingFactory(self.site)\n-        self.port = self._listen(self.wrapper)\n-        self.portno = self.port.getHost().port\n+        site = server.Site(r, timeout=None)\n+        return WrappingFactory(site)\n \n-    @inlineCallbacks\n-    def tearDown(self):\n-        yield self.port.stopListening()\n-        shutil.rmtree(self.tmpname)\n+    @async_yield_fixture\n+    async def server_port(self, wrapper):\n+        port = self._listen(wrapper)\n+\n+        yield port.getHost().port\n+\n+        await port.stopListening()\n \n-    def getURL(self, path):\n-        return f\"http://127.0.0.1:{self.portno}/{path}\"\n+    @pytest.fixture\n+    def server_url(self, server_port):\n+        return f\"http://127.0.0.1:{server_port}/\"\n \n     @inlineCallbacks\n-    def testPayload(self):\n+    def testPayload(self, server_url):\n         s = \"0123456789\" * 10\n-        body = yield getPage(self.getURL(\"payload\"), body=s)\n+        body = yield getPage(server_url + \"payload\", body=s)\n         assert body == to_bytes(s)\n \n     @inlineCallbacks\n-    def testHostHeader(self):\n+    def testHostHeader(self, server_port, server_url):\n         # if we pass Host header explicitly, it should be used, otherwise\n         # it should extract from url\n-        body = yield getPage(self.getURL(\"host\"))\n-        assert body == to_bytes(f\"127.0.0.1:{self.portno}\")\n-        body = yield getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"})\n+        body = yield getPage(server_url + \"host\")\n+        assert body == to_bytes(f\"127.0.0.1:{server_port}\")\n+        body = yield getPage(server_url + \"host\", headers={\"Host\": \"www.example.com\"})\n         assert body == to_bytes(\"www.example.com\")\n \n     @inlineCallbacks\n-    def test_getPage(self):\n+    def test_getPage(self, server_url):\n         \"\"\"\n         L{client.getPage} returns a L{Deferred} which is called back with\n         the body of the response if the default method B{GET} is used.\n         \"\"\"\n-        body = yield getPage(self.getURL(\"file\"))\n+        body = yield getPage(server_url + \"file\")\n         assert body == b\"0123456789\"\n \n     @inlineCallbacks\n-    def test_getPageHead(self):\n+    def test_getPageHead(self, server_url):\n         \"\"\"\n         L{client.getPage} returns a L{Deferred} which is called back with\n         the empty string if the method is C{HEAD} and there is a successful\n         response code.\n         \"\"\"\n \n         def _getPage(method):\n-            return getPage(self.getURL(\"file\"), method=method)\n+            return getPage(server_url + \"file\", method=method)\n \n         body = yield _getPage(\"head\")\n         assert body == b\"\"\n         body = yield _getPage(\"HEAD\")\n         assert body == b\"\"\n \n     @inlineCallbacks\n-    def test_timeoutNotTriggering(self):\n+    def test_timeoutNotTriggering(self, server_port, server_url):\n         \"\"\"\n         When a non-zero timeout is passed to L{getPage} and the page is\n         retrieved before the timeout period elapses, the L{Deferred} is\n         called back with the contents of the page.\n         \"\"\"\n-        body = yield getPage(self.getURL(\"host\"), timeout=100)\n-        assert body == to_bytes(f\"127.0.0.1:{self.portno}\")\n+        body = yield getPage(server_url + \"host\", timeout=100)\n+        assert body == to_bytes(f\"127.0.0.1:{server_port}\")\n \n     @inlineCallbacks\n-    def test_timeoutTriggering(self):\n+    def test_timeoutTriggering(self, wrapper, server_url):\n         \"\"\"\n         When a non-zero timeout is passed to L{getPage} and that many\n         seconds elapse before the server responds to the request. the\n         L{Deferred} is errbacked with a L{error.TimeoutError}.\n         \"\"\"\n         with pytest.raises(defer.TimeoutError):\n-            yield getPage(self.getURL(\"wait\"), timeout=0.000001)\n+            yield getPage(server_url + \"wait\", timeout=0.000001)\n         # Clean up the server which is hanging around not doing\n         # anything.\n-        connected = list(self.wrapper.protocols.keys())\n+        connected = list(wrapper.protocols.keys())\n         # There might be nothing here if the server managed to already see\n         # that the connection was lost.\n         if connected:\n             connected[0].transport.loseConnection()\n \n     @inlineCallbacks\nComment: This test uses `yield getPage(...)` but is not decorated with `@inlineCallbacks`, so the Deferred won\u2019t be awaited. Add `@inlineCallbacks` above the method or convert it to `async def` and use `await`.\n```suggestion\n    @inlineCallbacks\n    @inlineCallbacks\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "tests/test_webclient.py",
    "pr_number": 6938,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2188233341,
    "comment_created_at": "2025-07-06T12:29:12Z"
  },
  {
    "code": "@@ -389,7 +390,27 @@ def _incompatible(self):\n         scheduler.open(spider)\n \n     def test_incompatibility(self):\n-        with pytest.raises(\n-            ValueError, match=\"does not support CONCURRENT_REQUESTS_PER_IP\"\n-        ):\n-            self._incompatible()\n+        with warnings.catch_warnings():\n+            warnings.filterwarnings(\"ignore\")",
    "comment": "i thought add the deprecate test here but having a separate test seems better, i just filtering the warning to not show on the test logs.",
    "line_number": 394,
    "enriched": "File: tests/test_scheduler.py\nCode: @@ -389,7 +390,27 @@ def _incompatible(self):\n         scheduler.open(spider)\n \n     def test_incompatibility(self):\n-        with pytest.raises(\n-            ValueError, match=\"does not support CONCURRENT_REQUESTS_PER_IP\"\n-        ):\n-            self._incompatible()\n+        with warnings.catch_warnings():\n+            warnings.filterwarnings(\"ignore\")\nComment: I thought add the deprecate test here but having a separate test seems better, I just filtering the warning to not show on the test logs.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "tests/test_scheduler.py",
    "pr_number": 6921,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2173110992,
    "comment_created_at": "2025-06-28T03:56:06Z"
  },
  {
    "code": "@@ -217,17 +224,30 @@ async def _start_request_processing(self) -> None:\n         # Starts the processing of scheduled requests, as well as a periodic\n         # call to that processing method for scenarios where the scheduler\n         # reports having pending requests but returns none.\n-        assert self._slot is not None  # typing\n-        self._slot.nextcall.schedule()\n-        self._slot.heartbeat.start(self._SLOT_HEARTBEAT_INTERVAL)\n-\n-        while self._start and self.spider:\n-            await self._process_start_next()\n-            if not self.needs_backout():\n-                # Give room for the outcome of self._process_start_next() to be\n-                # processed before continuing with the next iteration.\n-                self._slot.nextcall.schedule()\n-                await self._slot.nextcall.wait()\n+        try:\n+            assert self._slot is not None  # typing\n+            self._slot.nextcall.schedule()\n+            self._slot.heartbeat.start(self._SLOT_HEARTBEAT_INTERVAL)\n+\n+            while self._start and self.spider:\n+                await self._process_start_next()\n+                if not self.needs_backout():\n+                    # Give room for the outcome of self._process_start_next() to be\n+                    # processed before continuing with the next iteration.\n+                    self._slot.nextcall.schedule()\n+                    await self._slot.nextcall.wait()\n+        except (asyncio.exceptions.CancelledError, CancelledError):\n+            # self.stop() has cancelled us, nothing to do\n+            return\n+        except Exception:\n+            # an error happened, log it and stop the engine\n+            self._start_request_processing_dfd = None\n+            logger.error(\n+                \"Error while processing requests from start()\",\n+                exc_info=True,\n+                extra={\"spider\": self.spider},\n+            )\n+            await maybe_deferred_to_future(self.stop())",
    "comment": "self.stop() can theoretically cause another unhandled exception here, but in practice i don't see it being possible (and, consequently, couldn't design a failing test), as self.running should be true here, and self.close_spider() is designed to swallow any possible exceptions.",
    "line_number": 250,
    "enriched": "File: scrapy/core/engine.py\nCode: @@ -217,17 +224,30 @@ async def _start_request_processing(self) -> None:\n         # Starts the processing of scheduled requests, as well as a periodic\n         # call to that processing method for scenarios where the scheduler\n         # reports having pending requests but returns none.\n-        assert self._slot is not None  # typing\n-        self._slot.nextcall.schedule()\n-        self._slot.heartbeat.start(self._SLOT_HEARTBEAT_INTERVAL)\n-\n-        while self._start and self.spider:\n-            await self._process_start_next()\n-            if not self.needs_backout():\n-                # Give room for the outcome of self._process_start_next() to be\n-                # processed before continuing with the next iteration.\n-                self._slot.nextcall.schedule()\n-                await self._slot.nextcall.wait()\n+        try:\n+            assert self._slot is not None  # typing\n+            self._slot.nextcall.schedule()\n+            self._slot.heartbeat.start(self._SLOT_HEARTBEAT_INTERVAL)\n+\n+            while self._start and self.spider:\n+                await self._process_start_next()\n+                if not self.needs_backout():\n+                    # Give room for the outcome of self._process_start_next() to be\n+                    # processed before continuing with the next iteration.\n+                    self._slot.nextcall.schedule()\n+                    await self._slot.nextcall.wait()\n+        except (asyncio.exceptions.CancelledError, CancelledError):\n+            # self.stop() has cancelled us, nothing to do\n+            return\n+        except Exception:\n+            # an error happened, log it and stop the engine\n+            self._start_request_processing_dfd = None\n+            logger.error(\n+                \"Error while processing requests from start()\",\n+                exc_info=True,\n+                extra={\"spider\": self.spider},\n+            )\n+            await maybe_deferred_to_future(self.stop())\nComment: `self.stop()` can theoretically cause another unhandled exception here, but in practice I don't see it being possible (and, consequently, couldn't design a failing test), as `self.running` should be True here, and `self.close_spider()` is designed to swallow any possible exceptions.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "scrapy/core/engine.py",
    "pr_number": 6900,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2160135080,
    "comment_created_at": "2025-06-21T19:54:52Z"
  },
  {
    "code": "@@ -155,6 +155,12 @@ def _assert_stores(\n         finally:\n             path.unlink()\n \n+    def test_preserves_windows_path_without_file_scheme(self, tmp_path):",
    "comment": "the fixture is not used and can be dropped.",
    "line_number": 158,
    "enriched": "File: tests/test_feedexport.py\nCode: @@ -155,6 +155,12 @@ def _assert_stores(\n         finally:\n             path.unlink()\n \n+    def test_preserves_windows_path_without_file_scheme(self, tmp_path):\nComment: The fixture is not used and can be dropped.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "tests/test_feedexport.py",
    "pr_number": 6897,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 2159917436,
    "comment_created_at": "2025-06-21T07:25:37Z"
  },
  {
    "code": "@@ -109,6 +113,24 @@ def _urllib3_request_context(\n     }\n     return host_params, pool_kwargs\n \n+def is_single_certificate(cert_path):",
    "comment": "it seems unlikely that special-casing single certs is the best approach \ud83e\udd14: we probably want to not care about the part of the chain that's above the trusted certificate.\r\ni wouldn't expect curl to special-case single certs either...\r\nprobably we want to match curl's behavior with regards to the openssl config here.",
    "line_number": 116,
    "enriched": "File: src/requests/adapters.py\nCode: @@ -109,6 +113,24 @@ def _urllib3_request_context(\n     }\n     return host_params, pool_kwargs\n \n+def is_single_certificate(cert_path):\nComment: It seems unlikely that special-casing single certs is the best approach \ud83e\udd14: we probably want to not care about the part of the chain that's above the trusted certificate.\r\nI wouldn't expect curl to special-case single certs either...\r\nProbably we want to match curl's behavior with regards to the openssl config here. ",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "src/requests/adapters.py",
    "pr_number": 7030,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 2347236444,
    "comment_created_at": "2025-09-14T10:16:36Z"
  },
  {
    "code": "@@ -663,10 +665,10 @@ def __init__(self):\n         #: Integer Code of responded HTTP Status, e.g. 404 or 200.\n         self.status_code = None\n \n-        #: Case-insensitive Dictionary of Response Headers.\n+        #: Case-insensitive Dictionary (email.EmailMessage) of Response Headers.",
    "comment": "this is incorrect for documentation linking (if that's what you're going for)",
    "line_number": 668,
    "enriched": "File: src/requests/models.py\nCode: @@ -663,10 +665,10 @@ def __init__(self):\n         #: Integer Code of responded HTTP Status, e.g. 404 or 200.\n         self.status_code = None\n \n-        #: Case-insensitive Dictionary of Response Headers.\n+        #: Case-insensitive Dictionary (email.EmailMessage) of Response Headers.\nComment: This is incorrect for documentation linking (if that's what you're going for)",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "src/requests/models.py",
    "pr_number": 7015,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 2291084175,
    "comment_created_at": "2025-08-21T13:34:06Z"
  },
  {
    "code": "@@ -236,16 +236,8 @@ def get_netrc_auth(url, raise_errors=False):\n             return\n \n         ri = urlparse(url)\n-\n-        # Strip port numbers from netloc. This weird `if...encode`` dance is\n-        # used for Python 3.2, which doesn't support unicode literals.\n-        splitstr = b\":\"\n-        if isinstance(url, str):\n-            splitstr = splitstr.decode(\"ascii\")\n-        host = ri.netloc.split(splitstr)[0]\n-\n         try:\n-            _netrc = netrc(netrc_path).authenticators(host)\n+            _netrc = netrc(netrc_path).authenticators(ri.hostname)",
    "comment": "this is fixed now in main: https://github.com/psf/requests/commit/96ba401c1296ab1dda74a2365ef36d88f7d144ef",
    "line_number": 240,
    "enriched": "File: src/requests/utils.py\nCode: @@ -236,16 +236,8 @@ def get_netrc_auth(url, raise_errors=False):\n             return\n \n         ri = urlparse(url)\n-\n-        # Strip port numbers from netloc. This weird `if...encode`` dance is\n-        # used for Python 3.2, which doesn't support unicode literals.\n-        splitstr = b\":\"\n-        if isinstance(url, str):\n-            splitstr = splitstr.decode(\"ascii\")\n-        host = ri.netloc.split(splitstr)[0]\n-\n         try:\n-            _netrc = netrc(netrc_path).authenticators(host)\n+            _netrc = netrc(netrc_path).authenticators(ri.hostname)\nComment: This is fixed now in main: https://github.com/psf/requests/commit/96ba401c1296ab1dda74a2365ef36d88f7d144ef",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "src/requests/utils.py",
    "pr_number": 6963,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 2128118428,
    "comment_created_at": "2025-06-05T07:12:17Z"
  },
  {
    "code": "@@ -956,3 +957,17 @@ def QueryValueEx(key, value_name):\n     monkeypatch.setattr(winreg, \"OpenKey\", OpenKey)\n     monkeypatch.setattr(winreg, \"QueryValueEx\", QueryValueEx)\n     assert should_bypass_proxies(\"http://example.com/\", None) is False\n+def test_super_len_stringio_multi_byte():\n+    s = \"\ud83d\udca9\"  # U+1F4A9 (4 bytes in UTF-8)\n+    sio = io.StringIO(s)\n+    assert super_len(sio) == 4, f\"Expected 4, got {super_len(sio)}\"\n+\n+def test_super_len_stringio_mixed_chars():\n+    s = \"A\ud83d\udca9B\"  # 1 byte + 4 bytes + 1 byte\n+    sio = io.StringIO(s)\n+    assert super_len(sio) == 6, f\"Expected 6, got {super_len(sio)}\"\n+\n+def test_super_len_stringio_empty():\n+    s = \"\"\n+    sio = io.StringIO(s)\n+    assert super_len(sio) == 0, f\"Expected 0, got {super_len(sio)}\"",
    "comment": "trailing space\r\n",
    "line_number": 973,
    "enriched": "File: tests/test_utils.py\nCode: @@ -956,3 +957,17 @@ def QueryValueEx(key, value_name):\n     monkeypatch.setattr(winreg, \"OpenKey\", OpenKey)\n     monkeypatch.setattr(winreg, \"QueryValueEx\", QueryValueEx)\n     assert should_bypass_proxies(\"http://example.com/\", None) is False\n+def test_super_len_stringio_multi_byte():\n+    s = \"\ud83d\udca9\"  # U+1F4A9 (4 bytes in UTF-8)\n+    sio = io.StringIO(s)\n+    assert super_len(sio) == 4, f\"Expected 4, got {super_len(sio)}\"\n+\n+def test_super_len_stringio_mixed_chars():\n+    s = \"A\ud83d\udca9B\"  # 1 byte + 4 bytes + 1 byte\n+    sio = io.StringIO(s)\n+    assert super_len(sio) == 6, f\"Expected 6, got {super_len(sio)}\"\n+\n+def test_super_len_stringio_empty():\n+    s = \"\"\n+    sio = io.StringIO(s)\n+    assert super_len(sio) == 0, f\"Expected 0, got {super_len(sio)}\"\nComment: Trailing space\r\n```suggestion\r\n    assert super_len(sio) == 0, f\"Expected 0, got {super_len(sio)}\"\r\n\r\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "tests/test_utils.py",
    "pr_number": 6955,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 2150138454,
    "comment_created_at": "2025-06-16T14:20:14Z"
  },
  {
    "code": "@@ -37,6 +37,8 @@ Exceptions\n .. autoexception:: requests.ReadTimeout\n .. autoexception:: requests.Timeout\n .. autoexception:: requests.JSONDecodeError\n+.. autoexception:: requests.ConnectTimeout",
    "comment": "not sure if i'm missing something. these already exist on lines 36 and 37. why are we adding them a second time?",
    "line_number": 40,
    "enriched": "File: docs/api.rst\nCode: @@ -37,6 +37,8 @@ Exceptions\n .. autoexception:: requests.ReadTimeout\n .. autoexception:: requests.Timeout\n .. autoexception:: requests.JSONDecodeError\n+.. autoexception:: requests.ConnectTimeout\nComment: Not sure if I'm missing something. These already exist on lines 36 and 37. Why are we adding them a second time?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/api.rst",
    "pr_number": 6879,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 1938333934,
    "comment_created_at": "2025-02-01T19:44:03Z"
  },
  {
    "code": "@@ -44,6 +44,9 @@ set with `headers=`.\n If credentials for the hostname are found, the request is sent with HTTP Basic\n Auth.\n \n+Requests will search for the netrc file at `~/.netrc`, `~/_netrc`, or at the path",
    "comment": "this doesn't list the windows paths or how to disable this altogether.",
    "line_number": 47,
    "enriched": "File: docs/user/authentication.rst\nCode: @@ -44,6 +44,9 @@ set with `headers=`.\n If credentials for the hostname are found, the request is sent with HTTP Basic\n Auth.\n \n+Requests will search for the netrc file at `~/.netrc`, `~/_netrc`, or at the path\nComment: This doesn't list the Windows paths or how to disable this altogether. ",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/user/authentication.rst",
    "pr_number": 6876,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 1931340274,
    "comment_created_at": "2025-01-27T23:56:20Z"
  },
  {
    "code": "@@ -128,11 +128,6 @@ jobs:\n       with:\n         python-version: ${{ matrix.version }}\n \n-    - uses: maxim-lobanov/setup-xcode@60606e260d2fc5762a71e64e74b2174e8ea3c8bd # v1.6.0",
    "comment": "this was probably required for a specific reason, now lost to time.",
    "line_number": 131,
    "enriched": "File: .github/workflows/macos.yml\nCode: @@ -128,11 +128,6 @@ jobs:\n       with:\n         python-version: ${{ matrix.version }}\n \n-    - uses: maxim-lobanov/setup-xcode@60606e260d2fc5762a71e64e74b2174e8ea3c8bd # v1.6.0\nComment: This was probably required for a specific reason, now lost to time.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": ".github/workflows/macos.yml",
    "pr_number": 29886,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2409385522,
    "comment_created_at": "2025-10-07T05:13:02Z"
  },
  {
    "code": "@@ -266,11 +266,6 @@ def test_scalar_coercion(self, scalar):\n             # Ensure we have a full-precision number if available\n             scalar = type(scalar)((scalar * 2)**0.5)\n \n-        if type(scalar) is rational:\n-            # Rational generally fails due to a missing cast. In the future\n-            # object casts should automatically be defined based on `setitem`.\n-            pytest.xfail(\"Rational to object cast is undefined currently.\")",
    "comment": "must have been working for many years...",
    "line_number": 272,
    "enriched": "File: numpy/_core/tests/test_array_coercion.py\nCode: @@ -266,11 +266,6 @@ def test_scalar_coercion(self, scalar):\n             # Ensure we have a full-precision number if available\n             scalar = type(scalar)((scalar * 2)**0.5)\n \n-        if type(scalar) is rational:\n-            # Rational generally fails due to a missing cast. In the future\n-            # object casts should automatically be defined based on `setitem`.\n-            pytest.xfail(\"Rational to object cast is undefined currently.\")\nComment: Must have been working for many years...",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "numpy/_core/tests/test_array_coercion.py",
    "pr_number": 29880,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2404625270,
    "comment_created_at": "2025-10-05T19:16:53Z"
  },
  {
    "code": "@@ -888,21 +891,20 @@ npy_uint64 _npy_halfbits_to_doublebits(npy_uint16 h){\n #    define _ROUND_TRIP(x) (x)\n #  elif @is_bool2@\n #    define _CONVERT_FN(x) ((npy_bool)!npy_half_iszero(x))\n-#    define _ROUND_TRIP(x) npy_float_to_half((float)(!npy_half_iszero(x)))\n+#    define _ROUND_TRIP(x) npy_float_to_half((!npy_half_iszero(x))\n #  else\n #    define _CONVERT_FN(x) ((_TYPE2)npy_half_to_float(x))\n #    define _ROUND_TRIP(x) npy_float_to_half((float)_CONVERT_FN(x))\n #  endif\n \n #elif @is_emu_half2@\n #  define _TO_RTYPE1(x) (@rtype1@)(x)\n-\n #  if @is_float1@\n #    define _CONVERT_FN(x) npy_floatbits_to_halfbits(x)\n-#    define _ROUND_TRIP(x) (@rtype1@)npy_halfbits_to_floatbits(_CONVERT_FN(x))\n+#    define _ROUND_TRIP(x) npy_half_to_float(npy_float_to_half(x))\n #  elif @is_double1@\n #    define _CONVERT_FN(x) npy_doublebits_to_halfbits(x)\n-#    define _ROUND_TRIP(x) (@rtype1@)_npy_halfbits_to_doublebits(_CONVERT_FN(x))\n+#    define _ROUND_TRIP(x) npy_half_to_double(npy_double_to_half(x))",
    "comment": "@mattip, it looks a bug to me, do we have test case covering it?",
    "line_number": 907,
    "enriched": "File: numpy/_core/src/multiarray/lowlevel_strided_loops.c.src\nCode: @@ -888,21 +891,20 @@ npy_uint64 _npy_halfbits_to_doublebits(npy_uint16 h){\n #    define _ROUND_TRIP(x) (x)\n #  elif @is_bool2@\n #    define _CONVERT_FN(x) ((npy_bool)!npy_half_iszero(x))\n-#    define _ROUND_TRIP(x) npy_float_to_half((float)(!npy_half_iszero(x)))\n+#    define _ROUND_TRIP(x) npy_float_to_half((!npy_half_iszero(x))\n #  else\n #    define _CONVERT_FN(x) ((_TYPE2)npy_half_to_float(x))\n #    define _ROUND_TRIP(x) npy_float_to_half((float)_CONVERT_FN(x))\n #  endif\n \n #elif @is_emu_half2@\n #  define _TO_RTYPE1(x) (@rtype1@)(x)\n-\n #  if @is_float1@\n #    define _CONVERT_FN(x) npy_floatbits_to_halfbits(x)\n-#    define _ROUND_TRIP(x) (@rtype1@)npy_halfbits_to_floatbits(_CONVERT_FN(x))\n+#    define _ROUND_TRIP(x) npy_half_to_float(npy_float_to_half(x))\n #  elif @is_double1@\n #    define _CONVERT_FN(x) npy_doublebits_to_halfbits(x)\n-#    define _ROUND_TRIP(x) (@rtype1@)_npy_halfbits_to_doublebits(_CONVERT_FN(x))\n+#    define _ROUND_TRIP(x) npy_half_to_double(npy_double_to_half(x))\nComment: @mattip, It looks a bug to me, do we have test case covering it?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "numpy/_core/src/multiarray/lowlevel_strided_loops.c.src",
    "pr_number": 29868,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2400472824,
    "comment_created_at": "2025-10-03T00:46:33Z"
  },
  {
    "code": "@@ -44,6 +56,10 @@ def run_lint(self, fix: bool) -> None:\n         retcode, c_API_errors = self.run_check_c_api()\n         c_API_errors and print(c_API_errors)\n ",
    "comment": "seems to miss an if retcode: sys.exit(retcode) now.",
    "line_number": 58,
    "enriched": "File: tools/linter.py\nCode: @@ -44,6 +56,10 @@ def run_lint(self, fix: bool) -> None:\n         retcode, c_API_errors = self.run_check_c_api()\n         c_API_errors and print(c_API_errors)\n \nComment: Seems to miss an `if retcode: sys.exit(retcode)` now.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "tools/linter.py",
    "pr_number": 29861,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2401414580,
    "comment_created_at": "2025-10-03T09:55:53Z"
  },
  {
    "code": "@@ -78,7 +78,7 @@ __all__ = [\n     \"vecdot\",\n ]\n \n-_ArrayT = TypeVar(\"_ArrayT\", bound=NDArray[Any])",
    "comment": "this one was unused",
    "line_number": 81,
    "enriched": "File: numpy/linalg/_linalg.pyi\nCode: @@ -78,7 +78,7 @@ __all__ = [\n     \"vecdot\",\n ]\n \n-_ArrayT = TypeVar(\"_ArrayT\", bound=NDArray[Any])\nComment: This one was unused",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "numpy/linalg/_linalg.pyi",
    "pr_number": 29846,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2393060390,
    "comment_created_at": "2025-09-30T23:34:23Z"
  },
  {
    "code": "@@ -9260,9 +9250,24 @@ def __int__(self):\n                     raise NotImplementedError\n             assert_raises(NotImplementedError,\n                 int_func, np.array(NotConvertible()))\n-            with pytest.warns(DeprecationWarning):\n-                assert_raises(NotImplementedError,\n-                    int_func, np.array([NotConvertible()]))\n+            assert_raises(TypeError,\n+                int_func, np.array([NotConvertible()]))\n+\n+    def test_to_float_scalar(self):\n+        float_funcs = (float, lambda x: x.__float__())\n+        for float_func in float_funcs:\n+            assert_equal(float_func(np.array(0)), 0.0)\n+            assert_equal(float_func(np.array(1.0, np.float64)), 1.0)\n+            assert_raises(TypeError, float_func, np.array([2]))\n+            assert_raises(TypeError, float_func, np.array([3.14]))\n+            assert_raises(TypeError, float_func, np.array([[4.0]]))\n+\n+            assert_equal(5.0, float_func(np.array('5')))\n+            assert_equal(5.1, float_func(np.array('5.1')))\n+            assert_equal(6.0, float_func(np.bytes_(b'6')))\n+            assert_equal(6.1, float_func(np.bytes_(b'6.1')))\n+            assert_equal(7.0, float_func(np.str_('7')))\n+            assert_equal(7.1, float_func(np.str_('7.1')))",
    "comment": "new tests?  i don't say no to new tests ever :).  (i think float and __float__ is a bit double, since float just calls __float__, but ok.)",
    "line_number": 9270,
    "enriched": "File: numpy/_core/tests/test_multiarray.py\nCode: @@ -9260,9 +9250,24 @@ def __int__(self):\n                     raise NotImplementedError\n             assert_raises(NotImplementedError,\n                 int_func, np.array(NotConvertible()))\n-            with pytest.warns(DeprecationWarning):\n-                assert_raises(NotImplementedError,\n-                    int_func, np.array([NotConvertible()]))\n+            assert_raises(TypeError,\n+                int_func, np.array([NotConvertible()]))\n+\n+    def test_to_float_scalar(self):\n+        float_funcs = (float, lambda x: x.__float__())\n+        for float_func in float_funcs:\n+            assert_equal(float_func(np.array(0)), 0.0)\n+            assert_equal(float_func(np.array(1.0, np.float64)), 1.0)\n+            assert_raises(TypeError, float_func, np.array([2]))\n+            assert_raises(TypeError, float_func, np.array([3.14]))\n+            assert_raises(TypeError, float_func, np.array([[4.0]]))\n+\n+            assert_equal(5.0, float_func(np.array('5')))\n+            assert_equal(5.1, float_func(np.array('5.1')))\n+            assert_equal(6.0, float_func(np.bytes_(b'6')))\n+            assert_equal(6.1, float_func(np.bytes_(b'6.1')))\n+            assert_equal(7.0, float_func(np.str_('7')))\n+            assert_equal(7.1, float_func(np.str_('7.1')))\nComment: New tests?  I don't say no to new tests ever :).  (I think `float` and `__float__` is a bit double, since `float` just calls `__float__`, but OK.)",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "numpy/_core/tests/test_multiarray.py",
    "pr_number": 29841,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2394058314,
    "comment_created_at": "2025-10-01T10:04:49Z"
  },
  {
    "code": "@@ -1687,9 +1687,9 @@ def test_integer_alias_names(self, int_, size):\n \n     @pytest.mark.parametrize(\"name\",\n             [\"Half\", \"Float\", \"Double\", \"CFloat\", \"CDouble\"])\n-    def test_float_alias_names(self, name):\n+    def test_float_alias_names_not_present(self, name):\n         with pytest.raises(AttributeError):\n-            getattr(numpy.dtypes, name + \"DType\") is numpy.dtypes.Float16DType\n+            getattr(numpy.dtypes, name + \"DType\")",
    "comment": "or just assert not hasattr(numpy.dtypes, f\"{name}dtype\") \ud83e\udd37\ud83c\udffb",
    "line_number": 1692,
    "enriched": "File: numpy/_core/tests/test_dtype.py\nCode: @@ -1687,9 +1687,9 @@ def test_integer_alias_names(self, int_, size):\n \n     @pytest.mark.parametrize(\"name\",\n             [\"Half\", \"Float\", \"Double\", \"CFloat\", \"CDouble\"])\n-    def test_float_alias_names(self, name):\n+    def test_float_alias_names_not_present(self, name):\n         with pytest.raises(AttributeError):\n-            getattr(numpy.dtypes, name + \"DType\") is numpy.dtypes.Float16DType\n+            getattr(numpy.dtypes, name + \"DType\")\nComment: or just `assert not hasattr(numpy.dtypes, f\"{name}DType\")` \ud83e\udd37\ud83c\udffb ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "numpy/_core/tests/test_dtype.py",
    "pr_number": 29796,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2373551442,
    "comment_created_at": "2025-09-23T22:01:23Z"
  },
  {
    "code": "@@ -187,6 +187,9 @@ def setbufsize(size):\n     8192\n \n     \"\"\"\n+    if size < 0:\n+        raise ValueError(\"buffer size must be non-negative\")\n+        ",
    "comment": "linting failure\r\n",
    "line_number": 192,
    "enriched": "File: numpy/_core/_ufunc_config.py\nCode: @@ -187,6 +187,9 @@ def setbufsize(size):\n     8192\n \n     \"\"\"\n+    if size < 0:\n+        raise ValueError(\"buffer size must be non-negative\")\n+        \nComment: Linting failure\r\n```suggestion\r\n\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "numpy/_core/_ufunc_config.py",
    "pr_number": 29774,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2366942627,
    "comment_created_at": "2025-09-22T07:16:45Z"
  },
  {
    "code": "@@ -572,6 +577,33 @@ def _init(self, dtype):\n         self._str_smallest_subnormal = machar._str_smallest_subnormal.strip()\n         return self\n \n+    def _init_from_finfo_obj(self, dtype, finfo_obj):\n+        self.dtype = numeric.dtype(dtype)\n+        self.precision = finfo_obj.get('precision')\n+        self.iexp = finfo_obj.get('iexp')\n+        self.maxexp = finfo_obj.get('maxexp')\n+        self.minexp = finfo_obj.get('minexp')\n+        self.negep = finfo_obj.get('negep')\n+        self.machep = finfo_obj.get('machep')\n+        self.resolution = finfo_obj.get('resolution')\n+        self.epsneg = finfo_obj.get('epsneg')\n+        self.smallest_subnormal = finfo_obj.get('smallest_subnormal')\n+        self.bits = self.dtype.itemsize * 8\n+        self.max = finfo_obj.get('max')\n+        self.min = finfo_obj.get('min')\n+        self.eps = finfo_obj.get('eps')\n+        self.nexp = finfo_obj.get('nexp')\n+        self.nmant = finfo_obj.get('nmant')\n+        self._machar = finfo_obj.get('machar')\n+        self._str_tiny = str(finfo_obj.get(\"smallest_normal\"))\n+        self._str_max = str(self.max)\n+        self._str_epsneg = str(self.epsneg)\n+        self._str_eps = str(self.eps)\n+        self._str_resolution = str(self.resolution)\n+        self._str_smallest_normal = str(finfo_obj.get(\"smallest_normal\"))\n+        self._str_smallest_subnormal = str(self.smallest_subnormal)",
    "comment": "these will be the string \"none\" if not present. i don't think that's what we want here.",
    "line_number": 604,
    "enriched": "File: numpy/_core/getlimits.py\nCode: @@ -572,6 +577,33 @@ def _init(self, dtype):\n         self._str_smallest_subnormal = machar._str_smallest_subnormal.strip()\n         return self\n \n+    def _init_from_finfo_obj(self, dtype, finfo_obj):\n+        self.dtype = numeric.dtype(dtype)\n+        self.precision = finfo_obj.get('precision')\n+        self.iexp = finfo_obj.get('iexp')\n+        self.maxexp = finfo_obj.get('maxexp')\n+        self.minexp = finfo_obj.get('minexp')\n+        self.negep = finfo_obj.get('negep')\n+        self.machep = finfo_obj.get('machep')\n+        self.resolution = finfo_obj.get('resolution')\n+        self.epsneg = finfo_obj.get('epsneg')\n+        self.smallest_subnormal = finfo_obj.get('smallest_subnormal')\n+        self.bits = self.dtype.itemsize * 8\n+        self.max = finfo_obj.get('max')\n+        self.min = finfo_obj.get('min')\n+        self.eps = finfo_obj.get('eps')\n+        self.nexp = finfo_obj.get('nexp')\n+        self.nmant = finfo_obj.get('nmant')\n+        self._machar = finfo_obj.get('machar')\n+        self._str_tiny = str(finfo_obj.get(\"smallest_normal\"))\n+        self._str_max = str(self.max)\n+        self._str_epsneg = str(self.epsneg)\n+        self._str_eps = str(self.eps)\n+        self._str_resolution = str(self.resolution)\n+        self._str_smallest_normal = str(finfo_obj.get(\"smallest_normal\"))\n+        self._str_smallest_subnormal = str(self.smallest_subnormal)\nComment: These will be the string `\"None\"` if not present. I don't think that's what we want here.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "numpy/_core/getlimits.py",
    "pr_number": 29771,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2364781388,
    "comment_created_at": "2025-09-19T23:13:54Z"
  },
  {
    "code": "@@ -616,6 +616,35 @@ def test_names_are_undersood_by_dtype(self, t):\n         assert np.dtype(t.__name__).type is t\n \n \n+class TestScalarTypeOrder:\n+    @pytest.mark.parametrize(('a', 'b'), [\n+        # signedinteger\n+        (np.byte, np.short),\n+        (np.short, np.intc),\n+        (np.intc, np.long),\n+        (np.long, np.longlong),\n+        # unsignedinteger\n+        (np.ubyte, np.ushort),\n+        (np.ushort, np.uintc),\n+        (np.uintc, np.ulong),\n+        (np.ulong, np.ulonglong),\n+        # floating\n+        (np.half, np.single),\n+        (np.single, np.double),\n+        (np.double, np.longdouble),\n+        # complexfloating\n+        (np.csingle, np.cdouble),\n+        (np.cdouble, np.clongdouble),\n+        # flexible\n+        (np.bytes_, np.str_),\n+        (np.str_, np.void),\n+        # bouncy castles\n+        (np.datetime64, np.timedelta64),",
    "comment": "total quibble but i think you can write this with just a single static list and then generate these tuples with e.g. itertools.pairwise.",
    "line_number": 642,
    "enriched": "File: numpy/_core/tests/test_numerictypes.py\nCode: @@ -616,6 +616,35 @@ def test_names_are_undersood_by_dtype(self, t):\n         assert np.dtype(t.__name__).type is t\n \n \n+class TestScalarTypeOrder:\n+    @pytest.mark.parametrize(('a', 'b'), [\n+        # signedinteger\n+        (np.byte, np.short),\n+        (np.short, np.intc),\n+        (np.intc, np.long),\n+        (np.long, np.longlong),\n+        # unsignedinteger\n+        (np.ubyte, np.ushort),\n+        (np.ushort, np.uintc),\n+        (np.uintc, np.ulong),\n+        (np.ulong, np.ulonglong),\n+        # floating\n+        (np.half, np.single),\n+        (np.single, np.double),\n+        (np.double, np.longdouble),\n+        # complexfloating\n+        (np.csingle, np.cdouble),\n+        (np.cdouble, np.clongdouble),\n+        # flexible\n+        (np.bytes_, np.str_),\n+        (np.str_, np.void),\n+        # bouncy castles\n+        (np.datetime64, np.timedelta64),\nComment: Total quibble but I think you can write this with just a single static list and then generate these tuples with e.g. `itertools.pairwise`.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "numpy/_core/tests/test_numerictypes.py",
    "pr_number": 29761,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2360522870,
    "comment_created_at": "2025-09-18T17:52:44Z"
  },
  {
    "code": "@@ -32,8 +32,17 @@ if [[ \"$INSTALL_OPENBLAS\" = \"true\" ]] ; then\n     echo pkgconf_path is $pkgconf_path, OPENBLAS is ${OPENBLAS}\n     rm -rf $pkgconf_path\n     mkdir -p $pkgconf_path\n-    python -m pip install -r $PROJECT_DIR/requirements/ci_requirements.txt\n-    python -c \"import scipy_${OPENBLAS}; print(scipy_${OPENBLAS}.get_pkg_config())\" > $pkgconf_path/scipy-openblas.pc\n+\n+    if [[ $CIBW_ARCHS_MACOS == \"x86_64\" ]]; then\n+        # We're cross compiling; the before-build hook isn't using Rosetta (see cibuildwheel#2592)\n+        mkdir host-env\n+        python -m pip install -r $PROJECT_DIR/requirements/ci_requirements.txt --platform macosx_10_13_x86_64 --only-binary :all: -U --target ./host-env\n+\t# Use a handwritten .pc file, because we can't run the cross Python to generate it",
    "comment": "tab instead of spaces - needs fixing",
    "line_number": 40,
    "enriched": "File: tools/wheels/cibw_before_build.sh\nCode: @@ -32,8 +32,17 @@ if [[ \"$INSTALL_OPENBLAS\" = \"true\" ]] ; then\n     echo pkgconf_path is $pkgconf_path, OPENBLAS is ${OPENBLAS}\n     rm -rf $pkgconf_path\n     mkdir -p $pkgconf_path\n-    python -m pip install -r $PROJECT_DIR/requirements/ci_requirements.txt\n-    python -c \"import scipy_${OPENBLAS}; print(scipy_${OPENBLAS}.get_pkg_config())\" > $pkgconf_path/scipy-openblas.pc\n+\n+    if [[ $CIBW_ARCHS_MACOS == \"x86_64\" ]]; then\n+        # We're cross compiling; the before-build hook isn't using Rosetta (see cibuildwheel#2592)\n+        mkdir host-env\n+        python -m pip install -r $PROJECT_DIR/requirements/ci_requirements.txt --platform macosx_10_13_x86_64 --only-binary :all: -U --target ./host-env\n+\t# Use a handwritten .pc file, because we can't run the cross Python to generate it\nComment: tab instead of spaces - needs fixing",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "tools/wheels/cibw_before_build.sh",
    "pr_number": 29756,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2347570239,
    "comment_created_at": "2025-09-14T20:48:11Z"
  },
  {
    "code": "@@ -864,11 +864,11 @@ result of multiplying the elements together, ``std`` to get the standard\n deviation, and more. ::\n \n   >>> data.max()",
    "comment": "on line 838, in the section about broadcasting, data is redefined data = np.array([1.0, 2.0]). so the text is correct, the image is wrong. i think the easiest thing would be to redefine data here to match the image (which will restore it to the same values as in the \"indexing and slicing\" section):\r\n\r\n\r\n\r\nthen at some point someone could regenerate the image in the \"broadcasting\" section and change the text so that the value of data remains consistent throughout this page. when they do that, it would be nice to add a .. note how they created the image.",
    "line_number": 867,
    "enriched": "File: doc/source/user/absolute_beginners.rst\nCode: @@ -864,11 +864,11 @@ result of multiplying the elements together, ``std`` to get the standard\n deviation, and more. ::\n \n   >>> data.max()\nComment: On line 838, in the section about Broadcasting, data is redefined `data = np.array([1.0, 2.0])`. So the text is correct, the image is wrong. I think the easiest thing would be to redefine `data` here to match the image (which will restore it to the same values as in the \"Indexing and slicing\" section):\r\n\r\n```suggestion\r\n  >>> data = np.array([1, 2, 3])\r\n  \r\n  >>> data.max()\r\n```\r\n\r\nThen at some point someone could regenerate the image in the \"Broadcasting\" section and change the text so that the value of `data` remains consistent throughout this page. When they do that, it would be nice to add a `.. note` how they created the image.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "doc/source/user/absolute_beginners.rst",
    "pr_number": 29753,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2347382996,
    "comment_created_at": "2025-09-14T15:08:45Z"
  },
  {
    "code": "@@ -74,8 +74,7 @@ def __bool__(self):\n         raise TypeError(\"boolean value of NA is ambiguous\")\n \n     def __hash__(self):\n-        exponent = 31 if is_32bit else 61\n-        return 2**exponent - 1\n+        return 2**61 - 1",
    "comment": "i assume this is string specific? might be good to have \"string\" in the file name somewhere.",
    "line_number": 77,
    "enriched": "File: numpy/_core/tests/_natype.py\nCode: @@ -74,8 +74,7 @@ def __bool__(self):\n         raise TypeError(\"boolean value of NA is ambiguous\")\n \n     def __hash__(self):\n-        exponent = 31 if is_32bit else 61\n-        return 2**exponent - 1\n+        return 2**61 - 1\nComment: I assume this is string specific? Might be good to have \"string\" in the file name somewhere.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "numpy/_core/tests/_natype.py",
    "pr_number": 29744,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2350346334,
    "comment_created_at": "2025-09-15T23:53:43Z"
  },
  {
    "code": "@@ -489,4 +491,9 @@ typedef PyArray_Descr *(PyArrayDTypeMeta_FinalizeDescriptor)(PyArray_Descr *dtyp\n typedef int(PyArrayDTypeMeta_SetItem)(PyArray_Descr *, PyObject *, char *);\n typedef PyObject *(PyArrayDTypeMeta_GetItem)(PyArray_Descr *, char *);\n \n+typedef struct {",
    "comment": "shouldn't we be able to just pass on the flags that were defined directly? i.e., wouldn't you'd want something like\r\n\r\ntypedef struct {\r\n    npy_sortkind flags;\r\n} pyarraymethod_sortparameters;\r\n\r\nthat avoids having to set .stable and .descending attributes in the code.\r\n\r\np.s. not 100% the indirection via a struct is even needed, but maybe it will become useful to pass on other information later on, without having to extend context with yet another slot.",
    "line_number": 494,
    "enriched": "File: numpy/_core/include/numpy/dtype_api.h\nCode: @@ -489,4 +491,9 @@ typedef PyArray_Descr *(PyArrayDTypeMeta_FinalizeDescriptor)(PyArray_Descr *dtyp\n typedef int(PyArrayDTypeMeta_SetItem)(PyArray_Descr *, PyObject *, char *);\n typedef PyObject *(PyArrayDTypeMeta_GetItem)(PyArray_Descr *, char *);\n \n+typedef struct {\nComment: Shouldn't we be able to just pass on the flags that were defined directly? I.e., wouldn't you'd want something like\r\n```\r\ntypedef struct {\r\n    NPY_SORTKIND flags;\r\n} PyArrayMethod_SortParameters;\r\n```\r\nThat avoids having to set `.stable` and `.descending` attributes in the code.\r\n\r\np.s. Not 100% the indirection via a `struct` is even needed, but maybe it will become useful to pass on other information later on, without having to extend `context` with yet another slot.\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "numpy/_core/include/numpy/dtype_api.h",
    "pr_number": 29737,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 2369124616,
    "comment_created_at": "2025-09-22T15:51:22Z"
  },
  {
    "code": "@@ -2429,8 +2429,8 @@ def _reverse_indexer(self) -> dict[Hashable, npt.NDArray[np.intp]]:\n             ensure_platform_int(self.codes), categories.size\n         )\n         counts = ensure_int64(counts).cumsum()\n-        _result = (r[start:end] for start, end in zip(counts, counts[1:]))\n-        return dict(zip(categories, _result))\n+        _result = (r[start:end] for start, end in zip(counts, counts[1:], strict=False))",
    "comment": "could you use itertools.pairwise here?",
    "line_number": 2432,
    "enriched": "File: pandas/core/arrays/categorical.py\nCode: @@ -2429,8 +2429,8 @@ def _reverse_indexer(self) -> dict[Hashable, npt.NDArray[np.intp]]:\n             ensure_platform_int(self.codes), categories.size\n         )\n         counts = ensure_int64(counts).cumsum()\n-        _result = (r[start:end] for start, end in zip(counts, counts[1:]))\n-        return dict(zip(categories, _result))\n+        _result = (r[start:end] for start, end in zip(counts, counts[1:], strict=False))\nComment: Could you use `itertools.pairwise` here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/core/arrays/categorical.py",
    "pr_number": 62596,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2407287317,
    "comment_created_at": "2025-10-06T16:21:20Z"
  },
  {
    "code": "@@ -3097,3 +3097,28 @@ def test_merge_categorical_key_recursion():\n         right.astype(\"float64\"), on=\"key\", how=\"outer\"\n     )\n     tm.assert_frame_equal(result, expected)\n+\n+\n+def test_merge_pyarrow_datetime_duplicates():\n+    # GH#61926\n+    # Regression test for merge failing on pyarrow datetime columns with duplicates",
    "comment": "can you remove all these comments except for the # gh#61926 above?",
    "line_number": 3104,
    "enriched": "File: pandas/tests/reshape/merge/test_merge.py\nCode: @@ -3097,3 +3097,28 @@ def test_merge_categorical_key_recursion():\n         right.astype(\"float64\"), on=\"key\", how=\"outer\"\n     )\n     tm.assert_frame_equal(result, expected)\n+\n+\n+def test_merge_pyarrow_datetime_duplicates():\n+    # GH#61926\n+    # Regression test for merge failing on pyarrow datetime columns with duplicates\nComment: Can you remove all these comments except for the `# GH#61926` above?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/tests/reshape/merge/test_merge.py",
    "pr_number": 62592,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2404627476,
    "comment_created_at": "2025-10-05T19:22:02Z"
  },
  {
    "code": "@@ -1143,89 +1397,102 @@ def line(\n \n         This function is useful to plot lines using DataFrame's values\n         as coordinates.\n-        \"\"\"\n-        if color is not None:\n-            kwargs[\"color\"] = color\n-        return self(kind=\"line\", x=x, y=y, **kwargs)\n \n-    @Appender(\n-        \"\"\"\n-        See Also\n-        --------\n-        DataFrame.plot.barh : Horizontal bar plot.\n-        DataFrame.plot : Make plots of a DataFrame.\n-        matplotlib.pyplot.bar : Make a bar plot with matplotlib.\n+        Parameters\n+        ----------\n+        x : label or position, optional\n+            Allows plotting of one column versus another. If not specified,\n+            the index of the DataFrame is used.\n+        y : label or position, optional\n+            Allows plotting of one column versus another. If not specified,\n+            all numerical columns are used.\n+        color : str, array-like, or dict, optional\n+            The color for each of the DataFrame's columns. Possible values are:\n \n-        Examples\n-        --------\n-        Basic plot.\n+            - A single color string referred to by name, RGB or RGBA code,\n+              for instance 'red' or '#a98d19'.\n \n-        .. plot::\n-            :context: close-figs\n+            - A sequence of color strings referred to by name, RGB or RGBA\n+              code, which will be used for each column recursively. For\n+              instance ['green','yellow'] each column's line will be filled in\n+              green or yellow, alternatively. If there is only a single column to\n+              be plotted, then only the first color from the color list will be\n+              used.\n \n-            >>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n-            >>> ax = df.plot.bar(x='lab', y='val', rot=0)\n+            - A dict of the form {column name : color}, so that each column will be\n+              colored accordingly. For example, if your columns are called `a` and\n+              `b`, then passing {'a': 'green', 'b': 'red'} will color lines for\n+              column `a` in green and lines for column `b` in red.\n+\n+        **kwargs\n+            Additional keyword arguments are documented in\n+            :meth:`DataFrame.plot`.\n \n-        Plot a whole dataframe to a bar plot. Each column is assigned a\n-        distinct color, and each row is nested in a group along the\n-        horizontal axis.\n+        Returns\n+        -------\n+        matplotlib.axes.Axes or np.ndarray of them\n+            An ndarray is returned with one :class:`matplotlib.axes.Axes`\n+            per column when ``subplots=True``.\n \n-        .. plot::\n-            :context: close-figs\n+                See Also",
    "comment": "i don't think the indentations of the see alsos in your pr are correct",
    "line_number": 1437,
    "enriched": "File: pandas/plotting/_core.py\nCode: @@ -1143,89 +1397,102 @@ def line(\n \n         This function is useful to plot lines using DataFrame's values\n         as coordinates.\n-        \"\"\"\n-        if color is not None:\n-            kwargs[\"color\"] = color\n-        return self(kind=\"line\", x=x, y=y, **kwargs)\n \n-    @Appender(\n-        \"\"\"\n-        See Also\n-        --------\n-        DataFrame.plot.barh : Horizontal bar plot.\n-        DataFrame.plot : Make plots of a DataFrame.\n-        matplotlib.pyplot.bar : Make a bar plot with matplotlib.\n+        Parameters\n+        ----------\n+        x : label or position, optional\n+            Allows plotting of one column versus another. If not specified,\n+            the index of the DataFrame is used.\n+        y : label or position, optional\n+            Allows plotting of one column versus another. If not specified,\n+            all numerical columns are used.\n+        color : str, array-like, or dict, optional\n+            The color for each of the DataFrame's columns. Possible values are:\n \n-        Examples\n-        --------\n-        Basic plot.\n+            - A single color string referred to by name, RGB or RGBA code,\n+              for instance 'red' or '#a98d19'.\n \n-        .. plot::\n-            :context: close-figs\n+            - A sequence of color strings referred to by name, RGB or RGBA\n+              code, which will be used for each column recursively. For\n+              instance ['green','yellow'] each column's line will be filled in\n+              green or yellow, alternatively. If there is only a single column to\n+              be plotted, then only the first color from the color list will be\n+              used.\n \n-            >>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n-            >>> ax = df.plot.bar(x='lab', y='val', rot=0)\n+            - A dict of the form {column name : color}, so that each column will be\n+              colored accordingly. For example, if your columns are called `a` and\n+              `b`, then passing {'a': 'green', 'b': 'red'} will color lines for\n+              column `a` in green and lines for column `b` in red.\n+\n+        **kwargs\n+            Additional keyword arguments are documented in\n+            :meth:`DataFrame.plot`.\n \n-        Plot a whole dataframe to a bar plot. Each column is assigned a\n-        distinct color, and each row is nested in a group along the\n-        horizontal axis.\n+        Returns\n+        -------\n+        matplotlib.axes.Axes or np.ndarray of them\n+            An ndarray is returned with one :class:`matplotlib.axes.Axes`\n+            per column when ``subplots=True``.\n \n-        .. plot::\n-            :context: close-figs\n+                See Also\nComment: I don't think the indentations of the `See Also`s in your PR are correct ",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "pandas/plotting/_core.py",
    "pr_number": 62584,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2404630522,
    "comment_created_at": "2025-10-05T19:29:11Z"
  },
  {
    "code": "@@ -1669,7 +1669,7 @@ def _has_valid_setitem_indexer(self, indexer) -> bool:\n         if not isinstance(indexer, tuple):\n             indexer = _tuplify(self.ndim, indexer)\n ",
    "comment": "[nitpick] consider adding a comment explaining why strict=false is intentionally used here, as it's the only exception to the strict zip pattern in this pr.\n",
    "line_number": 1671,
    "enriched": "File: pandas/core/indexing.py\nCode: @@ -1669,7 +1669,7 @@ def _has_valid_setitem_indexer(self, indexer) -> bool:\n         if not isinstance(indexer, tuple):\n             indexer = _tuplify(self.ndim, indexer)\n \nComment: [nitpick] Consider adding a comment explaining why `strict=False` is intentionally used here, as it's the only exception to the strict zip pattern in this PR.\n```suggestion\n\n        # Intentionally use strict=False here: in some cases, indexer may be shorter or longer\n        # than self.obj.axes, and we want to ignore any extra elements rather than raise an error.\n        # This is the only exception to the strict zip pattern in this codebase.\n```",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "pandas/core/indexing.py",
    "pr_number": 62577,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2403941605,
    "comment_created_at": "2025-10-04T12:12:13Z"
  },
  {
    "code": "@@ -1226,6 +1226,7 @@ Other\n - Fixed bug in the :meth:`Series.rank` with object dtype and extremely small float values (:issue:`62036`)\n - Fixed bug where the :class:`DataFrame` constructor misclassified array-like objects with a ``.name`` attribute as :class:`Series` or :class:`Index` (:issue:`61443`)\n - Fixed regression in :meth:`DataFrame.from_records` not initializing subclasses properly (:issue:`57008`)\n+- Fixed regression in :meth:`Series.pow` on Series with all-NA ``float64[pyarrow]`` values; this now returns Series with :class:`NA` values (:issue:`62520`)",
    "comment": "i dont think the problem is in a released version, so no note needed",
    "line_number": 1229,
    "enriched": "File: doc/source/whatsnew/v3.0.0.rst\nCode: @@ -1226,6 +1226,7 @@ Other\n - Fixed bug in the :meth:`Series.rank` with object dtype and extremely small float values (:issue:`62036`)\n - Fixed bug where the :class:`DataFrame` constructor misclassified array-like objects with a ``.name`` attribute as :class:`Series` or :class:`Index` (:issue:`61443`)\n - Fixed regression in :meth:`DataFrame.from_records` not initializing subclasses properly (:issue:`57008`)\n+- Fixed regression in :meth:`Series.pow` on Series with all-NA ``float64[pyarrow]`` values; this now returns Series with :class:`NA` values (:issue:`62520`)\nComment: i dont think the problem is in a released version, so no note needed",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "doc/source/whatsnew/v3.0.0.rst",
    "pr_number": 62572,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2404012775,
    "comment_created_at": "2025-10-04T14:43:19Z"
  },
  {
    "code": "@@ -97,11 +97,11 @@ def test_nat_parse(all_parsers):\n     )\n     df.iloc[3:6, :] = np.nan\n \n-    with tm.ensure_clean(\"__nat_parse_.csv\") as path:\n-        df.to_csv(path)\n+    path = temp_file.parent / \"__nat_parse_.csv\"",
    "comment": "just use temp_file directly (no need to create the __nat_parse_.csv",
    "line_number": 100,
    "enriched": "File: pandas/tests/io/parser/test_parse_dates.py\nCode: @@ -97,11 +97,11 @@ def test_nat_parse(all_parsers):\n     )\n     df.iloc[3:6, :] = np.nan\n \n-    with tm.ensure_clean(\"__nat_parse_.csv\") as path:\n-        df.to_csv(path)\n+    path = temp_file.parent / \"__nat_parse_.csv\"\nComment: Just use `temp_file` directly (no need to create the `__nat_parse_.csv`",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/tests/io/parser/test_parse_dates.py",
    "pr_number": 62556,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2402704189,
    "comment_created_at": "2025-10-03T17:14:42Z"
  },
  {
    "code": "@@ -5359,6 +5371,11 @@ cpdef to_offset(freq, bint is_period=False):\n \n             tups = zip(split[0::4], split[1::4], split[2::4])\n             for n, (sep, stride, name) in enumerate(tups):\n+                if name in deprec_to_valid_alias:\n+                    raise ValueError(INVALID_FREQ_ERR_MSG.format(",
    "comment": "instead, could you make invalid_freq_err_msg a function like:\r\n\r\npython\r\ndef raise_invalid_freq(freq: str, extra_message: str | none = none): -> none:\r\n    msg = f\"invalid frequency: {freq}\"\r\n    if extra_message is not none:\r\n        msg += extra_message\r\n    if name in deprc_to_valid_alias:\r\n        msg += \"did you mean {deprec_to_valid_alias[name]}?\"\r\n    raise valueerror(msg)\r\n\r\n\r\ncan call this function wherever invalid_freq_err_msg. it appears this variable is used in multiple places so it would be nice to consistently provide this hint",
    "line_number": 5375,
    "enriched": "File: pandas/_libs/tslibs/offsets.pyx\nCode: @@ -5359,6 +5371,11 @@ cpdef to_offset(freq, bint is_period=False):\n \n             tups = zip(split[0::4], split[1::4], split[2::4])\n             for n, (sep, stride, name) in enumerate(tups):\n+                if name in deprec_to_valid_alias:\n+                    raise ValueError(INVALID_FREQ_ERR_MSG.format(\nComment: Instead, could you make `INVALID_FREQ_ERR_MSG` a function like:\r\n\r\n```python\r\ndef raise_invalid_freq(freq: str, extra_message: str | None = None): -> None:\r\n    msg = f\"Invalid frequency: {freq}\"\r\n    if extra_message is not None:\r\n        msg += extra_message\r\n    if name in deprc_to_valid_alias:\r\n        msg += \"Did you mean {deprec_to_valid_alias[name]}?\"\r\n    raise ValueError(msg)\r\n```\r\n\r\nCan call this function wherever `INVALID_FREQ_ERR_MSG`. It appears this variable is used in multiple places so it would be nice to consistently provide this hint",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/_libs/tslibs/offsets.pyx",
    "pr_number": 62539,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2399372480,
    "comment_created_at": "2025-10-02T16:24:31Z"
  },
  {
    "code": "@@ -271,3 +271,12 @@ def test_multiindex_assign_aligns_as_implicit_tuple(self):\n         df1[\"C\"] = s1\n         tm.assert_frame_equal(df1, df2)\n         tm.assert_frame_equal(df1, df3)\n+\n+        # GH 62518\n+        df4 = DataFrame(\n+            columns=MultiIndex.from_arrays(\n+                [[\"a\", \"a\", \"z\", \"z\"], pd.Categorical([1, 2, 1, 2])],\n+            ),\n+            dtype=object,\n+        )\n+        df4[\"z\"] = df4[\"z\"].astype(\"int64\")",
    "comment": "could you make this a separate test that uses the exact code snippet in the original issue?",
    "line_number": 282,
    "enriched": "File: pandas/tests/indexing/multiindex/test_multiindex.py\nCode: @@ -271,3 +271,12 @@ def test_multiindex_assign_aligns_as_implicit_tuple(self):\n         df1[\"C\"] = s1\n         tm.assert_frame_equal(df1, df2)\n         tm.assert_frame_equal(df1, df3)\n+\n+        # GH 62518\n+        df4 = DataFrame(\n+            columns=MultiIndex.from_arrays(\n+                [[\"a\", \"a\", \"z\", \"z\"], pd.Categorical([1, 2, 1, 2])],\n+            ),\n+            dtype=object,\n+        )\n+        df4[\"z\"] = df4[\"z\"].astype(\"int64\")\nComment: Could you make this a separate test that uses the exact code snippet in the original issue?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/tests/indexing/multiindex/test_multiindex.py",
    "pr_number": 62527,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2395152829,
    "comment_created_at": "2025-10-01T16:16:00Z"
  },
  {
    "code": "@@ -1082,28 +1080,133 @@ def test_rolling_sem(frame_or_series):\n     tm.assert_series_equal(result, expected)\n \n \n-@pytest.mark.xfail(\n-    is_platform_arm() or is_platform_power() or is_platform_riscv64(),\n-    reason=\"GH 38921\",\n-)\n @pytest.mark.parametrize(\n-    (\"func\", \"third_value\", \"values\"),\n+    (\"func\", \"values\", \"window\", \"ddof\", \"exp_value\"),\n     [\n-        (\"var\", 1, [5e33, 0, 0.5, 0.5, 2, 0]),\n-        (\"std\", 1, [7.071068e16, 0, 0.7071068, 0.7071068, 1.414214, 0]),\n-        (\"var\", 2, [5e33, 0.5, 0, 0.5, 2, 0]),\n-        (\"std\", 2, [7.071068e16, 0.7071068, 0, 0.7071068, 1.414214, 0]),\n+        (\n+            \"var\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            3,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\n+            \"std\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            3,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\n+            \"var\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            2,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\n+            \"std\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            2,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\"var\", [99999999999999999, 1, 1, 2, 3, 1, 1], 2, 1, 0),\n+        (\"std\", [99999999999999999, 1, 1, 2, 3, 1, 1], 2, 1, 0),\n+        (\"var\", [99999999999999999, 1, 2, 2, 3, 1, 1], 2, 1, 0),\n+        (\"std\", [99999999999999999, 1, 2, 2, 3, 1, 1], 2, 1, 0),\n+        (\"var\", [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], 5, 0, \"numpy_compute\"),\n     ],\n )\n-def test_rolling_var_numerical_issues(func, third_value, values):\n-    # GH: 37051\n-    ds = Series([99999999999999999, 1, third_value, 2, 3, 1, 1])\n-    result = getattr(ds.rolling(2), func)()\n-    expected = Series([np.nan] + values)\n-    tm.assert_series_equal(result, expected)\n+def test_rolling_var_correctness(func, values, window, ddof, exp_value):\n+    # This tests subsume the previous tests under test_rolling_var_numerical_issues",
    "comment": "super nit but you can remove this first comment - if the user cared to see the history they can consult the git blame",
    "line_number": 1166,
    "enriched": "File: pandas/tests/window/test_rolling.py\nCode: @@ -1082,28 +1080,133 @@ def test_rolling_sem(frame_or_series):\n     tm.assert_series_equal(result, expected)\n \n \n-@pytest.mark.xfail(\n-    is_platform_arm() or is_platform_power() or is_platform_riscv64(),\n-    reason=\"GH 38921\",\n-)\n @pytest.mark.parametrize(\n-    (\"func\", \"third_value\", \"values\"),\n+    (\"func\", \"values\", \"window\", \"ddof\", \"exp_value\"),\n     [\n-        (\"var\", 1, [5e33, 0, 0.5, 0.5, 2, 0]),\n-        (\"std\", 1, [7.071068e16, 0, 0.7071068, 0.7071068, 1.414214, 0]),\n-        (\"var\", 2, [5e33, 0.5, 0, 0.5, 2, 0]),\n-        (\"std\", 2, [7.071068e16, 0.7071068, 0, 0.7071068, 1.414214, 0]),\n+        (\n+            \"var\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            3,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\n+            \"std\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            3,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\n+            \"var\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            2,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\n+            \"std\",\n+            [\n+                2.72993945,\n+                1.58444294,\n+                4.14371708,\n+                4.92961687,\n+                2.7138744,\n+                3.48168586,\n+                0.69505519,\n+                1.87511994,\n+                4.20167276,\n+                0.04797675,\n+            ],\n+            2,\n+            1,\n+            \"numpy_compute\",\n+        ),\n+        (\"var\", [99999999999999999, 1, 1, 2, 3, 1, 1], 2, 1, 0),\n+        (\"std\", [99999999999999999, 1, 1, 2, 3, 1, 1], 2, 1, 0),\n+        (\"var\", [99999999999999999, 1, 2, 2, 3, 1, 1], 2, 1, 0),\n+        (\"std\", [99999999999999999, 1, 2, 2, 3, 1, 1], 2, 1, 0),\n+        (\"var\", [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], 5, 0, \"numpy_compute\"),\n     ],\n )\n-def test_rolling_var_numerical_issues(func, third_value, values):\n-    # GH: 37051\n-    ds = Series([99999999999999999, 1, third_value, 2, 3, 1, 1])\n-    result = getattr(ds.rolling(2), func)()\n-    expected = Series([np.nan] + values)\n-    tm.assert_series_equal(result, expected)\n+def test_rolling_var_correctness(func, values, window, ddof, exp_value):\n+    # This tests subsume the previous tests under test_rolling_var_numerical_issues\nComment: Super nit but you can remove this first comment - if the user cared to see the history they can consult the git blame",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/tests/window/test_rolling.py",
    "pr_number": 62514,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2392136885,
    "comment_created_at": "2025-09-30T16:06:01Z"
  },
  {
    "code": "@@ -421,3 +423,39 @@ def test_hypothesis_delimited_date(\n \n     assert except_out_dateutil == except_in_dateutil\n     assert result == expected\n+\n+\n+@pytest.mark.parametrize(\"input\", [\"21-01-01\", \"01-01-21\"])\n+@pytest.mark.parametrize(\"dayfirst\", [True, False])\n+def test_parse_datetime_string_with_reso_dayfirst(dayfirst, input):\n+    set_option(\"display.date_dayfirst\", dayfirst)",
    "comment": "can you use the option_context context manager instead (and below)?",
    "line_number": 431,
    "enriched": "File: pandas/tests/tslibs/test_parsing.py\nCode: @@ -421,3 +423,39 @@ def test_hypothesis_delimited_date(\n \n     assert except_out_dateutil == except_in_dateutil\n     assert result == expected\n+\n+\n+@pytest.mark.parametrize(\"input\", [\"21-01-01\", \"01-01-21\"])\n+@pytest.mark.parametrize(\"dayfirst\", [True, False])\n+def test_parse_datetime_string_with_reso_dayfirst(dayfirst, input):\n+    set_option(\"display.date_dayfirst\", dayfirst)\nComment: Can you use the `option_context` context manager instead (and below)?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/tests/tslibs/test_parsing.py",
    "pr_number": 62511,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2392513598,
    "comment_created_at": "2025-09-30T18:44:08Z"
  },
  {
    "code": "@@ -22,8 +22,6 @@ become the default string dtype in pandas 3.0. See\n \n Bug fixes\n ^^^^^^^^^\n-- Fix :meth:`~Series.str.isdigit` to correctly recognize unicode superscript\n-  characters as digits for :class:`StringDtype` backed by PyArrow (:issue:`61466`)",
    "comment": "this was added to the wrong file, the fix is actually included in 2.3.3",
    "line_number": 26,
    "enriched": "File: doc/source/whatsnew/v2.3.2.rst\nCode: @@ -22,8 +22,6 @@ become the default string dtype in pandas 3.0. See\n \n Bug fixes\n ^^^^^^^^^\n-- Fix :meth:`~Series.str.isdigit` to correctly recognize unicode superscript\n-  characters as digits for :class:`StringDtype` backed by PyArrow (:issue:`61466`)\nComment: This was added to the wrong file, the fix is actually included in 2.3.3",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "doc/source/whatsnew/v2.3.2.rst",
    "pr_number": 62499,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2387190681,
    "comment_created_at": "2025-09-29T08:54:21Z"
  },
  {
    "code": "@@ -234,6 +236,16 @@ def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n             # e.g. test_np_max_nested_tuples\n             return result\n         else:\n+            if self.dtype.type is str:  # type: ignore[comparison-overlap]",
    "comment": "nit: would be a nice if this was elif so it could be un-indented one level, but not a big deal.",
    "line_number": 239,
    "enriched": "File: pandas/core/arrays/numpy_.py\nCode: @@ -234,6 +236,16 @@ def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n             # e.g. test_np_max_nested_tuples\n             return result\n         else:\n+            if self.dtype.type is str:  # type: ignore[comparison-overlap]\nComment: nit: Would be a nice if this was `elif` so it could be un-indented one level, but not a big deal.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "pandas/core/arrays/numpy_.py",
    "pr_number": 62498,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2388551302,
    "comment_created_at": "2025-09-29T16:22:00Z"
  },
  {
    "code": "@@ -1740,3 +1740,15 @@ def test_date_range_negative_freq_year_end_inbounds(self, unit):\n             freq=\"-1YE\",\n         )\n         tm.assert_index_equal(rng, exp)\n+\n+    def test_date_range_tzaware_endpoints_accept_ambiguous(self):\n+        # With tz-aware endpoints and a calendar offset (MS),\n+        # date_range should accept `ambiguous=True` and produce\n+        # the same result as passing tz explicitly with naive endpoints.\n+        start = Timestamp(\"1916-08-01\", tz=\"Europe/Oslo\")\n+        end = Timestamp(\"1916-12-01\", tz=\"Europe/Oslo\")\n+        res = date_range(start, end, freq=\"MS\", ambiguous=True)\n+        exp = date_range(\n+            \"1916-08-01\", \"1916-12-01\", freq=\"MS\", tz=\"Europe/Oslo\", ambiguous=True",
    "comment": "could you add another test that uses nonexistent?",
    "line_number": 1752,
    "enriched": "File: pandas/tests/indexes/datetimes/test_date_range.py\nCode: @@ -1740,3 +1740,15 @@ def test_date_range_negative_freq_year_end_inbounds(self, unit):\n             freq=\"-1YE\",\n         )\n         tm.assert_index_equal(rng, exp)\n+\n+    def test_date_range_tzaware_endpoints_accept_ambiguous(self):\n+        # With tz-aware endpoints and a calendar offset (MS),\n+        # date_range should accept `ambiguous=True` and produce\n+        # the same result as passing tz explicitly with naive endpoints.\n+        start = Timestamp(\"1916-08-01\", tz=\"Europe/Oslo\")\n+        end = Timestamp(\"1916-12-01\", tz=\"Europe/Oslo\")\n+        res = date_range(start, end, freq=\"MS\", ambiguous=True)\n+        exp = date_range(\n+            \"1916-08-01\", \"1916-12-01\", freq=\"MS\", tz=\"Europe/Oslo\", ambiguous=True\nComment: Could you add another test that uses `nonexistent`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/tests/indexes/datetimes/test_date_range.py",
    "pr_number": 62493,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2388586009,
    "comment_created_at": "2025-09-29T16:37:15Z"
  },
  {
    "code": "@@ -170,30 +170,30 @@ def parser(request):\n # FILE OUTPUT\n \n \n-def test_file_output_str_read(xml_books, parser, from_file_expected):\n+def test_file_output_str_read(xml_books, parser, from_file_expected, temp_file):\n     df_file = read_xml(xml_books, parser=parser)\n \n-    with tm.ensure_clean(\"test.xml\") as path:\n-        df_file.to_xml(path, parser=parser)\n-        with open(path, \"rb\") as f:\n-            output = f.read().decode(\"utf-8\").strip()\n+    path = temp_file.parent / \"test.xml\"",
    "comment": "for these cases where the test expects a specific file name, could you use the pytest tmp_path fixture instead to build this file instead i.e. path = tmp_path / \"test.xml\"",
    "line_number": 176,
    "enriched": "File: pandas/tests/io/xml/test_to_xml.py\nCode: @@ -170,30 +170,30 @@ def parser(request):\n # FILE OUTPUT\n \n \n-def test_file_output_str_read(xml_books, parser, from_file_expected):\n+def test_file_output_str_read(xml_books, parser, from_file_expected, temp_file):\n     df_file = read_xml(xml_books, parser=parser)\n \n-    with tm.ensure_clean(\"test.xml\") as path:\n-        df_file.to_xml(path, parser=parser)\n-        with open(path, \"rb\") as f:\n-            output = f.read().decode(\"utf-8\").strip()\n+    path = temp_file.parent / \"test.xml\"\nComment: For these cases where the test expects a specific file name, could you use the pytest `tmp_path` fixture instead to build this file instead i.e. `path = tmp_path / \"test.xml\"`",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "pandas/tests/io/xml/test_to_xml.py",
    "pr_number": 62475,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2388694997,
    "comment_created_at": "2025-09-29T17:23:33Z"
  },
  {
    "code": "@@ -295,29 +295,29 @@ def test_empty_with_nrows_chunksize(all_parsers, iterator):\n     tm.assert_frame_equal(result, expected)\n \n \n-def test_read_csv_memory_growth_chunksize(all_parsers):\n+def test_read_csv_memory_growth_chunksize(temp_file, all_parsers):\n     # see gh-24805\n     #\n     # Let's just make sure that we don't crash\n     # as we iteratively process all chunks.\n     parser = all_parsers\n \n-    with tm.ensure_clean() as path:\n-        with open(path, \"w\", encoding=\"utf-8\") as f:\n-            for i in range(1000):\n-                f.write(str(i) + \"\\n\")\n-\n-        if parser.engine == \"pyarrow\":\n-            msg = \"The 'chunksize' option is not supported with the 'pyarrow' engine\"\n-            with pytest.raises(ValueError, match=msg):\n-                with parser.read_csv(path, chunksize=20) as result:\n-                    for _ in result:\n-                        pass\n-            return\n-\n-        with parser.read_csv(path, chunksize=20) as result:\n-            for _ in result:\n-                pass\n+    path = str(temp_file)",
    "comment": "can you just use temp_file directly, instead of converting to a str? (unless the test appears to explicitly testing a string path)",
    "line_number": 305,
    "enriched": "File: pandas/tests/io/parser/common/test_chunksize.py\nCode: @@ -295,29 +295,29 @@ def test_empty_with_nrows_chunksize(all_parsers, iterator):\n     tm.assert_frame_equal(result, expected)\n \n \n-def test_read_csv_memory_growth_chunksize(all_parsers):\n+def test_read_csv_memory_growth_chunksize(temp_file, all_parsers):\n     # see gh-24805\n     #\n     # Let's just make sure that we don't crash\n     # as we iteratively process all chunks.\n     parser = all_parsers\n \n-    with tm.ensure_clean() as path:\n-        with open(path, \"w\", encoding=\"utf-8\") as f:\n-            for i in range(1000):\n-                f.write(str(i) + \"\\n\")\n-\n-        if parser.engine == \"pyarrow\":\n-            msg = \"The 'chunksize' option is not supported with the 'pyarrow' engine\"\n-            with pytest.raises(ValueError, match=msg):\n-                with parser.read_csv(path, chunksize=20) as result:\n-                    for _ in result:\n-                        pass\n-            return\n-\n-        with parser.read_csv(path, chunksize=20) as result:\n-            for _ in result:\n-                pass\n+    path = str(temp_file)\nComment: Can you just use `temp_file` directly, instead of converting to a `str`? (Unless the test appears to explicitly testing a string path) ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/tests/io/parser/common/test_chunksize.py",
    "pr_number": 62474,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2411252883,
    "comment_created_at": "2025-10-07T16:46:11Z"
  },
  {
    "code": "@@ -260,15 +261,15 @@ def _return_result_expected(\n                 kwargs[\"index_col\"] = list(range(rnlvl))\n             kwargs[\"header\"] = list(range(cnlvl))\n \n-            with tm.ensure_clean(\"__tmp_to_csv_moar__\") as path:\n-                df.to_csv(path, encoding=\"utf8\", chunksize=chunksize)\n-                recons = self.read_csv(path, **kwargs)\n+            path = str(temp_file)",
    "comment": "generally, could you try using temp_file as an argument instead of converting to a string when doing these modifications, unless the test appears to explicitly be testing a string argument?",
    "line_number": 264,
    "enriched": "File: pandas/tests/frame/methods/test_to_csv.py\nCode: @@ -260,15 +261,15 @@ def _return_result_expected(\n                 kwargs[\"index_col\"] = list(range(rnlvl))\n             kwargs[\"header\"] = list(range(cnlvl))\n \n-            with tm.ensure_clean(\"__tmp_to_csv_moar__\") as path:\n-                df.to_csv(path, encoding=\"utf8\", chunksize=chunksize)\n-                recons = self.read_csv(path, **kwargs)\n+            path = str(temp_file)\nComment: Generally, could you try using `temp_file` as an argument instead of converting to a string when doing these modifications, unless the test appears to explicitly be testing a string argument?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/tests/frame/methods/test_to_csv.py",
    "pr_number": 62461,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2388722795,
    "comment_created_at": "2025-09-29T17:36:34Z"
  },
  {
    "code": "@@ -803,7 +803,7 @@ def replace_list(\n         # Exclude anything that we know we won't contain\n         pairs = [\n             (x, y)\n-            for x, y in zip(src_list, dest_list)\n+            for x, y in zip(src_list, dest_list, strict=False)",
    "comment": "is strict=false necessary here?",
    "line_number": 806,
    "enriched": "File: pandas/core/internals/blocks.py\nCode: @@ -803,7 +803,7 @@ def replace_list(\n         # Exclude anything that we know we won't contain\n         pairs = [\n             (x, y)\n-            for x, y in zip(src_list, dest_list)\n+            for x, y in zip(src_list, dest_list, strict=False)\nComment: Is `strict=False` necessary here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pandas/core/internals/blocks.py",
    "pr_number": 62453,
    "repo": "pandas",
    "owner": "pandas-dev",
    "comment_id": 2380190384,
    "comment_created_at": "2025-09-25T19:56:29Z"
  },
  {
    "code": "@@ -1009,7 +1002,6 @@ def _fit(self, X, skip_num_points=0):\n         elif self.init == \"pca\":\n             pca = PCA(\n                 n_components=self.n_components,\n-                svd_solver=\"randomized\",",
    "comment": "note: this change might have a side effect for dense data, but i think our \"auto\" heuristic should behave reasonably well in most cases.",
    "line_number": 1012,
    "enriched": "File: sklearn/manifold/_t_sne.py\nCode: @@ -1009,7 +1002,6 @@ def _fit(self, X, skip_num_points=0):\n         elif self.init == \"pca\":\n             pca = PCA(\n                 n_components=self.n_components,\n-                svd_solver=\"randomized\",\nComment: Note: this change might have a side effect for dense data, but I think our \"auto\" heuristic should behave reasonably well in most cases.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "sklearn/manifold/_t_sne.py",
    "pr_number": 32433,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2413877575,
    "comment_created_at": "2025-10-08T13:31:54Z"
  },
  {
    "code": "@@ -23,7 +23,7 @@ jobs:\n         run: mkdir -p \"$ARTIFACTS_DIR\"\n \n       - name: Download artifact\n-        uses: actions/download-artifact@v4",
    "comment": "i haven't fully understood the issue with the cuda ci, but i am wondering whether the same issue with single artifact as in the cuda ci will happen here.\r\n\r\nthis action is not tested in prs, so ideally you need to test it in your fork. the alternative would be to merge it into main and cross your fingers.",
    "line_number": 26,
    "enriched": "File: .github/workflows/bot-lint-comment.yml\nCode: @@ -23,7 +23,7 @@ jobs:\n         run: mkdir -p \"$ARTIFACTS_DIR\"\n \n       - name: Download artifact\n-        uses: actions/download-artifact@v4\nComment: I haven't fully understood the issue with the CUDA CI, but I am wondering whether the same issue with single artifact as in the CUDA CI will happen here.\r\n\r\nThis action is not tested in PRs, so ideally you need to test it in your fork. The alternative would be to merge it into `main` and cross your fingers. ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".github/workflows/bot-lint-comment.yml",
    "pr_number": 32413,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2409997674,
    "comment_created_at": "2025-10-07T09:28:25Z"
  },
  {
    "code": "@@ -430,6 +437,8 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"ndcg_score\",\n     \"dcg_score\",\n     \"label_ranking_average_precision_score\",\n+    \"d2_log_loss_score\",\n+    \"d2_brier_score\",",
    "comment": "@ogrisel i added these in here because both log_loss and brier_score_loss are present but let me know if this needs to be changed. this helps skip one test that checks for a valueerror.",
    "line_number": 441,
    "enriched": "File: sklearn/metrics/tests/test_common.py\nCode: @@ -430,6 +437,8 @@ def precision_recall_curve_padded_thresholds(*args, **kwargs):\n     \"ndcg_score\",\n     \"dcg_score\",\n     \"label_ranking_average_precision_score\",\n+    \"d2_log_loss_score\",\n+    \"d2_brier_score\",\nComment: @ogrisel I added these in here because both `log_loss` and `brier_score_loss` are present but let me know if this needs to be changed. This helps skip one test that checks for a ValueError.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "sklearn/metrics/tests/test_common.py",
    "pr_number": 32356,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2405577172,
    "comment_created_at": "2025-10-06T10:05:13Z"
  },
  {
    "code": "@@ -137,21 +137,21 @@ def remove_from(alist, to_remove):\n         },\n     },\n     {\n-        \"name\": \"pylatest_conda_forge_mkl_osx-64\",\n+        \"name\": \"pylatest_conda_forge_osx-arm64\",\n         \"type\": \"conda\",\n         \"tag\": \"main-ci\",\n         \"folder\": \"build_tools/azure\",\n-        \"platform\": \"osx-64\",\n+        \"platform\": \"osx-arm64\",\n         \"channels\": [\"conda-forge\"],\n         \"conda_dependencies\": common_dependencies\n         + [\n             \"ccache\",\n             \"compilers\",\n             \"llvm-openmp\",\n+            \"pytorch\",\n+            \"pytorch-cpu\",\n+            \"array-api-strict\",\n         ],\n-        \"package_constraints\": {\n-            \"blas\": \"[build=mkl]\",",
    "comment": "mkl does not exist on conda-forge for osx-arm64 it seems so i removed it",
    "line_number": 153,
    "enriched": "File: build_tools/update_environments_and_lock_files.py\nCode: @@ -137,21 +137,21 @@ def remove_from(alist, to_remove):\n         },\n     },\n     {\n-        \"name\": \"pylatest_conda_forge_mkl_osx-64\",\n+        \"name\": \"pylatest_conda_forge_osx-arm64\",\n         \"type\": \"conda\",\n         \"tag\": \"main-ci\",\n         \"folder\": \"build_tools/azure\",\n-        \"platform\": \"osx-64\",\n+        \"platform\": \"osx-arm64\",\n         \"channels\": [\"conda-forge\"],\n         \"conda_dependencies\": common_dependencies\n         + [\n             \"ccache\",\n             \"compilers\",\n             \"llvm-openmp\",\n+            \"pytorch\",\n+            \"pytorch-cpu\",\n+            \"array-api-strict\",\n         ],\n-        \"package_constraints\": {\n-            \"blas\": \"[build=mkl]\",\nComment: mkl does not exist on conda-forge for osx-arm64 it seems so I removed it",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "build_tools/update_environments_and_lock_files.py",
    "pr_number": 32349,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2409895923,
    "comment_created_at": "2025-10-07T08:51:55Z"
  },
  {
    "code": "@@ -1048,6 +1049,7 @@ def test_param_is_non_default(default_value, test_value):\n         ((1, 2, 3), [1, 2, 3]),\n         ((1, 2, 3), np.array([1, 2, 3])),\n         (np.nan, np.nan),\n+        (np.nan, pd.NA),",
    "comment": "i don't think that we want to treat pd.na as being the same as np.nan.",
    "line_number": 1052,
    "enriched": "File: sklearn/tests/test_base.py\nCode: @@ -1048,6 +1049,7 @@ def test_param_is_non_default(default_value, test_value):\n         ((1, 2, 3), [1, 2, 3]),\n         ((1, 2, 3), np.array([1, 2, 3])),\n         (np.nan, np.nan),\n+        (np.nan, pd.NA),\nComment: I don't think that we want to treat `pd.NA` as being the same as `np.nan`.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "sklearn/tests/test_base.py",
    "pr_number": 32341,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2399018332,
    "comment_created_at": "2025-10-02T14:22:19Z"
  },
  {
    "code": "@@ -1,6 +1,6 @@\n from cython cimport final\n \n-from ...utils._typedefs cimport intp_t, float64_t\n+from sklearn.utils._typedefs cimport intp_t, float64_t",
    "comment": "this change is unrelated and already implemented in your previous pr. please revert it.",
    "line_number": 3,
    "enriched": "File: sklearn/metrics/_pairwise_distances_reduction/_base.pxd.tp\nCode: @@ -1,6 +1,6 @@\n from cython cimport final\n \n-from ...utils._typedefs cimport intp_t, float64_t\n+from sklearn.utils._typedefs cimport intp_t, float64_t\nComment: This change is unrelated and already implemented in your previous PR. Please revert it.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "sklearn/metrics/_pairwise_distances_reduction/_base.pxd.tp",
    "pr_number": 32324,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 2397984153,
    "comment_created_at": "2025-10-02T09:28:35Z"
  },
  {
    "code": "@@ -763,7 +763,7 @@\n         (\"pegasus\", \"Pegasus\"),\n         (\"pegasus_x\", \"PEGASUS-X\"),\n         (\"perceiver\", \"Perceiver\"),\n-        (\"perception_encoder\", \"PerceptionEncoder\"),\n+        (\"perception_encoder\", \"TimmWrapperConfig\"),",
    "comment": "we can just delete it. when models re-use existing vision backbone, we dont usually add them as separate models in auto-map",
    "line_number": 766,
    "enriched": "File: src/transformers/models/auto/configuration_auto.py\nCode: @@ -763,7 +763,7 @@\n         (\"pegasus\", \"Pegasus\"),\n         (\"pegasus_x\", \"PEGASUS-X\"),\n         (\"perceiver\", \"Perceiver\"),\n-        (\"perception_encoder\", \"PerceptionEncoder\"),\n+        (\"perception_encoder\", \"TimmWrapperConfig\"),\nComment: we can just delete it. When models re-use existing vision backbone, we dont usually add them as separate models in auto-map",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/models/auto/configuration_auto.py",
    "pr_number": 41464,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2416268670,
    "comment_created_at": "2025-10-09T10:13:06Z"
  },
  {
    "code": "@@ -170,8 +170,13 @@ def _lazy_load_causal_conv1d():\n     if is_kernels_available():\n         from kernels import get_kernel\n \n-        _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n+        try:\n+            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n+        except FileNotFoundError:\n+            # no kernel binary match, fallback to slow path\n+            _causal_conv1d_cache = (None, None)\n+        else:\n+            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)",
    "comment": "if we don't find an appropriate kernel binary on target platform, we should fall back to slow path, rather than crash.",
    "line_number": 179,
    "enriched": "File: src/transformers/models/falcon_mamba/modeling_falcon_mamba.py\nCode: @@ -170,8 +170,13 @@ def _lazy_load_causal_conv1d():\n     if is_kernels_available():\n         from kernels import get_kernel\n \n-        _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n+        try:\n+            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n+        except FileNotFoundError:\n+            # no kernel binary match, fallback to slow path\n+            _causal_conv1d_cache = (None, None)\n+        else:\n+            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\nComment: If we don't find an appropriate kernel binary on target platform, we should fall back to slow path, rather than crash.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
    "pr_number": 41428,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2411845952,
    "comment_created_at": "2025-10-07T20:36:37Z"
  },
  {
    "code": "@@ -427,7 +427,8 @@ def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n                 \"when creating this class.\"\n             )\n \n-        self.num_key_value_groups = config.num_experts_per_tok\n+        self.num_key_value_groups = 1  # We ignore this by setting it to 1 as we have different repeat patterns\n+        self.top_k = config.num_experts_per_tok",
    "comment": "is this what we have done before #40132, or it's kind new stuff?",
    "line_number": 431,
    "enriched": "File: src/transformers/models/jetmoe/modeling_jetmoe.py\nCode: @@ -427,7 +427,8 @@ def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n                 \"when creating this class.\"\n             )\n \n-        self.num_key_value_groups = config.num_experts_per_tok\n+        self.num_key_value_groups = 1  # We ignore this by setting it to 1 as we have different repeat patterns\n+        self.top_k = config.num_experts_per_tok\nComment: Is this what we have done before #40132, or it's kind new stuff?\r\n\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "src/transformers/models/jetmoe/modeling_jetmoe.py",
    "pr_number": 41423,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2411173953,
    "comment_created_at": "2025-10-07T16:16:02Z"
  },
  {
    "code": "@@ -123,11 +123,6 @@ class PreTrainedConfig(PushToHubMixin):\n         tie_encoder_decoder (`bool`, *optional*, defaults to `False`):\n             Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder\n             and decoder model to have the exact same parameter names.\n-        prune_heads (`dict[int, list[int]]`, *optional*, defaults to `{}`):\n-            Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of\n-            heads to prune in said layer.\n-\n-            For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.",
    "comment": "this won't break the loading if a saved config has this attribute?",
    "line_number": 130,
    "enriched": "File: src/transformers/configuration_utils.py\nCode: @@ -123,11 +123,6 @@ class PreTrainedConfig(PushToHubMixin):\n         tie_encoder_decoder (`bool`, *optional*, defaults to `False`):\n             Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder\n             and decoder model to have the exact same parameter names.\n-        prune_heads (`dict[int, list[int]]`, *optional*, defaults to `{}`):\n-            Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of\n-            heads to prune in said layer.\n-\n-            For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.\nComment: this won't break the loading if a saved config has this attribute?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "src/transformers/configuration_utils.py",
    "pr_number": 41417,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2411105360,
    "comment_created_at": "2025-10-07T15:50:45Z"
  },
  {
    "code": "@@ -607,7 +610,12 @@ def __init__(\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n             streaming: Whether to stream tokens as they are generated\n         \"\"\"\n-        model.set_attn_implementation(model.config._attn_implementation, use_paged=True)\n+        attn_implementation = f\"paged|{model.config._attn_implementation}\"\n+        attn_wrapper = ALL_ATTENTION_FUNCTIONS.get(attn_implementation, None)\n+        if attn_wrapper is None:",
    "comment": "maybe add a comment explaining that this code path is only taken if no match is found ie. the model.config._attn_implementation is a kernel (was not clear to me at first)",
    "line_number": 615,
    "enriched": "File: src/transformers/generation/continuous_batching/continuous_api.py\nCode: @@ -607,7 +610,12 @@ def __init__(\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n             streaming: Whether to stream tokens as they are generated\n         \"\"\"\n-        model.set_attn_implementation(model.config._attn_implementation, use_paged=True)\n+        attn_implementation = f\"paged|{model.config._attn_implementation}\"\n+        attn_wrapper = ALL_ATTENTION_FUNCTIONS.get(attn_implementation, None)\n+        if attn_wrapper is None:\nComment: Maybe add a comment explaining that this code path is only taken if no match is found ie. the `model.config._attn_implementation` is a kernel (was not clear to me at first)",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "src/transformers/generation/continuous_batching/continuous_api.py",
    "pr_number": 41413,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2413261691,
    "comment_created_at": "2025-10-08T09:48:12Z"
  },
  {
    "code": "@@ -2006,17 +1996,13 @@ def _prepare_cache_for_generation(\n             elif \"dynamic\" in generation_config.cache_implementation:\n                 model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n \n-        # Use DynamicCache instance by default. This will avoid back and forth from legacy format that\n-        # keeps copying the cache thus using much more memory\n-        # TODO (joao): remove this `else` when we remove the last traces of the legacy cache format (v4.58.0, search\n-        # for `instance(past_key_values, Cache)` as well). In general, if `cache_implementation` is unset, cache\n-        # initialization should happen inside the model at prefill time.\n-        else:\n-            model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n-\n         # TODO (joao): this logic is incomplete, e.g. `offloaded` should apply to both caches. Refactor this function\n         # to correctly pass parameterization to both caches.\n-        if requires_cross_attention_cache and not isinstance(model_kwargs[cache_name], EncoderDecoderCache):\n+        if (\n+            requires_cross_attention_cache\n+            and cache_name in model_kwargs",
    "comment": "ig this is for special models where name isn't \"past_key_values\". is it not going to fail later in model, if we leave the non-encoderdecodercache cache?",
    "line_number": 2003,
    "enriched": "File: src/transformers/generation/utils.py\nCode: @@ -2006,17 +1996,13 @@ def _prepare_cache_for_generation(\n             elif \"dynamic\" in generation_config.cache_implementation:\n                 model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n \n-        # Use DynamicCache instance by default. This will avoid back and forth from legacy format that\n-        # keeps copying the cache thus using much more memory\n-        # TODO (joao): remove this `else` when we remove the last traces of the legacy cache format (v4.58.0, search\n-        # for `instance(past_key_values, Cache)` as well). In general, if `cache_implementation` is unset, cache\n-        # initialization should happen inside the model at prefill time.\n-        else:\n-            model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n-\n         # TODO (joao): this logic is incomplete, e.g. `offloaded` should apply to both caches. Refactor this function\n         # to correctly pass parameterization to both caches.\n-        if requires_cross_attention_cache and not isinstance(model_kwargs[cache_name], EncoderDecoderCache):\n+        if (\n+            requires_cross_attention_cache\n+            and cache_name in model_kwargs\nComment: ig this is for special models where name isn't \"past_key_values\". Is it not going to fail later in model, if we leave the non-EncoderDecoderCache cache?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "src/transformers/generation/utils.py",
    "pr_number": 41405,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2410373191,
    "comment_created_at": "2025-10-07T11:54:44Z"
  },
  {
    "code": "@@ -573,8 +599,16 @@ def test_generation_siglip_backbone(self):\n         # Make sure that `generate` works\n         output = model.generate(**inputs, max_new_tokens=30)\n \n-        EXPECTED_DECODED_TEXT = \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\"\n-        self.assertTrue(processor.batch_decode(output, skip_special_tokens=True)[0] == EXPECTED_DECODED_TEXT)\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+                (\"xpu\", 3): \"user\\n\\nWhat are these?\\nassistant These are two cats, one with a green collar and the other with a black collar. They are lying on a pink blanket and appear to be sleeping\",\n+                (\"cuda\", None): \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\",\n+            }\n+        )  # fmt: skip\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+\n+        decoded_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n+        self.assertTrue(decoded_text == EXPECTED_DECODED_TEXT)",
    "comment": "let's use self.assertequal, so the error message would be more informative when it fails.",
    "line_number": 611,
    "enriched": "File: tests/models/llava/test_modeling_llava.py\nCode: @@ -573,8 +599,16 @@ def test_generation_siglip_backbone(self):\n         # Make sure that `generate` works\n         output = model.generate(**inputs, max_new_tokens=30)\n \n-        EXPECTED_DECODED_TEXT = \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\"\n-        self.assertTrue(processor.batch_decode(output, skip_special_tokens=True)[0] == EXPECTED_DECODED_TEXT)\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+                (\"xpu\", 3): \"user\\n\\nWhat are these?\\nassistant These are two cats, one with a green collar and the other with a black collar. They are lying on a pink blanket and appear to be sleeping\",\n+                (\"cuda\", None): \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\",\n+            }\n+        )  # fmt: skip\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+\n+        decoded_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n+        self.assertTrue(decoded_text == EXPECTED_DECODED_TEXT)\nComment: Let's use `self.assertEqual`, so the error message would be more informative when it fails.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "tests/models/llava/test_modeling_llava.py",
    "pr_number": 41386,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2410562961,
    "comment_created_at": "2025-10-07T13:02:56Z"
  },
  {
    "code": "@@ -1305,6 +1305,13 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n         # pass defaults to output dictionary\n         output_kwargs.update(default_kwargs)\n \n+        # For `common_kwargs` just update all modality-specific kwargs with same key/values\n+        common_kwargs = kwargs.get(\"common_kwargs\", {})\n+        common_kwargs.update(ModelProcessorKwargs._defaults.get(\"common_kwargs\", {}))",
    "comment": "in that case i think we want to change the order of these lines as well, instead of overriding user values by defaults",
    "line_number": 1310,
    "enriched": "File: src/transformers/processing_utils.py\nCode: @@ -1305,6 +1305,13 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n         # pass defaults to output dictionary\n         output_kwargs.update(default_kwargs)\n \n+        # For `common_kwargs` just update all modality-specific kwargs with same key/values\n+        common_kwargs = kwargs.get(\"common_kwargs\", {})\n+        common_kwargs.update(ModelProcessorKwargs._defaults.get(\"common_kwargs\", {}))\nComment: in that case i think we want to change the order of these lines as well, instead of overriding user values by defaults",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "src/transformers/processing_utils.py",
    "pr_number": 41381,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2409903065,
    "comment_created_at": "2025-10-07T08:54:26Z"
  },
  {
    "code": "@@ -796,12 +789,5 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, embeddings):\n         self.cpmant.input_embedding = embeddings\n \n-    def _reorder_cache(self, past_key_values, beam_idx):",
    "comment": "(good catch!)",
    "line_number": 799,
    "enriched": "File: src/transformers/models/cpmant/modeling_cpmant.py\nCode: @@ -796,12 +789,5 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, embeddings):\n         self.cpmant.input_embedding = embeddings\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\nComment: (good catch!)",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "src/transformers/models/cpmant/modeling_cpmant.py",
    "pr_number": 41378,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2413131680,
    "comment_created_at": "2025-10-08T09:00:56Z"
  },
  {
    "code": "@@ -127,10 +126,16 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n @require_torch\n class JetMoeIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     @slow\n     def test_model_8b_logits(self):\n         input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n-        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\")\n+        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\", device_map=\"auto\", torch_dtype=torch.bfloat16)",
    "comment": "my only nit, don't we want to use fp16 for more consistency or was the model saved in bf16?",
    "line_number": 138,
    "enriched": "File: tests/models/jetmoe/test_modeling_jetmoe.py\nCode: @@ -127,10 +126,16 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n @require_torch\n class JetMoeIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     @slow\n     def test_model_8b_logits(self):\n         input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n-        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\")\n+        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nComment: My only nit, don't we want to use fp16 for more consistency or was the model saved in bf16?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/models/jetmoe/test_modeling_jetmoe.py",
    "pr_number": 41377,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2411156916,
    "comment_created_at": "2025-10-07T16:10:22Z"
  },
  {
    "code": "@@ -5904,10 +5902,10 @@ class AttentionInterface(GeneralInterface):\n         \"flash_attention_3\": flash_attention_forward,\n         \"flash_attention_2\": flash_attention_forward,\n         \"flex_attention\": flex_attention_forward,\n-        \"paged_attention\": paged_attention_forward,\n         \"sdpa\": sdpa_attention_forward,\n-        \"sdpa_paged\": sdpa_attention_paged_forward,\n-        \"eager_paged\": eager_paged_attention_forward,\n+        \"paged|flash_attention2\": paged_attention_forward,\n+        \"paged|sdpa\": sdpa_attention_paged_forward,\n+        \"paged|eager\": eager_paged_attention_forward,",
    "comment": "should paged|flex_attention be an option as well? i see it listed below in the tests",
    "line_number": 5908,
    "enriched": "File: src/transformers/modeling_utils.py\nCode: @@ -5904,10 +5902,10 @@ class AttentionInterface(GeneralInterface):\n         \"flash_attention_3\": flash_attention_forward,\n         \"flash_attention_2\": flash_attention_forward,\n         \"flex_attention\": flex_attention_forward,\n-        \"paged_attention\": paged_attention_forward,\n         \"sdpa\": sdpa_attention_forward,\n-        \"sdpa_paged\": sdpa_attention_paged_forward,\n-        \"eager_paged\": eager_paged_attention_forward,\n+        \"paged|flash_attention2\": paged_attention_forward,\n+        \"paged|sdpa\": sdpa_attention_paged_forward,\n+        \"paged|eager\": eager_paged_attention_forward,\nComment: should `paged|flex_attention` be an option as well? I see it listed below in the tests",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "src/transformers/modeling_utils.py",
    "pr_number": 41370,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2406397043,
    "comment_created_at": "2025-10-06T13:41:47Z"
  },
  {
    "code": "@@ -435,14 +435,14 @@ def test_shortcat_generation(self):\n     @require_large_cpu_ram\n     def test_longcat_generation_cpu(self):\n         # takes absolutely forever and a lot RAM, but allows to test the output in the CI\n-        model = LongcatFlashForCausalLM.from_pretrained(self.model_id, device_map=\"cpu\", dtype=torch.bfloat16)\n+        model = LongcatFlashForCausalLM.from_pretrained(self.model_id, device_map=\"auto\", dtype=torch.bfloat16)\n         tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n \n         chat = [{\"role\": \"user\", \"content\": \"Paris is...\"}]\n         inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n \n         with torch.no_grad():\n-            outputs = model.generate(inputs, max_new_tokens=10, do_sample=False)\n+            outputs = model.generate(inputs, max_new_tokens=3, do_sample=False)\n \n         response = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n         expected_output = \"[Round 0] USER:Paris is... ASSISTANT:Paris is... a city of timeless charm, where\"",
    "comment": "should then be\r\n",
    "line_number": 448,
    "enriched": "File: tests/models/longcat_flash/test_modeling_longcat_flash.py\nCode: @@ -435,14 +435,14 @@ def test_shortcat_generation(self):\n     @require_large_cpu_ram\n     def test_longcat_generation_cpu(self):\n         # takes absolutely forever and a lot RAM, but allows to test the output in the CI\n-        model = LongcatFlashForCausalLM.from_pretrained(self.model_id, device_map=\"cpu\", dtype=torch.bfloat16)\n+        model = LongcatFlashForCausalLM.from_pretrained(self.model_id, device_map=\"auto\", dtype=torch.bfloat16)\n         tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n \n         chat = [{\"role\": \"user\", \"content\": \"Paris is...\"}]\n         inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n \n         with torch.no_grad():\n-            outputs = model.generate(inputs, max_new_tokens=10, do_sample=False)\n+            outputs = model.generate(inputs, max_new_tokens=3, do_sample=False)\n \n         response = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n         expected_output = \"[Round 0] USER:Paris is... ASSISTANT:Paris is... a city of timeless charm, where\"\nComment: should then be\r\n```suggestion\r\n        expected_output = \"[Round 0] USER:Paris is... ASSISTANT:Paris is...\"\r\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "tests/models/longcat_flash/test_modeling_longcat_flash.py",
    "pr_number": 41368,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2406172863,
    "comment_created_at": "2025-10-06T13:05:05Z"
  },
  {
    "code": "@@ -238,34 +251,40 @@ def __init__(self, hub_dataset: str):\n \n         self.models_root = MODELS_ROOT\n         self.hub_dataset = hub_dataset\n-        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n-        self.dtype = \"auto\"\n         self.tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n-        self.model = (\n-            AutoModel.from_pretrained(\n-                EMBEDDING_MODEL,\n-                torch_dtype=self.dtype if self.device.type == \"cuda\" else torch.float32,\n-            )\n-            .eval()\n-            .to(self.device)\n-        )\n+        self.model = AutoModel.from_pretrained(EMBEDDING_MODEL, torch_dtype=\"auto\", device_map=\"auto\").eval()\n+\n+        self.device = self.model.device\n+        self.index_dir: Path | None = None\n \n     # ---------- HUB IO ----------\n \n+    def _resolve_index_path(self, filename: str) -> Path:\n+        if self.index_dir is None:\n+            return Path(filename)\n+        return self.index_dir / filename\n+\n     def ensure_local_index(self) -> None:\n-        \"\"\"Download index files from Hub if they don't exist locally.\"\"\"\n-        have_all = Path(EMBEDDINGS_PATH).exists() and Path(INDEX_MAP_PATH).exists() and Path(TOKENS_PATH).exists()\n-        if have_all:\n+        \"\"\"Ensure index files are available locally, preferring Hub cache snapshots.\"\"\"\n+        if self.index_dir is not None and all(\n+            (self.index_dir / fname).exists() for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH)\n+        ):\n+            return\n+\n+        workspace_dir = Path.cwd()\n+        if all((workspace_dir / fname).exists() for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH)):\n+            self.index_dir = workspace_dir\n             return\n-        logging.info(f\"downloading index from hub: {self.hub_dataset}\")\n-        for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH):\n-            hf_hub_download(\n-                repo_id=self.hub_dataset,\n-                filename=fname,\n-                repo_type=\"dataset\",\n-                local_dir=\".\",\n-                local_dir_use_symlinks=False,\n-            )\n+\n+        logging.info(f\"downloading index from hub cache: {self.hub_dataset}\")\n+        snapshot_path = snapshot_download(repo_id=self.hub_dataset, repo_type=\"dataset\")\n+        snapshot_dir = Path(snapshot_path)\n+        missing = [\n+            fname for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH) if not (snapshot_dir / fname).exists()\n+        ]\n+        if missing:\n+            raise FileNotFoundError(\"Missing expected files in Hub snapshot: \" + \", \".join(missing))",
    "comment": "cleaner handling, thanks :d",
    "line_number": 286,
    "enriched": "File: utils/modular_model_detector.py\nCode: @@ -238,34 +251,40 @@ def __init__(self, hub_dataset: str):\n \n         self.models_root = MODELS_ROOT\n         self.hub_dataset = hub_dataset\n-        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n-        self.dtype = \"auto\"\n         self.tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n-        self.model = (\n-            AutoModel.from_pretrained(\n-                EMBEDDING_MODEL,\n-                torch_dtype=self.dtype if self.device.type == \"cuda\" else torch.float32,\n-            )\n-            .eval()\n-            .to(self.device)\n-        )\n+        self.model = AutoModel.from_pretrained(EMBEDDING_MODEL, torch_dtype=\"auto\", device_map=\"auto\").eval()\n+\n+        self.device = self.model.device\n+        self.index_dir: Path | None = None\n \n     # ---------- HUB IO ----------\n \n+    def _resolve_index_path(self, filename: str) -> Path:\n+        if self.index_dir is None:\n+            return Path(filename)\n+        return self.index_dir / filename\n+\n     def ensure_local_index(self) -> None:\n-        \"\"\"Download index files from Hub if they don't exist locally.\"\"\"\n-        have_all = Path(EMBEDDINGS_PATH).exists() and Path(INDEX_MAP_PATH).exists() and Path(TOKENS_PATH).exists()\n-        if have_all:\n+        \"\"\"Ensure index files are available locally, preferring Hub cache snapshots.\"\"\"\n+        if self.index_dir is not None and all(\n+            (self.index_dir / fname).exists() for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH)\n+        ):\n+            return\n+\n+        workspace_dir = Path.cwd()\n+        if all((workspace_dir / fname).exists() for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH)):\n+            self.index_dir = workspace_dir\n             return\n-        logging.info(f\"downloading index from hub: {self.hub_dataset}\")\n-        for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH):\n-            hf_hub_download(\n-                repo_id=self.hub_dataset,\n-                filename=fname,\n-                repo_type=\"dataset\",\n-                local_dir=\".\",\n-                local_dir_use_symlinks=False,\n-            )\n+\n+        logging.info(f\"downloading index from hub cache: {self.hub_dataset}\")\n+        snapshot_path = snapshot_download(repo_id=self.hub_dataset, repo_type=\"dataset\")\n+        snapshot_dir = Path(snapshot_path)\n+        missing = [\n+            fname for fname in (EMBEDDINGS_PATH, INDEX_MAP_PATH, TOKENS_PATH) if not (snapshot_dir / fname).exists()\n+        ]\n+        if missing:\n+            raise FileNotFoundError(\"Missing expected files in Hub snapshot: \" + \", \".join(missing))\nComment: Cleaner handling, thanks :D ",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "utils/modular_model_detector.py",
    "pr_number": 41361,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2405575211,
    "comment_created_at": "2025-10-06T10:04:19Z"
  },
  {
    "code": "@@ -14,6 +14,14 @@ See the License for the specific language governing permissions and\n limitations under the License.\n -->\n \n+# Setup\n+\n+```bash\n+pip install -e \".[dev]\"  # Installs dev dependencies\n+pip install git+https://github.com/huggingface/doc-builder  # Installs doc-builder",
    "comment": "hmm, i think this is a bit redundant as the package installation is detailed in lines 23 and 45 in main branch",
    "line_number": 21,
    "enriched": "File: docs/README.md\nCode: @@ -14,6 +14,14 @@ See the License for the specific language governing permissions and\n limitations under the License.\n -->\n \n+# Setup\n+\n+```bash\n+pip install -e \".[dev]\"  # Installs dev dependencies\n+pip install git+https://github.com/huggingface/doc-builder  # Installs doc-builder\nComment: hmm, i think this is a bit redundant as the package installation is detailed in lines 23 and 45 in main branch",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/README.md",
    "pr_number": 41338,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2405460321,
    "comment_created_at": "2025-10-06T09:20:53Z"
  },
  {
    "code": "@@ -1915,7 +1915,7 @@ def __init__(self, suppress_tokens, device: str = \"cpu\"):\n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)\n-        suppress_token_mask = isin_mps_friendly(vocab_tensor, self.suppress_tokens)\n+        suppress_token_mask = isin_mps_friendly(vocab_tensor, self.suppress_tokens.to(scores.device))\n         scores = torch.where(suppress_token_mask, -float(\"inf\"), scores)",
    "comment": "in multi-device cases(like put 2 devices to run):\r\nin current implementation, in assistant decoding case, assistant model will reuse main model's suppresstokenslogitsprocessor, which place the suppress_tokens in the same device as input_tensor (which is device 0).  assistant model will ingest encoder_outputs of the main model and do the decoder(in whisper case), while encoder_outputs may in device 1 but main model's suppress_tokens which is main model's is in device 0, so lead to runtimeerror: \r\n\r\n> runtimeerror: expected all tensors to be on the same device, but got test_elements is on xpu:0, different from other tensors on xpu:1 (when checking argument in method wrapper_xpu_isin_tensor_tensor)\r\n\r\nso based on current implementation(that assistant model shares main model's suppresstokenslogitsprocessor), i move suppress_tokens to scores.device while doing isin.",
    "line_number": 1919,
    "enriched": "File: src/transformers/generation/logits_process.py\nCode: @@ -1915,7 +1915,7 @@ def __init__(self, suppress_tokens, device: str = \"cpu\"):\n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)\n-        suppress_token_mask = isin_mps_friendly(vocab_tensor, self.suppress_tokens)\n+        suppress_token_mask = isin_mps_friendly(vocab_tensor, self.suppress_tokens.to(scores.device))\n         scores = torch.where(suppress_token_mask, -float(\"inf\"), scores)\nComment: In multi-device cases(like put 2 devices to run):\r\nin current implementation, in assistant decoding case, assistant model will reuse main model's `SuppressTokensLogitsProcessor`, which place the `suppress_tokens` in the same device as `input_tensor` (which is `device 0`).  assistant model will ingest `encoder_outputs` of the main model and do the decoder(in whisper case), while `encoder_outputs` may in `device 1` but main model's `suppress_tokens` which is main model's is in `device 0`, so lead to `RuntimeError`: \r\n\r\n> RuntimeError: Expected all tensors to be on the same device, but got test_elements is on xpu:0, different from other tensors on xpu:1 (when checking argument in method wrapper_XPU_isin_Tensor_Tensor)\r\n\r\nSo based on current implementation(that assistant model shares main model's `SuppressTokensLogitsProcessor`), I move `suppress_tokens` to `scores.device` while doing `isin`.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/generation/logits_process.py",
    "pr_number": 41332,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2403219068,
    "comment_created_at": "2025-10-03T20:00:39Z"
  },
  {
    "code": "@@ -251,53 +251,40 @@ def __init__(\n         **kwargs,\n     ):\n         if semantic_config is None:\n-            semantic_config = {}\n-            logger.info(\"semantic_config is None. initializing the semantic model with default values.\")\n+            semantic_config = BarkSemanticConfig()\n+            logger.info(\"`semantic_config` is `None`. Initializing the `BarkSemanticConfig` with default values.\")",
    "comment": "let's remove those logs instead wdyt? one of the v5 goals is to have fewer and more informatives warnings and logs!",
    "line_number": 255,
    "enriched": "File: src/transformers/models/bark/configuration_bark.py\nCode: @@ -251,53 +251,40 @@ def __init__(\n         **kwargs,\n     ):\n         if semantic_config is None:\n-            semantic_config = {}\n-            logger.info(\"semantic_config is None. initializing the semantic model with default values.\")\n+            semantic_config = BarkSemanticConfig()\n+            logger.info(\"`semantic_config` is `None`. Initializing the `BarkSemanticConfig` with default values.\")\nComment: Let's remove those logs instead wdyt? One of the v5 goals is to have fewer and more informatives warnings and logs!",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/models/bark/configuration_bark.py",
    "pr_number": 41314,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2406203499,
    "comment_created_at": "2025-10-06T13:10:11Z"
  },
  {
    "code": "@@ -148,7 +169,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # take any layer that contains cache and not empty tensor\n         layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n-        if len(self.key_cache) <= layer_idx:\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].shape[-1] == 0:",
    "comment": "without this check, get_seq_length would return the batch size when the cache is empty, because layers are initialized as torch.tensor([[]] * batch_size, device=device), which is batch size dim for shape[-2].... \ud83e\udee0\ud83e\udee0\ud83e\udee0\ud83e\udee0",
    "line_number": 172,
    "enriched": "File: src/transformers/models/jamba/modeling_jamba.py\nCode: @@ -148,7 +169,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # take any layer that contains cache and not empty tensor\n         layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n-        if len(self.key_cache) <= layer_idx:\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].shape[-1] == 0:\nComment: Without this check, `get_seq_length` would return the batch size when the cache is empty, because layers are initialized as `torch.tensor([[]] * batch_size, device=device)`, which is batch size dim for `shape[-2]`.... \ud83e\udee0\ud83e\udee0\ud83e\udee0\ud83e\udee0",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "src/transformers/models/jamba/modeling_jamba.py",
    "pr_number": 41309,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2402160119,
    "comment_created_at": "2025-10-03T14:28:27Z"
  },
  {
    "code": "@@ -135,6 +135,7 @@ class ConditionalDetrConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"conditional_detr\"\n+    sub_configs = {\"backbone_config\": AutoConfig}",
    "comment": "i am somehow worried that this will cause the problem of permanent changes if a user change something in sub_configs in order to initialize a specific instance.\r\n\r\nthat happens before and quite difficult to debug.\r\n\r\nwould it be possible to do like\r\n\r\n\r\nclass conditionaldetrconfig:\r\n    _ sub_configs  = ...\r\n\r\n    # could make this class method if necessary\r\n    def sub_configs():\r\n        return copy.deepcopy(self._sub_configs)\r\n\r\n\r\n\r\n",
    "line_number": 138,
    "enriched": "File: src/transformers/models/conditional_detr/configuration_conditional_detr.py\nCode: @@ -135,6 +135,7 @@ class ConditionalDetrConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"conditional_detr\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\nComment: I am somehow worried that this will cause the problem of permanent changes if a user change something in `sub_configs` in order to initialize a specific instance.\r\n\r\nThat happens before and quite difficult to debug.\r\n\r\nWould it be possible to do like\r\n\r\n```\r\nclass ConditionalDetrConfig:\r\n    _ sub_configs  = ...\r\n\r\n    # could make this class method if necessary\r\n    def sub_configs():\r\n        return copy.deepcopy(self._sub_configs)\r\n```\r\n\r\n\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/models/conditional_detr/configuration_conditional_detr.py",
    "pr_number": 41308,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2401353760,
    "comment_created_at": "2025-10-03T09:29:04Z"
  },
  {
    "code": "@@ -1335,21 +1335,60 @@ def recursive_diff_dict(dict_a, dict_b, config_obj=None):\n     default = config_obj.__class__().to_dict() if config_obj is not None else {}\n     for key, value in dict_a.items():\n         obj_value = getattr(config_obj, str(key), None)\n-        if isinstance(obj_value, PretrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n+        if isinstance(obj_value, PreTrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n             diff_value = recursive_diff_dict(value, dict_b[key], config_obj=obj_value)\n             diff[key] = diff_value\n         elif key not in dict_b or (value != default[key]):\n             diff[key] = value\n     return diff\n \n \n-PretrainedConfig.push_to_hub = copy_func(PretrainedConfig.push_to_hub)\n-if PretrainedConfig.push_to_hub.__doc__ is not None:\n-    PretrainedConfig.push_to_hub.__doc__ = PretrainedConfig.push_to_hub.__doc__.format(\n+PreTrainedConfig.push_to_hub = copy_func(PreTrainedConfig.push_to_hub)\n+if PreTrainedConfig.push_to_hub.__doc__ is not None:\n+    PreTrainedConfig.push_to_hub.__doc__ = PreTrainedConfig.push_to_hub.__doc__.format(\n         object=\"config\", object_class=\"AutoConfig\", object_files=\"configuration file\"\n     )\n \n \n+class DummyMeta(type):\n+    \"\"\"Dummy metaclass so that `isinstance`/`issubclass` checks against `PretrainedConfig` return True as well\n+    for instance/class of `PreTrainedConfig`.\n+\n+    E.g., thanks to this we have the following (note that LlamaConfig only inherits from PreTrainedConfig, not PretrainedConfig):\n+\n+    ```python\n+    from transformers import LlamaConfig, PretrainedConfig  # the old config name\n+\n+    isinstance(LlamaConfig(), PretrainedConfig)\n+    >>> True\n+    isinstance(LlamaConfig, PretrainedConfig)\n+    >>> True\n+    ```\n+\n+    This is because `isinstance(x, C)` actually maps to `C.__instancecheck__(x)`.\n+    \"\"\"\n+\n+    def __instancecheck__(cls, inst):\n+        \"\"\"Implement isinstance(inst, cls).\"\"\"\n+        return any(cls.__subclasscheck__(c) for c in {type(inst), inst.__class__})\n+\n+    def __subclasscheck__(cls, sub):\n+        \"\"\"Implement issubclass(sub, cls).\"\"\"\n+        logger.warning_once(\n+            \"`PretrainedConfig` is deprecated and will be removed in v5. Please use `PreTrainedConfig` instead!\"\n+        )\n+        candidates = {cls, PreTrainedConfig}\n+        return any(c in candidates for c in sub.mro())\n+\n+\n+class PretrainedConfig(PreTrainedConfig, metaclass=DummyMeta):\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`PretrainedConfig` is deprecated and will be removed in v5. Please use `PreTrainedConfig` instead!\"\n+        )\n+        super().__init__(*args, **kwargs)",
    "comment": "this is a bit fancy but it's the most simple way to keep full bc on the old pretrainedconfig object, while still showing deprecation warning when it's used. the other solution would have been a simple class alias such as\r\n\r\npython\r\npretrainedconfig = pretrainedconfig\r\n\r\n\r\nbut then it's impossible to surface warnings, and users are not advised of the deprecation going on on the object!\r\n\r\na simple class inheritance was not enough imo, as i feel that patterns such as isinstance(my_config, pretrainedconfig) are quite common, and would otherwise break old code because all configs are now pretrainedconfig descendants, not pretrainedconfig descendants\r\n\r\ncurrent code ensures that using the old class in isinstance/issubclass checks still works as expected, i.e.\r\n\r\npython\r\nfrom transformers import pretrainedconfig, llamaconfig\r\n\r\n# this raises a warning and returns true\r\nisinstance(llamaconfig(), pretrainedconfig)\r\n>>> pretrainedconfig is deprecated and will be removed in the future. please use pretrainedconfig instead!\r\n>>> true  # true even though llamaconfig inherits from pretrainedconfig, not pretrainedconfig\r\n",
    "line_number": 1389,
    "enriched": "File: src/transformers/configuration_utils.py\nCode: @@ -1335,21 +1335,60 @@ def recursive_diff_dict(dict_a, dict_b, config_obj=None):\n     default = config_obj.__class__().to_dict() if config_obj is not None else {}\n     for key, value in dict_a.items():\n         obj_value = getattr(config_obj, str(key), None)\n-        if isinstance(obj_value, PretrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n+        if isinstance(obj_value, PreTrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n             diff_value = recursive_diff_dict(value, dict_b[key], config_obj=obj_value)\n             diff[key] = diff_value\n         elif key not in dict_b or (value != default[key]):\n             diff[key] = value\n     return diff\n \n \n-PretrainedConfig.push_to_hub = copy_func(PretrainedConfig.push_to_hub)\n-if PretrainedConfig.push_to_hub.__doc__ is not None:\n-    PretrainedConfig.push_to_hub.__doc__ = PretrainedConfig.push_to_hub.__doc__.format(\n+PreTrainedConfig.push_to_hub = copy_func(PreTrainedConfig.push_to_hub)\n+if PreTrainedConfig.push_to_hub.__doc__ is not None:\n+    PreTrainedConfig.push_to_hub.__doc__ = PreTrainedConfig.push_to_hub.__doc__.format(\n         object=\"config\", object_class=\"AutoConfig\", object_files=\"configuration file\"\n     )\n \n \n+class DummyMeta(type):\n+    \"\"\"Dummy metaclass so that `isinstance`/`issubclass` checks against `PretrainedConfig` return True as well\n+    for instance/class of `PreTrainedConfig`.\n+\n+    E.g., thanks to this we have the following (note that LlamaConfig only inherits from PreTrainedConfig, not PretrainedConfig):\n+\n+    ```python\n+    from transformers import LlamaConfig, PretrainedConfig  # the old config name\n+\n+    isinstance(LlamaConfig(), PretrainedConfig)\n+    >>> True\n+    isinstance(LlamaConfig, PretrainedConfig)\n+    >>> True\n+    ```\n+\n+    This is because `isinstance(x, C)` actually maps to `C.__instancecheck__(x)`.\n+    \"\"\"\n+\n+    def __instancecheck__(cls, inst):\n+        \"\"\"Implement isinstance(inst, cls).\"\"\"\n+        return any(cls.__subclasscheck__(c) for c in {type(inst), inst.__class__})\n+\n+    def __subclasscheck__(cls, sub):\n+        \"\"\"Implement issubclass(sub, cls).\"\"\"\n+        logger.warning_once(\n+            \"`PretrainedConfig` is deprecated and will be removed in v5. Please use `PreTrainedConfig` instead!\"\n+        )\n+        candidates = {cls, PreTrainedConfig}\n+        return any(c in candidates for c in sub.mro())\n+\n+\n+class PretrainedConfig(PreTrainedConfig, metaclass=DummyMeta):\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`PretrainedConfig` is deprecated and will be removed in v5. Please use `PreTrainedConfig` instead!\"\n+        )\n+        super().__init__(*args, **kwargs)\nComment: This is a bit fancy but it's the most simple way to keep full BC on the old `PretrainedConfig` object, while still showing deprecation warning when it's used. The other solution would have been a simple class alias such as\r\n\r\n```python\r\nPretrainedConfig = PreTrainedConfig\r\n```\r\n\r\nbut then it's impossible to surface warnings, and users are not advised of the deprecation going on on the object!\r\n\r\nA simple class inheritance was not enough IMO, as I feel that patterns such as `isinstance(my_config, PretrainedConfig)` are quite common, and would otherwise break old code because all configs are now `PreTrainedConfig` descendants, not `PretrainedConfig` descendants\r\n\r\nCurrent code ensures that using the old class in `isinstance/issubclass` checks still works as expected, i.e.\r\n\r\n```python\r\nfrom transformers import PretrainedConfig, LlamaConfig\r\n\r\n# This raises a warning and returns True\r\nisinstance(LlamaConfig(), PretrainedConfig)\r\n>>> `PretrainedConfig` is deprecated and will be removed in the future. Please use `PreTrainedConfig` instead!\r\n>>> True  # True even though LlamaConfig inherits from PreTrainedConfig, not PretrainedConfig\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/configuration_utils.py",
    "pr_number": 41300,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2400078325,
    "comment_created_at": "2025-10-02T21:13:41Z"
  },
  {
    "code": "@@ -84,7 +84,6 @@ class TextToAudioPipeline(Pipeline):\n     _load_processor = True\n \n     _pipeline_calls_generate = True\n-    _load_processor = False",
    "comment": "the wrong value has been kept here - only false should remain (the last value that was set)",
    "line_number": 87,
    "enriched": "File: src/transformers/pipelines/text_to_audio.py\nCode: @@ -84,7 +84,6 @@ class TextToAudioPipeline(Pipeline):\n     _load_processor = True\n \n     _pipeline_calls_generate = True\n-    _load_processor = False\nComment: The wrong value has been kept here - only False should remain (the last value that was set)",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "src/transformers/pipelines/text_to_audio.py",
    "pr_number": 41293,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2405889975,
    "comment_created_at": "2025-10-06T11:44:45Z"
  },
  {
    "code": "@@ -64,8 +64,6 @@\n     title: Entrenador\n   - local: sagemaker\n     title: Ejecutar el entrenamiento en Amazon SageMaker\n-  - local: serialization",
    "comment": "in es, it, and pt: serialization.md was very outdated and doesn't match our recent practices -> delete",
    "line_number": 67,
    "enriched": "File: docs/source/es/_toctree.yml\nCode: @@ -64,8 +64,6 @@\n     title: Entrenador\n   - local: sagemaker\n     title: Ejecutar el entrenamiento en Amazon SageMaker\n-  - local: serialization\nComment: in `es`, `it`, and `pt`: `serialization.md` was very outdated and doesn't match our recent practices -> delete",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/source/es/_toctree.yml",
    "pr_number": 41286,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2398735437,
    "comment_created_at": "2025-10-02T12:51:43Z"
  },
  {
    "code": "@@ -123,55 +124,6 @@ def get_max_height_width(\n     return (max_height, max_width)\n \n \n-def get_size_with_aspect_ratio(",
    "comment": "not the same as in image_transforms! (mode_size)",
    "line_number": 126,
    "enriched": "File: src/transformers/models/yolos/image_processing_yolos.py\nCode: @@ -123,55 +124,6 @@ def get_max_height_width(\n     return (max_height, max_width)\n \n \n-def get_size_with_aspect_ratio(\nComment: Not the same as in image_transforms! (mode_size)",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "src/transformers/models/yolos/image_processing_yolos.py",
    "pr_number": 41284,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2403224624,
    "comment_created_at": "2025-10-03T20:03:54Z"
  },
  {
    "code": "@@ -80,30 +81,15 @@ def __init__(self, quantization_config, **kwargs):\n     def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n             raise ImportError(\n-                f\"Using `bitsandbytes` 4-bit quantization requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n+                f\"Using `bitsandbytes` 4-bit quantization requires accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n             )\n-        if not is_bitsandbytes_available(check_library_only=True):\n+        if not is_bitsandbytes_available():\n             raise ImportError(\n-                \"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\n+                f\"Using `bitsandbytes` 4-bit quantization requires bitsandbytes: `pip install -U bitsandbytes>={BITSANDBYTES_MIN_VERSION}`\"\n             )\n-        if not is_torch_available():\n-            raise ImportError(\n-                \"The bitsandbytes library requires PyTorch but it was not found in your environment. \"\n-                \"You can install it with `pip install torch`.\"\n-            )",
    "comment": "why removing the torch check ?",
    "line_number": 93,
    "enriched": "File: src/transformers/quantizers/quantizer_bnb_4bit.py\nCode: @@ -80,30 +81,15 @@ def __init__(self, quantization_config, **kwargs):\n     def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n             raise ImportError(\n-                f\"Using `bitsandbytes` 4-bit quantization requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n+                f\"Using `bitsandbytes` 4-bit quantization requires accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n             )\n-        if not is_bitsandbytes_available(check_library_only=True):\n+        if not is_bitsandbytes_available():\n             raise ImportError(\n-                \"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\n+                f\"Using `bitsandbytes` 4-bit quantization requires bitsandbytes: `pip install -U bitsandbytes>={BITSANDBYTES_MIN_VERSION}`\"\n             )\n-        if not is_torch_available():\n-            raise ImportError(\n-                \"The bitsandbytes library requires PyTorch but it was not found in your environment. \"\n-                \"You can install it with `pip install torch`.\"\n-            )\nComment: why removing the torch check ?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "src/transformers/quantizers/quantizer_bnb_4bit.py",
    "pr_number": 41283,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2398696061,
    "comment_created_at": "2025-10-02T12:42:18Z"
  },
  {
    "code": "@@ -1007,6 +1007,7 @@ def get_video_features(\n \n     @auto_docstring\n     @can_return_tuple\n+    @check_model_inputs",
    "comment": "we don't need both afaik, check_model_inputs will handle tuple/non-tuple as well",
    "line_number": 1010,
    "enriched": "File: src/transformers/models/qwen3_vl/modular_qwen3_vl.py\nCode: @@ -1007,6 +1007,7 @@ def get_video_features(\n \n     @auto_docstring\n     @can_return_tuple\n+    @check_model_inputs\nComment: we don't need both afaik, `check_model_inputs` will handle tuple/non-tuple as well",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
    "pr_number": 41277,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2398014444,
    "comment_created_at": "2025-10-02T09:35:41Z"
  },
  {
    "code": "@@ -615,7 +615,9 @@ def test_torch_whisper_batched(self):\n             {\"text\": \" Nor is Mr. Quilters' manner less interesting than his matter.\"},\n         ]\n \n-        output = speech_recognizer(ds[\"audio\"], batch_size=2)\n+\n+        audio_arrays = [x.get_all_samples().data for x in ds[\"audio\"]]\n+        output = speech_recognizer(audio_arrays, batch_size=2)",
    "comment": "preprocess, as of now, doesn't support multi audiodecoder. for this case, should decode outside then put them in.",
    "line_number": 620,
    "enriched": "File: tests/pipelines/test_pipelines_automatic_speech_recognition.py\nCode: @@ -615,7 +615,9 @@ def test_torch_whisper_batched(self):\n             {\"text\": \" Nor is Mr. Quilters' manner less interesting than his matter.\"},\n         ]\n \n-        output = speech_recognizer(ds[\"audio\"], batch_size=2)\n+\n+        audio_arrays = [x.get_all_samples().data for x in ds[\"audio\"]]\n+        output = speech_recognizer(audio_arrays, batch_size=2)\nComment: `preprocess`, as of now, doesn't support multi `AudioDecoder`. For this case, should decode outside then put them in.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
    "pr_number": 41275,
    "repo": "transformers",
    "owner": "huggingface",
    "comment_id": 2396911965,
    "comment_created_at": "2025-10-02T05:09:08Z"
  },
  {
    "code": "@@ -130,10 +130,9 @@ def step(self, action):\n         obs, rews, terminateds, truncateds, infos = self.env.step(action)\n         if not self.is_vector_env:\n             rews = np.array([rews])\n-        self.returns = self.returns * self.gamma + rews\n-        rews = self.normalize(rews)\n-        dones = np.logical_or(terminateds, truncateds)\n-        self.returns[dones] = 0.0\n+        dones = np.logical_or(terminateds, truncateds)            \n+        self.returns = self.returns * self.gamma * (1-dones) + rews",
    "comment": "should this not be (1-terminated) as in cases of truncation, the state values should still be boostrapped.",
    "line_number": 134,
    "enriched": "File: gym/wrappers/normalize.py\nCode: @@ -130,10 +130,9 @@ def step(self, action):\n         obs, rews, terminateds, truncateds, infos = self.env.step(action)\n         if not self.is_vector_env:\n             rews = np.array([rews])\n-        self.returns = self.returns * self.gamma + rews\n-        rews = self.normalize(rews)\n-        dones = np.logical_or(terminateds, truncateds)\n-        self.returns[dones] = 0.0\n+        dones = np.logical_or(terminateds, truncateds)            \n+        self.returns = self.returns * self.gamma * (1-dones) + rews\nComment: Should this not be `(1-terminated)` as in cases of truncation, the state values should still be boostrapped.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "gym/wrappers/normalize.py",
    "pr_number": 3152,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 1029924050,
    "comment_created_at": "2022-11-23T00:02:49Z"
  },
  {
    "code": "@@ -608,8 +608,8 @@ def step(self, action: np.ndarray):\n     def render(self):\n         if self.render_mode is None:\n             gym.logger.warn(",
    "comment": "should this be a warning or an error? i think it should be an error as otherwise users will ignore it and they shouldn't be ignoring it. \r\nalso i think, we should have the render_mode defaulting to human as that was the previous default used",
    "line_number": 610,
    "enriched": "File: gym/envs/box2d/bipedal_walker.py\nCode: @@ -608,8 +608,8 @@ def step(self, action: np.ndarray):\n     def render(self):\n         if self.render_mode is None:\n             gym.logger.warn(\nComment: Should this be a warning or an error? I think it should be an error as otherwise users will ignore it and they shouldn't be ignoring it. \r\nAlso I think, we should have the render_mode defaulting to human as that was the previous default used",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/envs/box2d/bipedal_walker.py",
    "pr_number": 3112,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 986611894,
    "comment_created_at": "2022-10-04T08:57:46Z"
  },
  {
    "code": "@@ -95,10 +95,9 @@ def __init__(\n \n         default_render_kwargs = {}\n         if not env.render_mode:\n-            default_render_kwargs = {\"mode\": \"rgb_array_list\"}\n-            logger.warn(\n+            raise AttributeError(\n                 \"env.render_mode must be specified to use PixelObservationWrapper:\"\n-                \"`gym.make(env_name, render_mode='rgb_array')`.\"\n+                \"`gym.make(env_name, render_mode='single_rgb_array')`.\"",
    "comment": "we can reverted the meaning of the render_modes in v25 back to the previous meaning where rgb_array means single render obs and rgb_array_list will mean list of render obs",
    "line_number": 100,
    "enriched": "File: gym/wrappers/pixel_observation.py\nCode: @@ -95,10 +95,9 @@ def __init__(\n \n         default_render_kwargs = {}\n         if not env.render_mode:\n-            default_render_kwargs = {\"mode\": \"rgb_array_list\"}\n-            logger.warn(\n+            raise AttributeError(\n                 \"env.render_mode must be specified to use PixelObservationWrapper:\"\n-                \"`gym.make(env_name, render_mode='rgb_array')`.\"\n+                \"`gym.make(env_name, render_mode='single_rgb_array')`.\"\nComment: We can reverted the meaning of the render_modes in v25 back to the previous meaning where `rgb_array` means single render obs and `rgb_array_list` will mean list of render obs",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "gym/wrappers/pixel_observation.py",
    "pr_number": 3076,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 965972694,
    "comment_created_at": "2022-09-08T13:29:32Z"
  },
  {
    "code": "@@ -0,0 +1,120 @@\n+\"\"\"A compatibility wrapper converting an old-style environment into a valid environment.\"\"\"\n+import sys\n+from typing import Any, Dict, Optional, Tuple\n+\n+import gym\n+from gym.core import ObsType\n+from gym.utils.step_api_compatibility import convert_to_terminated_truncated_step_api\n+\n+if sys.version_info >= (3, 8):\n+    from typing import Protocol, runtime_checkable\n+elif sys.version_info >= (3, 7):\n+    from typing_extensions import Protocol, runtime_checkable\n+else:\n+    Protocol = object\n+    runtime_checkable = lambda x: x  # noqa: E731\n+\n+\n+@runtime_checkable\n+class LegacyEnv(Protocol):\n+    \"\"\"A protocol for environments using the old step API.\"\"\"\n+\n+    observation_space: gym.Space\n+    action_space: gym.Space\n+\n+    def reset(self) -> Any:\n+        \"\"\"Reset the environment and return the initial observation.\"\"\"\n+        ...\n+\n+    def step(self, action: Any) -> Tuple[Any, float, bool, Dict]:\n+        \"\"\"Run one timestep of the environment's dynamics.\"\"\"\n+        ...\n+\n+    def render(self, mode: Optional[str] = \"human\") -> Any:\n+        \"\"\"Render the environment.\"\"\"\n+        ...\n+\n+    def close(self):\n+        \"\"\"Close the environment.\"\"\"\n+        ...\n+\n+    def seed(self, seed: Optional[int] = None):\n+        \"\"\"Set the seed for this env's random number generator(s).\"\"\"\n+        ...\n+\n+\n+class EnvCompatibility(gym.Env):\n+    r\"\"\"A wrapper which can transform an environment from new step API to old and vice-versa.\n+\n+    Old step API refers to step() method returning (observation, reward, done, info)\n+    New step API refers to step() method returning (observation, reward, terminated, truncated, info)\n+    (Refer to docs for details on the API change)\n+\n+    Known limitations:\n+    - Environments that use `self.np_random` might not work as expected.\n+    - `env.render_mode` cannot be set in `make` if the environment doesn't accept it as a parameter (e.g. via kwargs).\n+\n+    Args:\n+        old_env (gym.Env): the env to wrap. Can be in old or new API\n+\n+    \"\"\"\n+\n+    def __init__(self, old_env: LegacyEnv, render_mode: Optional[str] = None):\n+        \"\"\"A wrapper which converts old-style envs to valid modern envs.\n+\n+        Some information may be lost in the conversion, so we recommend updating your environment.\n+\n+        Args:\n+            old_env (LegacyEnv): the env to wrap, implemented with the old API\n+            render_mode (str): the render mode to use when rendering the environment, passed automatically to env.render\n+        \"\"\"\n+        self.metadata = getattr(old_env, \"metadata\", {\"render_modes\": []})\n+        self.render_mode = render_mode\n+        self.reward_range = getattr(old_env, \"reward_range\", None)\n+        self.spec = getattr(old_env, \"spec\", None)\n+        self.env = old_env\n+\n+        self.observation_space = old_env.observation_space\n+        self.action_space = old_env.action_space\n+\n+    def reset(\n+        self, seed: Optional[int] = None, options: Optional[dict] = None\n+    ) -> Tuple[ObsType, dict]:\n+        \"\"\"Resets the environment.\n+\n+        Args:\n+            seed: the seed to reset the environment with\n+            options: the options to reset the environment with\n+\n+        Returns:\n+            (observation, info)\n+        \"\"\"\n+        if seed is not None:\n+            self.env.seed(seed)\n+        # Options are ignored\n+        return self.env.reset(), {}\n+\n+    def step(self, action: Any) -> Tuple[Any, float, bool, bool, Dict]:\n+        \"\"\"Steps through the environment, returning 5 or 4 items depending on `apply_step_compatibility`.",
    "comment": "does apply_step_compatibility exist?",
    "line_number": 98,
    "enriched": "File: gym/wrappers/compatibility.py\nCode: @@ -0,0 +1,120 @@\n+\"\"\"A compatibility wrapper converting an old-style environment into a valid environment.\"\"\"\n+import sys\n+from typing import Any, Dict, Optional, Tuple\n+\n+import gym\n+from gym.core import ObsType\n+from gym.utils.step_api_compatibility import convert_to_terminated_truncated_step_api\n+\n+if sys.version_info >= (3, 8):\n+    from typing import Protocol, runtime_checkable\n+elif sys.version_info >= (3, 7):\n+    from typing_extensions import Protocol, runtime_checkable\n+else:\n+    Protocol = object\n+    runtime_checkable = lambda x: x  # noqa: E731\n+\n+\n+@runtime_checkable\n+class LegacyEnv(Protocol):\n+    \"\"\"A protocol for environments using the old step API.\"\"\"\n+\n+    observation_space: gym.Space\n+    action_space: gym.Space\n+\n+    def reset(self) -> Any:\n+        \"\"\"Reset the environment and return the initial observation.\"\"\"\n+        ...\n+\n+    def step(self, action: Any) -> Tuple[Any, float, bool, Dict]:\n+        \"\"\"Run one timestep of the environment's dynamics.\"\"\"\n+        ...\n+\n+    def render(self, mode: Optional[str] = \"human\") -> Any:\n+        \"\"\"Render the environment.\"\"\"\n+        ...\n+\n+    def close(self):\n+        \"\"\"Close the environment.\"\"\"\n+        ...\n+\n+    def seed(self, seed: Optional[int] = None):\n+        \"\"\"Set the seed for this env's random number generator(s).\"\"\"\n+        ...\n+\n+\n+class EnvCompatibility(gym.Env):\n+    r\"\"\"A wrapper which can transform an environment from new step API to old and vice-versa.\n+\n+    Old step API refers to step() method returning (observation, reward, done, info)\n+    New step API refers to step() method returning (observation, reward, terminated, truncated, info)\n+    (Refer to docs for details on the API change)\n+\n+    Known limitations:\n+    - Environments that use `self.np_random` might not work as expected.\n+    - `env.render_mode` cannot be set in `make` if the environment doesn't accept it as a parameter (e.g. via kwargs).\n+\n+    Args:\n+        old_env (gym.Env): the env to wrap. Can be in old or new API\n+\n+    \"\"\"\n+\n+    def __init__(self, old_env: LegacyEnv, render_mode: Optional[str] = None):\n+        \"\"\"A wrapper which converts old-style envs to valid modern envs.\n+\n+        Some information may be lost in the conversion, so we recommend updating your environment.\n+\n+        Args:\n+            old_env (LegacyEnv): the env to wrap, implemented with the old API\n+            render_mode (str): the render mode to use when rendering the environment, passed automatically to env.render\n+        \"\"\"\n+        self.metadata = getattr(old_env, \"metadata\", {\"render_modes\": []})\n+        self.render_mode = render_mode\n+        self.reward_range = getattr(old_env, \"reward_range\", None)\n+        self.spec = getattr(old_env, \"spec\", None)\n+        self.env = old_env\n+\n+        self.observation_space = old_env.observation_space\n+        self.action_space = old_env.action_space\n+\n+    def reset(\n+        self, seed: Optional[int] = None, options: Optional[dict] = None\n+    ) -> Tuple[ObsType, dict]:\n+        \"\"\"Resets the environment.\n+\n+        Args:\n+            seed: the seed to reset the environment with\n+            options: the options to reset the environment with\n+\n+        Returns:\n+            (observation, info)\n+        \"\"\"\n+        if seed is not None:\n+            self.env.seed(seed)\n+        # Options are ignored\n+        return self.env.reset(), {}\n+\n+    def step(self, action: Any) -> Tuple[Any, float, bool, bool, Dict]:\n+        \"\"\"Steps through the environment, returning 5 or 4 items depending on `apply_step_compatibility`.\nComment: Does `apply_step_compatibility` exist? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/wrappers/compatibility.py",
    "pr_number": 3066,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 963060206,
    "comment_created_at": "2022-09-05T17:10:00Z"
  },
  {
    "code": "@@ -61,11 +61,11 @@ def generate_random_map(size: int = 8, p: float = 0.8) -> List[str]:\n         A random valid map\n     \"\"\"\n     valid = False\n-    board = []  # initialize to make pyright happy\n+    board: List[List[str]] = []  # initialize to make pyright happy",
    "comment": "np.ndarray? though from experience typing with numpy is a pain",
    "line_number": 64,
    "enriched": "File: gym/envs/toy_text/frozen_lake.py\nCode: @@ -61,11 +61,11 @@ def generate_random_map(size: int = 8, p: float = 0.8) -> List[str]:\n         A random valid map\n     \"\"\"\n     valid = False\n-    board = []  # initialize to make pyright happy\n+    board: List[List[str]] = []  # initialize to make pyright happy\nComment: `np.ndarray`? Though from experience typing with numpy is a pain",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/envs/toy_text/frozen_lake.py",
    "pr_number": 3061,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 962341394,
    "comment_created_at": "2022-09-04T16:45:11Z"
  },
  {
    "code": "@@ -52,13 +53,14 @@ def test_make():\n \n \n def test_make_deprecated():\n-    with pytest.raises(\n-        gym.error.Error,\n-        match=re.escape(\n-            \"Environment version v0 for `Humanoid` is deprecated. Please use `Humanoid-v4` instead.\"\n-        ),\n-    ):\n-        gym.make(\"Humanoid-v0\", disable_env_checker=True)\n+    with warnings.catch_warnings(record=True):",
    "comment": "don't we want to check whether the caught warnings can be safely ignored? as we do in /tests/envs/test_envs.py?",
    "line_number": 56,
    "enriched": "File: tests/envs/test_make.py\nCode: @@ -52,13 +53,14 @@ def test_make():\n \n \n def test_make_deprecated():\n-    with pytest.raises(\n-        gym.error.Error,\n-        match=re.escape(\n-            \"Environment version v0 for `Humanoid` is deprecated. Please use `Humanoid-v4` instead.\"\n-        ),\n-    ):\n-        gym.make(\"Humanoid-v0\", disable_env_checker=True)\n+    with warnings.catch_warnings(record=True):\nComment: Don't we want to check whether the caught warnings can be safely ignored? As we do in /tests/envs/test_envs.py?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/envs/test_make.py",
    "pr_number": 3050,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 958669364,
    "comment_created_at": "2022-08-30T16:01:28Z"
  },
  {
    "code": "@@ -0,0 +1,140 @@\n+from collections import OrderedDict\n+\n+import numpy as np\n+import pytest\n+\n+from gym.spaces import Box, Dict, Discrete\n+\n+\n+def test_dict_init():\n+    with pytest.raises(\n+        AssertionError,\n+        match=r\"^Unexpected Dict space input, expecting dict, OrderedDict or Sequence, actual type: \",\n+    ):\n+        Dict(Discrete(2))\n+\n+    with pytest.raises(\n+        ValueError,\n+        match=\"Dict space keyword 'a' already exists in the spaces dictionary\",\n+    ):\n+        Dict({\"a\": Discrete(3)}, a=Box(0, 1))\n+\n+    with pytest.raises(\n+        AssertionError,\n+        match=\"Dict space element is not an instance of Space: key='b', space=Box\",\n+    ):\n+        Dict(a=Discrete(2), b=\"Box\")\n+\n+    with pytest.warns(None) as warnings:\n+        a = Dict({\"a\": Discrete(2), \"b\": Box(low=0.0, high=1.0)})\n+        b = Dict(OrderedDict(a=Discrete(2), b=Box(low=0.0, high=1.0)))\n+        c = Dict(((\"a\", Discrete(2)), (\"b\", Box(low=0.0, high=1.0))))\n+        d = Dict(a=Discrete(2), b=Box(low=0.0, high=1.0))\n+\n+        assert a == b == c == d\n+    assert len(warnings) == 0\n+\n+    with pytest.warns(None) as warnings:\n+        Dict({1: Discrete(2), \"a\": Discrete(3)})\n+    assert len(warnings) == 0\n+\n+\n+DICT_SPACE = Dict(\n+    {\n+        \"a\": Box(low=0, high=1, shape=(3, 3)),\n+        \"b\": Dict(\n+            {\n+                \"b_1\": Box(low=-100, high=100, shape=(2,)),\n+                \"b_2\": Box(low=-1, high=1, shape=(2,)),\n+            }\n+        ),\n+        \"c\": Discrete(5),\n+    }\n+)\n+\n+\n+def test_dict_seeding():\n+    seeds = DICT_SPACE.seed(\n+        {\n+            \"a\": 0,\n+            \"b\": {\n+                \"b_1\": 1,\n+                \"b_2\": 2,\n+            },\n+            \"c\": 3,\n+        }\n+    )\n+    assert all(isinstance(seed, int) for seed in seeds)\n+\n+    # \"Unpack\" the dict sub-spaces into individual spaces\n+    a = Box(low=0, high=1, shape=(3, 3), seed=0)\n+    b_1 = Box(low=-100, high=100, shape=(2,), seed=1)\n+    b_2 = Box(low=-1, high=1, shape=(2,), seed=2)\n+    c = Discrete(5, seed=3)\n+\n+    for i in range(10):\n+        dict_sample = DICT_SPACE.sample()\n+        assert np.all(dict_sample[\"a\"] == a.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_1\"] == b_1.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_2\"] == b_2.sample())\n+        assert dict_sample[\"c\"] == c.sample()\n+\n+\n+def test_int_seeding():\n+    seeds = DICT_SPACE.seed(1)\n+    assert all(isinstance(seed, int) for seed in seeds)\n+\n+    # rng, seeds = seeding.np_random(1)\n+    # subseeds = rng.choice(np.iinfo(int).max, size=3, replace=False)\n+    # b_rng, b_seeds = seeding.np_random(int(subseeds[1]))\n+    # b_subseeds = b_rng.choice(np.iinfo(int).max, size=2, replace=False)\n+\n+    # \"Unpack\" the dict sub-spaces into individual spaces\n+    a = Box(low=0, high=1, shape=(3, 3), seed=seeds[1])\n+    b_1 = Box(low=-100, high=100, shape=(2,), seed=seeds[3])\n+    b_2 = Box(low=-1, high=1, shape=(2,), seed=seeds[4])\n+    c = Discrete(5, seed=seeds[5])\n+\n+    for i in range(10):\n+        dict_sample = DICT_SPACE.sample()\n+        assert np.all(dict_sample[\"a\"] == a.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_1\"] == b_1.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_2\"] == b_2.sample())\n+        assert dict_sample[\"c\"] == c.sample()\n+\n+\n+def test_none_seeding():\n+    seeds = DICT_SPACE.seed(None)\n+    assert len(seeds) == 4 and all(isinstance(seed, int) for seed in seeds)",
    "comment": "maybe good to add a line to see if results don't repeat for none? \r\neg. \r\npython\r\nseeds1 = dict_space.seed(none)\r\nseeds2 = dict_space.seed(none)\r\n\r\nassert any([seed1 != seed2 for seed1, seed2 in zip(seeds1, seeds2)]) \r\n\r\ncan use all instead of any if we're confident that two same seeds won't be sampled",
    "line_number": 108,
    "enriched": "File: tests/spaces/test_dict.py\nCode: @@ -0,0 +1,140 @@\n+from collections import OrderedDict\n+\n+import numpy as np\n+import pytest\n+\n+from gym.spaces import Box, Dict, Discrete\n+\n+\n+def test_dict_init():\n+    with pytest.raises(\n+        AssertionError,\n+        match=r\"^Unexpected Dict space input, expecting dict, OrderedDict or Sequence, actual type: \",\n+    ):\n+        Dict(Discrete(2))\n+\n+    with pytest.raises(\n+        ValueError,\n+        match=\"Dict space keyword 'a' already exists in the spaces dictionary\",\n+    ):\n+        Dict({\"a\": Discrete(3)}, a=Box(0, 1))\n+\n+    with pytest.raises(\n+        AssertionError,\n+        match=\"Dict space element is not an instance of Space: key='b', space=Box\",\n+    ):\n+        Dict(a=Discrete(2), b=\"Box\")\n+\n+    with pytest.warns(None) as warnings:\n+        a = Dict({\"a\": Discrete(2), \"b\": Box(low=0.0, high=1.0)})\n+        b = Dict(OrderedDict(a=Discrete(2), b=Box(low=0.0, high=1.0)))\n+        c = Dict(((\"a\", Discrete(2)), (\"b\", Box(low=0.0, high=1.0))))\n+        d = Dict(a=Discrete(2), b=Box(low=0.0, high=1.0))\n+\n+        assert a == b == c == d\n+    assert len(warnings) == 0\n+\n+    with pytest.warns(None) as warnings:\n+        Dict({1: Discrete(2), \"a\": Discrete(3)})\n+    assert len(warnings) == 0\n+\n+\n+DICT_SPACE = Dict(\n+    {\n+        \"a\": Box(low=0, high=1, shape=(3, 3)),\n+        \"b\": Dict(\n+            {\n+                \"b_1\": Box(low=-100, high=100, shape=(2,)),\n+                \"b_2\": Box(low=-1, high=1, shape=(2,)),\n+            }\n+        ),\n+        \"c\": Discrete(5),\n+    }\n+)\n+\n+\n+def test_dict_seeding():\n+    seeds = DICT_SPACE.seed(\n+        {\n+            \"a\": 0,\n+            \"b\": {\n+                \"b_1\": 1,\n+                \"b_2\": 2,\n+            },\n+            \"c\": 3,\n+        }\n+    )\n+    assert all(isinstance(seed, int) for seed in seeds)\n+\n+    # \"Unpack\" the dict sub-spaces into individual spaces\n+    a = Box(low=0, high=1, shape=(3, 3), seed=0)\n+    b_1 = Box(low=-100, high=100, shape=(2,), seed=1)\n+    b_2 = Box(low=-1, high=1, shape=(2,), seed=2)\n+    c = Discrete(5, seed=3)\n+\n+    for i in range(10):\n+        dict_sample = DICT_SPACE.sample()\n+        assert np.all(dict_sample[\"a\"] == a.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_1\"] == b_1.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_2\"] == b_2.sample())\n+        assert dict_sample[\"c\"] == c.sample()\n+\n+\n+def test_int_seeding():\n+    seeds = DICT_SPACE.seed(1)\n+    assert all(isinstance(seed, int) for seed in seeds)\n+\n+    # rng, seeds = seeding.np_random(1)\n+    # subseeds = rng.choice(np.iinfo(int).max, size=3, replace=False)\n+    # b_rng, b_seeds = seeding.np_random(int(subseeds[1]))\n+    # b_subseeds = b_rng.choice(np.iinfo(int).max, size=2, replace=False)\n+\n+    # \"Unpack\" the dict sub-spaces into individual spaces\n+    a = Box(low=0, high=1, shape=(3, 3), seed=seeds[1])\n+    b_1 = Box(low=-100, high=100, shape=(2,), seed=seeds[3])\n+    b_2 = Box(low=-1, high=1, shape=(2,), seed=seeds[4])\n+    c = Discrete(5, seed=seeds[5])\n+\n+    for i in range(10):\n+        dict_sample = DICT_SPACE.sample()\n+        assert np.all(dict_sample[\"a\"] == a.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_1\"] == b_1.sample())\n+        assert np.all(dict_sample[\"b\"][\"b_2\"] == b_2.sample())\n+        assert dict_sample[\"c\"] == c.sample()\n+\n+\n+def test_none_seeding():\n+    seeds = DICT_SPACE.seed(None)\n+    assert len(seeds) == 4 and all(isinstance(seed, int) for seed in seeds)\nComment: Maybe good to add a line to see if results don't repeat for `None`? \r\nEg. \r\n```python\r\nseeds1 = DICT_SPACE.seed(None)\r\nseeds2 = DICT_SPACE.seed(None)\r\n\r\nassert any([seed1 != seed2 for seed1, seed2 in zip(seeds1, seeds2)]) \r\n```\r\nCan use `all` instead of `any` if we're confident that two same seeds won't be sampled ",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "tests/spaces/test_dict.py",
    "pr_number": 3047,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 962111237,
    "comment_created_at": "2022-09-03T06:17:27Z"
  },
  {
    "code": "@@ -143,7 +143,7 @@ def test_env_version_suggestions(\n def test_register_versioned_unversioned():\n     # Register versioned then unversioned\n     versioned_env = \"Test/MyEnv-v0\"\n-    gym.register(versioned_env)\n+    gym.register(versioned_env, None)",
    "comment": "is this being type checked? because you're simultaneously disallowing entry_point=none in the type system, and adding an explicit entry_point=none in the usage.",
    "line_number": 146,
    "enriched": "File: tests/envs/test_register.py\nCode: @@ -143,7 +143,7 @@ def test_env_version_suggestions(\n def test_register_versioned_unversioned():\n     # Register versioned then unversioned\n     versioned_env = \"Test/MyEnv-v0\"\n-    gym.register(versioned_env)\n+    gym.register(versioned_env, None)\nComment: Is this being type checked? Because you're simultaneously disallowing `entry_point=None` in the type system, and adding an explicit `entry_point=None` in the usage.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/envs/test_register.py",
    "pr_number": 3041,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 953208688,
    "comment_created_at": "2022-08-24T00:24:11Z"
  },
  {
    "code": "@@ -10,31 +10,18 @@\n \n import gym\n from gym.utils.play import MissingKeysToAction, PlayableGame, play\n+from tests.testing_env import GenericTestEnv\n \n RELEVANT_KEY_1 = ord(\"a\")  # 97\n RELEVANT_KEY_2 = ord(\"d\")  # 100\n IRRELEVANT_KEY = 1\n \n \n-@dataclass\n-class DummyEnvSpec:\n-    id: str\n-\n-\n-class DummyPlayEnv(gym.Env):\n-    def __init__(self, render_mode: Optional[str] = None):\n-        self.render_mode = render_mode\n-\n-    def step(self, action):\n-        obs = np.zeros((1, 1))\n-        rew, terminated, truncated, info = 1, False, False, {}\n-        return obs, rew, terminated, truncated, info\n-\n-    def reset(self, seed=None):\n-        ...\n-\n-    def render(self):\n-        return np.zeros((1, 1))\n+PlayableEnv = partial(\n+    GenericTestEnv,\n+    render_modes=[\"rgb_array\"],",
    "comment": "just a comment unrelated to this pr: i would vote to have metadata arg instead of render_modes and render_fps; render_modes can be easily confused with render_mode. also metadata is more customizable (e.g., i want to test an env with invalid metadata or i want other fields in it).",
    "line_number": 22,
    "enriched": "File: tests/utils/test_play.py\nCode: @@ -10,31 +10,18 @@\n \n import gym\n from gym.utils.play import MissingKeysToAction, PlayableGame, play\n+from tests.testing_env import GenericTestEnv\n \n RELEVANT_KEY_1 = ord(\"a\")  # 97\n RELEVANT_KEY_2 = ord(\"d\")  # 100\n IRRELEVANT_KEY = 1\n \n \n-@dataclass\n-class DummyEnvSpec:\n-    id: str\n-\n-\n-class DummyPlayEnv(gym.Env):\n-    def __init__(self, render_mode: Optional[str] = None):\n-        self.render_mode = render_mode\n-\n-    def step(self, action):\n-        obs = np.zeros((1, 1))\n-        rew, terminated, truncated, info = 1, False, False, {}\n-        return obs, rew, terminated, truncated, info\n-\n-    def reset(self, seed=None):\n-        ...\n-\n-    def render(self):\n-        return np.zeros((1, 1))\n+PlayableEnv = partial(\n+    GenericTestEnv,\n+    render_modes=[\"rgb_array\"],\nComment: Just a comment unrelated to this PR: I would vote to have `metadata` arg instead of `render_modes` and `render_fps`; `render_modes` can be easily confused with `render_mode`. Also `metadata` is more customizable (e.g., I want to test an env with invalid metadata or I want other fields in it).",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "tests/utils/test_play.py",
    "pr_number": 3040,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 959869348,
    "comment_created_at": "2022-08-31T17:57:11Z"
  },
  {
    "code": "@@ -15,33 +18,33 @@ class StepAPICompatibility(gym.Wrapper):\n \n     Args:\n         env (gym.Env): the env to wrap. Can be in old or new API\n-        new_step_api (bool): True to use env with new step API, False to use env with old step API. (False by default)\n+        to_termination_truncation_api (bool): True to use env with new step API, False to use env with old step API. (False by default)",
    "comment": "i think it's better to use to_terminated_truncated_api to be consistent with the actual variable names",
    "line_number": 21,
    "enriched": "File: gym/wrappers/step_api_compatibility.py\nCode: @@ -15,33 +18,33 @@ class StepAPICompatibility(gym.Wrapper):\n \n     Args:\n         env (gym.Env): the env to wrap. Can be in old or new API\n-        new_step_api (bool): True to use env with new step API, False to use env with old step API. (False by default)\n+        to_termination_truncation_api (bool): True to use env with new step API, False to use env with old step API. (False by default)\nComment: I think it's better to use `to_terminated_truncated_api` to be consistent with the actual variable names",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "gym/wrappers/step_api_compatibility.py",
    "pr_number": 3028,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 946587481,
    "comment_created_at": "2022-08-16T10:13:09Z"
  },
  {
    "code": "@@ -22,6 +22,9 @@ class DummyEnvSpec:\n \n \n class DummyPlayEnv(gym.Env):\n+    def __init__(self, render_mode=None):",
    "comment": "type annotation for the argument? i know it's only a test, but let's keep it clean. i think it'd be optional[str], right?",
    "line_number": 25,
    "enriched": "File: tests/utils/test_play.py\nCode: @@ -22,6 +22,9 @@ class DummyEnvSpec:\n \n \n class DummyPlayEnv(gym.Env):\n+    def __init__(self, render_mode=None):\nComment: Type annotation for the argument? I know it's only a test, but let's keep it clean. I think it'd be `Optional[str]`, right?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/utils/test_play.py",
    "pr_number": 3027,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 949999938,
    "comment_created_at": "2022-08-19T09:28:23Z"
  },
  {
    "code": "@@ -193,6 +202,14 @@ def __repr__(self) -> str:\n         \"\"\"Gives a string representation of this space.\"\"\"\n         return \"Dict(\" + \", \".join([f\"{k}: {s}\" for k, s in self.spaces.items()]) + \")\"\n \n+    def __eq__(self, other) -> bool:\n+        \"\"\"Check whether `other` is equivalent to this instance.\"\"\"\n+        return (\n+            isinstance(other, Dict)\n+            # Comparison of `OrderedDict`s is order-sensitive",
    "comment": "why are we not checking the keys are the same? \r\nstrange that we only checking the spaces",
    "line_number": 209,
    "enriched": "File: gym/spaces/dict.py\nCode: @@ -193,6 +202,14 @@ def __repr__(self) -> str:\n         \"\"\"Gives a string representation of this space.\"\"\"\n         return \"Dict(\" + \", \".join([f\"{k}: {s}\" for k, s in self.spaces.items()]) + \")\"\n \n+    def __eq__(self, other) -> bool:\n+        \"\"\"Check whether `other` is equivalent to this instance.\"\"\"\n+        return (\n+            isinstance(other, Dict)\n+            # Comparison of `OrderedDict`s is order-sensitive\nComment: Why are we not checking the keys are the same? \r\nStrange that we only checking the spaces",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/spaces/dict.py",
    "pr_number": 3024,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 945169037,
    "comment_created_at": "2022-08-13T17:00:07Z"
  },
  {
    "code": "@@ -31,201 +27,4 @@ def np_random(seed: Optional[int] = None) -> Tuple[\"RandomNumberGenerator\", Any]\n     return rng, np_seed\n \n \n-# TODO: Remove this class and make it alias to `Generator` in a future Gym release\n-# RandomNumberGenerator = np.random.Generator\n-class RandomNumberGenerator(np.random.Generator):\n-    \"\"\"Random number generator class that inherits from numpy's random Generator class.\"\"\"\n-\n-    def rand(self, *size):\n-        \"\"\"Deprecated rand function using random.\"\"\"\n-        deprecation(\n-            \"Function `rng.rand(*size)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `Generator.random(size)` instead.\"\n-        )\n-\n-        return self.random(size)\n-\n-    random_sample = rand\n-\n-    def randn(self, *size):\n-        \"\"\"Deprecated random standard normal function use standard_normal.\"\"\"\n-        deprecation(\n-            \"Function `rng.randn(*size)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.standard_normal(size)` instead.\"\n-        )\n-\n-        return self.standard_normal(size)\n-\n-    def randint(self, low, high=None, size=None, dtype=int):\n-        \"\"\"Deprecated random integer function use integers.\"\"\"\n-        deprecation(\n-            \"Function `rng.randint(low, [high, size, dtype])` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.integers(low, [high, size, dtype])` instead.\"\n-        )\n-\n-        return self.integers(low=low, high=high, size=size, dtype=dtype)\n-\n-    random_integers = randint\n-\n-    def get_state(self):\n-        \"\"\"Deprecated get rng state use bit_generator.state.\"\"\"\n-        deprecation(\n-            \"Function `rng.get_state()` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.bit_generator.state` instead.\"\n-        )\n-\n-        return self.bit_generator.state\n-\n-    def set_state(self, state):\n-        \"\"\"Deprecated set rng state function use bit_generator.state = state.\"\"\"\n-        deprecation(\n-            \"Function `rng.set_state(state)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.bit_generator.state = state` instead.\"\n-        )\n-\n-        self.bit_generator.state = state\n-\n-    def seed(self, seed=None):\n-        \"\"\"Deprecated seed function use gym.utils.seeding.np_random(seed).\"\"\"\n-        deprecation(\n-            \"Function `rng.seed(seed)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng, seed = gym.utils.seeding.np_random(seed)` to create a separate generator instead.\"\n-        )\n-\n-        self.bit_generator.state = type(self.bit_generator)(seed).state\n-\n-    rand.__doc__ = np.random.rand.__doc__\n-    randn.__doc__ = np.random.randn.__doc__\n-    randint.__doc__ = np.random.randint.__doc__\n-    get_state.__doc__ = np.random.get_state.__doc__\n-    set_state.__doc__ = np.random.set_state.__doc__\n-    seed.__doc__ = np.random.seed.__doc__\n-\n-    def __reduce__(self):\n-        \"\"\"Reduces the Random Number Generator to a RandomNumberGenerator, init_args and additional args.\"\"\"\n-        # np.random.Generator defines __reduce__, but it's hard-coded to\n-        # return a Generator instead of its subclass RandomNumberGenerator.\n-        # We need to override it here, otherwise sampling from a Space will\n-        # be broken after pickling and unpickling, due to using the deprecated\n-        # methods defined above.\n-        # See: https://github.com/numpy/numpy/blob/41d37b714caa1eef72f984d529f1d40ed48ce535/numpy/random/_generator.pyx#L221-L223\n-        # And: https://github.com/numpy/numpy/blob/41d37b714caa1eef72f984d529f1d40ed48ce535/numpy/random/_pickle.py#L17-L37\n-        _, init_args, *args = np.random.Generator.__reduce__(self)\n-        return (RandomNumberGenerator._generator_ctor, init_args, *args)\n-\n-    @staticmethod\n-    def _generator_ctor(bit_generator_name=\"MT19937\"):\n-        # Workaround method for RandomNumberGenerator pickling, see __reduce__ above.\n-        # Ported from numpy.random._pickle.__generator_ctor function.\n-        from numpy.random._pickle import BitGenerators\n-\n-        if bit_generator_name in BitGenerators:\n-            bit_generator = BitGenerators[bit_generator_name]\n-        else:\n-            raise ValueError(\n-                f\"{bit_generator_name} is not a known BitGenerator module.\"\n-            )\n-        return RandomNumberGenerator(bit_generator())\n-\n-\n-RNG = RandomNumberGenerator\n-\n-# Legacy functions\n-\n-\n-def hash_seed(seed: Optional[int] = None, max_bytes: int = 8) -> int:\n-    \"\"\"Any given evaluation is likely to have many PRNG's active at once.\n-\n-    (Most commonly, because the environment is running in multiple processes.)\n-    There's literature indicating that having linear correlations between seeds of multiple PRNG's can correlate the outputs:\n-        http://blogs.unity3d.com/2015/01/07/a-primer-on-repeatable-random-numbers/\n-        http://stackoverflow.com/questions/1554958/how-different-do-random-seeds-need-to-be\n-        http://dl.acm.org/citation.cfm?id=1276928\n-    Thus, for sanity we hash the seeds before using them. (This scheme is likely not crypto-strength, but it should be good enough to get rid of simple correlations.)\n-\n-    Args:\n-        seed: None seeds from an operating system specific randomness source.\n-        max_bytes: Maximum number of bytes to use in the hashed seed.\n-\n-    Returns:\n-        The hashed seed\n-    \"\"\"\n-    deprecation(\n-        \"Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \"\n-    )\n-    if seed is None:\n-        seed = create_seed(max_bytes=max_bytes)\n-    hash = hashlib.sha512(str(seed).encode(\"utf8\")).digest()\n-    return _bigint_from_bytes(hash[:max_bytes])\n-\n-\n-def create_seed(a: Optional[Union[int, str]] = None, max_bytes: int = 8) -> int:\n-    \"\"\"Create a strong random seed.\n-\n-    Otherwise, Python 2 would seed using the system time, which might be non-robust especially in the presence of concurrency.\n-\n-    Args:\n-        a: None seeds from an operating system specific randomness source.\n-        max_bytes: Maximum number of bytes to use in the seed.\n-\n-    Returns:\n-        A seed\n-\n-    Raises:\n-        Error: Invalid type for seed, expects None or str or int\n-    \"\"\"\n-    deprecation(\n-        \"Function `create_seed(a, max_bytes)` is marked as deprecated and will be removed in the future. \"\n-    )\n-    # Adapted from https://svn.python.org/projects/python/tags/r32/Lib/random.py\n-    if a is None:\n-        a = _bigint_from_bytes(os.urandom(max_bytes))\n-    elif isinstance(a, str):\n-        bt = a.encode(\"utf8\")\n-        bt += hashlib.sha512(bt).digest()\n-        a = _bigint_from_bytes(bt[:max_bytes])\n-    elif isinstance(a, int):\n-        a = int(a % 2 ** (8 * max_bytes))\n-    else:\n-        raise error.Error(f\"Invalid type for seed: {type(a)} ({a})\")\n-\n-    return a\n-\n-\n-# TODO: don't hardcode sizeof_int here\n-def _bigint_from_bytes(bt: bytes) -> int:\n-    deprecation(\n-        \"Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \"\n-    )\n-    sizeof_int = 4\n-    padding = sizeof_int - len(bt) % sizeof_int\n-    bt += b\"\\0\" * padding\n-    int_count = int(len(bt) / sizeof_int)\n-    unpacked = struct.unpack(f\"{int_count}I\", bt)\n-    accum = 0\n-    for i, val in enumerate(unpacked):\n-        accum += 2 ** (sizeof_int * 8 * i) * val\n-    return accum\n-\n-\n-def _int_list_from_bigint(bigint: int) -> List[int]:\n-    deprecation(\n-        \"Function `_int_list_from_bigint` is marked as deprecated and will be removed in the future. \"\n-    )\n-    # Special case 0\n-    if bigint < 0:\n-        raise error.Error(f\"Seed must be non-negative, not {bigint}\")\n-    elif bigint == 0:\n-        return [0]\n-\n-    ints: List[int] = []\n-    while bigint > 0:\n-        bigint, mod = divmod(bigint, 2**32)\n-        ints.append(mod)\n-    return ints\n+RandomNumberGenerator = np.random.Generator",
    "comment": "\r\n\r\ni think we shall leave the short name convenience (line 137).",
    "line_number": 30,
    "enriched": "File: gym/utils/seeding.py\nCode: @@ -31,201 +27,4 @@ def np_random(seed: Optional[int] = None) -> Tuple[\"RandomNumberGenerator\", Any]\n     return rng, np_seed\n \n \n-# TODO: Remove this class and make it alias to `Generator` in a future Gym release\n-# RandomNumberGenerator = np.random.Generator\n-class RandomNumberGenerator(np.random.Generator):\n-    \"\"\"Random number generator class that inherits from numpy's random Generator class.\"\"\"\n-\n-    def rand(self, *size):\n-        \"\"\"Deprecated rand function using random.\"\"\"\n-        deprecation(\n-            \"Function `rng.rand(*size)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `Generator.random(size)` instead.\"\n-        )\n-\n-        return self.random(size)\n-\n-    random_sample = rand\n-\n-    def randn(self, *size):\n-        \"\"\"Deprecated random standard normal function use standard_normal.\"\"\"\n-        deprecation(\n-            \"Function `rng.randn(*size)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.standard_normal(size)` instead.\"\n-        )\n-\n-        return self.standard_normal(size)\n-\n-    def randint(self, low, high=None, size=None, dtype=int):\n-        \"\"\"Deprecated random integer function use integers.\"\"\"\n-        deprecation(\n-            \"Function `rng.randint(low, [high, size, dtype])` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.integers(low, [high, size, dtype])` instead.\"\n-        )\n-\n-        return self.integers(low=low, high=high, size=size, dtype=dtype)\n-\n-    random_integers = randint\n-\n-    def get_state(self):\n-        \"\"\"Deprecated get rng state use bit_generator.state.\"\"\"\n-        deprecation(\n-            \"Function `rng.get_state()` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.bit_generator.state` instead.\"\n-        )\n-\n-        return self.bit_generator.state\n-\n-    def set_state(self, state):\n-        \"\"\"Deprecated set rng state function use bit_generator.state = state.\"\"\"\n-        deprecation(\n-            \"Function `rng.set_state(state)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng.bit_generator.state = state` instead.\"\n-        )\n-\n-        self.bit_generator.state = state\n-\n-    def seed(self, seed=None):\n-        \"\"\"Deprecated seed function use gym.utils.seeding.np_random(seed).\"\"\"\n-        deprecation(\n-            \"Function `rng.seed(seed)` is marked as deprecated \"\n-            \"and will be removed in the future. \"\n-            \"Please use `rng, seed = gym.utils.seeding.np_random(seed)` to create a separate generator instead.\"\n-        )\n-\n-        self.bit_generator.state = type(self.bit_generator)(seed).state\n-\n-    rand.__doc__ = np.random.rand.__doc__\n-    randn.__doc__ = np.random.randn.__doc__\n-    randint.__doc__ = np.random.randint.__doc__\n-    get_state.__doc__ = np.random.get_state.__doc__\n-    set_state.__doc__ = np.random.set_state.__doc__\n-    seed.__doc__ = np.random.seed.__doc__\n-\n-    def __reduce__(self):\n-        \"\"\"Reduces the Random Number Generator to a RandomNumberGenerator, init_args and additional args.\"\"\"\n-        # np.random.Generator defines __reduce__, but it's hard-coded to\n-        # return a Generator instead of its subclass RandomNumberGenerator.\n-        # We need to override it here, otherwise sampling from a Space will\n-        # be broken after pickling and unpickling, due to using the deprecated\n-        # methods defined above.\n-        # See: https://github.com/numpy/numpy/blob/41d37b714caa1eef72f984d529f1d40ed48ce535/numpy/random/_generator.pyx#L221-L223\n-        # And: https://github.com/numpy/numpy/blob/41d37b714caa1eef72f984d529f1d40ed48ce535/numpy/random/_pickle.py#L17-L37\n-        _, init_args, *args = np.random.Generator.__reduce__(self)\n-        return (RandomNumberGenerator._generator_ctor, init_args, *args)\n-\n-    @staticmethod\n-    def _generator_ctor(bit_generator_name=\"MT19937\"):\n-        # Workaround method for RandomNumberGenerator pickling, see __reduce__ above.\n-        # Ported from numpy.random._pickle.__generator_ctor function.\n-        from numpy.random._pickle import BitGenerators\n-\n-        if bit_generator_name in BitGenerators:\n-            bit_generator = BitGenerators[bit_generator_name]\n-        else:\n-            raise ValueError(\n-                f\"{bit_generator_name} is not a known BitGenerator module.\"\n-            )\n-        return RandomNumberGenerator(bit_generator())\n-\n-\n-RNG = RandomNumberGenerator\n-\n-# Legacy functions\n-\n-\n-def hash_seed(seed: Optional[int] = None, max_bytes: int = 8) -> int:\n-    \"\"\"Any given evaluation is likely to have many PRNG's active at once.\n-\n-    (Most commonly, because the environment is running in multiple processes.)\n-    There's literature indicating that having linear correlations between seeds of multiple PRNG's can correlate the outputs:\n-        http://blogs.unity3d.com/2015/01/07/a-primer-on-repeatable-random-numbers/\n-        http://stackoverflow.com/questions/1554958/how-different-do-random-seeds-need-to-be\n-        http://dl.acm.org/citation.cfm?id=1276928\n-    Thus, for sanity we hash the seeds before using them. (This scheme is likely not crypto-strength, but it should be good enough to get rid of simple correlations.)\n-\n-    Args:\n-        seed: None seeds from an operating system specific randomness source.\n-        max_bytes: Maximum number of bytes to use in the hashed seed.\n-\n-    Returns:\n-        The hashed seed\n-    \"\"\"\n-    deprecation(\n-        \"Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \"\n-    )\n-    if seed is None:\n-        seed = create_seed(max_bytes=max_bytes)\n-    hash = hashlib.sha512(str(seed).encode(\"utf8\")).digest()\n-    return _bigint_from_bytes(hash[:max_bytes])\n-\n-\n-def create_seed(a: Optional[Union[int, str]] = None, max_bytes: int = 8) -> int:\n-    \"\"\"Create a strong random seed.\n-\n-    Otherwise, Python 2 would seed using the system time, which might be non-robust especially in the presence of concurrency.\n-\n-    Args:\n-        a: None seeds from an operating system specific randomness source.\n-        max_bytes: Maximum number of bytes to use in the seed.\n-\n-    Returns:\n-        A seed\n-\n-    Raises:\n-        Error: Invalid type for seed, expects None or str or int\n-    \"\"\"\n-    deprecation(\n-        \"Function `create_seed(a, max_bytes)` is marked as deprecated and will be removed in the future. \"\n-    )\n-    # Adapted from https://svn.python.org/projects/python/tags/r32/Lib/random.py\n-    if a is None:\n-        a = _bigint_from_bytes(os.urandom(max_bytes))\n-    elif isinstance(a, str):\n-        bt = a.encode(\"utf8\")\n-        bt += hashlib.sha512(bt).digest()\n-        a = _bigint_from_bytes(bt[:max_bytes])\n-    elif isinstance(a, int):\n-        a = int(a % 2 ** (8 * max_bytes))\n-    else:\n-        raise error.Error(f\"Invalid type for seed: {type(a)} ({a})\")\n-\n-    return a\n-\n-\n-# TODO: don't hardcode sizeof_int here\n-def _bigint_from_bytes(bt: bytes) -> int:\n-    deprecation(\n-        \"Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \"\n-    )\n-    sizeof_int = 4\n-    padding = sizeof_int - len(bt) % sizeof_int\n-    bt += b\"\\0\" * padding\n-    int_count = int(len(bt) / sizeof_int)\n-    unpacked = struct.unpack(f\"{int_count}I\", bt)\n-    accum = 0\n-    for i, val in enumerate(unpacked):\n-        accum += 2 ** (sizeof_int * 8 * i) * val\n-    return accum\n-\n-\n-def _int_list_from_bigint(bigint: int) -> List[int]:\n-    deprecation(\n-        \"Function `_int_list_from_bigint` is marked as deprecated and will be removed in the future. \"\n-    )\n-    # Special case 0\n-    if bigint < 0:\n-        raise error.Error(f\"Seed must be non-negative, not {bigint}\")\n-    elif bigint == 0:\n-        return [0]\n-\n-    ints: List[int] = []\n-    while bigint > 0:\n-        bigint, mod = divmod(bigint, 2**32)\n-        ints.append(mod)\n-    return ints\n+RandomNumberGenerator = np.random.Generator\nComment: ```suggestion\r\nRNG = RandomNumberGenerator = np.random.Generator\r\n```\r\n\r\nI think we shall leave the short name convenience (line 137).",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "gym/utils/seeding.py",
    "pr_number": 3022,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 945105377,
    "comment_created_at": "2022-08-13T07:16:15Z"
  },
  {
    "code": "@@ -547,7 +547,7 @@ def make(\n     id: Union[str, EnvSpec],\n     max_episode_steps: Optional[int] = None,\n     autoreset: bool = False,\n-    new_step_api: bool = False,\n+    new_step_api: bool = True,",
    "comment": "rename to 'apply_step_compatibility' as in a few years time new or old won't be descriptive",
    "line_number": 550,
    "enriched": "File: gym/envs/registration.py\nCode: @@ -547,7 +547,7 @@ def make(\n     id: Union[str, EnvSpec],\n     max_episode_steps: Optional[int] = None,\n     autoreset: bool = False,\n-    new_step_api: bool = False,\n+    new_step_api: bool = True,\nComment: Rename to 'apply_step_compatibility' as in a few years time new or old won't be descriptive",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "gym/envs/registration.py",
    "pr_number": 3019,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 939439238,
    "comment_created_at": "2022-08-05T23:09:28Z"
  },
  {
    "code": "@@ -0,0 +1,105 @@\n+\"\"\"Wrapper for adding time aware observations to environment observation.\"\"\"\n+from collections import OrderedDict\n+\n+import jumpy as jp\n+\n+import gym\n+import gym.spaces as spaces\n+from gym.core import ActType, ObsType\n+from gym.spaces import Box, Dict\n+\n+\n+class TimeAwareObservationV0(gym.ObservationWrapper):\n+    \"\"\"Augment the observation with the current time step in the episode.\n+\n+    Example:\n+        >>> import gym\n+        >>> env = gym.make('CartPole-v1')\n+        >>> env = TimeAwareObservationV0(env)\n+        >>> env.observation_space\n+        Dict(obs: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32), time: Box(0.0, inf, (1,), float32))\n+        >>> _ = env.reset()\n+        >>> env.step(env.action_space.sample())[0]\n+        OrderedDict([('time', array([1.])),\n+        ...  ('obs',\n+        ...    array([ 0.02768888,  0.1745313 ,  0.03663293, -0.32239535], dtype=float32))])\n+\n+    Flatten observation space example:\n+        >>> env = gym.make('CartPole-v1')\n+        >>> env = TimeAwareObservationV0(env, flatten=True)\n+        >>> env.observation_space\n+        Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38  0.0000000e+00], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38 inf], (5,), float32)\n+        >>> _ = env.reset()\n+        >>> env.step(env.action_space.sample())[0]\n+        array([-0.01232257,  0.19335455, -0.02244143, -0.32388705,  1. ], dtype=float32)\n+    \"\"\"\n+\n+    def __init__(self, env: gym.Env, flatten=False):\n+        \"\"\"Initialize :class:`TimeAwareObservationV0`.\n+\n+        Args:\n+            env: The environment to apply the wrapper\n+            flatten: Flatten the observation to a `Box` of a single dimension\n+        \"\"\"\n+        super().__init__(env)\n+        self.flatten = flatten\n+        self.num_envs = getattr(env, \"num_envs\", 1)\n+\n+        self.time_aware_observation_space = Dict(\n+            obs=env.observation_space, time=Box(0, jp.inf, (self.num_envs,))\n+        )\n+\n+        if self.flatten:\n+            self.observation_space = spaces.flatten_space(\n+                self.time_aware_observation_space\n+            )\n+        else:\n+            self.observation_space = self.time_aware_observation_space\n+\n+    def observation(self, observation: ObsType):\n+        \"\"\"Adds to the observation with the current time step.\n+\n+        Args:\n+            observation: The observation to add the time step to\n+\n+        Returns:\n+            The observation with the time step appended to\n+        \"\"\"\n+        observation = OrderedDict(obs=observation, time=self.t)",
    "comment": "why not use the remaining time (as suggested by https://arxiv.org/abs/1712.00378 or done in https://sb3-contrib.readthedocs.io/en/master/common/wrappers.html#timefeaturewrapper)\r\nunbounded observation sounds like a bad idea.",
    "line_number": 68,
    "enriched": "File: gym/dev_wrappers/time_aware_observation.py\nCode: @@ -0,0 +1,105 @@\n+\"\"\"Wrapper for adding time aware observations to environment observation.\"\"\"\n+from collections import OrderedDict\n+\n+import jumpy as jp\n+\n+import gym\n+import gym.spaces as spaces\n+from gym.core import ActType, ObsType\n+from gym.spaces import Box, Dict\n+\n+\n+class TimeAwareObservationV0(gym.ObservationWrapper):\n+    \"\"\"Augment the observation with the current time step in the episode.\n+\n+    Example:\n+        >>> import gym\n+        >>> env = gym.make('CartPole-v1')\n+        >>> env = TimeAwareObservationV0(env)\n+        >>> env.observation_space\n+        Dict(obs: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32), time: Box(0.0, inf, (1,), float32))\n+        >>> _ = env.reset()\n+        >>> env.step(env.action_space.sample())[0]\n+        OrderedDict([('time', array([1.])),\n+        ...  ('obs',\n+        ...    array([ 0.02768888,  0.1745313 ,  0.03663293, -0.32239535], dtype=float32))])\n+\n+    Flatten observation space example:\n+        >>> env = gym.make('CartPole-v1')\n+        >>> env = TimeAwareObservationV0(env, flatten=True)\n+        >>> env.observation_space\n+        Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38  0.0000000e+00], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38 inf], (5,), float32)\n+        >>> _ = env.reset()\n+        >>> env.step(env.action_space.sample())[0]\n+        array([-0.01232257,  0.19335455, -0.02244143, -0.32388705,  1. ], dtype=float32)\n+    \"\"\"\n+\n+    def __init__(self, env: gym.Env, flatten=False):\n+        \"\"\"Initialize :class:`TimeAwareObservationV0`.\n+\n+        Args:\n+            env: The environment to apply the wrapper\n+            flatten: Flatten the observation to a `Box` of a single dimension\n+        \"\"\"\n+        super().__init__(env)\n+        self.flatten = flatten\n+        self.num_envs = getattr(env, \"num_envs\", 1)\n+\n+        self.time_aware_observation_space = Dict(\n+            obs=env.observation_space, time=Box(0, jp.inf, (self.num_envs,))\n+        )\n+\n+        if self.flatten:\n+            self.observation_space = spaces.flatten_space(\n+                self.time_aware_observation_space\n+            )\n+        else:\n+            self.observation_space = self.time_aware_observation_space\n+\n+    def observation(self, observation: ObsType):\n+        \"\"\"Adds to the observation with the current time step.\n+\n+        Args:\n+            observation: The observation to add the time step to\n+\n+        Returns:\n+            The observation with the time step appended to\n+        \"\"\"\n+        observation = OrderedDict(obs=observation, time=self.t)\nComment: why not use the remaining time (as suggested by https://arxiv.org/abs/1712.00378 or done in https://sb3-contrib.readthedocs.io/en/master/common/wrappers.html#timefeaturewrapper)\r\nunbounded observation sounds like a bad idea.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "gym/dev_wrappers/time_aware_observation.py",
    "pr_number": 3013,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 938795417,
    "comment_created_at": "2022-08-05T13:08:23Z"
  },
  {
    "code": "@@ -115,19 +115,14 @@ def seed(self, seed: Optional[Union[dict, int]] = None) -> list:\n                 seeds += self.spaces[key].seed(seed[seed_key])\n         elif isinstance(seed, int):\n             seeds = super().seed(seed)\n-            try:\n-                subseeds = self.np_random.choice(\n-                    np.iinfo(int).max,\n-                    size=len(self.spaces),\n-                    replace=False,  # unique subseed for each subspace\n-                )\n-            except ValueError:\n-                subseeds = self.np_random.choice(\n-                    np.iinfo(int).max,\n-                    size=len(self.spaces),\n-                    replace=True,  # we get more than INT_MAX subspaces\n-                )\n-\n+            subseeds = []\n+            assert (\n+                len(self.spaces) <= np.iinfo(np.int32).max\n+            ), f\"Expected spaces.length <= 2147483647, got {len(self.spaces)}\"",
    "comment": "we're introducing a new exception here (so de facto deprecating a new behavior), admittedly for an *extremely* niche use case, and if anyone encounters it i'd be concerned.\r\n\r\nstill, it's better to maintain backwards compatibility than not. can't we go the same way as in the previous version, so if len(self.spaces) > np.iinfo(np.int32).max, we just relax the uniqueness property?",
    "line_number": 121,
    "enriched": "File: gym/spaces/dict.py\nCode: @@ -115,19 +115,14 @@ def seed(self, seed: Optional[Union[dict, int]] = None) -> list:\n                 seeds += self.spaces[key].seed(seed[seed_key])\n         elif isinstance(seed, int):\n             seeds = super().seed(seed)\n-            try:\n-                subseeds = self.np_random.choice(\n-                    np.iinfo(int).max,\n-                    size=len(self.spaces),\n-                    replace=False,  # unique subseed for each subspace\n-                )\n-            except ValueError:\n-                subseeds = self.np_random.choice(\n-                    np.iinfo(int).max,\n-                    size=len(self.spaces),\n-                    replace=True,  # we get more than INT_MAX subspaces\n-                )\n-\n+            subseeds = []\n+            assert (\n+                len(self.spaces) <= np.iinfo(np.int32).max\n+            ), f\"Expected spaces.length <= 2147483647, got {len(self.spaces)}\"\nComment: We're introducing a new exception here (so de facto deprecating a new behavior), admittedly for an *extremely* niche use case, and if anyone encounters it I'd be concerned.\r\n\r\nStill, it's better to maintain backwards compatibility than not. Can't we go the same way as in the previous version, so if `len(self.spaces) > np.iinfo(np.int32).max`, we just relax the uniqueness property?",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "gym/spaces/dict.py",
    "pr_number": 3011,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 934385408,
    "comment_created_at": "2022-08-01T10:37:39Z"
  },
  {
    "code": "@@ -120,3 +128,20 @@ def test_render_modes(spec):\n             new_env.render()\n             new_env.close()\n     env.close()\n+\n+\n+@pytest.mark.parametrize(\n+    \"env\",\n+    all_testing_initialised_envs,\n+    ids=[env.spec.id for env in all_testing_initialised_envs],\n+)\n+def test_pickle_env(env: gym.Env):\n+    env.np_random, _ = seeding.np_random(0)",
    "comment": "you're not resetting the env before pickling which is likely to mess things up (and i don't think it's supported behavior anyways, since you need to reset an env before doing literally anything with it)",
    "line_number": 139,
    "enriched": "File: tests/envs/test_envs.py\nCode: @@ -120,3 +128,20 @@ def test_render_modes(spec):\n             new_env.render()\n             new_env.close()\n     env.close()\n+\n+\n+@pytest.mark.parametrize(\n+    \"env\",\n+    all_testing_initialised_envs,\n+    ids=[env.spec.id for env in all_testing_initialised_envs],\n+)\n+def test_pickle_env(env: gym.Env):\n+    env.np_random, _ = seeding.np_random(0)\nComment: You're not resetting the env before pickling which is likely to mess things up (and I don't think it's supported behavior anyways, since you need to reset an env before doing literally anything with it)",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "tests/envs/test_envs.py",
    "pr_number": 3006,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 934403655,
    "comment_created_at": "2022-08-01T11:01:32Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+\"\"\"A set of utility functions for lambda wrappers.\"\"\"\n+from functools import singledispatch\n+from typing import Any\n+from typing import Dict as TypingDict\n+from typing import Tuple as TypingTuple\n+\n+from gym.dev_wrappers import FuncArgType\n+from gym.spaces import Box, Dict, Discrete, MultiBinary, MultiDiscrete, Space, Tuple\n+\n+\n+@singledispatch\n+def filter_space(space: Space, args: FuncArgType) -> Any:\n+    \"\"\"Filter space with the provided args.\"\"\"\n+",
    "comment": "raise notimplementederror",
    "line_number": 14,
    "enriched": "File: gym/dev_wrappers/utils/filter_space.py\nCode: @@ -0,0 +1,39 @@\n+\"\"\"A set of utility functions for lambda wrappers.\"\"\"\n+from functools import singledispatch\n+from typing import Any\n+from typing import Dict as TypingDict\n+from typing import Tuple as TypingTuple\n+\n+from gym.dev_wrappers import FuncArgType\n+from gym.spaces import Box, Dict, Discrete, MultiBinary, MultiDiscrete, Space, Tuple\n+\n+\n+@singledispatch\n+def filter_space(space: Space, args: FuncArgType) -> Any:\n+    \"\"\"Filter space with the provided args.\"\"\"\n+\nComment: `raise NotImplementedError`",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "gym/dev_wrappers/utils/filter_space.py",
    "pr_number": 3002,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 955543453,
    "comment_created_at": "2022-08-26T01:06:10Z"
  },
  {
    "code": "@@ -106,6 +87,12 @@ class Env(Generic[ObsType, ActType], metaclass=decorator):\n     Note: a default reward range set to :math:`(-\\infty,+\\infty)` already exists. Set it if you want a narrower range.\n     \"\"\"\n \n+    def __init_subclass__(cls) -> None:\n+        \"\"\"Hook used for wrapping render method.\"\"\"\n+        super().__init_subclass__()\n+        if \"render\" in vars(cls):\n+            cls.render = _deprecate_mode(vars(cls)[\"render\"])",
    "comment": "any reason why this can't be cls.render = _deprecate_mode(cls.render)? if not, i'd prefer that for simplicity",
    "line_number": 94,
    "enriched": "File: gym/core.py\nCode: @@ -106,6 +87,12 @@ class Env(Generic[ObsType, ActType], metaclass=decorator):\n     Note: a default reward range set to :math:`(-\\infty,+\\infty)` already exists. Set it if you want a narrower range.\n     \"\"\"\n \n+    def __init_subclass__(cls) -> None:\n+        \"\"\"Hook used for wrapping render method.\"\"\"\n+        super().__init_subclass__()\n+        if \"render\" in vars(cls):\n+            cls.render = _deprecate_mode(vars(cls)[\"render\"])\nComment: Any reason why this can't be `cls.render = _deprecate_mode(cls.render)`? If not, I'd prefer that for simplicity",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/core.py",
    "pr_number": 3001,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 935811631,
    "comment_created_at": "2022-08-02T16:43:23Z"
  },
  {
    "code": "@@ -24,15 +24,16 @@ class CliffWalkingEnv(Env):\n     by Sutton and Barto](http://incompleteideas.net/book/bookdraft2018jan1.pdf).\n \n     With inspiration from:\n-    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n+    [https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py]\n+    (https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py)",
    "comment": "is this meant to be repeated",
    "line_number": 28,
    "enriched": "File: gym/envs/toy_text/cliffwalking.py\nCode: @@ -24,15 +24,16 @@ class CliffWalkingEnv(Env):\n     by Sutton and Barto](http://incompleteideas.net/book/bookdraft2018jan1.pdf).\n \n     With inspiration from:\n-    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n+    [https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py]\n+    (https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py)\nComment: Is this meant to be repeated ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/envs/toy_text/cliffwalking.py",
    "pr_number": 2997,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 929805836,
    "comment_created_at": "2022-07-26T10:38:20Z"
  },
  {
    "code": "@@ -621,30 +621,48 @@ def make(\n     mode = _kwargs.get(\"render_mode\")\n     apply_human_rendering = False\n \n-    # If we have access to metadata we check that \"render_mode\" is valid\n-    if hasattr(env_creator, \"metadata\"):\n-        render_modes = env_creator.metadata[\"render_modes\"]\n-\n-        # We might be able to fall back to the HumanRendering wrapper if 'human' rendering is not supported natively\n-        if (\n-            mode == \"human\"\n-            and \"human\" not in render_modes\n-            and (\"single_rgb_array\" in render_modes or \"rgb_array\" in render_modes)\n-        ):\n+    # If we have access to metadata we check that \"render_mode\" is valid and see if the HumanRendering wrapper needs to be applied\n+    if mode is not None:\n+        if hasattr(env_creator, \"metadata\"):\n+            if isinstance(env_creator.metadata, dict):",
    "comment": "what else could metadata be?",
    "line_number": 627,
    "enriched": "File: gym/envs/registration.py\nCode: @@ -621,30 +621,48 @@ def make(\n     mode = _kwargs.get(\"render_mode\")\n     apply_human_rendering = False\n \n-    # If we have access to metadata we check that \"render_mode\" is valid\n-    if hasattr(env_creator, \"metadata\"):\n-        render_modes = env_creator.metadata[\"render_modes\"]\n-\n-        # We might be able to fall back to the HumanRendering wrapper if 'human' rendering is not supported natively\n-        if (\n-            mode == \"human\"\n-            and \"human\" not in render_modes\n-            and (\"single_rgb_array\" in render_modes or \"rgb_array\" in render_modes)\n-        ):\n+    # If we have access to metadata we check that \"render_mode\" is valid and see if the HumanRendering wrapper needs to be applied\n+    if mode is not None:\n+        if hasattr(env_creator, \"metadata\"):\n+            if isinstance(env_creator.metadata, dict):\nComment: What else could `metadata` be?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "gym/envs/registration.py",
    "pr_number": 2990,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 928278809,
    "comment_created_at": "2022-07-24T15:58:29Z"
  },
  {
    "code": "@@ -99,7 +99,7 @@ class LunarLander(gym.Env, EzPickle):\n     orientation engine, fire main engine, fire right orientation engine.\n \n     ### Observation Space\n-    There are 8 states: the coordinates of the lander in `x` & `y`, its linear\n+    States are eight (8) dimensional vector: the coordinates of the lander in `x` & `y`, its linear",
    "comment": "i'd rephrase this to \"the state is an 8-dimensional vector\"",
    "line_number": 102,
    "enriched": "File: gym/envs/box2d/lunar_lander.py\nCode: @@ -99,7 +99,7 @@ class LunarLander(gym.Env, EzPickle):\n     orientation engine, fire main engine, fire right orientation engine.\n \n     ### Observation Space\n-    There are 8 states: the coordinates of the lander in `x` & `y`, its linear\n+    States are eight (8) dimensional vector: the coordinates of the lander in `x` & `y`, its linear\nComment: I'd rephrase this to \"The state is an 8-dimensional vector\"",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "gym/envs/box2d/lunar_lander.py",
    "pr_number": 2987,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 927780322,
    "comment_created_at": "2022-07-22T15:42:32Z"
  },
  {
    "code": "@@ -0,0 +1,95 @@\n+\"\"\"Lambda reward wrappers that uses jumpy for compatibility with jax (i.e. brax) and numpy environments.\"\"\"\n+from typing import Callable, Optional, Union\n+\n+import jax.numpy as jnp\n+import jumpy as jp\n+import numpy as np\n+\n+import gym\n+from gym.dev_wrappers import FuncArgType\n+from gym.error import InvalidBound\n+\n+\n+class lambda_reward_v0(gym.RewardWrapper):",
    "comment": "please no snake_case class names. i'd even take lambdareward_v0, or lambdarewardv0 over this",
    "line_number": 13,
    "enriched": "File: gym/dev_wrappers/lambda_reward.py\nCode: @@ -0,0 +1,95 @@\n+\"\"\"Lambda reward wrappers that uses jumpy for compatibility with jax (i.e. brax) and numpy environments.\"\"\"\n+from typing import Callable, Optional, Union\n+\n+import jax.numpy as jnp\n+import jumpy as jp\n+import numpy as np\n+\n+import gym\n+from gym.dev_wrappers import FuncArgType\n+from gym.error import InvalidBound\n+\n+\n+class lambda_reward_v0(gym.RewardWrapper):\nComment: Please no snake_case class names. I'd even take LambdaReward_v0, or LambdaRewardV0 over this",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "gym/dev_wrappers/lambda_reward.py",
    "pr_number": 2985,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 927556330,
    "comment_created_at": "2022-07-22T11:21:43Z"
  },
  {
    "code": "@@ -56,7 +56,7 @@ def __init__(\n         low: Union[SupportsFloat, np.ndarray],\n         high: Union[SupportsFloat, np.ndarray],\n         shape: Optional[Sequence[int]] = None,\n-        dtype: Type = np.float32,\n+        dtype: Union[Type, object] = np.float32,",
    "comment": "i'm not so sure about this annotation, since pretty much anything is an object anyways (maaaaaaybe except for some extremely exotic metaclass constructs, i'm not sure)\r\n\r\nwe want it to be either a python type, or a numpy datatype, right? i think numpy has a type annotation for this somewhere in its typing module",
    "line_number": 59,
    "enriched": "File: gym/spaces/box.py\nCode: @@ -56,7 +56,7 @@ def __init__(\n         low: Union[SupportsFloat, np.ndarray],\n         high: Union[SupportsFloat, np.ndarray],\n         shape: Optional[Sequence[int]] = None,\n-        dtype: Type = np.float32,\n+        dtype: Union[Type, object] = np.float32,\nComment: I'm not so sure about this annotation, since pretty much anything is an `object` anyways (maaaaaaybe except for some extremely exotic metaclass constructs, I'm not sure)\r\n\r\nWe want it to be either a python type, or a numpy datatype, right? I think numpy has a type annotation for this somewhere in its typing module",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "gym/spaces/box.py",
    "pr_number": 2977,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 936744773,
    "comment_created_at": "2022-08-03T14:35:21Z"
  },
  {
    "code": "@@ -1,90 +1,118 @@\n import numpy as np\n import pytest\n \n-import gym\n+from gym.spaces import Box, Discrete\n from gym.wrappers import AtariPreprocessing\n+from tests.testing_env import GenericTestEnv, old_step_fn\n \n-pytest.importorskip(\"gym.envs.atari\")\n \n+class AleTesting:",
    "comment": "docstring on class; docstring and type hint on methods",
    "line_number": 9,
    "enriched": "File: tests/wrappers/test_atari_preprocessing.py\nCode: @@ -1,90 +1,118 @@\n import numpy as np\n import pytest\n \n-import gym\n+from gym.spaces import Box, Discrete\n from gym.wrappers import AtariPreprocessing\n+from tests.testing_env import GenericTestEnv, old_step_fn\n \n-pytest.importorskip(\"gym.envs.atari\")\n \n+class AleTesting:\nComment: docstring on class; docstring and type hint on methods",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "tests/wrappers/test_atari_preprocessing.py",
    "pr_number": 2976,
    "repo": "gym",
    "owner": "openai",
    "comment_id": 926082671,
    "comment_created_at": "2022-07-20T21:49:23Z"
  },
  {
    "code": "@@ -493,6 +493,34 @@ def test_symtable_entry_repr(self):\n         expected = f\"<symtable entry top({self.top.get_id()}), line {self.top.get_lineno()}>\"\n         self.assertEqual(repr(self.top._table), expected)\n \n+    def test__symtable_refleak(self):\n+        # Regression test for reference leak in PyUnicode_FSDecoder.\n+        # See https://github.com/python/cpython/issues/139748.\n+        mortal_str = 'this is a mortal string'\n+        # check error path when 'compile_type' AC conversion failed\n+        self.assertRaises(TypeError, symtable.symtable, '', mortal_str, 1)\n+\n+\n+class ComprehensionTests(unittest.TestCase):",
    "comment": "i think those tests are extra ones",
    "line_number": 504,
    "enriched": "File: Lib/test/test_symtable.py\nCode: @@ -493,6 +493,34 @@ def test_symtable_entry_repr(self):\n         expected = f\"<symtable entry top({self.top.get_id()}), line {self.top.get_lineno()}>\"\n         self.assertEqual(repr(self.top._table), expected)\n \n+    def test__symtable_refleak(self):\n+        # Regression test for reference leak in PyUnicode_FSDecoder.\n+        # See https://github.com/python/cpython/issues/139748.\n+        mortal_str = 'this is a mortal string'\n+        # check error path when 'compile_type' AC conversion failed\n+        self.assertRaises(TypeError, symtable.symtable, '', mortal_str, 1)\n+\n+\n+class ComprehensionTests(unittest.TestCase):\nComment: I think those tests are extra ones",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "Lib/test/test_symtable.py",
    "pr_number": 139792,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2414292819,
    "comment_created_at": "2025-10-08T15:39:20Z"
  },
  {
    "code": "@@ -128,7 +128,7 @@ jobs:\n           choco install llvm --allow-downgrade --no-progress --version ${{ matrix.llvm }}.1.0\n           ./PCbuild/build.bat --experimental-jit ${{ matrix.debug && '-d' || '' }} -p ${{ matrix.architecture }}\n \n-      - name: Native macOS\n+      - name: macOS",
    "comment": "let's keep the old name, because there's also \"native linux\" and so on in this file:\r\n",
    "line_number": 131,
    "enriched": "File: .github/workflows/jit.yml\nCode: @@ -128,7 +128,7 @@ jobs:\n           choco install llvm --allow-downgrade --no-progress --version ${{ matrix.llvm }}.1.0\n           ./PCbuild/build.bat --experimental-jit ${{ matrix.debug && '-d' || '' }} -p ${{ matrix.architecture }}\n \n-      - name: Native macOS\n+      - name: macOS\nComment: Let's keep the old name, because there's also \"Native Linux\" and so on in this file:\r\n```suggestion\r\n      - name: Native macOS\r\n```",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": ".github/workflows/jit.yml",
    "pr_number": 139790,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2414336936,
    "comment_created_at": "2025-10-08T15:54:03Z"
  },
  {
    "code": "@@ -527,26 +527,12 @@ def test_symtable_entry_repr(self):\n         expected = f\"<symtable entry top({self.top.get_id()}), line {self.top.get_lineno()}>\"\n         self.assertEqual(repr(self.top._table), expected)\n \n-\n-class ComprehensionTests(unittest.TestCase):\n-    def get_identifiers_recursive(self, st, res):",
    "comment": "what happened to those tests?",
    "line_number": 532,
    "enriched": "File: Lib/test/test_symtable.py\nCode: @@ -527,26 +527,12 @@ def test_symtable_entry_repr(self):\n         expected = f\"<symtable entry top({self.top.get_id()}), line {self.top.get_lineno()}>\"\n         self.assertEqual(repr(self.top._table), expected)\n \n-\n-class ComprehensionTests(unittest.TestCase):\n-    def get_identifiers_recursive(self, st, res):\nComment: What happened to those tests?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "Lib/test/test_symtable.py",
    "pr_number": 139789,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2414290668,
    "comment_created_at": "2025-10-08T15:38:30Z"
  },
  {
    "code": "@@ -20,9 +20,3 @@ select = [\n     \"W\",       # pycodestyle\n     \"YTT\",     # flake8-2020\n ]\n-ignore = [\n-    \"E501\",    # Line too long",
    "comment": "keeping this one makes likely sense given we're auto-formatting.",
    "line_number": 24,
    "enriched": "File: Tools/wasm/.ruff.toml\nCode: @@ -20,9 +20,3 @@ select = [\n     \"W\",       # pycodestyle\n     \"YTT\",     # flake8-2020\n ]\n-ignore = [\n-    \"E501\",    # Line too long\nComment: Keeping this one makes likely sense given we're auto-formatting.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "Tools/wasm/.ruff.toml",
    "pr_number": 139752,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2413220312,
    "comment_created_at": "2025-10-08T09:32:20Z"
  },
  {
    "code": "@@ -420,9 +420,10 @@ static void clear_global_interned_strings(void)\n         return unicode_get_empty();  \\\n     } while (0)\n \n-static inline void\n-unicode_fill(int kind, void *data, Py_UCS4 value,\n-             Py_ssize_t start, Py_ssize_t length)\n+\n+void\n+_PyUnicode_Fill(int kind, void *data, Py_UCS4 value,\n+                Py_ssize_t start, Py_ssize_t length)",
    "comment": "are you sure that it will be inlined? maybe keep static inline unicode_fill() and add a wrapper _pyunicode_fill()?",
    "line_number": 426,
    "enriched": "File: Objects/unicodeobject.c\nCode: @@ -420,9 +420,10 @@ static void clear_global_interned_strings(void)\n         return unicode_get_empty();  \\\n     } while (0)\n \n-static inline void\n-unicode_fill(int kind, void *data, Py_UCS4 value,\n-             Py_ssize_t start, Py_ssize_t length)\n+\n+void\n+_PyUnicode_Fill(int kind, void *data, Py_UCS4 value,\n+                Py_ssize_t start, Py_ssize_t length)\nComment: Are you sure that it will be inlined? Maybe keep static inline `unicode_fill()` and add a wrapper `_PyUnicode_Fill()`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "Objects/unicodeobject.c",
    "pr_number": 139723,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2413526532,
    "comment_created_at": "2025-10-08T11:21:40Z"
  },
  {
    "code": "@@ -1859,6 +1861,12 @@ def __format__(self, format):\n         # Test multiple format specs in same raw f-string\n         self.assertEqual(rf\"{UnchangedFormat():\\xFF} {UnchangedFormat():\\n}\", '\\\\xFF \\\\n')\n \n+    def test_gh139516(self):\n+        # gh-139516\n+        # The '\\n' is explicit to ensure no trailing whitespace which would invalidate the test.\n+        # Must use tokenize instead of compile so that source is parsed by line which exposes the bug.\n+        list(tokenize.tokenize(BytesIO('''f\"{f(a=lambda: '\u00e0'\\n)}\"'''.encode()).readline))",
    "comment": "i am confused. isn't it possible to trigger this in an exec or eval call? or perhaps a file with an encoding?",
    "line_number": 1868,
    "enriched": "File: Lib/test/test_fstring.py\nCode: @@ -1859,6 +1861,12 @@ def __format__(self, format):\n         # Test multiple format specs in same raw f-string\n         self.assertEqual(rf\"{UnchangedFormat():\\xFF} {UnchangedFormat():\\n}\", '\\\\xFF \\\\n')\n \n+    def test_gh139516(self):\n+        # gh-139516\n+        # The '\\n' is explicit to ensure no trailing whitespace which would invalidate the test.\n+        # Must use tokenize instead of compile so that source is parsed by line which exposes the bug.\n+        list(tokenize.tokenize(BytesIO('''f\"{f(a=lambda: '\u00e0'\\n)}\"'''.encode()).readline))\nComment: I am confused. Isn't it possible to trigger this in an `exec` or `eval` call? Or perhaps a file with an encoding?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "Lib/test/test_fstring.py",
    "pr_number": 139657,
    "repo": "cpython",
    "owner": "python",
    "comment_id": 2407787516,
    "comment_created_at": "2025-10-06T18:12:08Z"
  },
  {
    "code": "@@ -895,3 +895,101 @@ def scale_and_translate(\n         method,\n         antialias,\n     )\n+\n+\n+def extract_volume_patches(\n+    volumes, size, strides, dilation_rate, padding, data_format\n+):\n+    \"\"\"JAX implementation of extract_volume_patches.\"\"\"\n+    import jax.numpy as jnp\n+\n+    # Convert to channels_last if needed\n+    if data_format == \"channels_first\":\n+        # (B, C, D, H, W) -> (B, D, H, W, C)\n+        volumes = jnp.transpose(volumes, [0, 2, 3, 4, 1])\n+\n+    batch_size, depth, height, width, channels = volumes.shape\n+\n+    # Normalize size, strides, and dilation_rate to tuples\n+    if isinstance(size, int):\n+        pd, ph, pw = size, size, size\n+    else:\n+        pd, ph, pw = size\n+\n+    if isinstance(strides, int):\n+        sd, sh, sw = strides, strides, strides\n+    elif strides is None:\n+        sd, sh, sw = pd, ph, pw\n+    else:\n+        sd, sh, sw = strides\n+\n+    if isinstance(dilation_rate, int):\n+        dd, dh, dw = dilation_rate, dilation_rate, dilation_rate\n+    else:\n+        dd, dh, dw = dilation_rate\n+\n+    # Calculate effective patch size with dilation\n+    eff_pd = pd + (pd - 1) * (dd - 1)\n+    eff_ph = ph + (ph - 1) * (dh - 1)\n+    eff_pw = pw + (pw - 1) * (dw - 1)\n+\n+    # Calculate output dimensions\n+    if padding == \"valid\":\n+        out_d = (depth - eff_pd) // sd + 1\n+        out_h = (height - eff_ph) // sh + 1\n+        out_w = (width - eff_pw) // sw + 1\n+    else:  # same\n+        out_d = (depth + sd - 1) // sd\n+        out_h = (height + sh - 1) // sh\n+        out_w = (width + sw - 1) // sw\n+\n+        # Calculate and apply padding\n+        pad_d = max((out_d - 1) * sd + eff_pd - depth, 0)\n+        pad_h = max((out_h - 1) * sh + eff_ph - height, 0)\n+        pad_w = max((out_w - 1) * sw + eff_pw - width, 0)\n+\n+        pad_d_before = pad_d // 2\n+        pad_d_after = pad_d - pad_d_before\n+        pad_h_before = pad_h // 2\n+        pad_h_after = pad_h - pad_h_before\n+        pad_w_before = pad_w // 2\n+        pad_w_after = pad_w - pad_w_before\n+\n+        volumes = jnp.pad(\n+            volumes,\n+            (\n+                (0, 0),\n+                (pad_d_before, pad_d_after),\n+                (pad_h_before, pad_h_after),\n+                (pad_w_before, pad_w_after),\n+                (0, 0),\n+            ),\n+            mode=\"constant\",\n+        )\n+        depth, height, width = volumes.shape[1:4]\n+\n+    # Extract patches using advanced indexing\n+    patches_list = []\n+    for d_idx in range(out_d):\n+        for h_idx in range(out_h):\n+            for w_idx in range(out_w):\n+                d_start = d_idx * sd\n+                h_start = h_idx * sh\n+                w_start = w_idx * sw\n+\n+                # Extract patch with dilation\n+                patch = volumes[\n+                    :,\n+                    d_start : d_start + eff_pd : dd,\n+                    h_start : h_start + eff_ph : dh,\n+                    w_start : w_start + eff_pw : dw,\n+                    :,\n+                ]\n+\n+                patch_flat = patch.reshape(batch_size, -1)\n+                patches_list.append(patch_flat)\n+",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe use of nested python loops to extract patches is highly inefficient for jax. this approach will lead to very poor performance, especially when jit-compiled, as it can cause excessive unrolling of the loops or slow execution in general. \n\na vectorized approach is necessary for a performant jax implementation. i recommend exploring one of the following options:\n\n1.  **adapt the 2d extract_patches implementation**: the existing 2d extract_patches in keras for jax uses lax.conv_general_dilated, which is a highly optimized operation. this could potentially be extended for 3d volumes.\n2.  **use lax.gather**: construct indices for all patches and use lax.gather to extract them in a single, vectorized operation.\n3.  **use lax.scan**: if a fully vectorized solution is too complex, using lax.scan over one of the dimensions would be significantly more performant than python loops.",
    "line_number": 991,
    "enriched": "File: keras/src/backend/jax/image.py\nCode: @@ -895,3 +895,101 @@ def scale_and_translate(\n         method,\n         antialias,\n     )\n+\n+\n+def extract_volume_patches(\n+    volumes, size, strides, dilation_rate, padding, data_format\n+):\n+    \"\"\"JAX implementation of extract_volume_patches.\"\"\"\n+    import jax.numpy as jnp\n+\n+    # Convert to channels_last if needed\n+    if data_format == \"channels_first\":\n+        # (B, C, D, H, W) -> (B, D, H, W, C)\n+        volumes = jnp.transpose(volumes, [0, 2, 3, 4, 1])\n+\n+    batch_size, depth, height, width, channels = volumes.shape\n+\n+    # Normalize size, strides, and dilation_rate to tuples\n+    if isinstance(size, int):\n+        pd, ph, pw = size, size, size\n+    else:\n+        pd, ph, pw = size\n+\n+    if isinstance(strides, int):\n+        sd, sh, sw = strides, strides, strides\n+    elif strides is None:\n+        sd, sh, sw = pd, ph, pw\n+    else:\n+        sd, sh, sw = strides\n+\n+    if isinstance(dilation_rate, int):\n+        dd, dh, dw = dilation_rate, dilation_rate, dilation_rate\n+    else:\n+        dd, dh, dw = dilation_rate\n+\n+    # Calculate effective patch size with dilation\n+    eff_pd = pd + (pd - 1) * (dd - 1)\n+    eff_ph = ph + (ph - 1) * (dh - 1)\n+    eff_pw = pw + (pw - 1) * (dw - 1)\n+\n+    # Calculate output dimensions\n+    if padding == \"valid\":\n+        out_d = (depth - eff_pd) // sd + 1\n+        out_h = (height - eff_ph) // sh + 1\n+        out_w = (width - eff_pw) // sw + 1\n+    else:  # same\n+        out_d = (depth + sd - 1) // sd\n+        out_h = (height + sh - 1) // sh\n+        out_w = (width + sw - 1) // sw\n+\n+        # Calculate and apply padding\n+        pad_d = max((out_d - 1) * sd + eff_pd - depth, 0)\n+        pad_h = max((out_h - 1) * sh + eff_ph - height, 0)\n+        pad_w = max((out_w - 1) * sw + eff_pw - width, 0)\n+\n+        pad_d_before = pad_d // 2\n+        pad_d_after = pad_d - pad_d_before\n+        pad_h_before = pad_h // 2\n+        pad_h_after = pad_h - pad_h_before\n+        pad_w_before = pad_w // 2\n+        pad_w_after = pad_w - pad_w_before\n+\n+        volumes = jnp.pad(\n+            volumes,\n+            (\n+                (0, 0),\n+                (pad_d_before, pad_d_after),\n+                (pad_h_before, pad_h_after),\n+                (pad_w_before, pad_w_after),\n+                (0, 0),\n+            ),\n+            mode=\"constant\",\n+        )\n+        depth, height, width = volumes.shape[1:4]\n+\n+    # Extract patches using advanced indexing\n+    patches_list = []\n+    for d_idx in range(out_d):\n+        for h_idx in range(out_h):\n+            for w_idx in range(out_w):\n+                d_start = d_idx * sd\n+                h_start = h_idx * sh\n+                w_start = w_idx * sw\n+\n+                # Extract patch with dilation\n+                patch = volumes[\n+                    :,\n+                    d_start : d_start + eff_pd : dd,\n+                    h_start : h_start + eff_ph : dh,\n+                    w_start : w_start + eff_pw : dw,\n+                    :,\n+                ]\n+\n+                patch_flat = patch.reshape(batch_size, -1)\n+                patches_list.append(patch_flat)\n+\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe use of nested Python loops to extract patches is highly inefficient for JAX. This approach will lead to very poor performance, especially when JIT-compiled, as it can cause excessive unrolling of the loops or slow execution in general. \n\nA vectorized approach is necessary for a performant JAX implementation. I recommend exploring one of the following options:\n\n1.  **Adapt the 2D `extract_patches` implementation**: The existing 2D `extract_patches` in Keras for JAX uses `lax.conv_general_dilated`, which is a highly optimized operation. This could potentially be extended for 3D volumes.\n2.  **Use `lax.gather`**: Construct indices for all patches and use `lax.gather` to extract them in a single, vectorized operation.\n3.  **Use `lax.scan`**: If a fully vectorized solution is too complex, using `lax.scan` over one of the dimensions would be significantly more performant than Python loops.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/jax/image.py",
    "pr_number": 21727,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2413702921,
    "comment_created_at": "2025-10-08T12:31:54Z"
  },
  {
    "code": "@@ -5,15 +5,20 @@\n \"\"\"\n \n from keras.src.backend.config import backend as backend\n+from keras.src.backend.config import disable_debug_mode as disable_debug_mode\n from keras.src.backend.config import (\n     disable_flash_attention as disable_flash_attention,\n )\n+from keras.src.backend.config import enable_debug_mode as enable_debug_mode\n from keras.src.backend.config import (\n     enable_flash_attention as enable_flash_attention,\n )\n from keras.src.backend.config import epsilon as epsilon\n from keras.src.backend.config import floatx as floatx\n from keras.src.backend.config import image_data_format as image_data_format\n+from keras.src.backend.config import (\n+    is_debug_mode_enabled as is_debug_mode_enabled,\n+)\n from keras.src.backend.config import (\n     is_flash_attention_enabled as is_flash_attention_enabled,\n )",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe import statements from keras.src.backend.config are styled inconsistently. some are on single lines, while others are grouped using parentheses. for better readability and consistency, it's best to group all imports from the same module. since this file is autogenerated, the generation script might need to be updated to enforce a consistent style.\n\n",
    "line_number": 24,
    "enriched": "File: keras/api/_tf_keras/keras/config/__init__.py\nCode: @@ -5,15 +5,20 @@\n \"\"\"\n \n from keras.src.backend.config import backend as backend\n+from keras.src.backend.config import disable_debug_mode as disable_debug_mode\n from keras.src.backend.config import (\n     disable_flash_attention as disable_flash_attention,\n )\n+from keras.src.backend.config import enable_debug_mode as enable_debug_mode\n from keras.src.backend.config import (\n     enable_flash_attention as enable_flash_attention,\n )\n from keras.src.backend.config import epsilon as epsilon\n from keras.src.backend.config import floatx as floatx\n from keras.src.backend.config import image_data_format as image_data_format\n+from keras.src.backend.config import (\n+    is_debug_mode_enabled as is_debug_mode_enabled,\n+)\n from keras.src.backend.config import (\n     is_flash_attention_enabled as is_flash_attention_enabled,\n )\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe import statements from `keras.src.backend.config` are styled inconsistently. Some are on single lines, while others are grouped using parentheses. For better readability and consistency, it's best to group all imports from the same module. Since this file is autogenerated, the generation script might need to be updated to enforce a consistent style.\n\n```suggestion\nfrom keras.src.backend.config import (\n    backend as backend,\n    disable_debug_mode as disable_debug_mode,\n    disable_flash_attention as disable_flash_attention,\n    enable_debug_mode as enable_debug_mode,\n    enable_flash_attention as enable_flash_attention,\n    epsilon as epsilon,\n    floatx as floatx,\n    image_data_format as image_data_format,\n    is_debug_mode_enabled as is_debug_mode_enabled,\n    is_flash_attention_enabled as is_flash_attention_enabled,\n)\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/api/_tf_keras/keras/config/__init__.py",
    "pr_number": 21725,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2410447256,
    "comment_created_at": "2025-10-07T12:23:52Z"
  },
  {
    "code": "@@ -97,3 +97,7 @@ def lstsq(a, b, rcond=None):\n     a = convert_to_tensor(a)\n     b = convert_to_tensor(b)\n     return jnp.linalg.lstsq(a, b, rcond=rcond)[0]\n+\n+\n+def jvp(fun, primals, tangents, has_aux=False):\n+    return jax.jvp(fun, primals, tangents, has_aux=has_aux)",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe return format for jvp with has_aux=true is inconsistent with the api contract defined in keras.ops.linalg.jvp and the tensorflow backend implementation. jax.jvp returns ((primals_out, tangents_out), aux), but the keras api expects (primals_out, tangents_out, aux). the implementation should unpack the result from jax.jvp to match the expected signature.\n\n",
    "line_number": 103,
    "enriched": "File: keras/src/backend/jax/linalg.py\nCode: @@ -97,3 +97,7 @@ def lstsq(a, b, rcond=None):\n     a = convert_to_tensor(a)\n     b = convert_to_tensor(b)\n     return jnp.linalg.lstsq(a, b, rcond=rcond)[0]\n+\n+\n+def jvp(fun, primals, tangents, has_aux=False):\n+    return jax.jvp(fun, primals, tangents, has_aux=has_aux)\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe return format for `jvp` with `has_aux=True` is inconsistent with the API contract defined in `keras.ops.linalg.jvp` and the TensorFlow backend implementation. `jax.jvp` returns `((primals_out, tangents_out), aux)`, but the Keras API expects `(primals_out, tangents_out, aux)`. The implementation should unpack the result from `jax.jvp` to match the expected signature.\n\n```suggestion\ndef jvp(fun, primals, tangents, has_aux=False):\n    if not has_aux:\n        return jax.jvp(fun, primals, tangents, has_aux=False)\n    (primals_out, tangents_out), aux = jax.jvp(\n        fun, primals, tangents, has_aux=True\n    )\n    return primals_out, tangents_out, aux\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "keras/src/backend/jax/linalg.py",
    "pr_number": 21720,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2404041911,
    "comment_created_at": "2025-10-04T15:20:27Z"
  },
  {
    "code": "@@ -363,6 +363,7 @@ def dot(inputs, axes=-1, **kwargs):\n         inputs: A list of input tensors (at least 2).\n         axes: Integer or tuple of integers,\n             axis or axes along which to take the dot product.\n+            In which `0` should not be included.",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthanks for adding this important clarification. to improve clarity and provide more context to the user, could you please rephrase this line? explaining *why* axis 0 is disallowed (it's the batch axis) would be helpful.\n\n",
    "line_number": 366,
    "enriched": "File: keras/src/layers/merging/dot.py\nCode: @@ -363,6 +363,7 @@ def dot(inputs, axes=-1, **kwargs):\n         inputs: A list of input tensors (at least 2).\n         axes: Integer or tuple of integers,\n             axis or axes along which to take the dot product.\n+            In which `0` should not be included.\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThanks for adding this important clarification. To improve clarity and provide more context to the user, could you please rephrase this line? Explaining *why* axis 0 is disallowed (it's the batch axis) would be helpful.\n\n```suggestion\n            Note that axis `0` (the batch axis) cannot be included.\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/layers/merging/dot.py",
    "pr_number": 21718,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2401448661,
    "comment_created_at": "2025-10-03T10:14:04Z"
  },
  {
    "code": "@@ -43,10 +43,12 @@ class Normalization(DataLayer):\n             will be broadcast to the shape of the kept axes above;\n             if the value(s) cannot be broadcast, an error will be raised when\n             this layer's `build()` method is called.\n+            Should be specified when `variance` is specified.\n         variance: The variance value(s) to use during normalization. The passed\n             value(s) will be broadcast to the shape of the kept axes above;\n             if the value(s) cannot be broadcast, an error will be raised when\n             this layer's `build()` method is called.\n+            Should be specified when `mean` is specified.",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthanks for adding this clarification. to be more direct and accurate, since a valueerror is raised if only one of mean or variance is provided, you could phrase this as 'mean and variance must be specified together.' this is also less circular than specifying each in terms of the other.\n\n",
    "line_number": 51,
    "enriched": "File: keras/src/layers/preprocessing/normalization.py\nCode: @@ -43,10 +43,12 @@ class Normalization(DataLayer):\n             will be broadcast to the shape of the kept axes above;\n             if the value(s) cannot be broadcast, an error will be raised when\n             this layer's `build()` method is called.\n+            Should be specified when `variance` is specified.\n         variance: The variance value(s) to use during normalization. The passed\n             value(s) will be broadcast to the shape of the kept axes above;\n             if the value(s) cannot be broadcast, an error will be raised when\n             this layer's `build()` method is called.\n+            Should be specified when `mean` is specified.\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThanks for adding this clarification. To be more direct and accurate, since a `ValueError` is raised if only one of `mean` or `variance` is provided, you could phrase this as '`mean` and `variance` must be specified together.' This is also less circular than specifying each in terms of the other.\n\n```suggestion\n            `mean` and `variance` must be specified together.\n        variance: The variance value(s) to use during normalization. The passed\n            value(s) will be broadcast to the shape of the kept axes above;\n            if the value(s) cannot be broadcast, an error will be raised when\n            this layer's `build()` method is called.\n            `mean` and `variance` must be specified together.\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/layers/preprocessing/normalization.py",
    "pr_number": 21716,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2401389610,
    "comment_created_at": "2025-10-03T09:44:45Z"
  },
  {
    "code": "@@ -23,33 +29,228 @@\n IS_THREAD_SAFE = True\n \n \n+class _ProtectedShardedArray:\n+    \"\"\"Wrapper that prevents deletion of sharded JAX arrays.\n+\n+    This wrapper intercepts delete() calls from jax_memory_cleanup\n+    and prevents deletion of sharded arrays that are needed for inference.\n+    \"\"\"\n+\n+    def __init__(self, array):\n+        self._array = array\n+        self._is_sharded = hasattr(array, \"addressable_shards\")\n+\n+    def __getattr__(self, name):\n+        # Delegate all attribute access to the wrapped array\n+        return getattr(self._array, name)\n+\n+    def delete(self):\n+        \"\"\"Intercept delete() calls and prevent deletion of sharded arrays.\"\"\"\n+        if self._is_sharded:\n+            # Don't actually delete sharded arrays\n+            return\n+        else:\n+            # Allow deletion of non-sharded arrays\n+            self._array.delete()\n+\n+    def __repr__(self):\n+        return f\"_ProtectedShardedArray({self._array})\"\n+\n+\n class JaxVariable(KerasVariable):\n     def __init__(self, *args, layout=None, **kwargs):\n         # Intercept layout parameter so that it is available\n         # during initialization.\n         self._layout = layout\n         super().__init__(*args, **kwargs)\n \n+    def _maybe_create_strong_reference(self, value):\n+        \"\"\"Create a strong ref to a JAX array to prevent GC.\"\"\"\n+        if isinstance(value, jax.Array):\n+            if hasattr(value, \"addressable_shards\"):\n+                # For sharded arrays, hold references to the shards' data.\n+                shard_data = [shard.data for shard in value.addressable_shards]\n+                if not hasattr(self, \"_shard_references\"):\n+                    self._shard_references = []\n+                self._shard_references.append(shard_data)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe current implementation of _maybe_create_strong_reference appends to self._shard_references every time variable.value is accessed. this will cause a memory leak, as the _shard_references list will grow indefinitely during inference or training. the intention is to hold a reference to prevent garbage collection, not to accumulate references on every access.\n\ni suggest replacing the append logic with a direct assignment. this ensures that only the reference to the current sharded array data is held, preventing the memory leak.\n\npython\n                self._shard_references = [shard_data]\n",
    "line_number": 69,
    "enriched": "File: keras/src/backend/jax/core.py\nCode: @@ -23,33 +29,228 @@\n IS_THREAD_SAFE = True\n \n \n+class _ProtectedShardedArray:\n+    \"\"\"Wrapper that prevents deletion of sharded JAX arrays.\n+\n+    This wrapper intercepts delete() calls from jax_memory_cleanup\n+    and prevents deletion of sharded arrays that are needed for inference.\n+    \"\"\"\n+\n+    def __init__(self, array):\n+        self._array = array\n+        self._is_sharded = hasattr(array, \"addressable_shards\")\n+\n+    def __getattr__(self, name):\n+        # Delegate all attribute access to the wrapped array\n+        return getattr(self._array, name)\n+\n+    def delete(self):\n+        \"\"\"Intercept delete() calls and prevent deletion of sharded arrays.\"\"\"\n+        if self._is_sharded:\n+            # Don't actually delete sharded arrays\n+            return\n+        else:\n+            # Allow deletion of non-sharded arrays\n+            self._array.delete()\n+\n+    def __repr__(self):\n+        return f\"_ProtectedShardedArray({self._array})\"\n+\n+\n class JaxVariable(KerasVariable):\n     def __init__(self, *args, layout=None, **kwargs):\n         # Intercept layout parameter so that it is available\n         # during initialization.\n         self._layout = layout\n         super().__init__(*args, **kwargs)\n \n+    def _maybe_create_strong_reference(self, value):\n+        \"\"\"Create a strong ref to a JAX array to prevent GC.\"\"\"\n+        if isinstance(value, jax.Array):\n+            if hasattr(value, \"addressable_shards\"):\n+                # For sharded arrays, hold references to the shards' data.\n+                shard_data = [shard.data for shard in value.addressable_shards]\n+                if not hasattr(self, \"_shard_references\"):\n+                    self._shard_references = []\n+                self._shard_references.append(shard_data)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe current implementation of `_maybe_create_strong_reference` appends to `self._shard_references` every time `variable.value` is accessed. This will cause a memory leak, as the `_shard_references` list will grow indefinitely during inference or training. The intention is to hold a reference to prevent garbage collection, not to accumulate references on every access.\n\nI suggest replacing the append logic with a direct assignment. This ensures that only the reference to the current sharded array data is held, preventing the memory leak.\n\n```python\n                self._shard_references = [shard_data]\n```",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "keras/src/backend/jax/core.py",
    "pr_number": 21712,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2401167091,
    "comment_created_at": "2025-10-03T08:16:52Z"
  },
  {
    "code": "@@ -136,7 +136,9 @@ def increment():\n                 g\n                 if g is None or self._overwrite_variable_with_gradient(v)\n                 else ops.divide(g, scale)\n-                for g, v in zip(grads, trainable_variables)\n+                for g, v in zip(\n+                    grads, self.inner_optimizer._trainable_variables\n+                )",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis change accesses a \"private\" attribute _trainable_variables of the inner optimizer. this relies on an internal implementation detail of the optimizer class, which could change in the future and break this code. it also violates the principle of encapsulation.[^1]\n\na more robust solution would be to expose the trainable variables via a public property on the baseoptimizer class. since that would require changes in another file, i'll leave it to you to decide on the best path forward.\n\n#### style guide references\n[^1]: apis should not deal with internal implementation details. an api should be about the problem it solves, not about how the code works in the background.",
    "line_number": 141,
    "enriched": "File: keras/src/optimizers/loss_scale_optimizer.py\nCode: @@ -136,7 +136,9 @@ def increment():\n                 g\n                 if g is None or self._overwrite_variable_with_gradient(v)\n                 else ops.divide(g, scale)\n-                for g, v in zip(grads, trainable_variables)\n+                for g, v in zip(\n+                    grads, self.inner_optimizer._trainable_variables\n+                )\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis change accesses a \"private\" attribute `_trainable_variables` of the inner optimizer. This relies on an internal implementation detail of the `Optimizer` class, which could change in the future and break this code. It also violates the principle of encapsulation.[^1]\n\nA more robust solution would be to expose the trainable variables via a public property on the `BaseOptimizer` class. Since that would require changes in another file, I'll leave it to you to decide on the best path forward.\n\n#### Style Guide References\n[^1]: APIs should not deal with internal implementation details. An API should be about the problem it solves, not about how the code works in the background.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "keras/src/optimizers/loss_scale_optimizer.py",
    "pr_number": 21706,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2395417555,
    "comment_created_at": "2025-10-01T18:03:41Z"
  },
  {
    "code": "@@ -0,0 +1,611 @@\n+import re\n+from typing import Any\n+from typing import Dict\n+from typing import List\n+from typing import Optional\n+from typing import Tuple\n+\n+import numpy as np\n+\n+import keras\n+from keras.src import ops\n+from keras.src import optimizers\n+from keras.src.backend.distributed import backend_resolver\n+\n+\n+class CoordinatedOptimizer:\n+    \"\"\"Manages an optimizer's state for distributed training.\n+\n+    This class is an internal coordinator that handles the complexities of\n+    sharding optimizer states across multiple devices (shards) and\n+    synchronizing gradients according to tensor parallelism rules. It is not\n+    intended to be used directly by the end-user but is a core component of\n+    the `TensorParallelOptimizer`.\n+\n+    Args:\n+        base_optimizer: The Keras optimizer instance\n+            (e.g., `keras.optimizers.Adam`) whose state will be managed.\n+        world_size: The total number of devices/processes in the distributed\n+            setup.\n+        distributed_backend: The distributed communication backend to use.\n+            Defaults to \"auto\".\n+        rank: The rank of the current process. Defaults to 0.\n+        shard_optimizer_states: If `True`, the optimizer's state variables\n+            (e.g., momentum, velocity) will be partitioned across `world_size`\n+            devices. Defaults to `True`.\n+        tensor_parallel_config: An optional configuration object that defines\n+            rules for tensor parallelism, such as which gradients to\n+            all-reduce. Defaults to `None`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        base_optimizer: optimizers.Optimizer,\n+        world_size: int,\n+        distributed_backend: str = \"auto\",\n+        rank: int = 0,\n+        shard_optimizer_states: bool = True,\n+        tensor_parallel_config=None,\n+    ):\n+        self.base_optimizer = base_optimizer\n+        self.world_size = world_size\n+        self.rank = rank\n+        self.shard_optimizer_states = shard_optimizer_states\n+        self.tensor_parallel_config = tensor_parallel_config\n+        self.sharded_states = {}\n+\n+        self.distributed_backend = backend_resolver.get_distributed_backend(\n+            distributed_backend\n+        )\n+\n+        if self.shard_optimizer_states:\n+            if getattr(self.base_optimizer, \"built\", False):\n+                self._initialize_sharded_states()\n+            else:\n+                self.shard_optimizer_states = False\n+\n+    def _parse_variable_name(\n+        self, var_name: str\n+    ) -> Tuple[Optional[str], Optional[str]]:\n+        \"\"\"Parses an optimizer variable name to find its type and parameter.\n+\n+        For ex, it maps 'dense/kernel/_momentum' to\n+        ('momentum', 'dense/kernel').\n+\n+        Args:\n+            var_name: The name of the optimizer state variable.\n+\n+        Returns:\n+            A tuple containing the state type (e.g., 'momentum') and the\n+            associated parameter name. Returns (None, None) if no known\n+            state suffix is found.\n+        \"\"\"\n+        var_name_lower = var_name.lower()\n+        state_map = {\n+            \"_momentum\": \"momentum\",\n+            \"_velocity\": \"velocity\",\n+            \"_m\": \"m\",\n+            \"_v\": \"v\",\n+        }\n+        for suffix, state_name in state_map.items():\n+            if var_name_lower.endswith(suffix):\n+                param_name = var_name[: -len(suffix)]\n+                return state_name, param_name\n+        return None, None\n+\n+    def _get_actual_optimizer_state(self) -> Dict[str, Any]:\n+        \"\"\"Extracts and structures the optimizer's state variables.\n+\n+        This method inspects the `variables` of the `base_optimizer` and\n+        organizes them into a nested dictionary structure based on their type\n+        (e.g., learning rate, iteration count, momentum).\n+\n+        Returns:\n+            A dictionary containing the optimizer's state, organized by\n+            state type. For example:\n+            {'t': <tf.Variable>, 'lr': <tf.Variable>,\n+             'momentum': {'param1': <tf.Variable>}}.\n+        \"\"\"\n+        state_dict = {}\n+        for var in self.base_optimizer.variables:\n+            identifier = getattr(var, \"path\", getattr(var, \"name\", str(var)))\n+            parts = identifier.split(\"/\")\n+            tail = parts[-1]\n+            tail_lower = tail.lower()\n+\n+            if \"iteration\" in tail_lower or tail_lower in {\"iter\", \"t\"}:\n+                state_dict[\"t\"] = var\n+                continue\n+            if \"learning_rate\" in tail_lower or tail_lower in {\"lr\"}:\n+                state_dict[\"lr\"] = var\n+                continue\n+\n+            state_name, param_name = self._parse_variable_name(tail)\n+            if not state_name:\n+                state_name = \"state\"\n+                param_name = tail\n+\n+            if state_name not in state_dict:\n+                state_dict[state_name] = {}\n+            state_dict[state_name][param_name] = var\n+        return state_dict\n+\n+    def _initialize_sharded_states(self):\n+        \"\"\"Partitions the optimizer's state variables across shards.\"\"\"\n+        if not self.shard_optimizer_states or not getattr(\n+            self.base_optimizer, \"built\", False\n+        ):\n+            return\n+\n+        base_state = self._get_actual_optimizer_state()\n+        if not base_state:\n+            self.shard_optimizer_states = False\n+            return\n+\n+        self.sharded_states = {}\n+        for state_name, state_value in base_state.items():\n+            if isinstance(state_value, dict):\n+                self.sharded_states[state_name] = {}\n+                for param_name, param_state_var in state_value.items():\n+                    sharding_dim = 0\n+                    if self.tensor_parallel_config:\n+                        norm_param = param_name.replace(\"/\", \".\")\n+                        for (\n+                            p,\n+                            a,\n+                        ) in self.tensor_parallel_config.state_rules.items():\n+                            if re.search(p, norm_param) and hasattr(a, \"dim\"):\n+                                sharding_dim = a.dim\n+                                break\n+                    self.sharded_states[state_name][param_name] = (\n+                        self._partition_state(param_state_var, dim=sharding_dim)\n+                    )\n+            else:\n+                self.sharded_states[state_name] = self._partition_state(\n+                    state_value, dim=0\n+                )\n+\n+    def _partition_state(\n+        self, state_variable: any, dim: int\n+    ) -> List[np.ndarray]:\n+        \"\"\"Splits a single state variable numpy array into chunks.\n+\n+        If the variable cannot be split along the given dimension, it is\n+        replicated across all shards.\n+\n+        Args:\n+            state_variable: The optimizer state variable.\n+            dim: The dimension along which to partition the variable.\n+\n+        Returns:\n+            A list of NumPy arrays, where each array is a partition of the\n+            original state variable for a specific shard.\n+        \"\"\"\n+        state_array = keras.ops.convert_to_numpy(state_variable)\n+        if state_array.ndim > dim and state_array.shape[dim] >= self.world_size:\n+            return np.array_split(state_array, self.world_size, axis=dim)\n+        else:\n+            return [np.copy(state_array) for _ in range(self.world_size)]\n+\n+    def get_config(self) -> Dict[str, Any]:\n+        return {\n+            \"base_optimizer\": self.base_optimizer.get_config(),\n+            \"world_size\": self.world_size,\n+            \"shard_optimizer_states\": self.shard_optimizer_states,\n+        }\n+\n+    def apply_gradients(\n+        self, gradients_and_vars: List[List[tuple]], shard_models: List\n+    ):\n+        \"\"\"Coordinates gradient synchronization and application.\n+\n+        This method first synchronizes gradients across all shards based on\n+        tensor parallelism rules. Then, it applies the gradients using either\n+        sharded optimizer states or replicated states.\n+\n+        Args:\n+            gradients_and_vars: A list of lists, where each inner list contains\n+                (gradient, variable) tuples for a specific model shard.\n+            shard_models: A list of the sharded model instances.\n+\n+        Raises:\n+            ValueError: If the number of gradient sets does not match the\n+                world size.\n+        \"\"\"\n+        if len(gradients_and_vars) != self.world_size:\n+            error_msg = (\n+                f\"Expected {self.world_size} gradient sets, \"\n+                f\"got {len(gradients_and_vars)}\"\n+            )\n+            raise ValueError(error_msg)\n+\n+        synchronized_gradients = self._synchronize_gradients(gradients_and_vars)\n+\n+        if self.shard_optimizer_states and self.sharded_states:\n+            self._apply_gradients_with_sharded_states(\n+                synchronized_gradients, shard_models\n+            )\n+        else:\n+            self._apply_gradients_with_replicated_states(\n+                synchronized_gradients, shard_models\n+            )\n+\n+    def _apply_gradients_with_sharded_states(\n+        self, synchronized_gradients: List[List[tuple]], shard_models: List\n+    ):\n+        \"\"\"Applies gradients to each shard using its local optimizer state.\n+\n+        For each shard, this method loads the corresponding partition of the\n+        optimizer state into the base optimizer and then applies the shard's\n+        gradients.\n+\n+        Args:\n+            synchronized_gradients: The gradients after synchronization.\n+            shard_models: The list of sharded models.\n+        \"\"\"\n+        for shard_idx, shard_grads in enumerate(synchronized_gradients):\n+            local_states = self._get_local_optimizer_states(shard_idx)\n+            self._update_optimizer_internal_state(\n+                self.base_optimizer, local_states\n+            )\n+            self.base_optimizer.apply_gradients(shard_grads)\n+\n+    def _apply_gradients_with_replicated_states(\n+        self, synchronized_gradients: List[List[tuple]], shard_models: List\n+    ):\n+        \"\"\"Averages gradients across all shards and applies them once.\n+\n+        This method is used when optimizer state sharding is disabled. It\n+        calculates the average of the gradients for each variable across all\n+        shards and applies the averaged gradients using the single, replicated\n+        optimizer state.\n+\n+        Args:\n+            synchronized_gradients: The gradients after synchronization.\n+            shard_models: The list of sharded models.\n+        \"\"\"\n+        num_vars = len(synchronized_gradients[0])\n+        averaged_grads_and_vars = []\n+\n+        for i in range(num_vars):\n+            variable = synchronized_gradients[0][i][1]\n+            grads_for_var = [\n+                shard_grads[i][0]\n+                for shard_grads in synchronized_gradients\n+                if shard_grads[i][0] is not None\n+            ]\n+\n+            if not grads_for_var:\n+                continue\n+\n+            summed_grad = grads_for_var[0]\n+            for grad in grads_for_var[1:]:\n+                summed_grad += grad\n+            averaged_grad = summed_grad / len(grads_for_var)\n+            averaged_grads_and_vars.append((averaged_grad, variable))\n+\n+        if averaged_grads_and_vars:\n+            self.base_optimizer.apply_gradients(averaged_grads_and_vars)\n+\n+    def _get_local_optimizer_states(self, shard_idx: int) -> Dict[str, Any]:\n+        \"\"\"Constructs the state dictionary for a single shard.\n+\n+        Args:\n+            shard_idx: The index of the shard for which to retrieve the state.\n+\n+        Returns:\n+            A dictionary containing the optimizer state variables specific to\n+            the given shard index.\n+        \"\"\"\n+        local_states = {}\n+        for state_name, state_value in self.sharded_states.items():\n+            if isinstance(state_value, dict):\n+                local_states[state_name] = {}\n+                for param_name, param_states in state_value.items():\n+                    local_states[state_name][param_name] = param_states[\n+                        shard_idx\n+                    ]\n+            else:\n+                local_states[state_name] = state_value[shard_idx]\n+        return local_states\n+\n+    def _update_optimizer_internal_state(self, optimizer, local_states: dict):\n+        \"\"\"Assigns local sharded state values to the optimizer's variables.\n+\n+        This method updates the `base_optimizer`'s internal state variables\n+        in-place with the values from a specific shard's state partition.\n+\n+        Args:\n+            optimizer: The Keras optimizer instance to update.\n+            local_states: A dictionary of state values for a single shard.\n+        \"\"\"\n+        if not hasattr(optimizer, \"variables\") or not optimizer.variables:\n+            return\n+\n+        for var in optimizer.variables:\n+            identifier = getattr(var, \"path\", getattr(var, \"name\", str(var)))\n+            parts = identifier.split(\"/\")\n+            tail = parts[-1]\n+            tail_lower = tail.lower()\n+\n+            if \"iteration\" in tail_lower or tail_lower in {\"iter\", \"t\"}:\n+                if \"t\" in local_states:\n+                    var.assign(local_states[\"t\"])\n+                continue\n+            if \"learning_rate\" in tail_lower or tail_lower in {\"lr\"}:\n+                if \"lr\" in local_states:\n+                    var.assign(local_states[\"lr\"])\n+                continue\n+\n+            state_name, param_name_in_opt = self._parse_variable_name(tail)\n+            if (\n+                state_name in local_states\n+                and param_name_in_opt in local_states[state_name]\n+            ):\n+                local_param_state = local_states[state_name][param_name_in_opt]\n+                if var.shape == local_param_state.shape:\n+                    var.assign(local_param_state)\n+\n+    def _synchronize_gradients(\n+        self, gradients_and_vars: List[List[tuple]]\n+    ) -> List[List[tuple]]:\n+        \"\"\"Synchronizes gradients across shards based on tensor parallel rules.\n+\n+        Specifically, it performs an all-reduce operation on gradients of\n+        weights that are split along a \"column\" dimension in tensor parallelism.\n+        Other gradients are passed through unchanged.\n+\n+        Args:\n+            gradients_and_vars: The list of (gradient, variable) lists from\n+                all shards.\n+\n+        Returns:\n+            The list of (gradient, variable) lists after synchronization.\n+        \"\"\"\n+        if not self.tensor_parallel_config:\n+            return gradients_and_vars\n+\n+        rules = self.tensor_parallel_config.state_rules.items()\n+        column_parallel_patterns = {\n+            pattern\n+            for pattern, action in rules\n+            if hasattr(action, \"sharding_type\")\n+            and action.sharding_type == \"column\"\n+        }\n+\n+        if not column_parallel_patterns:\n+            return gradients_and_vars\n+\n+        num_weights = len(gradients_and_vars[0])\n+        for i in range(num_weights):\n+            variable = gradients_and_vars[0][i][1]\n+            var_name = getattr(variable, \"path\", getattr(variable, \"name\", \"\"))\n+\n+            if any(\n+                re.search(pattern, var_name)\n+                for pattern in column_parallel_patterns\n+            ):\n+                grads_to_reduce = [\n+                    g_and_v[i][0]\n+                    for g_and_v in gradients_and_vars\n+                    if g_and_v[i][0] is not None\n+                ]\n+                if grads_to_reduce:\n+                    synced_grad = self._allreduce_gradients(grads_to_reduce)[0]\n+                    for shard_idx in range(self.world_size):\n+                        gradients_and_vars[shard_idx][i] = (\n+                            synced_grad,\n+                            variable,\n+                        )\n+        return gradients_and_vars\n+\n+    def _allreduce_gradients(self, gradients: List[Any]) -> List[Any]:\n+        \"\"\"Performs a mean all-reduce operation on a list of gradients.\n+\n+        If a distributed backend is available, it uses it. Otherwise, it\n+        falls back to a local mean calculation.\n+\n+        Args:\n+            gradients: A list of gradients (one from each shard) to be averaged.\n+\n+        Returns:\n+            A list where each element is the mean of the input gradients.\n+        \"\"\"\n+        if not gradients:\n+            return []\n+\n+        if (\n+            self.distributed_backend is not None\n+            and self.distributed_backend.is_initialized\n+        ):\n+            numpy_grad = keras.ops.convert_to_numpy(gradients[0])\n+            synced_numpy = self.distributed_backend.allreduce(\n+                numpy_grad, op=\"mean\"\n+            )\n+            synced_tensor = keras.ops.convert_to_tensor(synced_numpy)\n+            return [synced_tensor for _ in range(self.world_size)]\n+\n+        stacked_grads = keras.ops.stack(\n+            [keras.ops.convert_to_tensor(g) for g in gradients], axis=0\n+        )\n+        mean_grad = keras.ops.mean(stacked_grads, axis=0)\n+        return [mean_grad for _ in range(len(gradients))]\n+\n+    def get_weights(self) -> List[np.ndarray]:\n+        \"\"\"Returns the weights of the base optimizer.\"\"\"\n+        return self.base_optimizer.get_weights()\n+\n+    def set_weights(self, weights: List[np.ndarray]):\n+        \"\"\"Sets the weights of the base optimizer.\"\"\"\n+        self.base_optimizer.set_weights(weights)\n+\n+    def enable_optimizer_state_sharding(self):\n+        \"\"\"Enables and initializes optimizer state sharding.\n+\n+        If sharding is not already active, this method sets the flag and\n+        triggers the partitioning of the optimizer's states.\n+        \"\"\"\n+        if not self.shard_optimizer_states:\n+            self.shard_optimizer_states = True\n+            self._initialize_sharded_states()\n+\n+    def disable_optimizer_state_sharding(self):\n+        \"\"\"Disables sharding and clears any sharded states.\n+\n+        This reverts the optimizer to using a single, replicated state.\n+        \"\"\"\n+        if self.shard_optimizer_states:\n+            self.shard_optimizer_states = False\n+            self.sharded_states = {}\n+\n+\n+class TensorParallelOptimizer(optimizers.Optimizer):\n+    \"\"\"A Keras Optimizer wrapper for tensor-parallel distributed training.\n+\n+    This optimizer wraps a standard Keras optimizer (e.g., Adam, SGD) and\n+    delegates the complex tasks of state management and gradient synchronization\n+    to a `CoordinatedOptimizer` instance. It is designed to work with models\n+    that have been sharded for tensor parallelism.\n+\n+    When `apply_gradients` is called with a list of gradient lists (one for each\n+    model shard), it uses the `CoordinatedOptimizer` to handle synchronization\n+    and state sharding. Otherwise, it behaves like the base optimizer.\n+\n+    Args:\n+        base_optimizer: A Keras optimizer instance or a string identifier\n+            (e.g., 'adam', 'sgd').\n+        world_size: The total number of devices/processes in the distributed\n+            setup.\n+        distributed_backend: The distributed communication backend to use.\n+            Defaults to \"auto\".\n+        tensor_parallel_config: An optional configuration object that defines\n+            rules for tensor parallelism. Defaults to `None`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        base_optimizer: optimizers.Optimizer,\n+        world_size: int,\n+        distributed_backend: str = \"auto\",\n+        tensor_parallel_config=None,\n+    ):\n+        if isinstance(base_optimizer, str):\n+            opt_lower = base_optimizer.lower()\n+            if opt_lower == \"adam\":\n+                resolved_base_optimizer = optimizers.Adam()\n+            elif opt_lower == \"sgd\":\n+                resolved_base_optimizer = optimizers.SGD()\n+            elif opt_lower == \"rmsprop\":\n+                resolved_base_optimizer = optimizers.RMSprop()\n+            else:\n+                raise ValueError(f\"Unknown optimizer string: {base_optimizer}\")\n+        else:\n+            resolved_base_optimizer = base_optimizer\n+\n+        lr_value = float(\n+            ops.convert_to_numpy(resolved_base_optimizer.learning_rate)\n+        )",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe initialization of lr_value by casting resolved_base_optimizer.learning_rate to a float will fail if a keras.optimizers.schedules.learningrateschedule is used, as it cannot be converted to a single float. this is a critical bug that prevents the use of learning rate schedules with tensorparalleloptimizer. the learning_rate property of this class already delegates to the base optimizer, so the learning_rate argument to super().__init__ should be handled differently to support schedules.\n\npython\n        if isinstance(\n            resolved_base_optimizer.learning_rate,\n            keras.optimizers.schedules.learningrateschedule,\n        ):\n            lr_value = float(\n                ops.convert_to_numpy(\n                    resolved_base_optimizer.learning_rate.initial_learning_rate\n                )\n            )\n        else:\n            lr_value = float(\n                ops.convert_to_numpy(resolved_base_optimizer.learning_rate)\n            )\n",
    "line_number": 507,
    "enriched": "File: keras/src/distribution/tensor_parallel/coordinated_optimizer.py\nCode: @@ -0,0 +1,611 @@\n+import re\n+from typing import Any\n+from typing import Dict\n+from typing import List\n+from typing import Optional\n+from typing import Tuple\n+\n+import numpy as np\n+\n+import keras\n+from keras.src import ops\n+from keras.src import optimizers\n+from keras.src.backend.distributed import backend_resolver\n+\n+\n+class CoordinatedOptimizer:\n+    \"\"\"Manages an optimizer's state for distributed training.\n+\n+    This class is an internal coordinator that handles the complexities of\n+    sharding optimizer states across multiple devices (shards) and\n+    synchronizing gradients according to tensor parallelism rules. It is not\n+    intended to be used directly by the end-user but is a core component of\n+    the `TensorParallelOptimizer`.\n+\n+    Args:\n+        base_optimizer: The Keras optimizer instance\n+            (e.g., `keras.optimizers.Adam`) whose state will be managed.\n+        world_size: The total number of devices/processes in the distributed\n+            setup.\n+        distributed_backend: The distributed communication backend to use.\n+            Defaults to \"auto\".\n+        rank: The rank of the current process. Defaults to 0.\n+        shard_optimizer_states: If `True`, the optimizer's state variables\n+            (e.g., momentum, velocity) will be partitioned across `world_size`\n+            devices. Defaults to `True`.\n+        tensor_parallel_config: An optional configuration object that defines\n+            rules for tensor parallelism, such as which gradients to\n+            all-reduce. Defaults to `None`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        base_optimizer: optimizers.Optimizer,\n+        world_size: int,\n+        distributed_backend: str = \"auto\",\n+        rank: int = 0,\n+        shard_optimizer_states: bool = True,\n+        tensor_parallel_config=None,\n+    ):\n+        self.base_optimizer = base_optimizer\n+        self.world_size = world_size\n+        self.rank = rank\n+        self.shard_optimizer_states = shard_optimizer_states\n+        self.tensor_parallel_config = tensor_parallel_config\n+        self.sharded_states = {}\n+\n+        self.distributed_backend = backend_resolver.get_distributed_backend(\n+            distributed_backend\n+        )\n+\n+        if self.shard_optimizer_states:\n+            if getattr(self.base_optimizer, \"built\", False):\n+                self._initialize_sharded_states()\n+            else:\n+                self.shard_optimizer_states = False\n+\n+    def _parse_variable_name(\n+        self, var_name: str\n+    ) -> Tuple[Optional[str], Optional[str]]:\n+        \"\"\"Parses an optimizer variable name to find its type and parameter.\n+\n+        For ex, it maps 'dense/kernel/_momentum' to\n+        ('momentum', 'dense/kernel').\n+\n+        Args:\n+            var_name: The name of the optimizer state variable.\n+\n+        Returns:\n+            A tuple containing the state type (e.g., 'momentum') and the\n+            associated parameter name. Returns (None, None) if no known\n+            state suffix is found.\n+        \"\"\"\n+        var_name_lower = var_name.lower()\n+        state_map = {\n+            \"_momentum\": \"momentum\",\n+            \"_velocity\": \"velocity\",\n+            \"_m\": \"m\",\n+            \"_v\": \"v\",\n+        }\n+        for suffix, state_name in state_map.items():\n+            if var_name_lower.endswith(suffix):\n+                param_name = var_name[: -len(suffix)]\n+                return state_name, param_name\n+        return None, None\n+\n+    def _get_actual_optimizer_state(self) -> Dict[str, Any]:\n+        \"\"\"Extracts and structures the optimizer's state variables.\n+\n+        This method inspects the `variables` of the `base_optimizer` and\n+        organizes them into a nested dictionary structure based on their type\n+        (e.g., learning rate, iteration count, momentum).\n+\n+        Returns:\n+            A dictionary containing the optimizer's state, organized by\n+            state type. For example:\n+            {'t': <tf.Variable>, 'lr': <tf.Variable>,\n+             'momentum': {'param1': <tf.Variable>}}.\n+        \"\"\"\n+        state_dict = {}\n+        for var in self.base_optimizer.variables:\n+            identifier = getattr(var, \"path\", getattr(var, \"name\", str(var)))\n+            parts = identifier.split(\"/\")\n+            tail = parts[-1]\n+            tail_lower = tail.lower()\n+\n+            if \"iteration\" in tail_lower or tail_lower in {\"iter\", \"t\"}:\n+                state_dict[\"t\"] = var\n+                continue\n+            if \"learning_rate\" in tail_lower or tail_lower in {\"lr\"}:\n+                state_dict[\"lr\"] = var\n+                continue\n+\n+            state_name, param_name = self._parse_variable_name(tail)\n+            if not state_name:\n+                state_name = \"state\"\n+                param_name = tail\n+\n+            if state_name not in state_dict:\n+                state_dict[state_name] = {}\n+            state_dict[state_name][param_name] = var\n+        return state_dict\n+\n+    def _initialize_sharded_states(self):\n+        \"\"\"Partitions the optimizer's state variables across shards.\"\"\"\n+        if not self.shard_optimizer_states or not getattr(\n+            self.base_optimizer, \"built\", False\n+        ):\n+            return\n+\n+        base_state = self._get_actual_optimizer_state()\n+        if not base_state:\n+            self.shard_optimizer_states = False\n+            return\n+\n+        self.sharded_states = {}\n+        for state_name, state_value in base_state.items():\n+            if isinstance(state_value, dict):\n+                self.sharded_states[state_name] = {}\n+                for param_name, param_state_var in state_value.items():\n+                    sharding_dim = 0\n+                    if self.tensor_parallel_config:\n+                        norm_param = param_name.replace(\"/\", \".\")\n+                        for (\n+                            p,\n+                            a,\n+                        ) in self.tensor_parallel_config.state_rules.items():\n+                            if re.search(p, norm_param) and hasattr(a, \"dim\"):\n+                                sharding_dim = a.dim\n+                                break\n+                    self.sharded_states[state_name][param_name] = (\n+                        self._partition_state(param_state_var, dim=sharding_dim)\n+                    )\n+            else:\n+                self.sharded_states[state_name] = self._partition_state(\n+                    state_value, dim=0\n+                )\n+\n+    def _partition_state(\n+        self, state_variable: any, dim: int\n+    ) -> List[np.ndarray]:\n+        \"\"\"Splits a single state variable numpy array into chunks.\n+\n+        If the variable cannot be split along the given dimension, it is\n+        replicated across all shards.\n+\n+        Args:\n+            state_variable: The optimizer state variable.\n+            dim: The dimension along which to partition the variable.\n+\n+        Returns:\n+            A list of NumPy arrays, where each array is a partition of the\n+            original state variable for a specific shard.\n+        \"\"\"\n+        state_array = keras.ops.convert_to_numpy(state_variable)\n+        if state_array.ndim > dim and state_array.shape[dim] >= self.world_size:\n+            return np.array_split(state_array, self.world_size, axis=dim)\n+        else:\n+            return [np.copy(state_array) for _ in range(self.world_size)]\n+\n+    def get_config(self) -> Dict[str, Any]:\n+        return {\n+            \"base_optimizer\": self.base_optimizer.get_config(),\n+            \"world_size\": self.world_size,\n+            \"shard_optimizer_states\": self.shard_optimizer_states,\n+        }\n+\n+    def apply_gradients(\n+        self, gradients_and_vars: List[List[tuple]], shard_models: List\n+    ):\n+        \"\"\"Coordinates gradient synchronization and application.\n+\n+        This method first synchronizes gradients across all shards based on\n+        tensor parallelism rules. Then, it applies the gradients using either\n+        sharded optimizer states or replicated states.\n+\n+        Args:\n+            gradients_and_vars: A list of lists, where each inner list contains\n+                (gradient, variable) tuples for a specific model shard.\n+            shard_models: A list of the sharded model instances.\n+\n+        Raises:\n+            ValueError: If the number of gradient sets does not match the\n+                world size.\n+        \"\"\"\n+        if len(gradients_and_vars) != self.world_size:\n+            error_msg = (\n+                f\"Expected {self.world_size} gradient sets, \"\n+                f\"got {len(gradients_and_vars)}\"\n+            )\n+            raise ValueError(error_msg)\n+\n+        synchronized_gradients = self._synchronize_gradients(gradients_and_vars)\n+\n+        if self.shard_optimizer_states and self.sharded_states:\n+            self._apply_gradients_with_sharded_states(\n+                synchronized_gradients, shard_models\n+            )\n+        else:\n+            self._apply_gradients_with_replicated_states(\n+                synchronized_gradients, shard_models\n+            )\n+\n+    def _apply_gradients_with_sharded_states(\n+        self, synchronized_gradients: List[List[tuple]], shard_models: List\n+    ):\n+        \"\"\"Applies gradients to each shard using its local optimizer state.\n+\n+        For each shard, this method loads the corresponding partition of the\n+        optimizer state into the base optimizer and then applies the shard's\n+        gradients.\n+\n+        Args:\n+            synchronized_gradients: The gradients after synchronization.\n+            shard_models: The list of sharded models.\n+        \"\"\"\n+        for shard_idx, shard_grads in enumerate(synchronized_gradients):\n+            local_states = self._get_local_optimizer_states(shard_idx)\n+            self._update_optimizer_internal_state(\n+                self.base_optimizer, local_states\n+            )\n+            self.base_optimizer.apply_gradients(shard_grads)\n+\n+    def _apply_gradients_with_replicated_states(\n+        self, synchronized_gradients: List[List[tuple]], shard_models: List\n+    ):\n+        \"\"\"Averages gradients across all shards and applies them once.\n+\n+        This method is used when optimizer state sharding is disabled. It\n+        calculates the average of the gradients for each variable across all\n+        shards and applies the averaged gradients using the single, replicated\n+        optimizer state.\n+\n+        Args:\n+            synchronized_gradients: The gradients after synchronization.\n+            shard_models: The list of sharded models.\n+        \"\"\"\n+        num_vars = len(synchronized_gradients[0])\n+        averaged_grads_and_vars = []\n+\n+        for i in range(num_vars):\n+            variable = synchronized_gradients[0][i][1]\n+            grads_for_var = [\n+                shard_grads[i][0]\n+                for shard_grads in synchronized_gradients\n+                if shard_grads[i][0] is not None\n+            ]\n+\n+            if not grads_for_var:\n+                continue\n+\n+            summed_grad = grads_for_var[0]\n+            for grad in grads_for_var[1:]:\n+                summed_grad += grad\n+            averaged_grad = summed_grad / len(grads_for_var)\n+            averaged_grads_and_vars.append((averaged_grad, variable))\n+\n+        if averaged_grads_and_vars:\n+            self.base_optimizer.apply_gradients(averaged_grads_and_vars)\n+\n+    def _get_local_optimizer_states(self, shard_idx: int) -> Dict[str, Any]:\n+        \"\"\"Constructs the state dictionary for a single shard.\n+\n+        Args:\n+            shard_idx: The index of the shard for which to retrieve the state.\n+\n+        Returns:\n+            A dictionary containing the optimizer state variables specific to\n+            the given shard index.\n+        \"\"\"\n+        local_states = {}\n+        for state_name, state_value in self.sharded_states.items():\n+            if isinstance(state_value, dict):\n+                local_states[state_name] = {}\n+                for param_name, param_states in state_value.items():\n+                    local_states[state_name][param_name] = param_states[\n+                        shard_idx\n+                    ]\n+            else:\n+                local_states[state_name] = state_value[shard_idx]\n+        return local_states\n+\n+    def _update_optimizer_internal_state(self, optimizer, local_states: dict):\n+        \"\"\"Assigns local sharded state values to the optimizer's variables.\n+\n+        This method updates the `base_optimizer`'s internal state variables\n+        in-place with the values from a specific shard's state partition.\n+\n+        Args:\n+            optimizer: The Keras optimizer instance to update.\n+            local_states: A dictionary of state values for a single shard.\n+        \"\"\"\n+        if not hasattr(optimizer, \"variables\") or not optimizer.variables:\n+            return\n+\n+        for var in optimizer.variables:\n+            identifier = getattr(var, \"path\", getattr(var, \"name\", str(var)))\n+            parts = identifier.split(\"/\")\n+            tail = parts[-1]\n+            tail_lower = tail.lower()\n+\n+            if \"iteration\" in tail_lower or tail_lower in {\"iter\", \"t\"}:\n+                if \"t\" in local_states:\n+                    var.assign(local_states[\"t\"])\n+                continue\n+            if \"learning_rate\" in tail_lower or tail_lower in {\"lr\"}:\n+                if \"lr\" in local_states:\n+                    var.assign(local_states[\"lr\"])\n+                continue\n+\n+            state_name, param_name_in_opt = self._parse_variable_name(tail)\n+            if (\n+                state_name in local_states\n+                and param_name_in_opt in local_states[state_name]\n+            ):\n+                local_param_state = local_states[state_name][param_name_in_opt]\n+                if var.shape == local_param_state.shape:\n+                    var.assign(local_param_state)\n+\n+    def _synchronize_gradients(\n+        self, gradients_and_vars: List[List[tuple]]\n+    ) -> List[List[tuple]]:\n+        \"\"\"Synchronizes gradients across shards based on tensor parallel rules.\n+\n+        Specifically, it performs an all-reduce operation on gradients of\n+        weights that are split along a \"column\" dimension in tensor parallelism.\n+        Other gradients are passed through unchanged.\n+\n+        Args:\n+            gradients_and_vars: The list of (gradient, variable) lists from\n+                all shards.\n+\n+        Returns:\n+            The list of (gradient, variable) lists after synchronization.\n+        \"\"\"\n+        if not self.tensor_parallel_config:\n+            return gradients_and_vars\n+\n+        rules = self.tensor_parallel_config.state_rules.items()\n+        column_parallel_patterns = {\n+            pattern\n+            for pattern, action in rules\n+            if hasattr(action, \"sharding_type\")\n+            and action.sharding_type == \"column\"\n+        }\n+\n+        if not column_parallel_patterns:\n+            return gradients_and_vars\n+\n+        num_weights = len(gradients_and_vars[0])\n+        for i in range(num_weights):\n+            variable = gradients_and_vars[0][i][1]\n+            var_name = getattr(variable, \"path\", getattr(variable, \"name\", \"\"))\n+\n+            if any(\n+                re.search(pattern, var_name)\n+                for pattern in column_parallel_patterns\n+            ):\n+                grads_to_reduce = [\n+                    g_and_v[i][0]\n+                    for g_and_v in gradients_and_vars\n+                    if g_and_v[i][0] is not None\n+                ]\n+                if grads_to_reduce:\n+                    synced_grad = self._allreduce_gradients(grads_to_reduce)[0]\n+                    for shard_idx in range(self.world_size):\n+                        gradients_and_vars[shard_idx][i] = (\n+                            synced_grad,\n+                            variable,\n+                        )\n+        return gradients_and_vars\n+\n+    def _allreduce_gradients(self, gradients: List[Any]) -> List[Any]:\n+        \"\"\"Performs a mean all-reduce operation on a list of gradients.\n+\n+        If a distributed backend is available, it uses it. Otherwise, it\n+        falls back to a local mean calculation.\n+\n+        Args:\n+            gradients: A list of gradients (one from each shard) to be averaged.\n+\n+        Returns:\n+            A list where each element is the mean of the input gradients.\n+        \"\"\"\n+        if not gradients:\n+            return []\n+\n+        if (\n+            self.distributed_backend is not None\n+            and self.distributed_backend.is_initialized\n+        ):\n+            numpy_grad = keras.ops.convert_to_numpy(gradients[0])\n+            synced_numpy = self.distributed_backend.allreduce(\n+                numpy_grad, op=\"mean\"\n+            )\n+            synced_tensor = keras.ops.convert_to_tensor(synced_numpy)\n+            return [synced_tensor for _ in range(self.world_size)]\n+\n+        stacked_grads = keras.ops.stack(\n+            [keras.ops.convert_to_tensor(g) for g in gradients], axis=0\n+        )\n+        mean_grad = keras.ops.mean(stacked_grads, axis=0)\n+        return [mean_grad for _ in range(len(gradients))]\n+\n+    def get_weights(self) -> List[np.ndarray]:\n+        \"\"\"Returns the weights of the base optimizer.\"\"\"\n+        return self.base_optimizer.get_weights()\n+\n+    def set_weights(self, weights: List[np.ndarray]):\n+        \"\"\"Sets the weights of the base optimizer.\"\"\"\n+        self.base_optimizer.set_weights(weights)\n+\n+    def enable_optimizer_state_sharding(self):\n+        \"\"\"Enables and initializes optimizer state sharding.\n+\n+        If sharding is not already active, this method sets the flag and\n+        triggers the partitioning of the optimizer's states.\n+        \"\"\"\n+        if not self.shard_optimizer_states:\n+            self.shard_optimizer_states = True\n+            self._initialize_sharded_states()\n+\n+    def disable_optimizer_state_sharding(self):\n+        \"\"\"Disables sharding and clears any sharded states.\n+\n+        This reverts the optimizer to using a single, replicated state.\n+        \"\"\"\n+        if self.shard_optimizer_states:\n+            self.shard_optimizer_states = False\n+            self.sharded_states = {}\n+\n+\n+class TensorParallelOptimizer(optimizers.Optimizer):\n+    \"\"\"A Keras Optimizer wrapper for tensor-parallel distributed training.\n+\n+    This optimizer wraps a standard Keras optimizer (e.g., Adam, SGD) and\n+    delegates the complex tasks of state management and gradient synchronization\n+    to a `CoordinatedOptimizer` instance. It is designed to work with models\n+    that have been sharded for tensor parallelism.\n+\n+    When `apply_gradients` is called with a list of gradient lists (one for each\n+    model shard), it uses the `CoordinatedOptimizer` to handle synchronization\n+    and state sharding. Otherwise, it behaves like the base optimizer.\n+\n+    Args:\n+        base_optimizer: A Keras optimizer instance or a string identifier\n+            (e.g., 'adam', 'sgd').\n+        world_size: The total number of devices/processes in the distributed\n+            setup.\n+        distributed_backend: The distributed communication backend to use.\n+            Defaults to \"auto\".\n+        tensor_parallel_config: An optional configuration object that defines\n+            rules for tensor parallelism. Defaults to `None`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        base_optimizer: optimizers.Optimizer,\n+        world_size: int,\n+        distributed_backend: str = \"auto\",\n+        tensor_parallel_config=None,\n+    ):\n+        if isinstance(base_optimizer, str):\n+            opt_lower = base_optimizer.lower()\n+            if opt_lower == \"adam\":\n+                resolved_base_optimizer = optimizers.Adam()\n+            elif opt_lower == \"sgd\":\n+                resolved_base_optimizer = optimizers.SGD()\n+            elif opt_lower == \"rmsprop\":\n+                resolved_base_optimizer = optimizers.RMSprop()\n+            else:\n+                raise ValueError(f\"Unknown optimizer string: {base_optimizer}\")\n+        else:\n+            resolved_base_optimizer = base_optimizer\n+\n+        lr_value = float(\n+            ops.convert_to_numpy(resolved_base_optimizer.learning_rate)\n+        )\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe initialization of `lr_value` by casting `resolved_base_optimizer.learning_rate` to a float will fail if a `keras.optimizers.schedules.LearningRateSchedule` is used, as it cannot be converted to a single float. This is a critical bug that prevents the use of learning rate schedules with `TensorParallelOptimizer`. The `learning_rate` property of this class already delegates to the base optimizer, so the `learning_rate` argument to `super().__init__` should be handled differently to support schedules.\n\n```python\n        if isinstance(\n            resolved_base_optimizer.learning_rate,\n            keras.optimizers.schedules.LearningRateSchedule,\n        ):\n            lr_value = float(\n                ops.convert_to_numpy(\n                    resolved_base_optimizer.learning_rate.initial_learning_rate\n                )\n            )\n        else:\n            lr_value = float(\n                ops.convert_to_numpy(resolved_base_optimizer.learning_rate)\n            )\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "keras/src/distribution/tensor_parallel/coordinated_optimizer.py",
    "pr_number": 21703,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2393700437,
    "comment_created_at": "2025-10-01T07:30:32Z"
  },
  {
    "code": "@@ -113,6 +111,27 @@ def set_model(self, model):\n         for callback in self.callbacks:\n             callback.set_model(model)\n \n+    def _on_begin(self):\n+        \"\"\"Called by `on_train/test/predict_begin`.\"\"\"",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis docstring could be more descriptive about what the method does, in addition to where it's called from. this aligns with the style guide's recommendation for helpful documentation[^1]. a more descriptive docstring would improve code clarity for future maintainers.\n\n\n\n#### style guide references\n[^1]: the style guide states that docstrings should explain what a piece of code is about and why/how it should be used, to aid understanding.",
    "line_number": 115,
    "enriched": "File: keras/src/callbacks/callback_list.py\nCode: @@ -113,6 +111,27 @@ def set_model(self, model):\n         for callback in self.callbacks:\n             callback.set_model(model)\n \n+    def _on_begin(self):\n+        \"\"\"Called by `on_train/test/predict_begin`.\"\"\"\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis docstring could be more descriptive about what the method does, in addition to where it's called from. This aligns with the style guide's recommendation for helpful documentation[^1]. A more descriptive docstring would improve code clarity for future maintainers.\n\n```suggestion\n        \"\"\"Manages the start of an async block, creating the executor if it's the outermost block.\"\"\"\n```\n\n#### Style Guide References\n[^1]: The style guide states that docstrings should explain what a piece of code is about and why/how it should be used, to aid understanding.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/callbacks/callback_list.py",
    "pr_number": 21701,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2388868447,
    "comment_created_at": "2025-09-29T18:29:48Z"
  },
  {
    "code": "@@ -0,0 +1,40 @@\n+import logging\n+\n+from keras.src.backend.distributed.base import BaseDistributedBackend\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def get_distributed_backend(\n+    backend_name: str = \"auto\",\n+) -> BaseDistributedBackend:\n+    \"\"\"\n+    Factory to get the best available or a specific distributed backend.\n+    \"\"\"\n+    if backend_name == \"auto\":\n+        try:\n+            from keras.src.backend.jax.distributed_backend import (\n+                JaxDistributedBackend,\n+            )\n+\n+            logger.info(\"Auto-detected JAX for distributed backend.\")\n+            return JaxDistributedBackend()\n+        except ImportError:\n+            pass\n+\n+    elif backend_name == \"jax\":\n+        from keras.src.backend.jax.distributed_backend import (\n+            JaxDistributedBackend,\n+        )\n+\n+        return JaxDistributedBackend()\n+    elif backend_name == \"tensorflow\":\n+        pass\n+    elif backend_name == \"torch\":\n+        pass\n+\n+        return TorchDistributedBackend()\n+    elif backend_name == \"numpy\":\n+        pass",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthis part of the factory function has several issues:\n1. the return torchdistributedbackend() on line 36 is incorrectly indented and will be executed for tensorflow, torch, and numpy backends.\n2. torchdistributedbackend is not defined, which will cause a nameerror.\n3. the branches for tensorflow, torch, and numpy are not implemented. using pass can lead to unexpected behavior, as the function might fall through and attempt to execute code outside the if/elif chain.\n\nto make the code more robust and prevent runtime errors, i suggest explicitly raising a notimplementederror for backends that are not yet supported.\n\npython\n    elif backend_name == \"tensorflow\":\n        raise notimplementederror(\"tensorflow distributed backend is not yet implemented.\")\n    elif backend_name == \"torch\":\n        raise notimplementederror(\"torch distributed backend is not yet implemented.\")\n    elif backend_name == \"numpy\":\n        raise notimplementederror(\"numpy distributed backend is not yet implemented.\")\n",
    "line_number": 38,
    "enriched": "File: keras/src/backend/distributed/factory.py\nCode: @@ -0,0 +1,40 @@\n+import logging\n+\n+from keras.src.backend.distributed.base import BaseDistributedBackend\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def get_distributed_backend(\n+    backend_name: str = \"auto\",\n+) -> BaseDistributedBackend:\n+    \"\"\"\n+    Factory to get the best available or a specific distributed backend.\n+    \"\"\"\n+    if backend_name == \"auto\":\n+        try:\n+            from keras.src.backend.jax.distributed_backend import (\n+                JaxDistributedBackend,\n+            )\n+\n+            logger.info(\"Auto-detected JAX for distributed backend.\")\n+            return JaxDistributedBackend()\n+        except ImportError:\n+            pass\n+\n+    elif backend_name == \"jax\":\n+        from keras.src.backend.jax.distributed_backend import (\n+            JaxDistributedBackend,\n+        )\n+\n+        return JaxDistributedBackend()\n+    elif backend_name == \"tensorflow\":\n+        pass\n+    elif backend_name == \"torch\":\n+        pass\n+\n+        return TorchDistributedBackend()\n+    elif backend_name == \"numpy\":\n+        pass\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThis part of the factory function has several issues:\n1. The `return TorchDistributedBackend()` on line 36 is incorrectly indented and will be executed for `tensorflow`, `torch`, and `numpy` backends.\n2. `TorchDistributedBackend` is not defined, which will cause a `NameError`.\n3. The branches for `tensorflow`, `torch`, and `numpy` are not implemented. Using `pass` can lead to unexpected behavior, as the function might fall through and attempt to execute code outside the `if/elif` chain.\n\nTo make the code more robust and prevent runtime errors, I suggest explicitly raising a `NotImplementedError` for backends that are not yet supported.\n\n```python\n    elif backend_name == \"tensorflow\":\n        raise NotImplementedError(\"TensorFlow distributed backend is not yet implemented.\")\n    elif backend_name == \"torch\":\n        raise NotImplementedError(\"Torch distributed backend is not yet implemented.\")\n    elif backend_name == \"numpy\":\n        raise NotImplementedError(\"NumPy distributed backend is not yet implemented.\")\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "keras/src/backend/distributed/factory.py",
    "pr_number": 21699,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2388310330,
    "comment_created_at": "2025-09-29T15:01:40Z"
  },
  {
    "code": "@@ -164,13 +164,14 @@ def celu(x, alpha=1.0):\n \n def glu(x, axis=-1):\n     x = convert_to_tensor(x)\n+    dtype = x.dtype\n     if x.shape[axis] % 2 != 0:\n         raise ValueError(\n             \"axis size must be divisible by 2. \"\n             f\"Received: x.shape={x.shape} with axis={axis}\"\n         )\n     x1, x2 = np.split(x, 2, axis)\n-    return x1 * (1 / (1 + np.exp(-x2)))\n+    return (x1 * (1 / (1 + np.exp(-x2)))).astype(dtype)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nfor better code reuse and maintainability, consider using the sigmoid function that is already defined in this file. the expression 1 / (1 + np.exp(-x2)) is an implementation of the sigmoid function. using the existing sigmoid function would make the code more readable and consistent with other backend implementations of glu.\n\n",
    "line_number": 174,
    "enriched": "File: keras/src/backend/numpy/nn.py\nCode: @@ -164,13 +164,14 @@ def celu(x, alpha=1.0):\n \n def glu(x, axis=-1):\n     x = convert_to_tensor(x)\n+    dtype = x.dtype\n     if x.shape[axis] % 2 != 0:\n         raise ValueError(\n             \"axis size must be divisible by 2. \"\n             f\"Received: x.shape={x.shape} with axis={axis}\"\n         )\n     x1, x2 = np.split(x, 2, axis)\n-    return x1 * (1 / (1 + np.exp(-x2)))\n+    return (x1 * (1 / (1 + np.exp(-x2)))).astype(dtype)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nFor better code reuse and maintainability, consider using the `sigmoid` function that is already defined in this file. The expression `1 / (1 + np.exp(-x2))` is an implementation of the sigmoid function. Using the existing `sigmoid` function would make the code more readable and consistent with other backend implementations of `glu`.\n\n```suggestion\n    return (x1 * sigmoid(x2)).astype(dtype)\n```",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "keras/src/backend/numpy/nn.py",
    "pr_number": 21696,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2379343972,
    "comment_created_at": "2025-09-25T14:23:08Z"
  },
  {
    "code": "@@ -316,9 +324,10 @@ def _build_metrics_set(\n         return flat_metrics\n \n     def _flatten_y(self, y):\n-        if isinstance(y, dict) and self.output_names:\n+        names = getattr(self, \"_resolved_output_names\", self.output_names)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe use of getattr with a default value here can be a bit confusing, as it suggests a fallback path that doesn't appear to be exercised in the current implementation. the _flatten_y method is consistently called after build() has been executed (either from build() itself or from update_state()), ensuring self._resolved_output_names is always set.\n\na direct assignment, names = self._resolved_output_names, would make the dependency on the build step more explicit. this would lead to a clear attributeerror if _flatten_y were ever called prematurely, which is preferable to silently falling back to a potentially incorrect value. this change would improve code clarity and maintainability by simplifying the mental model of the execution flow.[^1]\n\n\n\n#### style guide references\n[^1]: this suggestion aligns with the principle of designing simple and consistent workflows that reflect simple and consistent mental models. by removing the implicit fallback, the code becomes more straightforward and easier to reason about.",
    "line_number": 327,
    "enriched": "File: keras/src/trainers/compile_utils.py\nCode: @@ -316,9 +324,10 @@ def _build_metrics_set(\n         return flat_metrics\n \n     def _flatten_y(self, y):\n-        if isinstance(y, dict) and self.output_names:\n+        names = getattr(self, \"_resolved_output_names\", self.output_names)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe use of `getattr` with a default value here can be a bit confusing, as it suggests a fallback path that doesn't appear to be exercised in the current implementation. The `_flatten_y` method is consistently called after `build()` has been executed (either from `build()` itself or from `update_state()`), ensuring `self._resolved_output_names` is always set.\n\nA direct assignment, `names = self._resolved_output_names`, would make the dependency on the `build` step more explicit. This would lead to a clear `AttributeError` if `_flatten_y` were ever called prematurely, which is preferable to silently falling back to a potentially incorrect value. This change would improve code clarity and maintainability by simplifying the mental model of the execution flow.[^1]\n\n```suggestion\n        names = self._resolved_output_names\n```\n\n#### Style Guide References\n[^1]: This suggestion aligns with the principle of designing simple and consistent workflows that reflect simple and consistent mental models. By removing the implicit fallback, the code becomes more straightforward and easier to reason about.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "keras/src/trainers/compile_utils.py",
    "pr_number": 21694,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2377966776,
    "comment_created_at": "2025-09-25T07:03:40Z"
  },
  {
    "code": "@@ -86,6 +86,27 @@ def test_result_type_with_int64(self, dtype):\n         out = backend.result_type(x1.dtype, x2.dtype)\n         self.assertEqual(out, \"int64\")\n \n+    @parameterized.named_parameters(\n+        named_product(\n+            dtype=[\n+                \"float16\",\n+                \"bfloat16\",\n+                \"float32\",\n+                \"float64\",\n+            ]",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe test for float64 promotion currently only covers promotion with other float types. to make it more comprehensive and ensure correctness for all type interactions, it would be beneficial to also test promotion with integer types, as float64 should be the result type in those cases as well on the tensorflow backend.\n\npython\n            dtype=[\n                \"float16\",\n                \"bfloat16\",\n                \"float32\",\n                \"float64\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"uint8\",\n                \"uint16\",\n            ]\n",
    "line_number": 96,
    "enriched": "File: keras/src/backend/common/dtypes_test.py\nCode: @@ -86,6 +86,27 @@ def test_result_type_with_int64(self, dtype):\n         out = backend.result_type(x1.dtype, x2.dtype)\n         self.assertEqual(out, \"int64\")\n \n+    @parameterized.named_parameters(\n+        named_product(\n+            dtype=[\n+                \"float16\",\n+                \"bfloat16\",\n+                \"float32\",\n+                \"float64\",\n+            ]\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe test for `float64` promotion currently only covers promotion with other float types. To make it more comprehensive and ensure correctness for all type interactions, it would be beneficial to also test promotion with integer types, as `float64` should be the result type in those cases as well on the TensorFlow backend.\n\n```python\n            dtype=[\n                \"float16\",\n                \"bfloat16\",\n                \"float32\",\n                \"float64\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"uint8\",\n                \"uint16\",\n            ]\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/backend/common/dtypes_test.py",
    "pr_number": 21693,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2377672821,
    "comment_created_at": "2025-09-25T04:20:03Z"
  },
  {
    "code": "@@ -862,6 +862,17 @@ def test_legacy_load_own_variables(self):\n             # outputs_grad_amax_history.\n             \"7\": np.random.random((1024,)).astype(\"float32\"),\n         }\n+        gptq_store = {\n+            # bias\n+            \"0\": np.random.random((16,)).astype(\"float32\"),\n+            \"1\": np.random.randint(0, 16, size=(16, 8), dtype=\"uint8\"),\n+            # kernel_scale.\n+            \"2\": np.random.random((16, 1)).astype(\"float32\"),\n+            # kernel_zero\n+            \"3\": np.random.random((16, 1)).astype(\"uint8\"),",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe kernel_zero test data is being generated with np.random.random((16, 1)).astype(\"uint8\"). np.random.random() returns float values in the range [0.0, 1.0), which will all be truncated to 0 when cast to uint8. this means the test is only running with zero values for kernel_zero. to properly test with random unsigned 8-bit integers, you should use np.random.randint().\n\n",
    "line_number": 871,
    "enriched": "File: keras/src/layers/core/dense_test.py\nCode: @@ -862,6 +862,17 @@ def test_legacy_load_own_variables(self):\n             # outputs_grad_amax_history.\n             \"7\": np.random.random((1024,)).astype(\"float32\"),\n         }\n+        gptq_store = {\n+            # bias\n+            \"0\": np.random.random((16,)).astype(\"float32\"),\n+            \"1\": np.random.randint(0, 16, size=(16, 8), dtype=\"uint8\"),\n+            # kernel_scale.\n+            \"2\": np.random.random((16, 1)).astype(\"float32\"),\n+            # kernel_zero\n+            \"3\": np.random.random((16, 1)).astype(\"uint8\"),\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe `kernel_zero` test data is being generated with `np.random.random((16, 1)).astype(\"uint8\")`. `np.random.random()` returns float values in the range `[0.0, 1.0)`, which will all be truncated to `0` when cast to `uint8`. This means the test is only running with zero values for `kernel_zero`. To properly test with random unsigned 8-bit integers, you should use `np.random.randint()`.\n\n```suggestion\n            \"3\": np.random.randint(0, 256, size=(16, 1), dtype=\"uint8\"),\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/layers/core/dense_test.py",
    "pr_number": 21689,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2375157892,
    "comment_created_at": "2025-09-24T09:24:37Z"
  },
  {
    "code": "@@ -308,12 +308,11 @@ def append(x1, x2, axis=None):\n \n def arange(start, stop=None, step=1, dtype=None):\n     if dtype is None:\n-        dtypes_to_resolve = [\n-            getattr(start, \"dtype\", type(start)),\n-            getattr(step, \"dtype\", type(step)),\n-        ]\n+        dtypes_to_resolve = [getattr(start, \"dtype\", type(start))]\n         if stop is not None:\n             dtypes_to_resolve.append(getattr(stop, \"dtype\", type(stop)))\n+        if step is not None:\n+            dtypes_to_resolve.append(getattr(step, \"dtype\", type(step)))\n         dtype = dtypes.result_type(*dtypes_to_resolve)\n     dtype = standardize_dtype(dtype)\n     return jnp.arange(start, stop, step=step, dtype=dtype)",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe signature for arange should be updated to step=none to align with the public api and other backend implementations. currently, it's step=1, which will cause an error when none is passed from the ops layer. additionally, jnp.arange does not accept step=none, so a check is needed to default step to 1 if it is none.\n\n",
    "line_number": 318,
    "enriched": "File: keras/src/backend/jax/numpy.py\nCode: @@ -308,12 +308,11 @@ def append(x1, x2, axis=None):\n \n def arange(start, stop=None, step=1, dtype=None):\n     if dtype is None:\n-        dtypes_to_resolve = [\n-            getattr(start, \"dtype\", type(start)),\n-            getattr(step, \"dtype\", type(step)),\n-        ]\n+        dtypes_to_resolve = [getattr(start, \"dtype\", type(start))]\n         if stop is not None:\n             dtypes_to_resolve.append(getattr(stop, \"dtype\", type(stop)))\n+        if step is not None:\n+            dtypes_to_resolve.append(getattr(step, \"dtype\", type(step)))\n         dtype = dtypes.result_type(*dtypes_to_resolve)\n     dtype = standardize_dtype(dtype)\n     return jnp.arange(start, stop, step=step, dtype=dtype)\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe signature for `arange` should be updated to `step=None` to align with the public API and other backend implementations. Currently, it's `step=1`, which will cause an error when `None` is passed from the ops layer. Additionally, `jnp.arange` does not accept `step=None`, so a check is needed to default `step` to 1 if it is `None`.\n\n```suggestion\ndef arange(start, stop=None, step=None, dtype=None):\n    if dtype is None:\n        dtypes_to_resolve = [getattr(start, \"dtype\", type(start))]\n        if stop is not None:\n            dtypes_to_resolve.append(getattr(stop, \"dtype\", type(stop)))\n        if step is not None:\n            dtypes_to_resolve.append(getattr(step, \"dtype\", type(step)))\n        dtype = dtypes.result_type(*dtypes_to_resolve)\n    dtype = standardize_dtype(dtype)\n    if step is None:\n        step = 1\n    return jnp.arange(start, stop, step=step, dtype=dtype)\n\n```",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "keras/src/backend/jax/numpy.py",
    "pr_number": 21688,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2373772353,
    "comment_created_at": "2025-09-24T00:44:09Z"
  },
  {
    "code": "@@ -1593,7 +1593,47 @@ def size(x):\n \n \n def sort(x, axis=-1):\n-    raise NotImplementedError(\"`sort` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_shape = x.get_partial_shape()\n+    rank = x_shape.rank.get_length()\n+    \n+    if rank == 0:\n+        return OpenVINOKerasTensor(x)\n+    \n+    # Handle axis=None by flattening the input\n+    if axis is None:\n+        x = ov_opset.reshape(x, [-1], False).output(0)\n+        axis = 0\n+    # Handle negative axis\n+    elif axis < 0:\n+        axis = rank + axis\n+    \n+    # Get the size of the dimension to sort\n+    shape_tensor = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n+    k = ov_opset.gather(\n+        shape_tensor,\n+        ov_opset.constant([axis], Type.i32).output(0),\n+        ov_opset.constant(0, Type.i32).output(0)\n+    ).output(0)\n+    \n+    # Convert k to a scalar value\n+    k_scalar = ov_opset.squeeze(k, [0]).output(0)\n+    \n+    # Use topk with k=size_of_axis to get all elements sorted\n+    topk_outputs = ov_opset.topk(\n+        x,\n+        k=k_scalar,\n+        axis=axis,\n+        mode=\"min\",\n+        sort=\"value\",\n+        stable=True\n+    )\n+    \n+    # Get the sorted values\n+    sorted_values = topk_outputs.output(0)\n+    \n+    return OpenVINOKerasTensor(sorted_values)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe implementation is correct. however, for consistency with the rest of the file, it's better to explicitly create constants for operator arguments instead of passing python lists directly. this applies to reshape on line 1605 and squeeze on line 1620. [^1]\n\nfor example:\n*   ov_opset.reshape(x, [-1], false) should be ov_opset.reshape(x, ov_opset.constant([-1], type.i32), false)\n*   ov_opset.squeeze(k, [0]) should be ov_opset.squeeze(k, ov_opset.constant([0], type.i32))\n\ni'll provide a suggestion for the whole function to apply these changes.\n\npython\n    x = get_ov_output(x)\n    x_shape = x.get_partial_shape()\n    rank = x_shape.rank.get_length()\n    \n    if rank == 0:\n        return openvinokerastensor(x)\n    \n    # handle axis=none by flattening the input\n    if axis is none:\n        x = ov_opset.reshape(x, ov_opset.constant([-1], type.i32), false).output(0)\n        axis = 0\n    # handle negative axis\n    elif axis < 0:\n        axis = rank + axis\n    \n    # get the size of the dimension to sort\n    shape_tensor = ov_opset.shape_of(x, output_type=type.i32).output(0)\n    k = ov_opset.gather(\n        shape_tensor,\n        ov_opset.constant([axis], type.i32).output(0),\n        ov_opset.constant(0, type.i32).output(0)\n    ).output(0)\n    \n    # convert k to a scalar value\n    k_scalar = ov_opset.squeeze(k, ov_opset.constant([0], type.i32)).output(0)\n    \n    # use topk with k=size_of_axis to get all elements sorted\n    topk_outputs = ov_opset.topk(\n        x,\n        k=k_scalar,\n        axis=axis,\n        mode=\"min\",\n        sort=\"value\",\n        stable=true\n    )\n    \n    # get the sorted values\n    sorted_values = topk_outputs.output(0)\n    \n    return openvinokerastensor(sorted_values)\n\n\n#### style guide references\n[^1]: the style guide emphasizes internal consistency. the suggested change aligns the code with the prevalent pattern of explicitly creating constant tensors for operator arguments within this file, as seen in other functions like mean, ravel, and even within this function for the gather op. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#l71-l72)",
    "line_number": 1635,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -1593,7 +1593,47 @@ def size(x):\n \n \n def sort(x, axis=-1):\n-    raise NotImplementedError(\"`sort` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_shape = x.get_partial_shape()\n+    rank = x_shape.rank.get_length()\n+    \n+    if rank == 0:\n+        return OpenVINOKerasTensor(x)\n+    \n+    # Handle axis=None by flattening the input\n+    if axis is None:\n+        x = ov_opset.reshape(x, [-1], False).output(0)\n+        axis = 0\n+    # Handle negative axis\n+    elif axis < 0:\n+        axis = rank + axis\n+    \n+    # Get the size of the dimension to sort\n+    shape_tensor = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n+    k = ov_opset.gather(\n+        shape_tensor,\n+        ov_opset.constant([axis], Type.i32).output(0),\n+        ov_opset.constant(0, Type.i32).output(0)\n+    ).output(0)\n+    \n+    # Convert k to a scalar value\n+    k_scalar = ov_opset.squeeze(k, [0]).output(0)\n+    \n+    # Use topk with k=size_of_axis to get all elements sorted\n+    topk_outputs = ov_opset.topk(\n+        x,\n+        k=k_scalar,\n+        axis=axis,\n+        mode=\"min\",\n+        sort=\"value\",\n+        stable=True\n+    )\n+    \n+    # Get the sorted values\n+    sorted_values = topk_outputs.output(0)\n+    \n+    return OpenVINOKerasTensor(sorted_values)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe implementation is correct. However, for consistency with the rest of the file, it's better to explicitly create constants for operator arguments instead of passing Python lists directly. This applies to `reshape` on line 1605 and `squeeze` on line 1620. [^1]\n\nFor example:\n*   `ov_opset.reshape(x, [-1], False)` should be `ov_opset.reshape(x, ov_opset.constant([-1], Type.i32), False)`\n*   `ov_opset.squeeze(k, [0])` should be `ov_opset.squeeze(k, ov_opset.constant([0], Type.i32))`\n\nI'll provide a suggestion for the whole function to apply these changes.\n\n```python\n    x = get_ov_output(x)\n    x_shape = x.get_partial_shape()\n    rank = x_shape.rank.get_length()\n    \n    if rank == 0:\n        return OpenVINOKerasTensor(x)\n    \n    # Handle axis=None by flattening the input\n    if axis is None:\n        x = ov_opset.reshape(x, ov_opset.constant([-1], Type.i32), False).output(0)\n        axis = 0\n    # Handle negative axis\n    elif axis < 0:\n        axis = rank + axis\n    \n    # Get the size of the dimension to sort\n    shape_tensor = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n    k = ov_opset.gather(\n        shape_tensor,\n        ov_opset.constant([axis], Type.i32).output(0),\n        ov_opset.constant(0, Type.i32).output(0)\n    ).output(0)\n    \n    # Convert k to a scalar value\n    k_scalar = ov_opset.squeeze(k, ov_opset.constant([0], Type.i32)).output(0)\n    \n    # Use topk with k=size_of_axis to get all elements sorted\n    topk_outputs = ov_opset.topk(\n        x,\n        k=k_scalar,\n        axis=axis,\n        mode=\"min\",\n        sort=\"value\",\n        stable=True\n    )\n    \n    # Get the sorted values\n    sorted_values = topk_outputs.output(0)\n    \n    return OpenVINOKerasTensor(sorted_values)\n```\n\n#### Style Guide References\n[^1]: The style guide emphasizes internal consistency. The suggested change aligns the code with the prevalent pattern of explicitly creating constant tensors for operator arguments within this file, as seen in other functions like `mean`, `ravel`, and even within this function for the `gather` op. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#L71-L72)",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21687,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2372873570,
    "comment_created_at": "2025-09-23T16:28:43Z"
  },
  {
    "code": "@@ -916,3 +953,44 @@ def _get_gptq_group_size(self, config):\n                 \"either through a `dtype_policy` of type \"\n                 \"`GPTQDTypePolicy` or the `config` argument.\"\n             )\n+\n+    def _get_gptq_weight_bits(self, config):",
    "comment": "should we move this function to quantizers.gptq or another location? it might be needed by any layers that support gptq.",
    "line_number": 956,
    "enriched": "File: keras/src/layers/core/dense.py\nCode: @@ -916,3 +953,44 @@ def _get_gptq_group_size(self, config):\n                 \"either through a `dtype_policy` of type \"\n                 \"`GPTQDTypePolicy` or the `config` argument.\"\n             )\n+\n+    def _get_gptq_weight_bits(self, config):\nComment: Should we move this function to `quantizers.gptq` or another location? It might be needed by any layers that support GPTQ.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "keras/src/layers/core/dense.py",
    "pr_number": 21686,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2373901496,
    "comment_created_at": "2025-09-24T02:43:40Z"
  },
  {
    "code": "@@ -228,3 +228,25 @@ def unstack(x, axis=0):\n         lax.index_in_dim(x, i, axis, keepdims=False)\n         for i in range(x.shape[axis])\n     ]\n+\n+\n+def unfold(input, kernel_size, dilation=1, padding=0, stride=1):\n+    filter_shape = (kernel_size, kernel_size)\n+\n+    window_strides = (stride, stride)\n+    lhs_dilation = (dilation, dilation)\n+\n+    if isinstance(padding, int):\n+        padding = ((padding, padding), (padding, padding))\n+    else:\n+        padding = ((padding[0], padding[0]), (padding[1], padding[1]))\n+\n+    patches = lax.conv_general_dilated_patches(\n+        lhs=input,\n+        filter_shape=filter_shape,\n+        window_strides=window_strides,\n+        padding=padding,\n+        lhs_dilation=lhs_dilation,\n+    )\n+\n+    return patches",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe current implementation assumes that kernel_size, dilation, and stride are integers. however, the api contract allows them to be tuples of two integers as well. this will lead to incorrect behavior when tuples are passed. please add a helper function to handle both integer and tuple inputs, similar to the tensorflow backend implementation.\n\npython\ndef unfold(input, kernel_size, dilation=1, padding=0, stride=1):\n    def _pair(x):\n        return (x, x) if isinstance(x, int) else x\n\n    filter_shape = _pair(kernel_size)\n    window_strides = _pair(stride)\n    lhs_dilation = _pair(dilation)\n\n    if isinstance(padding, int):\n        padding = ((padding, padding), (padding, padding))\n    else:\n        padding = ((padding[0], padding[0]), (padding[1], padding[1]))\n\n    patches = lax.conv_general_dilated_patches(\n        lhs=input,\n        filter_shape=filter_shape,\n        window_strides=window_strides,\n        padding=padding,\n        lhs_dilation=lhs_dilation,\n    )\n\n    return patches\n",
    "line_number": 252,
    "enriched": "File: keras/src/backend/jax/rnn.py\nCode: @@ -228,3 +228,25 @@ def unstack(x, axis=0):\n         lax.index_in_dim(x, i, axis, keepdims=False)\n         for i in range(x.shape[axis])\n     ]\n+\n+\n+def unfold(input, kernel_size, dilation=1, padding=0, stride=1):\n+    filter_shape = (kernel_size, kernel_size)\n+\n+    window_strides = (stride, stride)\n+    lhs_dilation = (dilation, dilation)\n+\n+    if isinstance(padding, int):\n+        padding = ((padding, padding), (padding, padding))\n+    else:\n+        padding = ((padding[0], padding[0]), (padding[1], padding[1]))\n+\n+    patches = lax.conv_general_dilated_patches(\n+        lhs=input,\n+        filter_shape=filter_shape,\n+        window_strides=window_strides,\n+        padding=padding,\n+        lhs_dilation=lhs_dilation,\n+    )\n+\n+    return patches\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe current implementation assumes that `kernel_size`, `dilation`, and `stride` are integers. However, the API contract allows them to be tuples of two integers as well. This will lead to incorrect behavior when tuples are passed. Please add a helper function to handle both integer and tuple inputs, similar to the TensorFlow backend implementation.\n\n```python\ndef unfold(input, kernel_size, dilation=1, padding=0, stride=1):\n    def _pair(x):\n        return (x, x) if isinstance(x, int) else x\n\n    filter_shape = _pair(kernel_size)\n    window_strides = _pair(stride)\n    lhs_dilation = _pair(dilation)\n\n    if isinstance(padding, int):\n        padding = ((padding, padding), (padding, padding))\n    else:\n        padding = ((padding[0], padding[0]), (padding[1], padding[1]))\n\n    patches = lax.conv_general_dilated_patches(\n        lhs=input,\n        filter_shape=filter_shape,\n        window_strides=window_strides,\n        padding=padding,\n        lhs_dilation=lhs_dilation,\n    )\n\n    return patches\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "keras/src/backend/jax/rnn.py",
    "pr_number": 21685,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2368908088,
    "comment_created_at": "2025-09-22T15:07:24Z"
  },
  {
    "code": "@@ -1036,3 +1037,16 @@ def test_gptq_serialization(self):\n         new_layer = layers.EinsumDense.from_config(config)\n         new_layer.build((None, 3))\n         self.assertEqual(new_layer.quantization_mode, \"gptq\")\n+\n+    def test_int4_kernel_returns_unpacked_form(self):\n+        \"\"\"Test that the `kernel` property returns the unpacked int4 kernel.\"\"\"\n+        layer = layers.EinsumDense(\n+            equation=\"ab,bc->ac\",\n+            output_shape=(2,),\n+        )\n+        layer.build((None, 2))\n+        layer.quantize(\"int4\")\n+        packed_kernel = layer._kernel\n+        self.assertAllClose(\n+            layer.kernel, quantizers.unpack_int4(packed_kernel, orig_len=2)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nfor consistency with the test in dense_test.py, it's better to call unpack_int4 with positional arguments instead of keyword arguments.[^1]\n\nwhile using a keyword argument here is functionally correct, using positional arguments consistently across the test suite improves readability and maintainability.\n\n\n\n#### style guide references\n[^1]:",
    "line_number": 1051,
    "enriched": "File: keras/src/layers/core/einsum_dense_test.py\nCode: @@ -1036,3 +1037,16 @@ def test_gptq_serialization(self):\n         new_layer = layers.EinsumDense.from_config(config)\n         new_layer.build((None, 3))\n         self.assertEqual(new_layer.quantization_mode, \"gptq\")\n+\n+    def test_int4_kernel_returns_unpacked_form(self):\n+        \"\"\"Test that the `kernel` property returns the unpacked int4 kernel.\"\"\"\n+        layer = layers.EinsumDense(\n+            equation=\"ab,bc->ac\",\n+            output_shape=(2,),\n+        )\n+        layer.build((None, 2))\n+        layer.quantize(\"int4\")\n+        packed_kernel = layer._kernel\n+        self.assertAllClose(\n+            layer.kernel, quantizers.unpack_int4(packed_kernel, orig_len=2)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nFor consistency with the test in `dense_test.py`, it's better to call `unpack_int4` with positional arguments instead of keyword arguments.[^1]\n\nWhile using a keyword argument here is functionally correct, using positional arguments consistently across the test suite improves readability and maintainability.\n\n```suggestion\n            layer.kernel, quantizers.unpack_int4(packed_kernel, 2)\n```\n\n#### Style Guide References\n[^1]: ",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/layers/core/einsum_dense_test.py",
    "pr_number": 21684,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2365741747,
    "comment_created_at": "2025-09-20T17:56:16Z"
  },
  {
    "code": "@@ -382,6 +350,27 @@ def _check_load_own_variables(self, store):\n                 f\"Expected: {[v.name for v in all_vars]}\"\n             )\n \n+    # Per-mode variable spec.\n+    MODE_SPEC = {",
    "comment": "i suggest renaming mode_spec to quantization_variable_spec. the current name is now somewhat ambiguous because the spec is stored as a class attribute. additionally, given that this value is a constant, it might be better to define it as a private constant outside the class rather than keeping it at the class level. wdyt?",
    "line_number": 354,
    "enriched": "File: keras/src/layers/core/dense.py\nCode: @@ -382,6 +350,27 @@ def _check_load_own_variables(self, store):\n                 f\"Expected: {[v.name for v in all_vars]}\"\n             )\n \n+    # Per-mode variable spec.\n+    MODE_SPEC = {\nComment: I suggest renaming `MODE_SPEC` to `QUANTIZATION_VARIABLE_SPEC`. The current name is now somewhat ambiguous because the spec is stored as a class attribute. Additionally, given that this value is a constant, it might be better to define it as a private constant outside the class rather than keeping it at the class level. WDYT?",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "keras/src/layers/core/dense.py",
    "pr_number": 21681,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2366752821,
    "comment_created_at": "2025-09-22T05:23:38Z"
  },
  {
    "code": "@@ -328,6 +328,10 @@ def test_variable_path_creation(self):\n         v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n         self.assertEqual(v.path, \"test_var\")\n \n+        with backend.name_scope(\"test_scope\"):\n+            v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n+            self.assertEqual(v.path, \"test_scope/test_var\")",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis is a good test case for a single level of name scope. to make the test more robust, consider adding a case for nested name scopes to ensure that path construction works correctly with multiple levels of hierarchy.\n\npython\n        with backend.name_scope(\"test_scope\"):\n            v = backend.variable(initializer=np.ones((2, 2)), name=\"test_var\")\n            self.assertequal(v.path, \"test_scope/test_var\")\n\n        with backend.name_scope(\"outer\"):\n            with backend.name_scope(\"inner\"):\n                v = backend.variable(initializer=np.ones((2, 2)), name=\"test_var\")\n                self.assertequal(v.path, \"outer/inner/test_var\")\n",
    "line_number": 333,
    "enriched": "File: keras/src/backend/common/variables_test.py\nCode: @@ -328,6 +328,10 @@ def test_variable_path_creation(self):\n         v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n         self.assertEqual(v.path, \"test_var\")\n \n+        with backend.name_scope(\"test_scope\"):\n+            v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n+            self.assertEqual(v.path, \"test_scope/test_var\")\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis is a good test case for a single level of name scope. To make the test more robust, consider adding a case for nested name scopes to ensure that path construction works correctly with multiple levels of hierarchy.\n\n```python\n        with backend.name_scope(\"test_scope\"):\n            v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n            self.assertEqual(v.path, \"test_scope/test_var\")\n\n        with backend.name_scope(\"outer\"):\n            with backend.name_scope(\"inner\"):\n                v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n                self.assertEqual(v.path, \"outer/inner/test_var\")\n```",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "keras/src/backend/common/variables_test.py",
    "pr_number": 21680,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2360515054,
    "comment_created_at": "2025-09-18T17:50:53Z"
  },
  {
    "code": "@@ -55,6 +60,25 @@ def test_result_type_with_tensor(self, dtype1, dtype2):\n         expected = jnp.result_type(x1_jax, x2_jax).name\n         self.assertEqual(out, expected)\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\", reason=\"TensorFlow only\"\n+    )\n+    def test_result_type_with_int64(self):\n+        # https://github.com/keras-team/keras/issues/21677\n+        x1 = ops.ones((1,), dtype=\"int64\")\n+        for dtype in (\n+            \"int8\",\n+            \"int16\",\n+            \"int32\",\n+            \"int64\",\n+            \"uint8\",\n+            \"uint16\",\n+            \"uint32\",\n+        ):\n+            x2 = ops.ones((1,), dtype=dtype)\n+            out = backend.result_type(x1.dtype, x2.dtype)\n+            self.assertEqual(out, \"int64\")\n+",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nfor better readability and consistency with other tests in this file, you could use @parameterized.named_parameters to test against different dtypes instead of a for loop. this makes each test case more explicit and easier to debug if one of them fails.[^1]\n\npython\n    @pytest.mark.skipif(\n        backend.backend() != \"tensorflow\", reason=\"tensorflow only\"\n    )\n    @parameterized.named_parameters(\n        named_product(\n            dtype=[\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n            ]\n        )\n    )\n    def test_result_type_with_int64(self, dtype):\n        # https://github.com/keras-team/keras/issues/21677\n        x1 = ops.ones((1,), dtype=\"int64\")\n        x2 = ops.ones((1,), dtype=dtype)\n        out = backend.result_type(x1.dtype, x2.dtype)\n        self.assertequal(out, \"int64\")\n\n\n#### style guide references\n[^1]: the style guide emphasizes consistency. using @parameterized.named_parameters would align this test with the established pattern in this file, improving consistency. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#l71-l72)",
    "line_number": 81,
    "enriched": "File: keras/src/backend/common/dtypes_test.py\nCode: @@ -55,6 +60,25 @@ def test_result_type_with_tensor(self, dtype1, dtype2):\n         expected = jnp.result_type(x1_jax, x2_jax).name\n         self.assertEqual(out, expected)\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\", reason=\"TensorFlow only\"\n+    )\n+    def test_result_type_with_int64(self):\n+        # https://github.com/keras-team/keras/issues/21677\n+        x1 = ops.ones((1,), dtype=\"int64\")\n+        for dtype in (\n+            \"int8\",\n+            \"int16\",\n+            \"int32\",\n+            \"int64\",\n+            \"uint8\",\n+            \"uint16\",\n+            \"uint32\",\n+        ):\n+            x2 = ops.ones((1,), dtype=dtype)\n+            out = backend.result_type(x1.dtype, x2.dtype)\n+            self.assertEqual(out, \"int64\")\n+\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nFor better readability and consistency with other tests in this file, you could use `@parameterized.named_parameters` to test against different dtypes instead of a `for` loop. This makes each test case more explicit and easier to debug if one of them fails.[^1]\n\n```python\n    @pytest.mark.skipif(\n        backend.backend() != \"tensorflow\", reason=\"TensorFlow only\"\n    )\n    @parameterized.named_parameters(\n        named_product(\n            dtype=[\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n            ]\n        )\n    )\n    def test_result_type_with_int64(self, dtype):\n        # https://github.com/keras-team/keras/issues/21677\n        x1 = ops.ones((1,), dtype=\"int64\")\n        x2 = ops.ones((1,), dtype=dtype)\n        out = backend.result_type(x1.dtype, x2.dtype)\n        self.assertEqual(out, \"int64\")\n```\n\n#### Style Guide References\n[^1]: The style guide emphasizes consistency. Using `@parameterized.named_parameters` would align this test with the established pattern in this file, improving consistency. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#L71-L72)",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/backend/common/dtypes_test.py",
    "pr_number": 21679,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2358969962,
    "comment_created_at": "2025-09-18T12:24:40Z"
  },
  {
    "code": "@@ -22,21 +22,14 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n-    ov_type = OPENVINO_DTYPES[dtype]\n-    seed = draw_seed(seed)\n-    if isinstance(seed, OpenVINOKerasTensor):\n-        seed1, seed2 = convert_to_numpy(seed)\n+    seed_val = draw_seed(seed)\n+    if isinstance(seed_val, OpenVINOKerasTensor):\n+        seed_data = convert_to_numpy(seed_val)\n     else:\n-        seed1, seed2 = draw_seed(seed).data\n-    minval_const = ov_opset.constant(minval, dtype=dtype)\n-    maxval_const = ov_opset.constant(maxval, dtype=dtype)\n-    if isinstance(shape, tuple):\n-        shape = list(shape)\n-    output_shape_const = ov_opset.constant(shape, dtype=Type.i32)\n-    random_uniform = ov_opset.random_uniform(\n-        output_shape_const, minval_const, maxval_const, ov_type, seed1, seed2\n-    )\n-    return OpenVINOKerasTensor(random_uniform.output(0))\n+        seed_data = seed_val.data\n+    rng = np.random.default_rng(seed_data)\n+    random_values = rng.uniform(minval, maxval, size=shape).astype(dtype)\n+    return OpenVINOKerasTensor(ov_opset.constant(random_values).output(0))",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthis change fixes the non-deterministic output of ov_opset.random_uniform by using numpy.random.uniform instead. however, this introduces a critical regression: it breaks support for symbolic shapes.\n\nthe previous implementation could handle shapes with none dimensions (e.g., (none, 32)) because it treated shape as a symbolic input to the openvino graph. the new implementation uses rng.uniform(..., size=shape), which will fail if shape contains none values, as numpy requires concrete shapes for array creation. this will break many common use cases, such as using random_uniform as an initializer in a layer that accepts inputs with a dynamic batch size.\n\nto address this while still providing a workaround for the openvino bug, i suggest a hybrid approach that checks if the shape is symbolic and uses the appropriate implementation.\n\nhere is a suggested implementation to replace the current uniform function. you will also need to add import warnings at the top of the file.\n\npython\ndef uniform(shape, minval=0.0, maxval=1.0, dtype=none, seed=none):\n    dtype = dtype or floatx()\n    if any(s is none for s in shape):\n        warnings.warn(\n            \"keras.random.uniform with symbolic shapes may be non-deterministic \"\n            \"in the openvino backend due to an upstream bug. see \"\n            \"https://github.com/openvinotoolkit/openvino/issues/32045 for \"\n            \"details.\"\n        )\n        ov_type = openvino_dtypes[dtype]\n        seed_val = draw_seed(seed)\n        seed1, seed2 = convert_to_numpy(seed_val)\n        minval_const = ov_opset.constant(minval, dtype=dtype)\n        maxval_const = ov_opset.constant(maxval, dtype=dtype)\n        shape_list = [-1 if s is none else s for s in shape]\n        output_shape_const = ov_opset.constant(shape_list, dtype=type.i32)\n        random_uniform = ov_opset.random_uniform(\n            output_shape_const, minval_const, maxval_const, ov_type, seed1, seed2\n        )\n        return openvinokerastensor(random_uniform.output(0))\n\n    seed_val = draw_seed(seed)\n    seed_data = convert_to_numpy(seed_val)\n    rng = np.random.default_rng(seed_data)\n    random_values = rng.uniform(minval, maxval, size=shape).astype(dtype)\n    return openvinokerastensor(ov_opset.constant(random_values).output(0))\n",
    "line_number": 32,
    "enriched": "File: keras/src/backend/openvino/random.py\nCode: @@ -22,21 +22,14 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n-    ov_type = OPENVINO_DTYPES[dtype]\n-    seed = draw_seed(seed)\n-    if isinstance(seed, OpenVINOKerasTensor):\n-        seed1, seed2 = convert_to_numpy(seed)\n+    seed_val = draw_seed(seed)\n+    if isinstance(seed_val, OpenVINOKerasTensor):\n+        seed_data = convert_to_numpy(seed_val)\n     else:\n-        seed1, seed2 = draw_seed(seed).data\n-    minval_const = ov_opset.constant(minval, dtype=dtype)\n-    maxval_const = ov_opset.constant(maxval, dtype=dtype)\n-    if isinstance(shape, tuple):\n-        shape = list(shape)\n-    output_shape_const = ov_opset.constant(shape, dtype=Type.i32)\n-    random_uniform = ov_opset.random_uniform(\n-        output_shape_const, minval_const, maxval_const, ov_type, seed1, seed2\n-    )\n-    return OpenVINOKerasTensor(random_uniform.output(0))\n+        seed_data = seed_val.data\n+    rng = np.random.default_rng(seed_data)\n+    random_values = rng.uniform(minval, maxval, size=shape).astype(dtype)\n+    return OpenVINOKerasTensor(ov_opset.constant(random_values).output(0))\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThis change fixes the non-deterministic output of `ov_opset.random_uniform` by using `numpy.random.uniform` instead. However, this introduces a critical regression: it breaks support for symbolic shapes.\n\nThe previous implementation could handle shapes with `None` dimensions (e.g., `(None, 32)`) because it treated `shape` as a symbolic input to the OpenVINO graph. The new implementation uses `rng.uniform(..., size=shape)`, which will fail if `shape` contains `None` values, as NumPy requires concrete shapes for array creation. This will break many common use cases, such as using `random_uniform` as an initializer in a layer that accepts inputs with a dynamic batch size.\n\nTo address this while still providing a workaround for the OpenVINO bug, I suggest a hybrid approach that checks if the `shape` is symbolic and uses the appropriate implementation.\n\nHere is a suggested implementation to replace the current `uniform` function. You will also need to add `import warnings` at the top of the file.\n\n```python\ndef uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    dtype = dtype or floatx()\n    if any(s is None for s in shape):\n        warnings.warn(\n            \"keras.random.uniform with symbolic shapes may be non-deterministic \"\n            \"in the OpenVINO backend due to an upstream bug. See \"\n            \"https://github.com/openvinotoolkit/openvino/issues/32045 for \"\n            \"details.\"\n        )\n        ov_type = OPENVINO_DTYPES[dtype]\n        seed_val = draw_seed(seed)\n        seed1, seed2 = convert_to_numpy(seed_val)\n        minval_const = ov_opset.constant(minval, dtype=dtype)\n        maxval_const = ov_opset.constant(maxval, dtype=dtype)\n        shape_list = [-1 if s is None else s for s in shape]\n        output_shape_const = ov_opset.constant(shape_list, dtype=Type.i32)\n        random_uniform = ov_opset.random_uniform(\n            output_shape_const, minval_const, maxval_const, ov_type, seed1, seed2\n        )\n        return OpenVINOKerasTensor(random_uniform.output(0))\n\n    seed_val = draw_seed(seed)\n    seed_data = convert_to_numpy(seed_val)\n    rng = np.random.default_rng(seed_data)\n    random_values = rng.uniform(minval, maxval, size=shape).astype(dtype)\n    return OpenVINOKerasTensor(ov_opset.constant(random_values).output(0))\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/random.py",
    "pr_number": 21670,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2353168494,
    "comment_created_at": "2025-09-16T17:21:24Z"
  },
  {
    "code": "@@ -1149,7 +1149,115 @@ def maximum(x1, x2):\n \n \n def median(x, axis=None, keepdims=False):\n-    raise NotImplementedError(\"`median` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_shape = x.get_partial_shape()\n+    rank = x_shape.rank.get_length()\n+    \n+    if rank == 0:\n+        return OpenVINOKerasTensor(x)\n+    \n+    # Handle axis=None by flattening the input\n+    original_shape = None\n+    flattened_all = False\n+    if axis is None:\n+        original_shape = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n+        x = ov_opset.reshape(x, [-1], False).output(0)\n+        axis = 0\n+        original_rank = rank\n+        rank = 1\n+        flattened_all = True\n+    else:\n+        # Handle tuple axis - for median, we only support single axis\n+        if isinstance(axis, (tuple, list)):\n+            if len(axis) != 1:\n+                raise ValueError(\"median only supports single axis reduction\")\n+            axis = axis[0]\n+        \n+        # Handle negative axis\n+        if axis < 0:\n+            axis = rank + axis\n+        original_rank = rank\n+    \n+    # Get the size of the dimension to sort\n+    shape_tensor = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n+    k = ov_opset.gather(\n+        shape_tensor,\n+        ov_opset.constant([axis], Type.i32).output(0),\n+        ov_opset.constant(0, Type.i32).output(0)\n+    ).output(0)\n+    \n+    # Convert k to a scalar value\n+    k_scalar = ov_opset.squeeze(k, [0]).output(0)\n+    \n+    # Use topk with k=size_of_axis to get all elements sorted\n+    topk_outputs = ov_opset.topk(\n+        x,\n+        k=k_scalar,\n+        axis=axis,\n+        mode=\"min\",\n+        sort=\"value\",\n+        stable=True\n+    )",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthis implementation does not handle cases where the reduction axis has a size of 0 (i.e., k=0). in such cases, numpy.median typically returns nan and raises a runtimewarning. the current logic will likely fail during graph execution because even_idx1 would be -1, leading to an attempt to gather from an empty tensor with a negative index.\n\nplease add logic to handle the k=0 case to align with numpy's behavior and prevent potential runtime errors. this could involve checking if k_scalar is zero and, if so, constructing a tensor of nans with the appropriate output shape.",
    "line_number": 1200,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -1149,7 +1149,115 @@ def maximum(x1, x2):\n \n \n def median(x, axis=None, keepdims=False):\n-    raise NotImplementedError(\"`median` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_shape = x.get_partial_shape()\n+    rank = x_shape.rank.get_length()\n+    \n+    if rank == 0:\n+        return OpenVINOKerasTensor(x)\n+    \n+    # Handle axis=None by flattening the input\n+    original_shape = None\n+    flattened_all = False\n+    if axis is None:\n+        original_shape = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n+        x = ov_opset.reshape(x, [-1], False).output(0)\n+        axis = 0\n+        original_rank = rank\n+        rank = 1\n+        flattened_all = True\n+    else:\n+        # Handle tuple axis - for median, we only support single axis\n+        if isinstance(axis, (tuple, list)):\n+            if len(axis) != 1:\n+                raise ValueError(\"median only supports single axis reduction\")\n+            axis = axis[0]\n+        \n+        # Handle negative axis\n+        if axis < 0:\n+            axis = rank + axis\n+        original_rank = rank\n+    \n+    # Get the size of the dimension to sort\n+    shape_tensor = ov_opset.shape_of(x, output_type=Type.i32).output(0)\n+    k = ov_opset.gather(\n+        shape_tensor,\n+        ov_opset.constant([axis], Type.i32).output(0),\n+        ov_opset.constant(0, Type.i32).output(0)\n+    ).output(0)\n+    \n+    # Convert k to a scalar value\n+    k_scalar = ov_opset.squeeze(k, [0]).output(0)\n+    \n+    # Use topk with k=size_of_axis to get all elements sorted\n+    topk_outputs = ov_opset.topk(\n+        x,\n+        k=k_scalar,\n+        axis=axis,\n+        mode=\"min\",\n+        sort=\"value\",\n+        stable=True\n+    )\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThis implementation does not handle cases where the reduction axis has a size of 0 (i.e., `k=0`). In such cases, `numpy.median` typically returns `nan` and raises a `RuntimeWarning`. The current logic will likely fail during graph execution because `even_idx1` would be -1, leading to an attempt to gather from an empty tensor with a negative index.\n\nPlease add logic to handle the `k=0` case to align with NumPy's behavior and prevent potential runtime errors. This could involve checking if `k_scalar` is zero and, if so, constructing a tensor of `NaN`s with the appropriate output shape.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21667,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2347932770,
    "comment_created_at": "2025-09-15T05:46:59Z"
  },
  {
    "code": "@@ -967,9 +967,30 @@ def isnan(x):\n \n \n def isneginf(x):\n-    raise NotImplementedError(\n-        \"`isneginf` is not supported with openvino backend\"\n-    )\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    \n+    if x_type.is_integral() or x_type == Type.boolean:\n+        shape = ov_opset.shape_of(x, \"i32\").output(0)\n+        false_const = ov_opset.constant(False, Type.boolean).output(0)\n+        return OpenVINOKerasTensor(ov_opset.broadcast(false_const, shape).output(0))\n+    \n+    if x_type == Type.bf16:\n+        x_f32 = ov_opset.convert(x, Type.f32).output(0)\n+        neg_inf = ov_opset.constant(-np.inf, Type.f32).output(0)\n+        is_neg_inf = ov_opset.equal(x_f32, neg_inf).output(0)\n+    else:\n+        if x_type == Type.f16:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f16).output(0)\n+        elif x_type == Type.f32:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f32).output(0)\n+        elif x_type == Type.f64:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f64).output(0)\n+        else:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f32).output(0)\n+        is_neg_inf = ov_opset.equal(x, neg_inf).output(0)\n+    \n+    return OpenVINOKerasTensor(is_neg_inf)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe implementation of isneginf can be significantly simplified and made more robust.\n\nfor integer and boolean types, you can use the existing zeros_like helper for better readability. for floating-point types, the openvino is_inf operation can directly detect negative infinity using attributes. this avoids manual comparisons and complex type handling, which also fixes a potential type mismatch bug in the current implementation.\n\nthis approach is cleaner and aligns with other functions in this file like nan_to_num. i've provided a suggested implementation.\n\npython\n    x = get_ov_output(x)\n    x_type = x.get_element_type()\n\n    if x_type.is_integral() or x_type == type.boolean:\n        # integers and booleans cannot be infinity.\n        return zeros_like(x, dtype=\"bool\")\n\n    # for floating point types, use is_inf with attributes.\n    is_neg_inf = ov_opset.is_inf(\n        x, attributes={\"detect_positive\": false, \"detect_negative\": true}\n    ).output(0)\n    return openvinokerastensor(is_neg_inf)\n",
    "line_number": 993,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -967,9 +967,30 @@ def isnan(x):\n \n \n def isneginf(x):\n-    raise NotImplementedError(\n-        \"`isneginf` is not supported with openvino backend\"\n-    )\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    \n+    if x_type.is_integral() or x_type == Type.boolean:\n+        shape = ov_opset.shape_of(x, \"i32\").output(0)\n+        false_const = ov_opset.constant(False, Type.boolean).output(0)\n+        return OpenVINOKerasTensor(ov_opset.broadcast(false_const, shape).output(0))\n+    \n+    if x_type == Type.bf16:\n+        x_f32 = ov_opset.convert(x, Type.f32).output(0)\n+        neg_inf = ov_opset.constant(-np.inf, Type.f32).output(0)\n+        is_neg_inf = ov_opset.equal(x_f32, neg_inf).output(0)\n+    else:\n+        if x_type == Type.f16:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f16).output(0)\n+        elif x_type == Type.f32:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f32).output(0)\n+        elif x_type == Type.f64:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f64).output(0)\n+        else:\n+            neg_inf = ov_opset.constant(-np.inf, Type.f32).output(0)\n+        is_neg_inf = ov_opset.equal(x, neg_inf).output(0)\n+    \n+    return OpenVINOKerasTensor(is_neg_inf)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe implementation of `isneginf` can be significantly simplified and made more robust.\n\nFor integer and boolean types, you can use the existing `zeros_like` helper for better readability. For floating-point types, the OpenVINO `is_inf` operation can directly detect negative infinity using attributes. This avoids manual comparisons and complex type handling, which also fixes a potential type mismatch bug in the current implementation.\n\nThis approach is cleaner and aligns with other functions in this file like `nan_to_num`. I've provided a suggested implementation.\n\n```python\n    x = get_ov_output(x)\n    x_type = x.get_element_type()\n\n    if x_type.is_integral() or x_type == Type.boolean:\n        # Integers and booleans cannot be infinity.\n        return zeros_like(x, dtype=\"bool\")\n\n    # For floating point types, use is_inf with attributes.\n    is_neg_inf = ov_opset.is_inf(\n        x, attributes={\"detect_positive\": False, \"detect_negative\": True}\n    ).output(0)\n    return OpenVINOKerasTensor(is_neg_inf)\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21665,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2347334930,
    "comment_created_at": "2025-09-14T13:35:15Z"
  },
  {
    "code": "@@ -46,6 +46,10 @@ def subtract(x1, x2):\n     x1 = get_ov_output(x1, element_type)\n     x2 = get_ov_output(x2, element_type)\n     x1, x2 = _align_operand_types(x1, x2, \"subtract()\")\n+    if x1.get_element_type() == Type.boolean:\n+        return OpenVINOKerasTensor(\n+            ov_opset.logical_xor(x1, x2).output(0)\n+        )",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthis change correctly handles subtraction for two boolean tensors by using logical_xor. however, it introduces incorrect behavior for mixed-type operations, such as subtracting a numeric scalar from a boolean tensor.\n\nfor example, an operation like subtract(bool_tensor, 5) will be incorrectly computed. the current implementation's preceding boilerplate logic will cast the numeric literal 5 to a boolean (true), and then this new logic will apply logical_xor. the expected behavior, consistent with numpy, is to promote the boolean tensor to a numeric type and perform numeric subtraction, resulting in a numeric tensor.\n\nthis is a critical issue as it leads to incorrect results. the root cause lies in the operand preparation logic (lines 41-47) which is outside the scope of this change. a complete fix would require refactoring that section. given the constraint to only modify the added lines, it's hard to provide a suggestion, but this implementation is incorrect as it stands.",
    "line_number": 52,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -46,6 +46,10 @@ def subtract(x1, x2):\n     x1 = get_ov_output(x1, element_type)\n     x2 = get_ov_output(x2, element_type)\n     x1, x2 = _align_operand_types(x1, x2, \"subtract()\")\n+    if x1.get_element_type() == Type.boolean:\n+        return OpenVINOKerasTensor(\n+            ov_opset.logical_xor(x1, x2).output(0)\n+        )\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThis change correctly handles subtraction for two boolean tensors by using `logical_xor`. However, it introduces incorrect behavior for mixed-type operations, such as subtracting a numeric scalar from a boolean tensor.\n\nFor example, an operation like `subtract(bool_tensor, 5)` will be incorrectly computed. The current implementation's preceding boilerplate logic will cast the numeric literal `5` to a boolean (`True`), and then this new logic will apply `logical_xor`. The expected behavior, consistent with NumPy, is to promote the boolean tensor to a numeric type and perform numeric subtraction, resulting in a numeric tensor.\n\nThis is a critical issue as it leads to incorrect results. The root cause lies in the operand preparation logic (lines 41-47) which is outside the scope of this change. A complete fix would require refactoring that section. Given the constraint to only modify the added lines, it's hard to provide a suggestion, but this implementation is incorrect as it stands.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21664,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2346569383,
    "comment_created_at": "2025-09-13T12:52:19Z"
  },
  {
    "code": "@@ -193,6 +193,27 @@ def tree_unflatten(cls, metadata, children):\n         # For optree / dmtree\n         return cls(children)\n \n+    def torchtree_flatten(self):\n+        # For torchtree\n+        # Returns (values, metadata)\n+        return (self, None)",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe current implementation of torchtree_flatten returns (self, none). when torch.utils._pytree.tree_flatten processes a registered custom node, it expects the flatten function to return its children. by returning self, you are making the trackedlist instance a child of itself, which will lead to infinite recursion when tree_flatten is called. the flatten function should return the list's elements as children.\n\n",
    "line_number": 199,
    "enriched": "File: keras/src/utils/tracking.py\nCode: @@ -193,6 +193,27 @@ def tree_unflatten(cls, metadata, children):\n         # For optree / dmtree\n         return cls(children)\n \n+    def torchtree_flatten(self):\n+        # For torchtree\n+        # Returns (values, metadata)\n+        return (self, None)\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe current implementation of `torchtree_flatten` returns `(self, None)`. When `torch.utils._pytree.tree_flatten` processes a registered custom node, it expects the flatten function to return its children. By returning `self`, you are making the `TrackedList` instance a child of itself, which will lead to infinite recursion when `tree_flatten` is called. The flatten function should return the list's elements as children.\n\n```suggestion\n    def torchtree_flatten(self):\n        # For torchtree\n        # Returns (values, metadata)\n        return list(self), None\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "keras/src/utils/tracking.py",
    "pr_number": 21661,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2352654762,
    "comment_created_at": "2025-09-16T14:11:40Z"
  },
  {
    "code": "@@ -92,11 +92,14 @@ def _parameterize_data(self, data):\n         return parametrize_data\n \n     def _get_compiled_model(self, data):\n-        if (\n-            self.ov_compiled_model is not None\n-            and get_device() == self.ov_device\n-        ):\n-            return self.ov_compiled_model\n+        # OpenVINO compiled model cache is disabled for now.\n+        # For more information, please visit:\n+        # https://github.com/openvinotoolkit/openvino/issues/32045\n+        # if (\n+        #     self.ov_compiled_model is not None\n+        #     and get_device() == self.ov_device\n+        # ):\n+        #     return self.ov_compiled_model",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\ninstead of commenting out the caching logic, consider using a boolean flag to disable it. this approach is cleaner, self-documenting, and makes it trivial to re-enable the caching mechanism once the underlying bug is fixed. it also avoids leaving commented-out 'dead code' in the codebase.\n\n",
    "line_number": 102,
    "enriched": "File: keras/src/backend/openvino/trainer.py\nCode: @@ -92,11 +92,14 @@ def _parameterize_data(self, data):\n         return parametrize_data\n \n     def _get_compiled_model(self, data):\n-        if (\n-            self.ov_compiled_model is not None\n-            and get_device() == self.ov_device\n-        ):\n-            return self.ov_compiled_model\n+        # OpenVINO compiled model cache is disabled for now.\n+        # For more information, please visit:\n+        # https://github.com/openvinotoolkit/openvino/issues/32045\n+        # if (\n+        #     self.ov_compiled_model is not None\n+        #     and get_device() == self.ov_device\n+        # ):\n+        #     return self.ov_compiled_model\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nInstead of commenting out the caching logic, consider using a boolean flag to disable it. This approach is cleaner, self-documenting, and makes it trivial to re-enable the caching mechanism once the underlying bug is fixed. It also avoids leaving commented-out 'dead code' in the codebase.\n\n```suggestion\n        # OpenVINO compiled model cache is disabled for now.\n        # For more information, please visit:\n        # https://github.com/openvinotoolkit/openvino/issues/32045\n        use_cache = False\n        if (\n            use_cache\n            and self.ov_compiled_model is not None\n            and get_device() == self.ov_device\n        ):\n            return self.ov_compiled_model\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/backend/openvino/trainer.py",
    "pr_number": 21659,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2341306299,
    "comment_created_at": "2025-09-11T14:59:44Z"
  },
  {
    "code": "@@ -2403,7 +2403,9 @@ def take(x, indices, axis=None):\n \n     def fix_negative_indices(i):\n         # Correct the indices using \"fill\" mode which is the same as in jax\n-        return tf.where(i < 0, i + tf.cast(tf.shape(x)[axis], i.dtype), i)\n+        return tf.where(\n+            i < 0, tf.cast(i + tf.cast(tf.shape(x)[axis], i.dtype), i.dtype), i\n+        )",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nwhile this fix is correct, the implementation is a bit dense. refactoring it to use an intermediate variable for the corrected indices would make the code more readable and easier to maintain. this also provides a good place to add a comment explaining why this explicit cast is necessary, which is helpful for future contributors.\n\n",
    "line_number": 2408,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -2403,7 +2403,9 @@ def take(x, indices, axis=None):\n \n     def fix_negative_indices(i):\n         # Correct the indices using \"fill\" mode which is the same as in jax\n-        return tf.where(i < 0, i + tf.cast(tf.shape(x)[axis], i.dtype), i)\n+        return tf.where(\n+            i < 0, tf.cast(i + tf.cast(tf.shape(x)[axis], i.dtype), i.dtype), i\n+        )\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nWhile this fix is correct, the implementation is a bit dense. Refactoring it to use an intermediate variable for the corrected indices would make the code more readable and easier to maintain. This also provides a good place to add a comment explaining why this explicit cast is necessary, which is helpful for future contributors.\n\n```suggestion\n        positive_i = tf.cast(i + tf.cast(tf.shape(x)[axis], i.dtype), i.dtype)\n        # Explicitly cast back to original dtype to fix TF casting issue for int64.\n        return tf.where(i < 0, positive_i, i)\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21656,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2337351878,
    "comment_created_at": "2025-09-10T16:57:21Z"
  },
  {
    "code": "@@ -442,6 +442,7 @@ def quantize(self, mode, config=None, **kwargs):\n                     \"`keras.quantizers.GPTQConfig`.\"\n                 )\n             gptq_quantize(self, config)\n+            self._post_quantize(mode, **kwargs)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nas the _post_quantize method does not use the mode and **kwargs parameters, this call should be simplified by removing them. this change depends on the suggested refactoring of the _post_quantize method itself.\n\n",
    "line_number": 445,
    "enriched": "File: keras/src/models/model.py\nCode: @@ -442,6 +442,7 @@ def quantize(self, mode, config=None, **kwargs):\n                     \"`keras.quantizers.GPTQConfig`.\"\n                 )\n             gptq_quantize(self, config)\n+            self._post_quantize(mode, **kwargs)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nAs the `_post_quantize` method does not use the `mode` and `**kwargs` parameters, this call should be simplified by removing them. This change depends on the suggested refactoring of the `_post_quantize` method itself.\n\n```suggestion\n            self._post_quantize()\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/models/model.py",
    "pr_number": 21654,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2336821105,
    "comment_created_at": "2025-09-10T13:44:53Z"
  },
  {
    "code": "@@ -2302,8 +2302,11 @@ def searchsorted(sorted_sequence, values, side=\"left\"):\n             \"to extend it to N-D sequences. Received: \"\n             f\"sorted_sequence.shape={sorted_sequence.shape}\"\n         )\n+    sequence_len = sorted_sequence.shape[0]\n     out_type = (\n-        \"int32\" if len(sorted_sequence) <= np.iinfo(np.int32).max else \"int64\"\n+        \"int32\"\n+        if sequence_len is None or sequence_len <= np.iinfo(np.int32).max\n+        else \"int64\"",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\ndefaulting to int32 when sequence_len is none is risky. if the tensor at runtime has a length greater than int32.max, this could lead to an integer overflow, which can be a silent error. it would be safer to default to int64 when the length is unknown to prevent potential data corruption. this would also align better with the behavior of numpy.searchsorted which typically returns int64 on 64-bit systems.\n\n",
    "line_number": 2309,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -2302,8 +2302,11 @@ def searchsorted(sorted_sequence, values, side=\"left\"):\n             \"to extend it to N-D sequences. Received: \"\n             f\"sorted_sequence.shape={sorted_sequence.shape}\"\n         )\n+    sequence_len = sorted_sequence.shape[0]\n     out_type = (\n-        \"int32\" if len(sorted_sequence) <= np.iinfo(np.int32).max else \"int64\"\n+        \"int32\"\n+        if sequence_len is None or sequence_len <= np.iinfo(np.int32).max\n+        else \"int64\"\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nDefaulting to `int32` when `sequence_len` is `None` is risky. If the tensor at runtime has a length greater than `int32.max`, this could lead to an integer overflow, which can be a silent error. It would be safer to default to `int64` when the length is unknown to prevent potential data corruption. This would also align better with the behavior of `numpy.searchsorted` which typically returns `int64` on 64-bit systems.\n\n```suggestion\n        \"int32\"\n        if sequence_len is not None and sequence_len <= np.iinfo(np.int32).max\n        else \"int64\"\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21652,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2334943095,
    "comment_created_at": "2025-09-09T22:19:55Z"
  },
  {
    "code": "@@ -34,7 +34,8 @@ class Iterator(PyDataset):\n \n     white_list_formats = (\"png\", \"jpg\", \"jpeg\", \"bmp\", \"ppm\", \"tif\", \"tiff\")\n \n-    def __init__(self, n, batch_size, shuffle, seed):\n+    def __init__(self, n, batch_size, shuffle, seed, **kwargs):",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nwhile adding **kwargs and calling super().__init__ is correct, the class docstring for iterator should also be updated to reflect the newly available arguments from pydataset (workers, use_multiprocessing, max_queue_size). this improves api discoverability for users.[^1]\n\n#### style guide references\n[^1]: the style guide emphasizes that documentation is an integral part of the api and key to a good user experience. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#l137-l137)",
    "line_number": 37,
    "enriched": "File: keras/src/legacy/preprocessing/image.py\nCode: @@ -34,7 +34,8 @@ class Iterator(PyDataset):\n \n     white_list_formats = (\"png\", \"jpg\", \"jpeg\", \"bmp\", \"ppm\", \"tif\", \"tiff\")\n \n-    def __init__(self, n, batch_size, shuffle, seed):\n+    def __init__(self, n, batch_size, shuffle, seed, **kwargs):\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nWhile adding `**kwargs` and calling `super().__init__` is correct, the class docstring for `Iterator` should also be updated to reflect the newly available arguments from `PyDataset` (`workers`, `use_multiprocessing`, `max_queue_size`). This improves API discoverability for users.[^1]\n\n#### Style Guide References\n[^1]: The style guide emphasizes that documentation is an integral part of the API and key to a good user experience. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#L137-L137)",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/legacy/preprocessing/image.py",
    "pr_number": 21651,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2334505828,
    "comment_created_at": "2025-09-09T19:01:25Z"
  },
  {
    "code": "@@ -80,6 +80,12 @@ def export_onnx(\n                 \"The model provided has never called. \"\n                 \"It must be called at least once before export.\"\n             )\n+    input_names = []\n+    for i, spec in enumerate(input_signature):\n+        name = getattr(spec, \"name\", None)\n+        if name is None:\n+            name = f\"input_{i}\"\n+        input_names.append(name)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis logic for creating input_names can be made more concise and pythonic by using a list comprehension. this also improves robustness by handling cases where spec.name might be an empty string, not just none.\n\npython\n    input_names = [\n        getattr(spec, \"name\", none) or f\"input_{i}\"\n        for i, spec in enumerate(input_signature)\n    ]\n",
    "line_number": 88,
    "enriched": "File: keras/src/export/onnx.py\nCode: @@ -80,6 +80,12 @@ def export_onnx(\n                 \"The model provided has never called. \"\n                 \"It must be called at least once before export.\"\n             )\n+    input_names = []\n+    for i, spec in enumerate(input_signature):\n+        name = getattr(spec, \"name\", None)\n+        if name is None:\n+            name = f\"input_{i}\"\n+        input_names.append(name)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis logic for creating `input_names` can be made more concise and Pythonic by using a list comprehension. This also improves robustness by handling cases where `spec.name` might be an empty string, not just `None`.\n\n```python\n    input_names = [\n        getattr(spec, \"name\", None) or f\"input_{i}\"\n        for i, spec in enumerate(input_signature)\n    ]\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/export/onnx.py",
    "pr_number": 21646,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2332579763,
    "comment_created_at": "2025-09-09T08:52:06Z"
  },
  {
    "code": "@@ -594,7 +594,7 @@ def test_quantize_gptq_combinations(self, dataset, config):\n         kl = _mean_kl(p_ref, p_q)\n \n         self.assertGreaterEqual(\n-            top1_match, 0.6, f\"Top-1 agreement too low: {top1_match:.3f}\"\n+            top1_match, 0.5, f\"Top-1 agreement too low: {top1_match:.3f}\"",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nfor better readability and maintainability, consider defining 0.5 as a named constant, for example top1_match_threshold. this makes the purpose of the value clearer and easier to adjust in the future if needed. the same could be applied to the kl divergence threshold 0.50 in the next assertion for consistency.",
    "line_number": 597,
    "enriched": "File: keras/src/quantizers/gptq_test.py\nCode: @@ -594,7 +594,7 @@ def test_quantize_gptq_combinations(self, dataset, config):\n         kl = _mean_kl(p_ref, p_q)\n \n         self.assertGreaterEqual(\n-            top1_match, 0.6, f\"Top-1 agreement too low: {top1_match:.3f}\"\n+            top1_match, 0.5, f\"Top-1 agreement too low: {top1_match:.3f}\"\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nFor better readability and maintainability, consider defining `0.5` as a named constant, for example `TOP1_MATCH_THRESHOLD`. This makes the purpose of the value clearer and easier to adjust in the future if needed. The same could be applied to the KL divergence threshold `0.50` in the next assertion for consistency.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/quantizers/gptq_test.py",
    "pr_number": 21645,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2331941610,
    "comment_created_at": "2025-09-09T03:55:25Z"
  },
  {
    "code": "@@ -0,0 +1,1918 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Test suite for model verification with Parallax (Keras AutoShardDistribution),\n+using KerasNLP presets and the Tiny Shakespeare dataset.\n+\n+This script compares the training performance (loss and perplexity) of a\n+baseline model against its Parallax-sharded equivalent. The goal is to verify\n+the correctness and performance of Keras's automatic sharding capabilities.\n+\n+It is generalized to run tests for multiple model architectures, including:\n+- Gemma\n+- GPT-2\n+- Bloom\n+- OPT\n+\"\"\"\n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"\n+os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=2\"\n+\n+# Add these imports at the top of your script\n+import psutil\n+import os\n+\n+# Get the current process to monitor its memory. Place this near the top.\n+PROCESS = psutil.Process(os.getpid())\n+\n+# Replace your ENTIRE old log_memory_usage function with this new one.\n+def log_memory_usage(stage_name):\n+    \"\"\"\n+    Logs the Resident Set Size (RSS) of the current process using psutil.\n+    This is compatible with CPU, GPU, and other backends.\n+    \"\"\"\n+    print(f\"\\n--- Process Memory Usage at: {stage_name} ---\")\n+    mem_info = PROCESS.memory_info()\n+    rss_mb = mem_info.rss / 1024 / 1024  # rss is the Resident Set Size\n+    # print(f\"  Process RSS: {rss_mb:.2f} MB\")\n+    # print(\"-\" * 45)\n+\n+\"\"\"Utilities for distribution strategy with JAX backend.\"\"\"\n+class MergeableGraph:\n+    \"\"\"A graph that supports merging nodes.\"\"\"\n+\n+    def __init__(self):\n+        self._parent = {}\n+        self._edges = set()\n+\n+    def get_root(self, node):\n+        if node not in self._parent:\n+            self._parent[node] = node\n+            return node\n+        if self._parent[node] == node:\n+            return node\n+        self._parent[node] = self.get_root(self._parent[node])\n+        return self._parent[node]\n+\n+    def merge_nodes(self, node1, node2):\n+        root1 = self.get_root(node1)\n+        root2 = self.get_root(node2)\n+        if root1 != root2:\n+            self._parent[root1] = root2\n+\n+    def add_edge(self, node1, node2):\n+        root1 = self.get_root(node1)\n+        root2 = self.get_root(node2)\n+        if root1 != root2:\n+            self._edges.add(tuple(sorted((root1, root2))))\n+\n+    def get_edges(self):\n+        return self._edges\n+    \n+import jax\n+import numpy as np\n+import collections\n+import itertools\n+from keras.src.backend.common import global_state\n+from keras.src.random import seed_generator\n+from keras.src.utils import jax_utils\n+from keras.src.utils import rng_utils\n+from jax import tree_util\n+\n+\n+def list_devices(device_type=None):\n+    \"\"\"Return all the available devices based on the device type.\n+\n+    Note that this should return the global devices in a distributed setting.\n+\n+    Args:\n+        device_type: string of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`. Defaults to `\"gpu\"`\n+            or `\"tpu\"` if available when device_type is not provided. Otherwise\n+            will return the `\"cpu\"` devices.\n+\n+    Return:\n+        List of devices that are available for distribute computation.\n+    \"\"\"\n+    device_type = device_type.lower() if device_type else None\n+    jax_devices = jax.devices(backend=device_type)\n+    return [f\"{device.platform}:{device.id}\" for device in jax_devices]\n+\n+\n+def distribute_variable(value, layout):\n+    \"\"\"Create a distributed variable for JAX.\n+\n+    Since JAX doesn't have a variable class, this will just return a `jax.Array`\n+    with the corresponding layout/sharding specified.\n+\n+    Note that this function should be used in eager context, not in jitted\n+    function.\n+\n+    Args:\n+        value: the initial value of the variable.\n+        layout: `TensorLayout` for the created variable, or a\n+            JAX-supported layout instance (e.g. `jax.sharding.Sharding`).\n+\n+    Returns:\n+        jax.Array which is the distributed variable.\n+    \"\"\"\n+    return distribute_tensor(value, layout)\n+\n+\n+def distribute_tensor(tensor, layout):\n+    \"\"\"Distribute the tensor based on the layout.\n+\n+    Note that this function can be used both in eager context, or within a\n+    jitted function.\n+\n+    Args:\n+        tensor: `jax.Array` that need to be distributed.\n+        layout: `TensorLayout` for the created variable, or a\n+            JAX-supported layout instance (e.g. `jax.sharding.Sharding`).\n+\n+    Returns:\n+        Distributed value.\n+    \"\"\"\n+    # Avoid circular imports.\n+    from keras.src.distribution import TensorLayout\n+\n+    if isinstance(layout, TensorLayout):\n+        layout = layout.backend_layout\n+\n+    # TODO(scottzhu): This might not be a cheap check, we should consider\n+    # have some proper JAX API for doing this check.\n+    if jax_utils.is_in_jax_tracing_scope():\n+        return jax.lax.with_sharding_constraint(tensor, layout)\n+\n+    # Skip relayout if unnecessary.\n+    if isinstance(tensor, jax.Array):\n+        if isinstance(\n+            layout, jax.sharding.Sharding\n+        ) and tensor.sharding.is_equivalent_to(layout, ndim=len(tensor.shape)):\n+            return tensor\n+        # JAX explicit \"layout\" support.\n+        elif hasattr(layout, \"layout\"):\n+            current_layout = getattr(tensor, \"layout\", None)\n+            if current_layout == layout:\n+                return tensor\n+        # JAX explicit \"format\" support.\n+        elif hasattr(layout, \"format\"):\n+            current_layout = getattr(tensor, \"format\", None)\n+            if current_layout == layout:\n+                return tensor\n+\n+    return jax.device_put(tensor, layout)\n+\n+\n+def distribute_data_input(per_process_batch, layout, batch_dim_name):\n+    \"\"\"Distribute the input data with the corresponding layout.\n+\n+    Note that the inputs here is a local worker batch. Within the local worker,\n+    the data need to be further partitioned to map to each of the devices.\n+\n+    Args:\n+        inputs: `jax.Array` that is already sharded to a local process size.\n+        layout: `TensorLayout` for the distribution information, or a\n+            `jax.sharding.Sharding` instance.\n+\n+    Returns:\n+        A global batch distributed according to `layout`.\n+    \"\"\"\n+    # Avoid circular imports.\n+    from keras.src.distribution import TensorLayout\n+\n+    if isinstance(layout, TensorLayout):\n+        layout = layout.backend_layout\n+\n+    return jax.make_array_from_process_local_data(layout, per_process_batch)\n+\n+\n+def initialize_rng():\n+    \"\"\"Initializes the global random number generator across processes.\n+\n+    This is required for consistent initialization in multi-host settings.\n+    \"\"\"\n+    global_seed = rng_utils.get_random_seed()\n+    # Only set a random seed if not already set\n+    # via keras.config.set_random_seed()\n+    if global_seed is None:\n+        # Generate a random seed on each CPU host and psum them to get a single\n+        # consistent seed across all processes.\n+        cpu_devices = jax.devices(\"cpu\")\n+        num_local_cpu_devices = jax.local_device_count(\"cpu\")\n+        # Seed must be in range [0, 2^32 - 1], so to ensure proper range and\n+        # avoid signed integer overflow, we use uint32.\n+        local_seed = jax.numpy.asarray(\n+            [seed_generator.make_default_seed()] * num_local_cpu_devices,\n+            dtype=jax.numpy.uint32,\n+        )\n+        # Sum across processes and pull out the first item.\n+        global_seed = jax.pmap(\n+            lambda x: jax.lax.psum(x, \"all\"),\n+            axis_name=\"all\",\n+            devices=cpu_devices,\n+        )(local_seed).item(0)\n+        # Set the global seed.\n+        rng_utils.set_random_seed(global_seed)\n+\n+    # Check if the global seed generator is set and ensure it has an initialized\n+    # seed.  Otherwise, reset the seed to the global seed.\n+    global_seed_generator = global_state.get_global_attribute(\n+        \"global_seed_generator\"\n+    )\n+    if global_seed_generator is not None:\n+        seed = global_seed_generator.get_config()[\"seed\"]\n+        if seed is None:\n+            global_state.set_global_attribute(\n+                \"global_seed_generator\",\n+                seed_generator.SeedGenerator(\n+                    seed=global_seed,\n+                    name=global_seed_generator.name,\n+                    backend=global_seed_generator.backend,\n+                ),\n+            )\n+\n+\n+def initialize(job_addresses, num_processes, process_id):\n+    if job_addresses and \",\" in job_addresses:\n+        # When user provide all the job addresses, we will split and get the\n+        # first one, which is the coordinator.\n+        job_addresses = job_addresses.split(\",\")\n+        # Do a sanity check to make sure the number of addresses also match\n+        # the num_processes.\n+        if num_processes is not None and num_processes != len(job_addresses):\n+            raise ValueError(\n+                f\"The provided job_addresses {job_addresses} has \"\n+                f\"{len(job_addresses)} jobs, but num_processes is \"\n+                f\"{num_processes}\"\n+            )\n+        coordinator_address = job_addresses[0]\n+    else:\n+        coordinator_address = job_addresses\n+\n+    jax.distributed.initialize(\n+        coordinator_address=coordinator_address,\n+        num_processes=num_processes,\n+        process_id=process_id,\n+    )\n+\n+    # Ensure the random number generator is initialized across processes.\n+    initialize_rng()\n+\n+\n+def num_processes():\n+    \"\"\"Return the number of processes for the current distribution setting.\"\"\"\n+    return jax.process_count()\n+\n+\n+def process_id():\n+    \"\"\"Return the current process ID for the distribution setting.\"\"\"\n+    return jax.process_index()\n+\n+\n+def _to_backend_device(device_name):\n+    if isinstance(device_name, jax.Device):\n+        return device_name\n+    device_name = str(device_name)\n+    if \":\" not in device_name:\n+        device_type, device_id = device_name, 0\n+    else:\n+        device_type, device_id = device_name.split(\":\")\n+\n+    devices = jax.devices(backend=device_type)\n+    for device in devices:\n+        if device.platform == device_type and device.id == int(device_id):\n+            return device\n+    raise ValueError(f\"Device not found: {device_name}\")\n+\n+\n+def _to_backend_mesh(device_mesh):\n+    \"\"\"Convert the DeviceMesh to JAX backend specific Mesh.\n+\n+    Args:\n+        device_mesh: DeviceMesh instance to convert.\n+\n+    Returns:\n+        A `jax.sharding.Mesh` instance.\n+    \"\"\"\n+    shape = device_mesh.devices.shape\n+    devices = [_to_backend_device(d) for d in device_mesh.devices.flatten()]\n+    devices = np.array(devices).reshape(shape)\n+    return jax.sharding.Mesh(devices, device_mesh.axis_names)\n+\n+\n+def _to_backend_layout(tensor_layout):\n+    \"\"\"Convert the TensorLayout to JAX backend specific Sharding.\n+\n+    Args:\n+        tensor_layout: TensorLayout instance to convert.\n+\n+    Returns:\n+        A `jax.sharding.NamedSharding` instance.\n+    \"\"\"\n+    if tensor_layout.device_mesh is None:\n+        raise ValueError(\n+            \"Cannot create sharding when device mesh is not set \"\n+            \"for TensorLayout.\"\n+        )\n+    partition_spec = jax.sharding.PartitionSpec(*tensor_layout.axes)\n+    jax_mesh = tensor_layout.device_mesh.backend_mesh\n+    return jax.sharding.NamedSharding(jax_mesh, partition_spec)\n+\n+\n+_JAX_CLASSES_DEFINED = False\n+JaxGraph = None\n+JaxShardingPlanner = None\n+JaxShardApplier = None\n+\n+\n+def _define_and_register_jax_classes():\n+    global _JAX_CLASSES_DEFINED, JaxGraph, JaxShardingPlanner, JaxShardApplier\n+    if _JAX_CLASSES_DEFINED:\n+        return\n+\n+    # from keras.src.distribution.autoshard_utils import MergeableGraph\n+\n+    def parse_jaxpr(jaxpr) -> MergeableGraph:\n+        graph = MergeableGraph()\n+\n+        def same_axis(node1, node2):\n+            var1, axis1 = node1\n+            var2, axis2 = node2\n+            if var1.aval.shape[axis1] != var2.aval.shape[axis2]:\n+                return\n+            graph.merge_nodes(node1, node2)\n+\n+        def parse_dot_general(eqn):\n+            lhs, rhs = eqn.invars\n+            out = eqn.outvars[0]\n+            (lc, rc), (lb, rb) = eqn.params[\"dimension_numbers\"]\n+            for l, r in zip(lc, rc):\n+                same_axis((lhs, l), (rhs, r))\n+            o_offset = 0\n+            for l, r in zip(lb, rb):\n+                same_axis((lhs, l), (rhs, r))\n+                same_axis((lhs, l), (out, o_offset))\n+                o_offset += 1\n+            for i in range(lhs.aval.ndim):\n+                if i not in lb and i not in lc:\n+                    same_axis((lhs, i), (out, o_offset))\n+                    o_offset += 1\n+            for j in range(rhs.aval.ndim):\n+                if j not in rb and j not in rc:\n+                    same_axis((rhs, j), (out, o_offset))\n+                    o_offset += 1\n+\n+        def parse_reshape(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            in_idx, out_idx, in_prod, out_prod = 0, 0, 1, 1\n+            while in_idx < invar.aval.ndim and out_idx < out.aval.ndim:\n+                if (\n+                    in_prod == out_prod\n+                    and invar.aval.shape[in_idx] == out.aval.shape[out_idx]\n+                ):\n+                    if invar.aval.shape[in_idx] > 1:\n+                        same_axis((invar, in_idx), (out, out_idx))\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    out_prod *= out.aval.shape[out_idx]\n+                    in_idx += 1\n+                    out_idx += 1\n+                elif in_prod < out_prod:\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    in_idx += 1\n+                else:\n+                    out_prod *= out.aval.shape[out_idx]\n+                    out_idx += 1\n+\n+        def parse_transpose(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            for i, j in enumerate(eqn.params[\"permutation\"]):\n+                same_axis((invar, j), (out, i))\n+\n+        def parse_elementwise_with_broadcast(eqn):\n+            out = eqn.outvars[0]\n+            for invar in eqn.invars:\n+                if invar.aval.ndim == 0:\n+                    continue\n+                for i in range(1, min(invar.aval.ndim, out.aval.ndim) + 1):\n+                    in_axis, out_axis = -i, -i\n+                    if invar.aval.shape[in_axis] == out.aval.shape[out_axis]:\n+                        same_axis(\n+                            (invar, invar.aval.ndim + in_axis),\n+                            (out, out.aval.ndim + out_axis),\n+                        )\n+\n+        for var in jaxpr.jaxpr.invars:\n+            for i, j in itertools.combinations(range(var.aval.ndim), 2):\n+                graph.add_edge((var, i), (var, j))\n+\n+        for eqn in jaxpr.eqns:\n+            for outvar in eqn.outvars:\n+                for i, j in itertools.combinations(range(outvar.aval.ndim), 2):\n+                    graph.add_edge((outvar, i), (outvar, j))\n+\n+            primitive_parsers = {\n+                \"dot_general\": parse_dot_general,\n+                \"reshape\": parse_reshape,\n+                \"transpose\": parse_transpose,\n+            }\n+            parser = primitive_parsers.get(\n+                eqn.primitive.name, parse_elementwise_with_broadcast\n+            )\n+            parser(eqn)\n+        return graph\n+\n+    def shard_model(\n+        jaxpr,\n+        out_avals,\n+        trainable_params,\n+        non_trainable_params,\n+        args,\n+        kwargs,\n+        min_shard_size=1,\n+        data_axis_name=\"data\",\n+        model_axis_name=\"model\",\n+    ):\n+        graph = parse_jaxpr(jaxpr)\n+\n+        t_params_flat, t_params_treedef = tree_util.tree_flatten(\n+            trainable_params\n+        )\n+        nt_params_flat, nt_params_treedef = tree_util.tree_flatten(\n+            non_trainable_params\n+        )\n+        args_flat, args_treedef = tree_util.tree_flatten(args)\n+        kwargs_flat, kwargs_treedef = tree_util.tree_flatten(kwargs)\n+        _, outputs_treedef = tree_util.tree_flatten(out_avals)\n+\n+        pos = 0\n+        t_param_invars = jaxpr.jaxpr.invars[pos : pos + len(t_params_flat)]\n+        pos += len(t_params_flat)\n+        nt_param_invars = jaxpr.jaxpr.invars[pos : pos + len(nt_params_flat)]\n+        pos += len(nt_params_flat)\n+        arg_invars = jaxpr.jaxpr.invars[pos : pos + len(args_flat)]\n+        pos += len(args_flat)\n+        kwarg_invars = jaxpr.jaxpr.invars[pos:]\n+\n+        all_param_invars = t_param_invars + nt_param_invars\n+        data_invars = arg_invars + kwarg_invars\n+\n+        seen = collections.Counter()\n+        for var in all_param_invars:\n+            for i in range(var.aval.ndim):\n+                if var.aval.shape[i] >= min_shard_size:\n+                    seen.update([graph.get_root((var, i))])\n+\n+        model_axis_root = max(seen, key=seen.get) if seen else None\n+\n+        data_axes_roots = []\n+        for var in data_invars:\n+            for i in range(var.aval.ndim):\n+                root = graph.get_root((var, i))\n+                if root not in seen and root not in data_axes_roots:\n+                    data_axes_roots.append(root)\n+\n+        def assign_layouts(vars_flat, is_params=False):\n+            assignments = []\n+            for var in vars_flat:\n+                layout = [None] * var.aval.ndim\n+                for i in range(var.aval.ndim):\n+                    if var.aval.shape[i] < min_shard_size:\n+                        continue\n+                    root = graph.get_root((var, i))\n+                    if (\n+                        is_params\n+                        and model_axis_root\n+                        and root == model_axis_root\n+                    ):\n+                        layout[i] = model_axis_name\n+                    elif not is_params and root in data_axes_roots:\n+                        name = data_axis_name\n+                        if len(data_axes_roots) > 1:\n+                            name += str(data_axes_roots.index(root))\n+                        layout[i] = name\n+                assignments.append(layout)\n+            return assignments\n+\n+        params_assignments = tree_util.tree_unflatten(\n+            t_params_treedef, assign_layouts(t_param_invars, is_params=True)\n+        )\n+        return params_assignments\n+\n+    class _JaxGraph:\n+        def __init__(\n+            self,\n+            jaxpr,\n+            trainable_variables,\n+            non_trainable_variables,\n+            in_treedefs,\n+            out_avals,\n+        ):\n+            self.jaxpr = jaxpr\n+            self.trainable_variables = trainable_variables\n+            self.non_trainable_variables = non_trainable_variables\n+            self.in_treedefs = in_treedefs\n+            self.out_avals = out_avals\n+\n+        @classmethod\n+        def from_model(cls, model, *args, **kwargs):\n+            def stateless_fn(\n+                trainable_vars, non_trainable_vars, f_args, f_kwargs\n+            ):\n+                return model.stateless_call(\n+                    trainable_vars, non_trainable_vars, *f_args, **f_kwargs\n+                )\n+\n+            trainable_vars = model.trainable_variables\n+            non_trainable_vars = model.non_trainable_variables\n+            in_treedefs = tree_util.tree_structure(\n+                (trainable_vars, non_trainable_vars, args, kwargs)\n+            )\n+\n+            closed_jaxpr, out_avals = jax.make_jaxpr(\n+                stateless_fn, return_shape=True\n+            )(trainable_vars, non_trainable_vars, args, kwargs)\n+\n+            return cls(\n+                closed_jaxpr,\n+                trainable_vars,\n+                non_trainable_vars,\n+                in_treedefs,\n+                out_avals,\n+            )\n+\n+    class _JaxShardingPlanner:\n+        def plan(self, graph, device_mesh):\n+            all_in_avals = [var.aval for var in graph.jaxpr.jaxpr.invars]\n+            all_in_leaves = tree_util.tree_unflatten(\n+                graph.in_treedefs, all_in_avals\n+            )\n+            _, _, args_aval_tree, kwargs_aval_tree = all_in_leaves\n+\n+            dummy_args = tree_util.tree_map(\n+                lambda x: np.zeros(x.shape, x.dtype), args_aval_tree\n+            )\n+            dummy_kwargs = tree_util.tree_map(\n+                lambda x: np.zeros(x.shape, x.dtype), kwargs_aval_tree\n+            )\n+\n+            param_assignments = shard_model(\n+                jaxpr=graph.jaxpr,\n+                out_avals=graph.out_avals,\n+                trainable_params=graph.trainable_variables,\n+                non_trainable_params=graph.non_trainable_variables,\n+                args=dummy_args,\n+                kwargs=dummy_kwargs,\n+            )\n+\n+            param_vars_flat, _ = tree_util.tree_flatten(\n+                graph.trainable_variables\n+            )\n+            param_layouts_flat, _ = tree_util.tree_flatten(param_assignments)\n+\n+            parameter_layout_dict = {\n+                var.path: tuple(layout) if layout else None\n+                for var, layout in zip(param_vars_flat, param_layouts_flat)\n+            }\n+            return parameter_layout_dict\n+\n+    class _JaxShardApplier:\n+        def apply(self, model, plan):\n+            for var in model.variables:\n+                layout = plan.get(var.path)\n+                if layout:\n+                    var.layout = layout\n+\n+    JaxGraph = _JaxGraph\n+    JaxShardingPlanner = _JaxShardingPlanner\n+    JaxShardApplier = _JaxShardApplier\n+    _JAX_CLASSES_DEFINED = True\n+\n+\n+def get_sharding_planner():\n+    \"\"\"Returns an instance of the JAX sharding planner.\"\"\"\n+    _define_and_register_jax_classes()\n+    return JaxShardingPlanner()\n+\n+\n+def get_shard_applier():\n+    \"\"\"Returns an instance of the JAX shard applier.\"\"\"\n+    _define_and_register_jax_classes()\n+    return JaxShardApplier()\n+\n+\n+def create_graph_from_model(model, *args, **kwargs):\n+    \"\"\"Returns a JAX graph representation of the Keras model.\"\"\"\n+    _define_and_register_jax_classes()\n+    return JaxGraph.from_model(model, *args, **kwargs)\n+\n+\n+import os\n+import time\n+import logging\n+import numpy as np\n+import keras\n+import keras_nlp\n+import matplotlib.pyplot as plt\n+import tensorflow as tf\n+import tensorflow_datasets as tfds\n+import jax\n+from keras.src.distribution import distribution_lib\n+# --- Basic Configuration ---\n+logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n+logger = logging.getLogger(__name__)\n+import collections\n+import contextlib\n+import os\n+import re\n+import warnings\n+\n+import numpy as np\n+\n+from keras.src.api_export import keras_export\n+from keras.src.backend import KerasTensor\n+from keras.src.backend import distribution_lib\n+from keras.src.backend.common import global_state\n+\"\"\"Unified high-level distribution APIs across backends.\n+\n+Currently only the JAX backend is supported. The TensorFlow backend\n+will be supported in the future (via tf.dtensor API).\n+\"\"\"\n+\n+import collections\n+import contextlib\n+import os\n+import re\n+import warnings\n+\n+import numpy as np\n+\n+from keras.src.api_export import keras_export\n+from keras.src.backend import KerasTensor\n+from keras.src.backend import distribution_lib\n+from keras.src.backend.common import global_state\n+\n+DEFAULT_BATCH_DIM_NAME = \"batch\"\n+GLOBAL_ATTRIBUTE_NAME = \"distribution\"\n+\n+\n+@keras_export(\"keras.distribution.list_devices\")\n+def list_devices(device_type=None):\n+    \"\"\"Return all the available devices based on the device type.\n+\n+    Note: in a distributed setting, global devices are returned.\n+\n+    Args:\n+        device_type: string, one of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`.\n+            Defaults to `\"gpu\"` or `\"tpu\"` if available when\n+            `device_type` is not provided. Otherwise\n+            will return the `\"cpu\"` devices.\n+\n+    Return:\n+        List of devices that are available for distribute computation.\n+    \"\"\"\n+    return list_devices(device_type)\n+\n+\n+@keras_export(\"keras.distribution.initialize\")\n+def initialize(job_addresses=None, num_processes=None, process_id=None):\n+    \"\"\"Initialize the distribution system for multi-host/process setting.\n+\n+    Calling `initialize` will prepare the backend for execution on multi-host\n+    GPU or TPUs. It should be called before any computations.\n+\n+    Note that the parameters can also be injected via environment variables,\n+    which can be better controlled by the launch script at startup time.\n+    For certain backend that also rely on the environment variables to\n+    configure, Keras will properly forward them.\n+\n+    Args:\n+        job_addresses: string. Comma separated IP addresses for all the jobs\n+            that will form the whole computation cluster. Note that for JAX\n+            backend, only the address for job 0 (coodinator) is needed. For\n+            certain runtime like cloud TPU, this value can be `None`, and the\n+            backend will figure it out with the TPU environment variables. You\n+            can also config this value via environment variable\n+            `KERAS_DISTRIBUTION_JOB_ADDRESSES`.\n+        num_processes: int. The number of worker/processes that will form the\n+            whole computation cluster. For certain runtime like cloud TPU, this\n+            value can be `None`, and the backend will figure it out with the TPU\n+            environment variables. You can also configure this value via\n+            environment variable `KERAS_DISTRIBUTION_NUM_PROCESSES`.\n+        process_id: int. The ID number of the current worker/process. The value\n+            should be ranged from `0` to `num_processes - 1`. `0` will indicate\n+            the current worker/process is the master/coordinate job. You can\n+            also configure this value via environment variable\n+            `KERAS_DISTRIBUTION_PROCESS_ID`.\n+\n+        Example:\n+            Suppose there are two GPU processes, and process 0 is running at\n+            address `10.0.0.1:1234`, and process 1 is running at address\n+            `10.0.0.2:2345`. To configure such cluster, you can run\n+\n+        On process 0:\n+        ```python\n+        keras.distribute.initialize(\n+            job_addresses=\"10.0.0.1:1234,10.0.0.2:2345\",\n+            num_processes=2,\n+            process_id=0)\n+        ```\n+\n+        On process 1:\n+        ```python\n+        keras.distribute.initialize(\n+            job_addresses=\"10.0.0.1:1234,10.0.0.2:2345\",\n+            num_processes=2,\n+            process_id=1)\n+        ```\n+\n+        or via the environment variables:\n+        On process 0:\n+        ```python\n+        os.environ[\n+            \"KERAS_DISTRIBUTION_JOB_ADDRESSES\"] = \"10.0.0.1:1234,10.0.0.2:2345\"\n+        os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"] = \"2\"\n+        os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"] = \"0\"\n+        keras.distribute.initialize()\n+        ```\n+\n+        On process 1:\n+        ```python\n+        os.environ[\n+            \"KERAS_DISTRIBUTION_JOB_ADDRESSES\"] = \"10.0.0.1:1234,10.0.0.2:2345\"\n+        os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"] = \"2\"\n+        os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"] = \"1\"\n+        keras.distribute.initialize()\n+        ```\n+\n+        Also note that for JAX backend, the `job_addresses` can be further\n+        reduced to just the master/coordinator address, which is\n+        `10.0.0.1:1234`.\n+    \"\"\"\n+    if (\n+        job_addresses is None\n+        and \"KERAS_DISTRIBUTION_JOB_ADDRESSES\" in os.environ\n+    ):\n+        job_addresses = os.environ[\"KERAS_DISTRIBUTION_JOB_ADDRESSES\"]\n+    if (\n+        num_processes is None\n+        and \"KERAS_DISTRIBUTION_NUM_PROCESSES\" in os.environ\n+    ):\n+        num_processes = int(os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"])\n+    if process_id is None and \"KERAS_DISTRIBUTION_PROCESS_ID\" in os.environ:\n+        process_id = int(os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"])\n+    initialize(job_addresses, num_processes, process_id)\n+\n+\n+@keras_export(\"keras.distribution.DeviceMesh\")\n+class DeviceMesh:\n+    \"\"\"A cluster of computation devices for distributed computation.\n+\n+    This API is aligned with `jax.sharding.Mesh` and `tf.dtensor.Mesh`, which\n+    represents the computation devices in the global context.\n+\n+    See more details in [jax.sharding.Mesh](\n+        https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh)\n+    and [tf.dtensor.Mesh](\n+        https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Mesh).\n+\n+    Args:\n+        shape: tuple of list of integers. The shape of the overall\n+            `DeviceMesh`, e.g. `(8,)` for a data parallel only distribution,\n+            or `(4, 2)` for a model+data parallel distribution.\n+        axis_names: List of string. The logical name of the each axis for\n+            the `DeviceMesh`. The length of the `axis_names` should match to\n+            the rank of the `shape`. The `axis_names` will be used to\n+            match/create the `TensorLayout` when distribute the data and\n+            variables.\n+        devices: Optional list of devices. Defaults to all the available\n+            devices locally from `keras.distribution.list_devices()`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        shape,\n+        axis_names,\n+        devices=None,\n+    ):\n+        if not shape or not axis_names:\n+            raise ValueError(\n+                \"Shape and axis_names cannot be empty. Received: \"\n+                f\"shape={shape}, axis_names={axis_names}\"\n+            )\n+\n+        if len(shape) != len(axis_names):\n+            raise ValueError(\n+                \"Shape and axis_names should have same size. \"\n+                f\"Received: shape={shape}, axis_names={axis_names}\"\n+            )\n+        if devices is None:\n+            devices = list_devices()\n+        devices = np.array(devices)\n+        if np.prod(shape) != np.prod(devices.shape):\n+            raise ValueError(\n+                \"Shape does not match the number of devices. \"\n+                f\"Received: shape={shape}; devices.shape=\"\n+                f\"{devices.shape}\"\n+            )\n+\n+        self._shape = shape\n+        self._axis_names = axis_names\n+        self._devices = np.reshape(devices, shape)\n+\n+    @property\n+    def shape(self):\n+        return self._shape\n+\n+    @property\n+    def axis_names(self):\n+        return self._axis_names\n+\n+    @property\n+    def devices(self):\n+        return self._devices\n+\n+    @property\n+    def backend_mesh(self):\n+        if not hasattr(self, \"_backend_mesh\"):\n+            self._backend_mesh = _to_backend_mesh(self)\n+        return self._backend_mesh\n+\n+    def __repr__(self):\n+        return (\n+            f\"<{self.__class__.__name__} \"\n+            f\"shape={self.shape}, axis_names={self.axis_names}>\"\n+        )\n+\n+    def __str__(self):\n+        return self.__repr__()\n+\n+\n+@keras_export(\"keras.distribution.TensorLayout\")\n+class TensorLayout:\n+    \"\"\"A layout to apply to a tensor.\n+\n+    This API is aligned with `jax.sharding.NamedSharding`\n+    and `tf.dtensor.Layout`.\n+\n+    See more details in [jax.sharding.NamedSharding](\n+        https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding)\n+    and [tf.dtensor.Layout](\n+        https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Layout).\n+\n+    Args:\n+        axes: tuple of strings that should map to the `axis_names` in\n+            a `DeviceMesh`. For any dimensions that doesn't need any sharding,\n+            A `None` can be used a placeholder.\n+        device_mesh: Optional `DeviceMesh` that will be used to create\n+            the layout. The actual mapping of tensor to physical device\n+            is not known until the mesh is specified.\n+    \"\"\"\n+\n+    def __init__(self, axes, device_mesh=None):\n+        self._axes = tuple(axes)\n+        self._device_mesh = device_mesh\n+        self._validate_axes()\n+\n+    @property\n+    def axes(self):\n+        return self._axes\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    @device_mesh.setter\n+    def device_mesh(self, device_mesh):\n+        if self._device_mesh is not None:\n+            raise ValueError(\n+                \"Cannot override device mesh value. Existing \"\n+                f\"value is {self._device_mesh}\"\n+            )\n+        self._device_mesh = device_mesh\n+        self._validate_axes()\n+\n+    @property\n+    def backend_layout(self):\n+        if not hasattr(self, \"_backend_layout\"):\n+            self._backend_layout = _to_backend_layout(self)\n+        return self._backend_layout\n+\n+    def _validate_axes(self):\n+        if self._device_mesh:\n+            valid_axis_names = set(self._device_mesh.axis_names)\n+            axis_names = set(self._axes) - set([None])\n+            if axis_names - valid_axis_names:\n+                raise ValueError(\n+                    \"Invalid axis names for Layout. Valid axis \"\n+                    f\"names: {valid_axis_names}, Got {axis_names}\"\n+                )\n+\n+    def __repr__(self):\n+        return (\n+            f\"<{self.__class__.__name__} \"\n+            f\"axes={self.axes}, device_mesh={self.device_mesh}>\"\n+        )\n+\n+    def __str__(self):\n+        return self.__repr__()\n+\n+\n+class Distribution:\n+    \"\"\"Base class for variable distribution strategies.\n+\n+    A `Distribution` has following key functionalities:\n+\n+    1. Distribute the model variables to a `DeviceMesh`.\n+    2. Distribute the input data to a `DeviceMesh`.\n+    3. Distribute an intermediate state tensor in the model.\n+\n+    It can create a context scope so that the framework to properly detect the\n+    `Distribution` and distribute the variable/data accordingly.\n+\n+    Args:\n+        device_mesh: A `DeviceMesh` instance.\n+        batch_dim_name: Optional string name for the batch dimension.\n+            Defaults to None.\n+        auto_shard_dataset: Automatically shard the dataset amongst\n+            processes in a multi-process setting. Set to `False` if the dataset\n+            is already sharded across hosts.  Defaults to `True`.\n+    \"\"\"\n+\n+    def __init__(\n+        self, device_mesh, batch_dim_name=None, auto_shard_dataset=True\n+    ):\n+        self._device_mesh = device_mesh\n+        self._batch_dim_name = batch_dim_name\n+        self._auto_shard_dataset = auto_shard_dataset\n+\n+    def get_data_layout(self, data_shape):\n+        \"\"\"Retrieve the `TensorLayout` for the input data.\n+\n+        Args:\n+            data_shape: shape for the input data in list or tuple format.\n+\n+        Returns:\n+            The `TensorLayout` for the data, which can be used by\n+            `backend.distribute_value()` to redistribute a input data.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def get_variable_layout(self, variable):\n+        \"\"\"Retrieve the `TensorLayout` for the variable.\n+\n+        Args:\n+            variable: A `Variable` instance.\n+\n+        return:\n+            The `TensorLayout` for the variable, which can be used by\n+            `backend.distribute_value()` to redistribute a variable.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def get_tensor_layout(self, path):\n+        \"\"\"Retrieve the `TensorLayout` for the intermediate tensor.\n+\n+        Args:\n+            path: a string path for the corresponding tensor.\n+\n+        return:\n+            The `TensorLayout` for the intermediate tensor, which can be used\n+            by `backend.relayout()` to reshard the tensor. Could also return\n+            None.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    @contextlib.contextmanager\n+    def scope(self):\n+        \"\"\"Context manager to make the `Distribution` current.\"\"\"\n+        original_scope = distribution()\n+        set_distribution(self)\n+        try:\n+            yield\n+        finally:\n+            set_distribution(original_scope)\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    @property\n+    def batch_dim_name(self):\n+        return self._batch_dim_name\n+\n+    @property\n+    def auto_shard_dataset(self):\n+        return self._auto_shard_dataset\n+\n+    @auto_shard_dataset.setter\n+    def auto_shard_dataset(self, auto_shard_dataset):\n+        self._auto_shard_dataset = auto_shard_dataset\n+\n+    def distribute_dataset(self, dataset):\n+        \"\"\"Create a distributed dataset from the original global dataset.\n+\n+        Args:\n+            dataset: the original global dataset instance.\n+\n+        Returns:\n+            If `auto_shard_dataset` is `True`, returns a sharded dataset that\n+            only produces data for the current local worker/process.  Otherwise,\n+            returns the original dataset.\n+\n+        Raises:\n+            ValueError: if auto-sharding is requested in a multi-process\n+            setting, but the dataset type is not supported.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def __repr__(self):\n+        return f\"<{self.__class__.__name__} device_mesh={self.device_mesh}>\"\n+\n+    def __str__(self):\n+        return self.__repr__()\n+\n+@keras_export(\"keras.distribution.AutoShardDistribution\")\n+class AutoShardDistribution(Distribution):\n+    def __init__(\n+        self,\n+        device_mesh=None,\n+    ):\n+        if device_mesh is None:\n+            devices = np.array(list_devices())\n+            axis_names = [DEFAULT_BATCH_DIM_NAME] + [\n+                f\"model_{i}\" for i in range(devices.ndim - 1)\n+            ]\n+            device_mesh = DeviceMesh(\n+                shape=devices.shape,\n+                axis_names=axis_names,\n+                devices=devices,\n+            )\n+        super().__init__(device_mesh, device_mesh.axis_names[0])\n+        self._sharding_plan = None\n+        self._sharding_planner = None\n+        self._shard_applier = None\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+\n+    def _get_backend_components(self):\n+        if self._sharding_planner and self._shard_applier:\n+            return\n+        self._sharding_planner = get_sharding_planner()\n+        self._shard_applier = get_shard_applier()\n+\n+    def shard(self, model, *args, **kwargs):\n+        self._get_backend_components()\n+        graph = create_graph_from_model(model, *args, **kwargs)\n+\n+        plan = self._sharding_planner.plan(graph, self.device_mesh)\n+        self._sharding_plan = plan\n+        self._shard_applier.apply(model, self._sharding_plan)\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self.batch_dim_name\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+    def get_tensor_layout(self, path):\n+        return None\n+\n+    def distribute_dataset(self, dataset):\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        if not tf.available or not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for auto-sharding, \"\n+                f\"got {type(dataset)}\"\n+            )\n+\n+        from tensorflow.python.data.experimental.ops import (\n+            distribute as tf_data_distribute,\n+        )\n+\n+        batch_size = tf_data_distribute.compute_batch_size(dataset)\n+        if batch_size.numpy() < 0:\n+            raise ValueError(\n+                \"The batch size of the input dataset is \"\n+                \"unknown. Please config the batch size for \"\n+                \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n+            )\n+        per_worker_batch_size = tf_data_distribute.batch_sizes_for_worker(\n+            global_batch_size=batch_size,\n+            num_workers=self._num_process,\n+            num_replicas_per_worker=1,  # We hard code this for now.\n+            worker_index=self._process_id,\n+        )\n+        distributed_dataset = dataset.rebatch(per_worker_batch_size)\n+        distributed_dataset = tf_data_distribute._AutoShardDataset(\n+            distributed_dataset,\n+            num_workers=self._num_process,\n+            index=self._process_id,\n+            num_replicas=self._num_process,\n+        )\n+        return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+\n+@keras_export(\"keras.distribution.DataParallel\")\n+class DataParallel(Distribution):\n+    \"\"\"Distribution for data parallelism.\n+\n+    You can choose to create this instance by either specifying\n+    the `device_mesh` or `devices` arguments (but not both).\n+\n+    The `device_mesh` argument is expected to be a `DeviceMesh` instance,\n+    and is expected to be 1D only. In case that the mesh has multiple axes,\n+    then the first axis will be treated as the data parallel dimension\n+    (and a warning will be raised).\n+\n+    When a list of `devices` are provided, they will be used to construct a\n+    1D mesh.\n+\n+    When both `mesh` and `devices` are absent, then `list_devices()`\n+    will be used to detect any available devices and create a 1D mesh from\n+    them.\n+\n+    Args:\n+        device_mesh: Optional `DeviceMesh` instance.\n+        devices: Optional list of devices.\n+        auto_shard_dataset: Automatically shard the dataset amongst\n+            processes in a multi-process setting. Set to `False` if the dataset\n+            is already sharded across hosts.  Defaults to `True`.\n+    \"\"\"\n+\n+    def __init__(self, device_mesh=None, devices=None, auto_shard_dataset=True):\n+        if device_mesh:\n+            self._initialize_with_device_mesh(device_mesh, auto_shard_dataset)\n+        elif devices:\n+            self._initialize_mesh_from_devices(devices, auto_shard_dataset)\n+        else:\n+            self._initialize_mesh_from_list_devices(auto_shard_dataset)\n+\n+        # Those following attributes might get convert to public methods.\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+        self._is_multi_process = self._num_process > 1\n+\n+    def _initialize_with_device_mesh(self, device_mesh, auto_shard_dataset):\n+        if not isinstance(device_mesh, DeviceMesh):\n+            raise ValueError(\n+                \"Expect `mesh` to be an instance of `DeviceMesh`. \"\n+                f\"Received: mesh={device_mesh} (of type {type(device_mesh)})\"\n+            )\n+        super().__init__(\n+            device_mesh, device_mesh.axis_names[0], auto_shard_dataset\n+        )\n+        if self.device_mesh.devices.ndim != 1:\n+            warnings.warn(\n+                \"Expect the input mesh to be 1D, but received \"\n+                \"mesh.devices.ndim=%d. \"\n+                \"The first axis will be used for data-parallel sharding.\",\n+                device_mesh.devices.ndim,\n+            )\n+\n+    def _initialize_mesh_from_devices(self, devices, auto_shard_dataset):\n+        devices = np.array(devices)\n+        device_mesh = DeviceMesh(\n+            shape=devices.shape,\n+            axis_names=[DEFAULT_BATCH_DIM_NAME],\n+            devices=devices,\n+        )\n+        super().__init__(\n+            device_mesh, DEFAULT_BATCH_DIM_NAME, auto_shard_dataset\n+        )\n+\n+    def _initialize_mesh_from_list_devices(self, auto_shard_dataset):\n+        devices = np.array(list_devices())\n+        device_mesh = DeviceMesh(\n+            shape=devices.shape,\n+            axis_names=[DEFAULT_BATCH_DIM_NAME],\n+            devices=devices,\n+        )\n+        super().__init__(\n+            device_mesh, DEFAULT_BATCH_DIM_NAME, auto_shard_dataset\n+        )\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self.batch_dim_name  # Shard on the first dim\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        # First check if the variable already has a layout assigned.\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        # Otherwise, replicate variable.\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+    def get_tensor_layout(self, path):\n+        # For data parallel training, the intermediate state is not changed.\n+        return None\n+\n+    def distribute_dataset(self, dataset):\n+        if not self._is_multi_process or not self.auto_shard_dataset:\n+            return dataset\n+\n+        # Try to distribute a global tf.data.Dataset.\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        if not tf.available or not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for auto-sharding, \"\n+                f\"got {type(dataset)}\"\n+            )\n+\n+        from tensorflow.python.data.experimental.ops import (\n+            distribute as tf_data_distribute,\n+        )\n+\n+        batch_size = tf_data_distribute.compute_batch_size(dataset)\n+        if batch_size.numpy() < 0:\n+            raise ValueError(\n+                \"The batch size of the input dataset is \"\n+                \"unknown. Please config the batch size for \"\n+                \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n+            )\n+        per_worker_batch_size = tf_data_distribute.batch_sizes_for_worker(\n+            global_batch_size=batch_size,\n+            num_workers=self._num_process,\n+            num_replicas_per_worker=1,  # We hard code this for now.\n+            worker_index=self._process_id,\n+        )\n+        distributed_dataset = dataset.rebatch(per_worker_batch_size)\n+        distributed_dataset = tf_data_distribute._AutoShardDataset(\n+            distributed_dataset,\n+            num_workers=self._num_process,\n+            index=self._process_id,\n+            num_replicas=self._num_process,\n+        )\n+        return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+\n+\n+@keras_export(\"keras.distribution.ModelParallel\")\n+class ModelParallel(Distribution):\n+    \"\"\"Distribution that shards model variables.\n+\n+    Compare to `DataParallel` which replicates the variables across all devices,\n+    `ModelParallel` allows you to shard variables in addition to the input data.\n+\n+    To construct a `ModelParallel` distribution, you need to provide a\n+    `DeviceMesh` and a `LayoutMap`.\n+\n+    1. `DeviceMesh` contains physical device information. The axis names in\n+        the mesh will be used to map the variable and data layout.\n+    2. `LayoutMap` contains the mapping between variable paths to their\n+        corresponding `TensorLayout`.\n+\n+    Example:\n+\n+    ```python\n+    devices = list_devices()    # Assume there are 8 devices.\n+\n+    # Create a mesh with 2 devices for data parallelism and 4 devices for\n+    # model parallelism.\n+    device_mesh = DeviceMesh(shape=(2, 4), axis_names=('batch', 'model'),\n+                             devices=devices)\n+    # Create a layout map that shard the `Dense` layer and `Conv2D`\n+    # layer variables on the last dimension.\n+    # Based on the `device_mesh`, this means the variables\n+    # will be split across 4 devices. Any other variable that doesn't\n+    # match any key in the layout map will be fully replicated.\n+    layout_map = LayoutMap(device_mesh)\n+    layout_map['dense.*kernel'] = (None, 'model')\n+    layout_map['dense.*bias'] = ('model',)\n+    layout_map['conv2d.*kernel'] = (None, None, None, 'model')\n+    layout_map['conv2d.*bias'] = ('model',)\n+\n+    distribution = ModelParallel(\n+        layout_map=layout_map,\n+        batch_dim_name='batch',\n+    )\n+\n+    # Set the global distribution, or via `with distribution.scope():`\n+    set_distribution(distribution)\n+\n+    model = model_creation()\n+    model.compile()\n+    model.fit(data)\n+    ```\n+\n+    You can quickly update the device mesh shape to change the sharding factor\n+    of the variables. E.g.\n+\n+    ```python\n+    # With only the shape change for the device mesh, the variables will be\n+    # sharded across 8 devices instead of 4, which further reduces the memory\n+    # footprint of variables on each of the device.\n+    device_mesh = DeviceMesh(\n+        shape=(1, 8),\n+        axis_names=('batch', 'model'),\n+        devices=devices,\n+    )\n+    ```\n+\n+    To figure out a proper layout mapping rule for all the model variables, you\n+    can first list out all the model variable paths, which will be used as the\n+    key to map the variables to `TensorLayout`.\n+\n+    e.g.\n+\n+    ```python\n+    model = create_model()\n+    for v in model.variables:\n+        print(v.path)\n+    ```\n+\n+    Args:\n+        layout_map: `LayoutMap` instance which map the variable path to the\n+            corresponding tensor layout.\n+        batch_dim_name: Optional string, the axis name in the device mesh\n+            (of the `layout_map` object)\n+            that will be used to distribute data. If unspecified, the\n+            first axis from the device mesh will be used.\n+        auto_shard_dataset: Automatically shard the dataset amongst\n+            processes in a multi-process setting. Set to `False` if the dataset\n+            is already sharded across hosts.  Defaults to `True`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        layout_map=None,\n+        batch_dim_name=None,\n+        auto_shard_dataset=True,\n+        **kwargs,\n+    ):\n+        kwargs.pop(\"device_mesh\", None)\n+        if layout_map is None:\n+            raise ValueError(\"You must specify a layout_map argument.\")\n+        if not isinstance(layout_map, LayoutMap):\n+            raise ValueError(\n+                \"Argument `layout_map` must be a `LayoutMap` instance. \"\n+                f\"Received: layout_map={layout_map}\"\n+            )\n+        device_mesh = layout_map.device_mesh\n+        batch_dim_name = batch_dim_name or device_mesh.axis_names[0]\n+        super().__init__(device_mesh, batch_dim_name, auto_shard_dataset)\n+        self._layout_map = layout_map\n+\n+        # Those following attributes might get convert to public methods.\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+        self._is_multi_process = self._num_process > 1\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self.batch_dim_name  # Shard on the first dim\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        # First check if the variable already has a layout assigned.\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        # Check the layout map.\n+        variable_layout = self._layout_map[variable.path]\n+        if variable_layout is not None:\n+            return variable_layout\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+    def get_tensor_layout(self, path):\n+        return self._layout_map[path]\n+\n+    def distribute_dataset(self, dataset):\n+        if not self._is_multi_process or not self.auto_shard_dataset:\n+            return dataset\n+\n+        # Try to distribute a global tf.data.Dataset.\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        if not tf.available or not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for auto-sharding, \"\n+                f\"got {type(dataset)}\"\n+            )\n+\n+        from tensorflow.python.data.experimental.ops import (\n+            distribute as tf_data_distribute,\n+        )\n+\n+        global_batch_size = tf_data_distribute.compute_batch_size(dataset)\n+        if global_batch_size.numpy() < 0:\n+            raise ValueError(\n+                \"The batch size of the input dataset is \"\n+                \"unknown. Please config the batch size for \"\n+                \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n+            )\n+\n+        # We need to compute the per-process/worker/host batch size.\n+        # This will depend on how many model replicas we have on each process.\n+        # Note that this might be smaller than one if model replicas are sharded\n+        # across multiple processes.\n+        mesh_batch_dim_index = self.device_mesh.axis_names.index(\n+            self.batch_dim_name\n+        )\n+        num_model_replicas = self.device_mesh.shape[mesh_batch_dim_index]\n+        if num_model_replicas == 1:\n+            # No sharding is needed in this case. Each process will have the\n+            # global batch size, and data from the iterator will need to be\n+            # replicated across all processes.\n+            return dataset.prefetch(tf.data.AUTOTUNE)\n+        num_model_replicas_per_process = num_model_replicas / self._num_process\n+        if num_model_replicas_per_process >= 1:\n+            # Each process will have one or more full model replicas. Data will\n+            # be sharded across all processes without replication.\n+            if global_batch_size % self._num_process != 0:\n+                raise ValueError(\n+                    \"Global batch size must be divisible by the number of \"\n+                    f\"processes. `global_batch_size`={global_batch_size} and \"\n+                    f\"`num_process`={self._num_process}\"\n+                )\n+            per_process_batch_size = global_batch_size // self._num_process\n+            distributed_dataset = dataset.rebatch(per_process_batch_size)\n+            distributed_dataset = distributed_dataset.shard(\n+                num_shards=self._num_process,\n+                index=self._process_id,\n+            )\n+            return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+        else:\n+            # Model replicas are sharded across multiple processes. Data will be\n+            # sharded across model replicas, and replicated across processes\n+            # within the same model replica.\n+            if global_batch_size % num_model_replicas != 0:\n+                raise ValueError(\n+                    \"Global batch size must be divisible by the number of \"\n+                    f\"replicas. `global_batch_size`={global_batch_size} and \"\n+                    f\"`num_model_replicas`={num_model_replicas}\"\n+                )\n+            per_process_batch_size = global_batch_size // num_model_replicas\n+            distributed_dataset = dataset.rebatch(per_process_batch_size)\n+            processes_per_replica = self._num_process // num_model_replicas\n+            # TODO: Figure out what the convention is for data sharding id.\n+            data_shard_id = self._process_id % processes_per_replica\n+            distributed_dataset = distributed_dataset.shard(\n+                num_shards=num_model_replicas,\n+                index=data_shard_id,\n+            )\n+            return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+\n+\n+@keras_export(\"keras.distribution.LayoutMap\")\n+class LayoutMap(collections.abc.MutableMapping):\n+    \"\"\"A dict-like object that maps string to `TensorLayout` instances.\n+\n+    `LayoutMap` uses a string as key and a `TensorLayout` as value. There is a\n+    behavior difference between a normal Python dict and this class. The string\n+    key will be treated as a regex when retrieving the value. See the docstring\n+    of `get` for more details.\n+\n+    See below for a usage example. You can define the naming schema\n+    of the `TensorLayout`, and then retrieve the corresponding\n+    `TensorLayout` instance.\n+\n+    In the normal case, the key to query is usually the `variable.path`, which\n+    is the identifier of the variable.\n+\n+    As shortcut, tuple or list of axis names are also allowed when inserting\n+    as value, and will be converted to `TensorLayout`.\n+\n+    ```python\n+    layout_map = LayoutMap(device_mesh)\n+    layout_map['dense.*kernel'] = (None, 'model')\n+    layout_map['dense.*bias'] = ('model',)\n+    layout_map['conv2d.*kernel'] = (None, None, None, 'model')\n+    layout_map['conv2d.*bias'] = ('model',)\n+\n+    layout_1 = layout_map['dense_1.kernel']             # layout_1 == layout_2d\n+    layout_2 = layout_map['dense_1.bias']               # layout_2 == layout_1d\n+    layout_3 = layout_map['dense_2.kernel']             # layout_3 == layout_2d\n+    layout_4 = layout_map['dense_2.bias']               # layout_4 == layout_1d\n+    layout_5 = layout_map['my_model/conv2d_123/kernel'] # layout_5 == layout_4d\n+    layout_6 = layout_map['my_model/conv2d_123/bias']   # layout_6 == layout_1d\n+    layout_7 = layout_map['my_model/conv3d_1/kernel']   # layout_7 == None\n+    layout_8 = layout_map['my_model/conv3d_1/bias']     # layout_8 == None\n+    ```\n+\n+    Args:\n+        device_mesh: `keras.distribution.DeviceMesh` instance.\n+    \"\"\"\n+\n+    def __init__(self, device_mesh):\n+        self._layout_map = collections.OrderedDict()\n+        self._device_mesh = device_mesh\n+\n+    def __getitem__(self, key):\n+        \"\"\"Retrieves the corresponding layout by the string key.\n+\n+        When there isn't an exact match, all the existing keys in the layout map\n+        will be treated as a regex and map against the input key again. When\n+        there are multiple matches for the regex, an `ValueError` will be\n+        raised. Returns `None` if there isn't any match found.\n+\n+        Args:\n+            key: String key to query a layout.\n+\n+        Returns:\n+            Corresponding layout based on the query.\n+        \"\"\"\n+        if key in self._layout_map:\n+            return self._layout_map[key]\n+\n+        matching_keys = []\n+        for k in self._layout_map:\n+            if re.search(k, key):\n+                matching_keys.append(k)\n+        if len(matching_keys) > 1:\n+            raise ValueError(\n+                f\"Path '{key}' matches multiple layout \"\n+                f\"specification keys: {matching_keys}. Please make \"\n+                \"sure each tensor/variable path only matches at most \"\n+                \"one layout specification key in the LayoutMap.\"\n+            )\n+        elif len(matching_keys) == 1:\n+            return self._layout_map[matching_keys[0]]\n+        return None\n+\n+    def __setitem__(self, key, layout):\n+        \"\"\"Insert TensorLayout to the LayoutMap.\n+\n+        Args:\n+            key: String key for the `TensorLayout`.\n+            layout: The `TensorLayout`. As a shortcut, tuple of string and None\n+                are also acceptable, and will be converted to `TensorLayout`.\n+        \"\"\"\n+        if key in self._layout_map:\n+            raise ValueError(\n+                f\"{key} already exist in the LayoutMap with \"\n+                f\"value {self._layout_map[key]}. Please make sure to \"\n+                \"not use duplicated keys.\"\n+            )\n+        if isinstance(layout, tuple):\n+            layout = TensorLayout(axes=layout, device_mesh=None)\n+\n+        if not isinstance(layout, TensorLayout):\n+            raise ValueError(\n+                f\"{layout} should be a TensorLayout type, got {type(layout)}\"\n+            )\n+        self._maybe_populate_device_mesh(layout)\n+        self._layout_map[key] = layout\n+\n+    def __delitem__(self, key):\n+        # let the dict to handle the key missing error\n+        return self._layout_map.pop(key)\n+\n+    def __len__(self):\n+        return len(self._layout_map)\n+\n+    def __iter__(self):\n+        return iter(self._layout_map)\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    def _maybe_populate_device_mesh(self, layout):\n+        if layout.device_mesh is None and self.device_mesh is not None:\n+            layout.device_mesh = self.device_mesh\n+\n+\n+LayoutMap.get.__doc__ = LayoutMap.__getitem__.__doc__\n+\n+\n+@keras_export(\"keras.distribution.distribute_tensor\")\n+def distribute_tensor(tensor, layout):\n+    \"\"\"Change the layout of a Tensor value in the jit function execution.\n+\n+    Args:\n+        tensor: a Tensor to change the layout.\n+        layout: `TensorLayout` to be applied on the value.\n+\n+    Returns:\n+        a new value with the specified tensor layout.\n+    \"\"\"\n+    if isinstance(tensor, KerasTensor):\n+        return tensor\n+    return distribute_tensor(tensor, layout)\n+\n+\n+@keras_export(\"keras.distribution.distribution\")\n+def distribution():\n+    \"\"\"Retrieve the current distribution from global context.\"\"\"\n+    return global_state.get_global_attribute(GLOBAL_ATTRIBUTE_NAME)\n+\n+\n+@keras_export(\"keras.distribution.set_distribution\")\n+def set_distribution(value):\n+    \"\"\"Set the distribution as the global distribution setting.\n+\n+    Args:\n+        value: a `Distribution` instance.\n+    \"\"\"\n+    global_state.set_global_attribute(GLOBAL_ATTRIBUTE_NAME, value)\n+",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthis test file contains a very large amount of duplicated code from keras/src/backend/jax/distribution_lib.py and keras/src/distribution/distribution_lib.py. this is a significant maintenance concern, as any changes in the source files will not be reflected here, leading to outdated and incorrect tests. please refactor this file to import the necessary functions and classes directly instead of copy-pasting them.",
    "line_number": 1617,
    "enriched": "File: keras/src/distribution/test_memory.py\nCode: @@ -0,0 +1,1918 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Test suite for model verification with Parallax (Keras AutoShardDistribution),\n+using KerasNLP presets and the Tiny Shakespeare dataset.\n+\n+This script compares the training performance (loss and perplexity) of a\n+baseline model against its Parallax-sharded equivalent. The goal is to verify\n+the correctness and performance of Keras's automatic sharding capabilities.\n+\n+It is generalized to run tests for multiple model architectures, including:\n+- Gemma\n+- GPT-2\n+- Bloom\n+- OPT\n+\"\"\"\n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"\n+os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=2\"\n+\n+# Add these imports at the top of your script\n+import psutil\n+import os\n+\n+# Get the current process to monitor its memory. Place this near the top.\n+PROCESS = psutil.Process(os.getpid())\n+\n+# Replace your ENTIRE old log_memory_usage function with this new one.\n+def log_memory_usage(stage_name):\n+    \"\"\"\n+    Logs the Resident Set Size (RSS) of the current process using psutil.\n+    This is compatible with CPU, GPU, and other backends.\n+    \"\"\"\n+    print(f\"\\n--- Process Memory Usage at: {stage_name} ---\")\n+    mem_info = PROCESS.memory_info()\n+    rss_mb = mem_info.rss / 1024 / 1024  # rss is the Resident Set Size\n+    # print(f\"  Process RSS: {rss_mb:.2f} MB\")\n+    # print(\"-\" * 45)\n+\n+\"\"\"Utilities for distribution strategy with JAX backend.\"\"\"\n+class MergeableGraph:\n+    \"\"\"A graph that supports merging nodes.\"\"\"\n+\n+    def __init__(self):\n+        self._parent = {}\n+        self._edges = set()\n+\n+    def get_root(self, node):\n+        if node not in self._parent:\n+            self._parent[node] = node\n+            return node\n+        if self._parent[node] == node:\n+            return node\n+        self._parent[node] = self.get_root(self._parent[node])\n+        return self._parent[node]\n+\n+    def merge_nodes(self, node1, node2):\n+        root1 = self.get_root(node1)\n+        root2 = self.get_root(node2)\n+        if root1 != root2:\n+            self._parent[root1] = root2\n+\n+    def add_edge(self, node1, node2):\n+        root1 = self.get_root(node1)\n+        root2 = self.get_root(node2)\n+        if root1 != root2:\n+            self._edges.add(tuple(sorted((root1, root2))))\n+\n+    def get_edges(self):\n+        return self._edges\n+    \n+import jax\n+import numpy as np\n+import collections\n+import itertools\n+from keras.src.backend.common import global_state\n+from keras.src.random import seed_generator\n+from keras.src.utils import jax_utils\n+from keras.src.utils import rng_utils\n+from jax import tree_util\n+\n+\n+def list_devices(device_type=None):\n+    \"\"\"Return all the available devices based on the device type.\n+\n+    Note that this should return the global devices in a distributed setting.\n+\n+    Args:\n+        device_type: string of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`. Defaults to `\"gpu\"`\n+            or `\"tpu\"` if available when device_type is not provided. Otherwise\n+            will return the `\"cpu\"` devices.\n+\n+    Return:\n+        List of devices that are available for distribute computation.\n+    \"\"\"\n+    device_type = device_type.lower() if device_type else None\n+    jax_devices = jax.devices(backend=device_type)\n+    return [f\"{device.platform}:{device.id}\" for device in jax_devices]\n+\n+\n+def distribute_variable(value, layout):\n+    \"\"\"Create a distributed variable for JAX.\n+\n+    Since JAX doesn't have a variable class, this will just return a `jax.Array`\n+    with the corresponding layout/sharding specified.\n+\n+    Note that this function should be used in eager context, not in jitted\n+    function.\n+\n+    Args:\n+        value: the initial value of the variable.\n+        layout: `TensorLayout` for the created variable, or a\n+            JAX-supported layout instance (e.g. `jax.sharding.Sharding`).\n+\n+    Returns:\n+        jax.Array which is the distributed variable.\n+    \"\"\"\n+    return distribute_tensor(value, layout)\n+\n+\n+def distribute_tensor(tensor, layout):\n+    \"\"\"Distribute the tensor based on the layout.\n+\n+    Note that this function can be used both in eager context, or within a\n+    jitted function.\n+\n+    Args:\n+        tensor: `jax.Array` that need to be distributed.\n+        layout: `TensorLayout` for the created variable, or a\n+            JAX-supported layout instance (e.g. `jax.sharding.Sharding`).\n+\n+    Returns:\n+        Distributed value.\n+    \"\"\"\n+    # Avoid circular imports.\n+    from keras.src.distribution import TensorLayout\n+\n+    if isinstance(layout, TensorLayout):\n+        layout = layout.backend_layout\n+\n+    # TODO(scottzhu): This might not be a cheap check, we should consider\n+    # have some proper JAX API for doing this check.\n+    if jax_utils.is_in_jax_tracing_scope():\n+        return jax.lax.with_sharding_constraint(tensor, layout)\n+\n+    # Skip relayout if unnecessary.\n+    if isinstance(tensor, jax.Array):\n+        if isinstance(\n+            layout, jax.sharding.Sharding\n+        ) and tensor.sharding.is_equivalent_to(layout, ndim=len(tensor.shape)):\n+            return tensor\n+        # JAX explicit \"layout\" support.\n+        elif hasattr(layout, \"layout\"):\n+            current_layout = getattr(tensor, \"layout\", None)\n+            if current_layout == layout:\n+                return tensor\n+        # JAX explicit \"format\" support.\n+        elif hasattr(layout, \"format\"):\n+            current_layout = getattr(tensor, \"format\", None)\n+            if current_layout == layout:\n+                return tensor\n+\n+    return jax.device_put(tensor, layout)\n+\n+\n+def distribute_data_input(per_process_batch, layout, batch_dim_name):\n+    \"\"\"Distribute the input data with the corresponding layout.\n+\n+    Note that the inputs here is a local worker batch. Within the local worker,\n+    the data need to be further partitioned to map to each of the devices.\n+\n+    Args:\n+        inputs: `jax.Array` that is already sharded to a local process size.\n+        layout: `TensorLayout` for the distribution information, or a\n+            `jax.sharding.Sharding` instance.\n+\n+    Returns:\n+        A global batch distributed according to `layout`.\n+    \"\"\"\n+    # Avoid circular imports.\n+    from keras.src.distribution import TensorLayout\n+\n+    if isinstance(layout, TensorLayout):\n+        layout = layout.backend_layout\n+\n+    return jax.make_array_from_process_local_data(layout, per_process_batch)\n+\n+\n+def initialize_rng():\n+    \"\"\"Initializes the global random number generator across processes.\n+\n+    This is required for consistent initialization in multi-host settings.\n+    \"\"\"\n+    global_seed = rng_utils.get_random_seed()\n+    # Only set a random seed if not already set\n+    # via keras.config.set_random_seed()\n+    if global_seed is None:\n+        # Generate a random seed on each CPU host and psum them to get a single\n+        # consistent seed across all processes.\n+        cpu_devices = jax.devices(\"cpu\")\n+        num_local_cpu_devices = jax.local_device_count(\"cpu\")\n+        # Seed must be in range [0, 2^32 - 1], so to ensure proper range and\n+        # avoid signed integer overflow, we use uint32.\n+        local_seed = jax.numpy.asarray(\n+            [seed_generator.make_default_seed()] * num_local_cpu_devices,\n+            dtype=jax.numpy.uint32,\n+        )\n+        # Sum across processes and pull out the first item.\n+        global_seed = jax.pmap(\n+            lambda x: jax.lax.psum(x, \"all\"),\n+            axis_name=\"all\",\n+            devices=cpu_devices,\n+        )(local_seed).item(0)\n+        # Set the global seed.\n+        rng_utils.set_random_seed(global_seed)\n+\n+    # Check if the global seed generator is set and ensure it has an initialized\n+    # seed.  Otherwise, reset the seed to the global seed.\n+    global_seed_generator = global_state.get_global_attribute(\n+        \"global_seed_generator\"\n+    )\n+    if global_seed_generator is not None:\n+        seed = global_seed_generator.get_config()[\"seed\"]\n+        if seed is None:\n+            global_state.set_global_attribute(\n+                \"global_seed_generator\",\n+                seed_generator.SeedGenerator(\n+                    seed=global_seed,\n+                    name=global_seed_generator.name,\n+                    backend=global_seed_generator.backend,\n+                ),\n+            )\n+\n+\n+def initialize(job_addresses, num_processes, process_id):\n+    if job_addresses and \",\" in job_addresses:\n+        # When user provide all the job addresses, we will split and get the\n+        # first one, which is the coordinator.\n+        job_addresses = job_addresses.split(\",\")\n+        # Do a sanity check to make sure the number of addresses also match\n+        # the num_processes.\n+        if num_processes is not None and num_processes != len(job_addresses):\n+            raise ValueError(\n+                f\"The provided job_addresses {job_addresses} has \"\n+                f\"{len(job_addresses)} jobs, but num_processes is \"\n+                f\"{num_processes}\"\n+            )\n+        coordinator_address = job_addresses[0]\n+    else:\n+        coordinator_address = job_addresses\n+\n+    jax.distributed.initialize(\n+        coordinator_address=coordinator_address,\n+        num_processes=num_processes,\n+        process_id=process_id,\n+    )\n+\n+    # Ensure the random number generator is initialized across processes.\n+    initialize_rng()\n+\n+\n+def num_processes():\n+    \"\"\"Return the number of processes for the current distribution setting.\"\"\"\n+    return jax.process_count()\n+\n+\n+def process_id():\n+    \"\"\"Return the current process ID for the distribution setting.\"\"\"\n+    return jax.process_index()\n+\n+\n+def _to_backend_device(device_name):\n+    if isinstance(device_name, jax.Device):\n+        return device_name\n+    device_name = str(device_name)\n+    if \":\" not in device_name:\n+        device_type, device_id = device_name, 0\n+    else:\n+        device_type, device_id = device_name.split(\":\")\n+\n+    devices = jax.devices(backend=device_type)\n+    for device in devices:\n+        if device.platform == device_type and device.id == int(device_id):\n+            return device\n+    raise ValueError(f\"Device not found: {device_name}\")\n+\n+\n+def _to_backend_mesh(device_mesh):\n+    \"\"\"Convert the DeviceMesh to JAX backend specific Mesh.\n+\n+    Args:\n+        device_mesh: DeviceMesh instance to convert.\n+\n+    Returns:\n+        A `jax.sharding.Mesh` instance.\n+    \"\"\"\n+    shape = device_mesh.devices.shape\n+    devices = [_to_backend_device(d) for d in device_mesh.devices.flatten()]\n+    devices = np.array(devices).reshape(shape)\n+    return jax.sharding.Mesh(devices, device_mesh.axis_names)\n+\n+\n+def _to_backend_layout(tensor_layout):\n+    \"\"\"Convert the TensorLayout to JAX backend specific Sharding.\n+\n+    Args:\n+        tensor_layout: TensorLayout instance to convert.\n+\n+    Returns:\n+        A `jax.sharding.NamedSharding` instance.\n+    \"\"\"\n+    if tensor_layout.device_mesh is None:\n+        raise ValueError(\n+            \"Cannot create sharding when device mesh is not set \"\n+            \"for TensorLayout.\"\n+        )\n+    partition_spec = jax.sharding.PartitionSpec(*tensor_layout.axes)\n+    jax_mesh = tensor_layout.device_mesh.backend_mesh\n+    return jax.sharding.NamedSharding(jax_mesh, partition_spec)\n+\n+\n+_JAX_CLASSES_DEFINED = False\n+JaxGraph = None\n+JaxShardingPlanner = None\n+JaxShardApplier = None\n+\n+\n+def _define_and_register_jax_classes():\n+    global _JAX_CLASSES_DEFINED, JaxGraph, JaxShardingPlanner, JaxShardApplier\n+    if _JAX_CLASSES_DEFINED:\n+        return\n+\n+    # from keras.src.distribution.autoshard_utils import MergeableGraph\n+\n+    def parse_jaxpr(jaxpr) -> MergeableGraph:\n+        graph = MergeableGraph()\n+\n+        def same_axis(node1, node2):\n+            var1, axis1 = node1\n+            var2, axis2 = node2\n+            if var1.aval.shape[axis1] != var2.aval.shape[axis2]:\n+                return\n+            graph.merge_nodes(node1, node2)\n+\n+        def parse_dot_general(eqn):\n+            lhs, rhs = eqn.invars\n+            out = eqn.outvars[0]\n+            (lc, rc), (lb, rb) = eqn.params[\"dimension_numbers\"]\n+            for l, r in zip(lc, rc):\n+                same_axis((lhs, l), (rhs, r))\n+            o_offset = 0\n+            for l, r in zip(lb, rb):\n+                same_axis((lhs, l), (rhs, r))\n+                same_axis((lhs, l), (out, o_offset))\n+                o_offset += 1\n+            for i in range(lhs.aval.ndim):\n+                if i not in lb and i not in lc:\n+                    same_axis((lhs, i), (out, o_offset))\n+                    o_offset += 1\n+            for j in range(rhs.aval.ndim):\n+                if j not in rb and j not in rc:\n+                    same_axis((rhs, j), (out, o_offset))\n+                    o_offset += 1\n+\n+        def parse_reshape(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            in_idx, out_idx, in_prod, out_prod = 0, 0, 1, 1\n+            while in_idx < invar.aval.ndim and out_idx < out.aval.ndim:\n+                if (\n+                    in_prod == out_prod\n+                    and invar.aval.shape[in_idx] == out.aval.shape[out_idx]\n+                ):\n+                    if invar.aval.shape[in_idx] > 1:\n+                        same_axis((invar, in_idx), (out, out_idx))\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    out_prod *= out.aval.shape[out_idx]\n+                    in_idx += 1\n+                    out_idx += 1\n+                elif in_prod < out_prod:\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    in_idx += 1\n+                else:\n+                    out_prod *= out.aval.shape[out_idx]\n+                    out_idx += 1\n+\n+        def parse_transpose(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            for i, j in enumerate(eqn.params[\"permutation\"]):\n+                same_axis((invar, j), (out, i))\n+\n+        def parse_elementwise_with_broadcast(eqn):\n+            out = eqn.outvars[0]\n+            for invar in eqn.invars:\n+                if invar.aval.ndim == 0:\n+                    continue\n+                for i in range(1, min(invar.aval.ndim, out.aval.ndim) + 1):\n+                    in_axis, out_axis = -i, -i\n+                    if invar.aval.shape[in_axis] == out.aval.shape[out_axis]:\n+                        same_axis(\n+                            (invar, invar.aval.ndim + in_axis),\n+                            (out, out.aval.ndim + out_axis),\n+                        )\n+\n+        for var in jaxpr.jaxpr.invars:\n+            for i, j in itertools.combinations(range(var.aval.ndim), 2):\n+                graph.add_edge((var, i), (var, j))\n+\n+        for eqn in jaxpr.eqns:\n+            for outvar in eqn.outvars:\n+                for i, j in itertools.combinations(range(outvar.aval.ndim), 2):\n+                    graph.add_edge((outvar, i), (outvar, j))\n+\n+            primitive_parsers = {\n+                \"dot_general\": parse_dot_general,\n+                \"reshape\": parse_reshape,\n+                \"transpose\": parse_transpose,\n+            }\n+            parser = primitive_parsers.get(\n+                eqn.primitive.name, parse_elementwise_with_broadcast\n+            )\n+            parser(eqn)\n+        return graph\n+\n+    def shard_model(\n+        jaxpr,\n+        out_avals,\n+        trainable_params,\n+        non_trainable_params,\n+        args,\n+        kwargs,\n+        min_shard_size=1,\n+        data_axis_name=\"data\",\n+        model_axis_name=\"model\",\n+    ):\n+        graph = parse_jaxpr(jaxpr)\n+\n+        t_params_flat, t_params_treedef = tree_util.tree_flatten(\n+            trainable_params\n+        )\n+        nt_params_flat, nt_params_treedef = tree_util.tree_flatten(\n+            non_trainable_params\n+        )\n+        args_flat, args_treedef = tree_util.tree_flatten(args)\n+        kwargs_flat, kwargs_treedef = tree_util.tree_flatten(kwargs)\n+        _, outputs_treedef = tree_util.tree_flatten(out_avals)\n+\n+        pos = 0\n+        t_param_invars = jaxpr.jaxpr.invars[pos : pos + len(t_params_flat)]\n+        pos += len(t_params_flat)\n+        nt_param_invars = jaxpr.jaxpr.invars[pos : pos + len(nt_params_flat)]\n+        pos += len(nt_params_flat)\n+        arg_invars = jaxpr.jaxpr.invars[pos : pos + len(args_flat)]\n+        pos += len(args_flat)\n+        kwarg_invars = jaxpr.jaxpr.invars[pos:]\n+\n+        all_param_invars = t_param_invars + nt_param_invars\n+        data_invars = arg_invars + kwarg_invars\n+\n+        seen = collections.Counter()\n+        for var in all_param_invars:\n+            for i in range(var.aval.ndim):\n+                if var.aval.shape[i] >= min_shard_size:\n+                    seen.update([graph.get_root((var, i))])\n+\n+        model_axis_root = max(seen, key=seen.get) if seen else None\n+\n+        data_axes_roots = []\n+        for var in data_invars:\n+            for i in range(var.aval.ndim):\n+                root = graph.get_root((var, i))\n+                if root not in seen and root not in data_axes_roots:\n+                    data_axes_roots.append(root)\n+\n+        def assign_layouts(vars_flat, is_params=False):\n+            assignments = []\n+            for var in vars_flat:\n+                layout = [None] * var.aval.ndim\n+                for i in range(var.aval.ndim):\n+                    if var.aval.shape[i] < min_shard_size:\n+                        continue\n+                    root = graph.get_root((var, i))\n+                    if (\n+                        is_params\n+                        and model_axis_root\n+                        and root == model_axis_root\n+                    ):\n+                        layout[i] = model_axis_name\n+                    elif not is_params and root in data_axes_roots:\n+                        name = data_axis_name\n+                        if len(data_axes_roots) > 1:\n+                            name += str(data_axes_roots.index(root))\n+                        layout[i] = name\n+                assignments.append(layout)\n+            return assignments\n+\n+        params_assignments = tree_util.tree_unflatten(\n+            t_params_treedef, assign_layouts(t_param_invars, is_params=True)\n+        )\n+        return params_assignments\n+\n+    class _JaxGraph:\n+        def __init__(\n+            self,\n+            jaxpr,\n+            trainable_variables,\n+            non_trainable_variables,\n+            in_treedefs,\n+            out_avals,\n+        ):\n+            self.jaxpr = jaxpr\n+            self.trainable_variables = trainable_variables\n+            self.non_trainable_variables = non_trainable_variables\n+            self.in_treedefs = in_treedefs\n+            self.out_avals = out_avals\n+\n+        @classmethod\n+        def from_model(cls, model, *args, **kwargs):\n+            def stateless_fn(\n+                trainable_vars, non_trainable_vars, f_args, f_kwargs\n+            ):\n+                return model.stateless_call(\n+                    trainable_vars, non_trainable_vars, *f_args, **f_kwargs\n+                )\n+\n+            trainable_vars = model.trainable_variables\n+            non_trainable_vars = model.non_trainable_variables\n+            in_treedefs = tree_util.tree_structure(\n+                (trainable_vars, non_trainable_vars, args, kwargs)\n+            )\n+\n+            closed_jaxpr, out_avals = jax.make_jaxpr(\n+                stateless_fn, return_shape=True\n+            )(trainable_vars, non_trainable_vars, args, kwargs)\n+\n+            return cls(\n+                closed_jaxpr,\n+                trainable_vars,\n+                non_trainable_vars,\n+                in_treedefs,\n+                out_avals,\n+            )\n+\n+    class _JaxShardingPlanner:\n+        def plan(self, graph, device_mesh):\n+            all_in_avals = [var.aval for var in graph.jaxpr.jaxpr.invars]\n+            all_in_leaves = tree_util.tree_unflatten(\n+                graph.in_treedefs, all_in_avals\n+            )\n+            _, _, args_aval_tree, kwargs_aval_tree = all_in_leaves\n+\n+            dummy_args = tree_util.tree_map(\n+                lambda x: np.zeros(x.shape, x.dtype), args_aval_tree\n+            )\n+            dummy_kwargs = tree_util.tree_map(\n+                lambda x: np.zeros(x.shape, x.dtype), kwargs_aval_tree\n+            )\n+\n+            param_assignments = shard_model(\n+                jaxpr=graph.jaxpr,\n+                out_avals=graph.out_avals,\n+                trainable_params=graph.trainable_variables,\n+                non_trainable_params=graph.non_trainable_variables,\n+                args=dummy_args,\n+                kwargs=dummy_kwargs,\n+            )\n+\n+            param_vars_flat, _ = tree_util.tree_flatten(\n+                graph.trainable_variables\n+            )\n+            param_layouts_flat, _ = tree_util.tree_flatten(param_assignments)\n+\n+            parameter_layout_dict = {\n+                var.path: tuple(layout) if layout else None\n+                for var, layout in zip(param_vars_flat, param_layouts_flat)\n+            }\n+            return parameter_layout_dict\n+\n+    class _JaxShardApplier:\n+        def apply(self, model, plan):\n+            for var in model.variables:\n+                layout = plan.get(var.path)\n+                if layout:\n+                    var.layout = layout\n+\n+    JaxGraph = _JaxGraph\n+    JaxShardingPlanner = _JaxShardingPlanner\n+    JaxShardApplier = _JaxShardApplier\n+    _JAX_CLASSES_DEFINED = True\n+\n+\n+def get_sharding_planner():\n+    \"\"\"Returns an instance of the JAX sharding planner.\"\"\"\n+    _define_and_register_jax_classes()\n+    return JaxShardingPlanner()\n+\n+\n+def get_shard_applier():\n+    \"\"\"Returns an instance of the JAX shard applier.\"\"\"\n+    _define_and_register_jax_classes()\n+    return JaxShardApplier()\n+\n+\n+def create_graph_from_model(model, *args, **kwargs):\n+    \"\"\"Returns a JAX graph representation of the Keras model.\"\"\"\n+    _define_and_register_jax_classes()\n+    return JaxGraph.from_model(model, *args, **kwargs)\n+\n+\n+import os\n+import time\n+import logging\n+import numpy as np\n+import keras\n+import keras_nlp\n+import matplotlib.pyplot as plt\n+import tensorflow as tf\n+import tensorflow_datasets as tfds\n+import jax\n+from keras.src.distribution import distribution_lib\n+# --- Basic Configuration ---\n+logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n+logger = logging.getLogger(__name__)\n+import collections\n+import contextlib\n+import os\n+import re\n+import warnings\n+\n+import numpy as np\n+\n+from keras.src.api_export import keras_export\n+from keras.src.backend import KerasTensor\n+from keras.src.backend import distribution_lib\n+from keras.src.backend.common import global_state\n+\"\"\"Unified high-level distribution APIs across backends.\n+\n+Currently only the JAX backend is supported. The TensorFlow backend\n+will be supported in the future (via tf.dtensor API).\n+\"\"\"\n+\n+import collections\n+import contextlib\n+import os\n+import re\n+import warnings\n+\n+import numpy as np\n+\n+from keras.src.api_export import keras_export\n+from keras.src.backend import KerasTensor\n+from keras.src.backend import distribution_lib\n+from keras.src.backend.common import global_state\n+\n+DEFAULT_BATCH_DIM_NAME = \"batch\"\n+GLOBAL_ATTRIBUTE_NAME = \"distribution\"\n+\n+\n+@keras_export(\"keras.distribution.list_devices\")\n+def list_devices(device_type=None):\n+    \"\"\"Return all the available devices based on the device type.\n+\n+    Note: in a distributed setting, global devices are returned.\n+\n+    Args:\n+        device_type: string, one of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`.\n+            Defaults to `\"gpu\"` or `\"tpu\"` if available when\n+            `device_type` is not provided. Otherwise\n+            will return the `\"cpu\"` devices.\n+\n+    Return:\n+        List of devices that are available for distribute computation.\n+    \"\"\"\n+    return list_devices(device_type)\n+\n+\n+@keras_export(\"keras.distribution.initialize\")\n+def initialize(job_addresses=None, num_processes=None, process_id=None):\n+    \"\"\"Initialize the distribution system for multi-host/process setting.\n+\n+    Calling `initialize` will prepare the backend for execution on multi-host\n+    GPU or TPUs. It should be called before any computations.\n+\n+    Note that the parameters can also be injected via environment variables,\n+    which can be better controlled by the launch script at startup time.\n+    For certain backend that also rely on the environment variables to\n+    configure, Keras will properly forward them.\n+\n+    Args:\n+        job_addresses: string. Comma separated IP addresses for all the jobs\n+            that will form the whole computation cluster. Note that for JAX\n+            backend, only the address for job 0 (coodinator) is needed. For\n+            certain runtime like cloud TPU, this value can be `None`, and the\n+            backend will figure it out with the TPU environment variables. You\n+            can also config this value via environment variable\n+            `KERAS_DISTRIBUTION_JOB_ADDRESSES`.\n+        num_processes: int. The number of worker/processes that will form the\n+            whole computation cluster. For certain runtime like cloud TPU, this\n+            value can be `None`, and the backend will figure it out with the TPU\n+            environment variables. You can also configure this value via\n+            environment variable `KERAS_DISTRIBUTION_NUM_PROCESSES`.\n+        process_id: int. The ID number of the current worker/process. The value\n+            should be ranged from `0` to `num_processes - 1`. `0` will indicate\n+            the current worker/process is the master/coordinate job. You can\n+            also configure this value via environment variable\n+            `KERAS_DISTRIBUTION_PROCESS_ID`.\n+\n+        Example:\n+            Suppose there are two GPU processes, and process 0 is running at\n+            address `10.0.0.1:1234`, and process 1 is running at address\n+            `10.0.0.2:2345`. To configure such cluster, you can run\n+\n+        On process 0:\n+        ```python\n+        keras.distribute.initialize(\n+            job_addresses=\"10.0.0.1:1234,10.0.0.2:2345\",\n+            num_processes=2,\n+            process_id=0)\n+        ```\n+\n+        On process 1:\n+        ```python\n+        keras.distribute.initialize(\n+            job_addresses=\"10.0.0.1:1234,10.0.0.2:2345\",\n+            num_processes=2,\n+            process_id=1)\n+        ```\n+\n+        or via the environment variables:\n+        On process 0:\n+        ```python\n+        os.environ[\n+            \"KERAS_DISTRIBUTION_JOB_ADDRESSES\"] = \"10.0.0.1:1234,10.0.0.2:2345\"\n+        os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"] = \"2\"\n+        os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"] = \"0\"\n+        keras.distribute.initialize()\n+        ```\n+\n+        On process 1:\n+        ```python\n+        os.environ[\n+            \"KERAS_DISTRIBUTION_JOB_ADDRESSES\"] = \"10.0.0.1:1234,10.0.0.2:2345\"\n+        os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"] = \"2\"\n+        os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"] = \"1\"\n+        keras.distribute.initialize()\n+        ```\n+\n+        Also note that for JAX backend, the `job_addresses` can be further\n+        reduced to just the master/coordinator address, which is\n+        `10.0.0.1:1234`.\n+    \"\"\"\n+    if (\n+        job_addresses is None\n+        and \"KERAS_DISTRIBUTION_JOB_ADDRESSES\" in os.environ\n+    ):\n+        job_addresses = os.environ[\"KERAS_DISTRIBUTION_JOB_ADDRESSES\"]\n+    if (\n+        num_processes is None\n+        and \"KERAS_DISTRIBUTION_NUM_PROCESSES\" in os.environ\n+    ):\n+        num_processes = int(os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"])\n+    if process_id is None and \"KERAS_DISTRIBUTION_PROCESS_ID\" in os.environ:\n+        process_id = int(os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"])\n+    initialize(job_addresses, num_processes, process_id)\n+\n+\n+@keras_export(\"keras.distribution.DeviceMesh\")\n+class DeviceMesh:\n+    \"\"\"A cluster of computation devices for distributed computation.\n+\n+    This API is aligned with `jax.sharding.Mesh` and `tf.dtensor.Mesh`, which\n+    represents the computation devices in the global context.\n+\n+    See more details in [jax.sharding.Mesh](\n+        https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh)\n+    and [tf.dtensor.Mesh](\n+        https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Mesh).\n+\n+    Args:\n+        shape: tuple of list of integers. The shape of the overall\n+            `DeviceMesh`, e.g. `(8,)` for a data parallel only distribution,\n+            or `(4, 2)` for a model+data parallel distribution.\n+        axis_names: List of string. The logical name of the each axis for\n+            the `DeviceMesh`. The length of the `axis_names` should match to\n+            the rank of the `shape`. The `axis_names` will be used to\n+            match/create the `TensorLayout` when distribute the data and\n+            variables.\n+        devices: Optional list of devices. Defaults to all the available\n+            devices locally from `keras.distribution.list_devices()`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        shape,\n+        axis_names,\n+        devices=None,\n+    ):\n+        if not shape or not axis_names:\n+            raise ValueError(\n+                \"Shape and axis_names cannot be empty. Received: \"\n+                f\"shape={shape}, axis_names={axis_names}\"\n+            )\n+\n+        if len(shape) != len(axis_names):\n+            raise ValueError(\n+                \"Shape and axis_names should have same size. \"\n+                f\"Received: shape={shape}, axis_names={axis_names}\"\n+            )\n+        if devices is None:\n+            devices = list_devices()\n+        devices = np.array(devices)\n+        if np.prod(shape) != np.prod(devices.shape):\n+            raise ValueError(\n+                \"Shape does not match the number of devices. \"\n+                f\"Received: shape={shape}; devices.shape=\"\n+                f\"{devices.shape}\"\n+            )\n+\n+        self._shape = shape\n+        self._axis_names = axis_names\n+        self._devices = np.reshape(devices, shape)\n+\n+    @property\n+    def shape(self):\n+        return self._shape\n+\n+    @property\n+    def axis_names(self):\n+        return self._axis_names\n+\n+    @property\n+    def devices(self):\n+        return self._devices\n+\n+    @property\n+    def backend_mesh(self):\n+        if not hasattr(self, \"_backend_mesh\"):\n+            self._backend_mesh = _to_backend_mesh(self)\n+        return self._backend_mesh\n+\n+    def __repr__(self):\n+        return (\n+            f\"<{self.__class__.__name__} \"\n+            f\"shape={self.shape}, axis_names={self.axis_names}>\"\n+        )\n+\n+    def __str__(self):\n+        return self.__repr__()\n+\n+\n+@keras_export(\"keras.distribution.TensorLayout\")\n+class TensorLayout:\n+    \"\"\"A layout to apply to a tensor.\n+\n+    This API is aligned with `jax.sharding.NamedSharding`\n+    and `tf.dtensor.Layout`.\n+\n+    See more details in [jax.sharding.NamedSharding](\n+        https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding)\n+    and [tf.dtensor.Layout](\n+        https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Layout).\n+\n+    Args:\n+        axes: tuple of strings that should map to the `axis_names` in\n+            a `DeviceMesh`. For any dimensions that doesn't need any sharding,\n+            A `None` can be used a placeholder.\n+        device_mesh: Optional `DeviceMesh` that will be used to create\n+            the layout. The actual mapping of tensor to physical device\n+            is not known until the mesh is specified.\n+    \"\"\"\n+\n+    def __init__(self, axes, device_mesh=None):\n+        self._axes = tuple(axes)\n+        self._device_mesh = device_mesh\n+        self._validate_axes()\n+\n+    @property\n+    def axes(self):\n+        return self._axes\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    @device_mesh.setter\n+    def device_mesh(self, device_mesh):\n+        if self._device_mesh is not None:\n+            raise ValueError(\n+                \"Cannot override device mesh value. Existing \"\n+                f\"value is {self._device_mesh}\"\n+            )\n+        self._device_mesh = device_mesh\n+        self._validate_axes()\n+\n+    @property\n+    def backend_layout(self):\n+        if not hasattr(self, \"_backend_layout\"):\n+            self._backend_layout = _to_backend_layout(self)\n+        return self._backend_layout\n+\n+    def _validate_axes(self):\n+        if self._device_mesh:\n+            valid_axis_names = set(self._device_mesh.axis_names)\n+            axis_names = set(self._axes) - set([None])\n+            if axis_names - valid_axis_names:\n+                raise ValueError(\n+                    \"Invalid axis names for Layout. Valid axis \"\n+                    f\"names: {valid_axis_names}, Got {axis_names}\"\n+                )\n+\n+    def __repr__(self):\n+        return (\n+            f\"<{self.__class__.__name__} \"\n+            f\"axes={self.axes}, device_mesh={self.device_mesh}>\"\n+        )\n+\n+    def __str__(self):\n+        return self.__repr__()\n+\n+\n+class Distribution:\n+    \"\"\"Base class for variable distribution strategies.\n+\n+    A `Distribution` has following key functionalities:\n+\n+    1. Distribute the model variables to a `DeviceMesh`.\n+    2. Distribute the input data to a `DeviceMesh`.\n+    3. Distribute an intermediate state tensor in the model.\n+\n+    It can create a context scope so that the framework to properly detect the\n+    `Distribution` and distribute the variable/data accordingly.\n+\n+    Args:\n+        device_mesh: A `DeviceMesh` instance.\n+        batch_dim_name: Optional string name for the batch dimension.\n+            Defaults to None.\n+        auto_shard_dataset: Automatically shard the dataset amongst\n+            processes in a multi-process setting. Set to `False` if the dataset\n+            is already sharded across hosts.  Defaults to `True`.\n+    \"\"\"\n+\n+    def __init__(\n+        self, device_mesh, batch_dim_name=None, auto_shard_dataset=True\n+    ):\n+        self._device_mesh = device_mesh\n+        self._batch_dim_name = batch_dim_name\n+        self._auto_shard_dataset = auto_shard_dataset\n+\n+    def get_data_layout(self, data_shape):\n+        \"\"\"Retrieve the `TensorLayout` for the input data.\n+\n+        Args:\n+            data_shape: shape for the input data in list or tuple format.\n+\n+        Returns:\n+            The `TensorLayout` for the data, which can be used by\n+            `backend.distribute_value()` to redistribute a input data.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def get_variable_layout(self, variable):\n+        \"\"\"Retrieve the `TensorLayout` for the variable.\n+\n+        Args:\n+            variable: A `Variable` instance.\n+\n+        return:\n+            The `TensorLayout` for the variable, which can be used by\n+            `backend.distribute_value()` to redistribute a variable.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def get_tensor_layout(self, path):\n+        \"\"\"Retrieve the `TensorLayout` for the intermediate tensor.\n+\n+        Args:\n+            path: a string path for the corresponding tensor.\n+\n+        return:\n+            The `TensorLayout` for the intermediate tensor, which can be used\n+            by `backend.relayout()` to reshard the tensor. Could also return\n+            None.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    @contextlib.contextmanager\n+    def scope(self):\n+        \"\"\"Context manager to make the `Distribution` current.\"\"\"\n+        original_scope = distribution()\n+        set_distribution(self)\n+        try:\n+            yield\n+        finally:\n+            set_distribution(original_scope)\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    @property\n+    def batch_dim_name(self):\n+        return self._batch_dim_name\n+\n+    @property\n+    def auto_shard_dataset(self):\n+        return self._auto_shard_dataset\n+\n+    @auto_shard_dataset.setter\n+    def auto_shard_dataset(self, auto_shard_dataset):\n+        self._auto_shard_dataset = auto_shard_dataset\n+\n+    def distribute_dataset(self, dataset):\n+        \"\"\"Create a distributed dataset from the original global dataset.\n+\n+        Args:\n+            dataset: the original global dataset instance.\n+\n+        Returns:\n+            If `auto_shard_dataset` is `True`, returns a sharded dataset that\n+            only produces data for the current local worker/process.  Otherwise,\n+            returns the original dataset.\n+\n+        Raises:\n+            ValueError: if auto-sharding is requested in a multi-process\n+            setting, but the dataset type is not supported.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def __repr__(self):\n+        return f\"<{self.__class__.__name__} device_mesh={self.device_mesh}>\"\n+\n+    def __str__(self):\n+        return self.__repr__()\n+\n+@keras_export(\"keras.distribution.AutoShardDistribution\")\n+class AutoShardDistribution(Distribution):\n+    def __init__(\n+        self,\n+        device_mesh=None,\n+    ):\n+        if device_mesh is None:\n+            devices = np.array(list_devices())\n+            axis_names = [DEFAULT_BATCH_DIM_NAME] + [\n+                f\"model_{i}\" for i in range(devices.ndim - 1)\n+            ]\n+            device_mesh = DeviceMesh(\n+                shape=devices.shape,\n+                axis_names=axis_names,\n+                devices=devices,\n+            )\n+        super().__init__(device_mesh, device_mesh.axis_names[0])\n+        self._sharding_plan = None\n+        self._sharding_planner = None\n+        self._shard_applier = None\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+\n+    def _get_backend_components(self):\n+        if self._sharding_planner and self._shard_applier:\n+            return\n+        self._sharding_planner = get_sharding_planner()\n+        self._shard_applier = get_shard_applier()\n+\n+    def shard(self, model, *args, **kwargs):\n+        self._get_backend_components()\n+        graph = create_graph_from_model(model, *args, **kwargs)\n+\n+        plan = self._sharding_planner.plan(graph, self.device_mesh)\n+        self._sharding_plan = plan\n+        self._shard_applier.apply(model, self._sharding_plan)\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self.batch_dim_name\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+    def get_tensor_layout(self, path):\n+        return None\n+\n+    def distribute_dataset(self, dataset):\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        if not tf.available or not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for auto-sharding, \"\n+                f\"got {type(dataset)}\"\n+            )\n+\n+        from tensorflow.python.data.experimental.ops import (\n+            distribute as tf_data_distribute,\n+        )\n+\n+        batch_size = tf_data_distribute.compute_batch_size(dataset)\n+        if batch_size.numpy() < 0:\n+            raise ValueError(\n+                \"The batch size of the input dataset is \"\n+                \"unknown. Please config the batch size for \"\n+                \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n+            )\n+        per_worker_batch_size = tf_data_distribute.batch_sizes_for_worker(\n+            global_batch_size=batch_size,\n+            num_workers=self._num_process,\n+            num_replicas_per_worker=1,  # We hard code this for now.\n+            worker_index=self._process_id,\n+        )\n+        distributed_dataset = dataset.rebatch(per_worker_batch_size)\n+        distributed_dataset = tf_data_distribute._AutoShardDataset(\n+            distributed_dataset,\n+            num_workers=self._num_process,\n+            index=self._process_id,\n+            num_replicas=self._num_process,\n+        )\n+        return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+\n+@keras_export(\"keras.distribution.DataParallel\")\n+class DataParallel(Distribution):\n+    \"\"\"Distribution for data parallelism.\n+\n+    You can choose to create this instance by either specifying\n+    the `device_mesh` or `devices` arguments (but not both).\n+\n+    The `device_mesh` argument is expected to be a `DeviceMesh` instance,\n+    and is expected to be 1D only. In case that the mesh has multiple axes,\n+    then the first axis will be treated as the data parallel dimension\n+    (and a warning will be raised).\n+\n+    When a list of `devices` are provided, they will be used to construct a\n+    1D mesh.\n+\n+    When both `mesh` and `devices` are absent, then `list_devices()`\n+    will be used to detect any available devices and create a 1D mesh from\n+    them.\n+\n+    Args:\n+        device_mesh: Optional `DeviceMesh` instance.\n+        devices: Optional list of devices.\n+        auto_shard_dataset: Automatically shard the dataset amongst\n+            processes in a multi-process setting. Set to `False` if the dataset\n+            is already sharded across hosts.  Defaults to `True`.\n+    \"\"\"\n+\n+    def __init__(self, device_mesh=None, devices=None, auto_shard_dataset=True):\n+        if device_mesh:\n+            self._initialize_with_device_mesh(device_mesh, auto_shard_dataset)\n+        elif devices:\n+            self._initialize_mesh_from_devices(devices, auto_shard_dataset)\n+        else:\n+            self._initialize_mesh_from_list_devices(auto_shard_dataset)\n+\n+        # Those following attributes might get convert to public methods.\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+        self._is_multi_process = self._num_process > 1\n+\n+    def _initialize_with_device_mesh(self, device_mesh, auto_shard_dataset):\n+        if not isinstance(device_mesh, DeviceMesh):\n+            raise ValueError(\n+                \"Expect `mesh` to be an instance of `DeviceMesh`. \"\n+                f\"Received: mesh={device_mesh} (of type {type(device_mesh)})\"\n+            )\n+        super().__init__(\n+            device_mesh, device_mesh.axis_names[0], auto_shard_dataset\n+        )\n+        if self.device_mesh.devices.ndim != 1:\n+            warnings.warn(\n+                \"Expect the input mesh to be 1D, but received \"\n+                \"mesh.devices.ndim=%d. \"\n+                \"The first axis will be used for data-parallel sharding.\",\n+                device_mesh.devices.ndim,\n+            )\n+\n+    def _initialize_mesh_from_devices(self, devices, auto_shard_dataset):\n+        devices = np.array(devices)\n+        device_mesh = DeviceMesh(\n+            shape=devices.shape,\n+            axis_names=[DEFAULT_BATCH_DIM_NAME],\n+            devices=devices,\n+        )\n+        super().__init__(\n+            device_mesh, DEFAULT_BATCH_DIM_NAME, auto_shard_dataset\n+        )\n+\n+    def _initialize_mesh_from_list_devices(self, auto_shard_dataset):\n+        devices = np.array(list_devices())\n+        device_mesh = DeviceMesh(\n+            shape=devices.shape,\n+            axis_names=[DEFAULT_BATCH_DIM_NAME],\n+            devices=devices,\n+        )\n+        super().__init__(\n+            device_mesh, DEFAULT_BATCH_DIM_NAME, auto_shard_dataset\n+        )\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self.batch_dim_name  # Shard on the first dim\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        # First check if the variable already has a layout assigned.\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        # Otherwise, replicate variable.\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+    def get_tensor_layout(self, path):\n+        # For data parallel training, the intermediate state is not changed.\n+        return None\n+\n+    def distribute_dataset(self, dataset):\n+        if not self._is_multi_process or not self.auto_shard_dataset:\n+            return dataset\n+\n+        # Try to distribute a global tf.data.Dataset.\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        if not tf.available or not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for auto-sharding, \"\n+                f\"got {type(dataset)}\"\n+            )\n+\n+        from tensorflow.python.data.experimental.ops import (\n+            distribute as tf_data_distribute,\n+        )\n+\n+        batch_size = tf_data_distribute.compute_batch_size(dataset)\n+        if batch_size.numpy() < 0:\n+            raise ValueError(\n+                \"The batch size of the input dataset is \"\n+                \"unknown. Please config the batch size for \"\n+                \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n+            )\n+        per_worker_batch_size = tf_data_distribute.batch_sizes_for_worker(\n+            global_batch_size=batch_size,\n+            num_workers=self._num_process,\n+            num_replicas_per_worker=1,  # We hard code this for now.\n+            worker_index=self._process_id,\n+        )\n+        distributed_dataset = dataset.rebatch(per_worker_batch_size)\n+        distributed_dataset = tf_data_distribute._AutoShardDataset(\n+            distributed_dataset,\n+            num_workers=self._num_process,\n+            index=self._process_id,\n+            num_replicas=self._num_process,\n+        )\n+        return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+\n+\n+@keras_export(\"keras.distribution.ModelParallel\")\n+class ModelParallel(Distribution):\n+    \"\"\"Distribution that shards model variables.\n+\n+    Compare to `DataParallel` which replicates the variables across all devices,\n+    `ModelParallel` allows you to shard variables in addition to the input data.\n+\n+    To construct a `ModelParallel` distribution, you need to provide a\n+    `DeviceMesh` and a `LayoutMap`.\n+\n+    1. `DeviceMesh` contains physical device information. The axis names in\n+        the mesh will be used to map the variable and data layout.\n+    2. `LayoutMap` contains the mapping between variable paths to their\n+        corresponding `TensorLayout`.\n+\n+    Example:\n+\n+    ```python\n+    devices = list_devices()    # Assume there are 8 devices.\n+\n+    # Create a mesh with 2 devices for data parallelism and 4 devices for\n+    # model parallelism.\n+    device_mesh = DeviceMesh(shape=(2, 4), axis_names=('batch', 'model'),\n+                             devices=devices)\n+    # Create a layout map that shard the `Dense` layer and `Conv2D`\n+    # layer variables on the last dimension.\n+    # Based on the `device_mesh`, this means the variables\n+    # will be split across 4 devices. Any other variable that doesn't\n+    # match any key in the layout map will be fully replicated.\n+    layout_map = LayoutMap(device_mesh)\n+    layout_map['dense.*kernel'] = (None, 'model')\n+    layout_map['dense.*bias'] = ('model',)\n+    layout_map['conv2d.*kernel'] = (None, None, None, 'model')\n+    layout_map['conv2d.*bias'] = ('model',)\n+\n+    distribution = ModelParallel(\n+        layout_map=layout_map,\n+        batch_dim_name='batch',\n+    )\n+\n+    # Set the global distribution, or via `with distribution.scope():`\n+    set_distribution(distribution)\n+\n+    model = model_creation()\n+    model.compile()\n+    model.fit(data)\n+    ```\n+\n+    You can quickly update the device mesh shape to change the sharding factor\n+    of the variables. E.g.\n+\n+    ```python\n+    # With only the shape change for the device mesh, the variables will be\n+    # sharded across 8 devices instead of 4, which further reduces the memory\n+    # footprint of variables on each of the device.\n+    device_mesh = DeviceMesh(\n+        shape=(1, 8),\n+        axis_names=('batch', 'model'),\n+        devices=devices,\n+    )\n+    ```\n+\n+    To figure out a proper layout mapping rule for all the model variables, you\n+    can first list out all the model variable paths, which will be used as the\n+    key to map the variables to `TensorLayout`.\n+\n+    e.g.\n+\n+    ```python\n+    model = create_model()\n+    for v in model.variables:\n+        print(v.path)\n+    ```\n+\n+    Args:\n+        layout_map: `LayoutMap` instance which map the variable path to the\n+            corresponding tensor layout.\n+        batch_dim_name: Optional string, the axis name in the device mesh\n+            (of the `layout_map` object)\n+            that will be used to distribute data. If unspecified, the\n+            first axis from the device mesh will be used.\n+        auto_shard_dataset: Automatically shard the dataset amongst\n+            processes in a multi-process setting. Set to `False` if the dataset\n+            is already sharded across hosts.  Defaults to `True`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        layout_map=None,\n+        batch_dim_name=None,\n+        auto_shard_dataset=True,\n+        **kwargs,\n+    ):\n+        kwargs.pop(\"device_mesh\", None)\n+        if layout_map is None:\n+            raise ValueError(\"You must specify a layout_map argument.\")\n+        if not isinstance(layout_map, LayoutMap):\n+            raise ValueError(\n+                \"Argument `layout_map` must be a `LayoutMap` instance. \"\n+                f\"Received: layout_map={layout_map}\"\n+            )\n+        device_mesh = layout_map.device_mesh\n+        batch_dim_name = batch_dim_name or device_mesh.axis_names[0]\n+        super().__init__(device_mesh, batch_dim_name, auto_shard_dataset)\n+        self._layout_map = layout_map\n+\n+        # Those following attributes might get convert to public methods.\n+        self._num_process = num_processes()\n+        self._process_id = process_id()\n+        self._is_multi_process = self._num_process > 1\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self.batch_dim_name  # Shard on the first dim\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        # First check if the variable already has a layout assigned.\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        # Check the layout map.\n+        variable_layout = self._layout_map[variable.path]\n+        if variable_layout is not None:\n+            return variable_layout\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+    def get_tensor_layout(self, path):\n+        return self._layout_map[path]\n+\n+    def distribute_dataset(self, dataset):\n+        if not self._is_multi_process or not self.auto_shard_dataset:\n+            return dataset\n+\n+        # Try to distribute a global tf.data.Dataset.\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        if not tf.available or not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for auto-sharding, \"\n+                f\"got {type(dataset)}\"\n+            )\n+\n+        from tensorflow.python.data.experimental.ops import (\n+            distribute as tf_data_distribute,\n+        )\n+\n+        global_batch_size = tf_data_distribute.compute_batch_size(dataset)\n+        if global_batch_size.numpy() < 0:\n+            raise ValueError(\n+                \"The batch size of the input dataset is \"\n+                \"unknown. Please config the batch size for \"\n+                \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n+            )\n+\n+        # We need to compute the per-process/worker/host batch size.\n+        # This will depend on how many model replicas we have on each process.\n+        # Note that this might be smaller than one if model replicas are sharded\n+        # across multiple processes.\n+        mesh_batch_dim_index = self.device_mesh.axis_names.index(\n+            self.batch_dim_name\n+        )\n+        num_model_replicas = self.device_mesh.shape[mesh_batch_dim_index]\n+        if num_model_replicas == 1:\n+            # No sharding is needed in this case. Each process will have the\n+            # global batch size, and data from the iterator will need to be\n+            # replicated across all processes.\n+            return dataset.prefetch(tf.data.AUTOTUNE)\n+        num_model_replicas_per_process = num_model_replicas / self._num_process\n+        if num_model_replicas_per_process >= 1:\n+            # Each process will have one or more full model replicas. Data will\n+            # be sharded across all processes without replication.\n+            if global_batch_size % self._num_process != 0:\n+                raise ValueError(\n+                    \"Global batch size must be divisible by the number of \"\n+                    f\"processes. `global_batch_size`={global_batch_size} and \"\n+                    f\"`num_process`={self._num_process}\"\n+                )\n+            per_process_batch_size = global_batch_size // self._num_process\n+            distributed_dataset = dataset.rebatch(per_process_batch_size)\n+            distributed_dataset = distributed_dataset.shard(\n+                num_shards=self._num_process,\n+                index=self._process_id,\n+            )\n+            return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+        else:\n+            # Model replicas are sharded across multiple processes. Data will be\n+            # sharded across model replicas, and replicated across processes\n+            # within the same model replica.\n+            if global_batch_size % num_model_replicas != 0:\n+                raise ValueError(\n+                    \"Global batch size must be divisible by the number of \"\n+                    f\"replicas. `global_batch_size`={global_batch_size} and \"\n+                    f\"`num_model_replicas`={num_model_replicas}\"\n+                )\n+            per_process_batch_size = global_batch_size // num_model_replicas\n+            distributed_dataset = dataset.rebatch(per_process_batch_size)\n+            processes_per_replica = self._num_process // num_model_replicas\n+            # TODO: Figure out what the convention is for data sharding id.\n+            data_shard_id = self._process_id % processes_per_replica\n+            distributed_dataset = distributed_dataset.shard(\n+                num_shards=num_model_replicas,\n+                index=data_shard_id,\n+            )\n+            return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+\n+\n+@keras_export(\"keras.distribution.LayoutMap\")\n+class LayoutMap(collections.abc.MutableMapping):\n+    \"\"\"A dict-like object that maps string to `TensorLayout` instances.\n+\n+    `LayoutMap` uses a string as key and a `TensorLayout` as value. There is a\n+    behavior difference between a normal Python dict and this class. The string\n+    key will be treated as a regex when retrieving the value. See the docstring\n+    of `get` for more details.\n+\n+    See below for a usage example. You can define the naming schema\n+    of the `TensorLayout`, and then retrieve the corresponding\n+    `TensorLayout` instance.\n+\n+    In the normal case, the key to query is usually the `variable.path`, which\n+    is the identifier of the variable.\n+\n+    As shortcut, tuple or list of axis names are also allowed when inserting\n+    as value, and will be converted to `TensorLayout`.\n+\n+    ```python\n+    layout_map = LayoutMap(device_mesh)\n+    layout_map['dense.*kernel'] = (None, 'model')\n+    layout_map['dense.*bias'] = ('model',)\n+    layout_map['conv2d.*kernel'] = (None, None, None, 'model')\n+    layout_map['conv2d.*bias'] = ('model',)\n+\n+    layout_1 = layout_map['dense_1.kernel']             # layout_1 == layout_2d\n+    layout_2 = layout_map['dense_1.bias']               # layout_2 == layout_1d\n+    layout_3 = layout_map['dense_2.kernel']             # layout_3 == layout_2d\n+    layout_4 = layout_map['dense_2.bias']               # layout_4 == layout_1d\n+    layout_5 = layout_map['my_model/conv2d_123/kernel'] # layout_5 == layout_4d\n+    layout_6 = layout_map['my_model/conv2d_123/bias']   # layout_6 == layout_1d\n+    layout_7 = layout_map['my_model/conv3d_1/kernel']   # layout_7 == None\n+    layout_8 = layout_map['my_model/conv3d_1/bias']     # layout_8 == None\n+    ```\n+\n+    Args:\n+        device_mesh: `keras.distribution.DeviceMesh` instance.\n+    \"\"\"\n+\n+    def __init__(self, device_mesh):\n+        self._layout_map = collections.OrderedDict()\n+        self._device_mesh = device_mesh\n+\n+    def __getitem__(self, key):\n+        \"\"\"Retrieves the corresponding layout by the string key.\n+\n+        When there isn't an exact match, all the existing keys in the layout map\n+        will be treated as a regex and map against the input key again. When\n+        there are multiple matches for the regex, an `ValueError` will be\n+        raised. Returns `None` if there isn't any match found.\n+\n+        Args:\n+            key: String key to query a layout.\n+\n+        Returns:\n+            Corresponding layout based on the query.\n+        \"\"\"\n+        if key in self._layout_map:\n+            return self._layout_map[key]\n+\n+        matching_keys = []\n+        for k in self._layout_map:\n+            if re.search(k, key):\n+                matching_keys.append(k)\n+        if len(matching_keys) > 1:\n+            raise ValueError(\n+                f\"Path '{key}' matches multiple layout \"\n+                f\"specification keys: {matching_keys}. Please make \"\n+                \"sure each tensor/variable path only matches at most \"\n+                \"one layout specification key in the LayoutMap.\"\n+            )\n+        elif len(matching_keys) == 1:\n+            return self._layout_map[matching_keys[0]]\n+        return None\n+\n+    def __setitem__(self, key, layout):\n+        \"\"\"Insert TensorLayout to the LayoutMap.\n+\n+        Args:\n+            key: String key for the `TensorLayout`.\n+            layout: The `TensorLayout`. As a shortcut, tuple of string and None\n+                are also acceptable, and will be converted to `TensorLayout`.\n+        \"\"\"\n+        if key in self._layout_map:\n+            raise ValueError(\n+                f\"{key} already exist in the LayoutMap with \"\n+                f\"value {self._layout_map[key]}. Please make sure to \"\n+                \"not use duplicated keys.\"\n+            )\n+        if isinstance(layout, tuple):\n+            layout = TensorLayout(axes=layout, device_mesh=None)\n+\n+        if not isinstance(layout, TensorLayout):\n+            raise ValueError(\n+                f\"{layout} should be a TensorLayout type, got {type(layout)}\"\n+            )\n+        self._maybe_populate_device_mesh(layout)\n+        self._layout_map[key] = layout\n+\n+    def __delitem__(self, key):\n+        # let the dict to handle the key missing error\n+        return self._layout_map.pop(key)\n+\n+    def __len__(self):\n+        return len(self._layout_map)\n+\n+    def __iter__(self):\n+        return iter(self._layout_map)\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    def _maybe_populate_device_mesh(self, layout):\n+        if layout.device_mesh is None and self.device_mesh is not None:\n+            layout.device_mesh = self.device_mesh\n+\n+\n+LayoutMap.get.__doc__ = LayoutMap.__getitem__.__doc__\n+\n+\n+@keras_export(\"keras.distribution.distribute_tensor\")\n+def distribute_tensor(tensor, layout):\n+    \"\"\"Change the layout of a Tensor value in the jit function execution.\n+\n+    Args:\n+        tensor: a Tensor to change the layout.\n+        layout: `TensorLayout` to be applied on the value.\n+\n+    Returns:\n+        a new value with the specified tensor layout.\n+    \"\"\"\n+    if isinstance(tensor, KerasTensor):\n+        return tensor\n+    return distribute_tensor(tensor, layout)\n+\n+\n+@keras_export(\"keras.distribution.distribution\")\n+def distribution():\n+    \"\"\"Retrieve the current distribution from global context.\"\"\"\n+    return global_state.get_global_attribute(GLOBAL_ATTRIBUTE_NAME)\n+\n+\n+@keras_export(\"keras.distribution.set_distribution\")\n+def set_distribution(value):\n+    \"\"\"Set the distribution as the global distribution setting.\n+\n+    Args:\n+        value: a `Distribution` instance.\n+    \"\"\"\n+    global_state.set_global_attribute(GLOBAL_ATTRIBUTE_NAME, value)\n+\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThis test file contains a very large amount of duplicated code from `keras/src/backend/jax/distribution_lib.py` and `keras/src/distribution/distribution_lib.py`. This is a significant maintenance concern, as any changes in the source files will not be reflected here, leading to outdated and incorrect tests. Please refactor this file to import the necessary functions and classes directly instead of copy-pasting them.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/distribution/test_memory.py",
    "pr_number": 21644,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2331844224,
    "comment_created_at": "2025-09-09T02:35:39Z"
  },
  {
    "code": "@@ -65,28 +65,28 @@ class StringLookup(IndexLookup):\n             If this value is 0, OOV inputs will cause an error when calling\n             the layer. Defaults to `1`.\n         mask_token: A token that represents masked inputs. When `output_mode` is\n-            `\"int\"`, the token is included in vocabulary and mapped to index 0.\n-            In other output modes, the token will not appear\n-            in the vocabulary and instances of the mask token\n-            in the input will be dropped. If set to `None`,\n-            no mask term will be added. Defaults to `None`.\n+            `\"int\"`, the token is included in the vocabulary and mapped to index\n+            0.\n+            In other output modes, the token will not appear in the vocabulary\n+            and instances of the mask token in the input will be dropped.\n+            If set to `None`, no mask term will be added. Defaults to `None`.\n         oov_token: Only used when `invert` is True. The token to return for OOV\n             indices. Defaults to `\"[UNK]\"`.\n         vocabulary: Optional. Either an array of integers or a string path to a\n-            text file. If passing an array, can pass a tuple, list,\n-            1D NumPy array, or 1D tensor containing the integer vocbulary terms.\n-            If passing a file path, the file should contain one line per term\n-            in the vocabulary. If this argument is set,\n-            there is no need to `adapt()` the layer.\n+            text file. If passing an array, you can pass a tuple, list, 1D NumPy\n+            array, or 1D tensor containing the integer vocabulary terms.\n+            If passing a file path, the file should contain one line per term in\n+            the vocabulary. If this argument is set, there is no need to\n+            `adapt()` the layer.",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe description for the vocabulary argument is incorrect. it states that the array should contain \"integer vocabulary terms\" and can be an \"array of integers\". for stringlookup, the vocabulary consists of strings. this appears to be a copy-paste error, likely from integerlookup.\n\nto improve clarity and correctness, i suggest updating the description to specify string types.\n\n",
    "line_number": 80,
    "enriched": "File: keras/src/layers/preprocessing/string_lookup.py\nCode: @@ -65,28 +65,28 @@ class StringLookup(IndexLookup):\n             If this value is 0, OOV inputs will cause an error when calling\n             the layer. Defaults to `1`.\n         mask_token: A token that represents masked inputs. When `output_mode` is\n-            `\"int\"`, the token is included in vocabulary and mapped to index 0.\n-            In other output modes, the token will not appear\n-            in the vocabulary and instances of the mask token\n-            in the input will be dropped. If set to `None`,\n-            no mask term will be added. Defaults to `None`.\n+            `\"int\"`, the token is included in the vocabulary and mapped to index\n+            0.\n+            In other output modes, the token will not appear in the vocabulary\n+            and instances of the mask token in the input will be dropped.\n+            If set to `None`, no mask term will be added. Defaults to `None`.\n         oov_token: Only used when `invert` is True. The token to return for OOV\n             indices. Defaults to `\"[UNK]\"`.\n         vocabulary: Optional. Either an array of integers or a string path to a\n-            text file. If passing an array, can pass a tuple, list,\n-            1D NumPy array, or 1D tensor containing the integer vocbulary terms.\n-            If passing a file path, the file should contain one line per term\n-            in the vocabulary. If this argument is set,\n-            there is no need to `adapt()` the layer.\n+            text file. If passing an array, you can pass a tuple, list, 1D NumPy\n+            array, or 1D tensor containing the integer vocabulary terms.\n+            If passing a file path, the file should contain one line per term in\n+            the vocabulary. If this argument is set, there is no need to\n+            `adapt()` the layer.\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe description for the `vocabulary` argument is incorrect. It states that the array should contain \"integer vocabulary terms\" and can be an \"array of integers\". For `StringLookup`, the vocabulary consists of strings. This appears to be a copy-paste error, likely from `IntegerLookup`.\n\nTo improve clarity and correctness, I suggest updating the description to specify string types.\n\n```suggestion\n        vocabulary: Optional. Either an array of strings or a string path to a\n            text file. If passing an array, you can pass a tuple, list, 1D NumPy\n            array, or 1D tensor containing the string vocabulary terms.\n            If passing a file path, the file should contain one line per term in\n            the vocabulary. If this argument is set, there is no need to\n            `adapt()` the layer.\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/layers/preprocessing/string_lookup.py",
    "pr_number": 21643,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2331684906,
    "comment_created_at": "2025-09-09T00:39:43Z"
  },
  {
    "code": "@@ -288,6 +288,54 @@ def get_config(self):\n         return config\n \n \n+@keras_export(\"keras.dtype_policies.GPTQDTypePolicy\")\n+class GPTQDTypePolicy(QuantizedDTypePolicy):\n+    \"\"\"Quantized dtype policy for GPTQ quantization.\n+\n+    This policy helps propagate quantization settings for GPTQ\n+    when loading a GPTQ quantized model in Keras format.\n+\n+    Args:\n+        mode: The quantization mode, \"gptq\".\n+        source_name: The source dtype policy name, e.g. \"float32\".\n+        weight_bits: Number of bits to quantize weights to. Supported values\n+            are 2, 3, 4, and 8.\n+        group_size: The group size for quantization. Supported values are\n+            -1 (for whole-tensor quantization) or any positive integer.\n+            Typically a smaller group size leads to better accuracy but\n+            slower speed.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        mode,\n+        source_name=None,\n+        weight_bits=None,\n+        group_size=None,",
    "comment": "i don't think it will work if weight_bits is none or group_size is none.",
    "line_number": 314,
    "enriched": "File: keras/src/dtype_policies/dtype_policy.py\nCode: @@ -288,6 +288,54 @@ def get_config(self):\n         return config\n \n \n+@keras_export(\"keras.dtype_policies.GPTQDTypePolicy\")\n+class GPTQDTypePolicy(QuantizedDTypePolicy):\n+    \"\"\"Quantized dtype policy for GPTQ quantization.\n+\n+    This policy helps propagate quantization settings for GPTQ\n+    when loading a GPTQ quantized model in Keras format.\n+\n+    Args:\n+        mode: The quantization mode, \"gptq\".\n+        source_name: The source dtype policy name, e.g. \"float32\".\n+        weight_bits: Number of bits to quantize weights to. Supported values\n+            are 2, 3, 4, and 8.\n+        group_size: The group size for quantization. Supported values are\n+            -1 (for whole-tensor quantization) or any positive integer.\n+            Typically a smaller group size leads to better accuracy but\n+            slower speed.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        mode,\n+        source_name=None,\n+        weight_bits=None,\n+        group_size=None,\nComment: I don't think it will work if `weight_bits` is `None` or `group_size` is `None`.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "keras/src/dtype_policies/dtype_policy.py",
    "pr_number": 21641,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2338171832,
    "comment_created_at": "2025-09-10T23:43:27Z"
  },
  {
    "code": "@@ -7,59 +10,61 @@\n class TorchLayer(torch.nn.Module):\n     @property\n     def torch_params(self):\n-        if not hasattr(self, \"_torch_params\"):\n-            self._track_variables()\n-        return self._torch_params\n+        warnings.warn(\n+            \"`layer.torch_param` is deprecated and will be removed in a future \"\n+            \"release. Please use `layer.variables` (Keras style) or \"\n+            \"`layer.named_parameters()` (Torch style) instead.\"\n+        )\n+        return list(self.named_parameters())\n \n     def _post_build(self):\n         # Do not track variables when in a stateless scope.\n         # The variables are not initialized.\n         if in_stateless_scope():\n             return\n-        self._track_variables()\n-\n-    def _track_variables(self):\n-        # set torch_params attribute will have module automatically track\n-        # parameters.\n-        self._torch_params = torch.nn.ParameterDict(\n-            {variable.path: variable.value for variable in self.variables}\n-        )\n-\n-    def named_parameters(\n-        self,\n-        prefix=\"\",\n-        recurse=True,\n-        remove_duplicate=True,\n-    ):\n-        if not hasattr(self, \"_torch_params\"):\n-            self._track_variables()\n-        return torch.nn.Module.named_parameters(\n-            self, prefix, recurse, remove_duplicate\n-        )\n-\n-    def forward(self, *args, **kwargs):\n-        return Operation.__call__(self, *args, **kwargs)\n \n     def _setattr_hook(self, name, value):\n         from keras.src.layers import Layer\n \n-        if (\n-            isinstance(value, torch.nn.Module)\n-            and not isinstance(value, Layer)\n-            and not name == \"_torch_params\"\n-        ):\n+        if isinstance(value, torch.nn.Module) and not isinstance(value, Layer):\n             from keras.src.utils.torch_utils import TorchModuleWrapper\n \n             if not isinstance(self, TorchModuleWrapper):\n                 value = TorchModuleWrapper(value)\n         return name, value\n \n-    def _post_track_variable(self, variable):\n-        if hasattr(self, \"_torch_params\"):\n-            if variable.path not in self.torch_params:\n-                self.torch_params[variable.path] = variable.value\n+    # Overrided torch.nn.Module methods.\n+\n+    def forward(self, *args, **kwargs):\n+        return Operation.__call__(self, *args, **kwargs)\n+\n+    def named_parameters(self, prefix=\"\", recurse=True, remove_duplicate=True):\n+        seen = set()\n+        for layer in self._flatten_layers(\n+            include_self=False, recursive=recurse\n+        ):\n+            for variable in layer.variables:\n+                if remove_duplicate and id(variable) in seen:\n+                    continue\n+                seen.add(id(variable))\n+                name = prefix + variable.path\n+                yield name, variable.value\n+        for variable in self.variables:\n+            if remove_duplicate and id(variable) in seen:\n+                continue\n+            seen.add(id(variable))\n+            name = prefix + variable.path\n+            yield name, variable.value",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe current implementation of named_parameters is inefficient and has a few issues:\n\n1.  **inefficiency and redundancy**: it iterates through _flatten_layers and then separately through self.variables. since self.variables is a recursive property that already includes all variables from sublayers, the first loop is redundant. the seen set prevents duplication, but the approach is unnecessarily complex.\n2.  **incorrect recurse=false behavior**: when recurse=false, _flatten_layers correctly returns only direct child layers. however, the code then calls layer.variables on them, which is a recursive property, thus fetching variables from grandchildren and beyond. this contradicts the expectation for recurse=false.\n3.  **incorrectly includes non-parameters**: the implementation iterates over layer.variables, which includes seedgenerator states. in pytorch, \"parameters\" typically refer only to model weights, so iterating over layer.weights would be more appropriate.\n\ni suggest a simpler and more correct implementation that addresses these points.\n\n",
    "line_number": 57,
    "enriched": "File: keras/src/backend/torch/layer.py\nCode: @@ -7,59 +10,61 @@\n class TorchLayer(torch.nn.Module):\n     @property\n     def torch_params(self):\n-        if not hasattr(self, \"_torch_params\"):\n-            self._track_variables()\n-        return self._torch_params\n+        warnings.warn(\n+            \"`layer.torch_param` is deprecated and will be removed in a future \"\n+            \"release. Please use `layer.variables` (Keras style) or \"\n+            \"`layer.named_parameters()` (Torch style) instead.\"\n+        )\n+        return list(self.named_parameters())\n \n     def _post_build(self):\n         # Do not track variables when in a stateless scope.\n         # The variables are not initialized.\n         if in_stateless_scope():\n             return\n-        self._track_variables()\n-\n-    def _track_variables(self):\n-        # set torch_params attribute will have module automatically track\n-        # parameters.\n-        self._torch_params = torch.nn.ParameterDict(\n-            {variable.path: variable.value for variable in self.variables}\n-        )\n-\n-    def named_parameters(\n-        self,\n-        prefix=\"\",\n-        recurse=True,\n-        remove_duplicate=True,\n-    ):\n-        if not hasattr(self, \"_torch_params\"):\n-            self._track_variables()\n-        return torch.nn.Module.named_parameters(\n-            self, prefix, recurse, remove_duplicate\n-        )\n-\n-    def forward(self, *args, **kwargs):\n-        return Operation.__call__(self, *args, **kwargs)\n \n     def _setattr_hook(self, name, value):\n         from keras.src.layers import Layer\n \n-        if (\n-            isinstance(value, torch.nn.Module)\n-            and not isinstance(value, Layer)\n-            and not name == \"_torch_params\"\n-        ):\n+        if isinstance(value, torch.nn.Module) and not isinstance(value, Layer):\n             from keras.src.utils.torch_utils import TorchModuleWrapper\n \n             if not isinstance(self, TorchModuleWrapper):\n                 value = TorchModuleWrapper(value)\n         return name, value\n \n-    def _post_track_variable(self, variable):\n-        if hasattr(self, \"_torch_params\"):\n-            if variable.path not in self.torch_params:\n-                self.torch_params[variable.path] = variable.value\n+    # Overrided torch.nn.Module methods.\n+\n+    def forward(self, *args, **kwargs):\n+        return Operation.__call__(self, *args, **kwargs)\n+\n+    def named_parameters(self, prefix=\"\", recurse=True, remove_duplicate=True):\n+        seen = set()\n+        for layer in self._flatten_layers(\n+            include_self=False, recursive=recurse\n+        ):\n+            for variable in layer.variables:\n+                if remove_duplicate and id(variable) in seen:\n+                    continue\n+                seen.add(id(variable))\n+                name = prefix + variable.path\n+                yield name, variable.value\n+        for variable in self.variables:\n+            if remove_duplicate and id(variable) in seen:\n+                continue\n+            seen.add(id(variable))\n+            name = prefix + variable.path\n+            yield name, variable.value\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe current implementation of `named_parameters` is inefficient and has a few issues:\n\n1.  **Inefficiency and Redundancy**: It iterates through `_flatten_layers` and then separately through `self.variables`. Since `self.variables` is a recursive property that already includes all variables from sublayers, the first loop is redundant. The `seen` set prevents duplication, but the approach is unnecessarily complex.\n2.  **Incorrect `recurse=False` Behavior**: When `recurse=False`, `_flatten_layers` correctly returns only direct child layers. However, the code then calls `layer.variables` on them, which is a recursive property, thus fetching variables from grandchildren and beyond. This contradicts the expectation for `recurse=False`.\n3.  **Incorrectly Includes Non-Parameters**: The implementation iterates over `layer.variables`, which includes `SeedGenerator` states. In PyTorch, \"parameters\" typically refer only to model weights, so iterating over `layer.weights` would be more appropriate.\n\nI suggest a simpler and more correct implementation that addresses these points.\n\n```suggestion\n    def named_parameters(self, prefix=\"\", recurse=True, remove_duplicate=True):\n        if recurse:\n            # self.weights is recursive and does not include seeds.\n            variables_to_iterate = self.weights\n        else:\n            # Direct weights of this layer.\n            variables_to_iterate = (\n                self._trainable_variables + self._non_trainable_variables\n            )\n\n        seen = set()\n        for variable in variables_to_iterate:\n            if remove_duplicate and id(variable) in seen:\n                continue\n            seen.add(id(variable))\n            name = prefix + variable.path\n            yield name, variable.value\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "keras/src/backend/torch/layer.py",
    "pr_number": 21640,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2330447631,
    "comment_created_at": "2025-09-08T14:33:33Z"
  },
  {
    "code": "@@ -3888,6 +3888,37 @@ def kron(x1, x2):\n     return backend.numpy.kron(x1, x2)\n \n \n+class Lcm(Operation):\n+    def call(self, x1, x2):\n+        return backend.numpy.lcm(x1, x2)\n+\n+    def compute_output_spec(self, x1, x2):\n+        x1_shape = getattr(x1, \"shape\", [])\n+        x2_shape = getattr(x2, \"shape\", [])\n+        output_shape = broadcast_shapes(x1_shape, x2_shape)\n+\n+        x1_type = backend.standardize_dtype(getattr(x1, \"dtype\", type(x1)))\n+        x2_type = backend.standardize_dtype(getattr(x2, \"dtype\", type(x2)))\n+        dtype = dtypes.result_type(x1_type, x2_type)\n+        return KerasTensor(output_shape, dtype=dtype)\n+\n+\n+@keras_export([\"keras.ops.lcm\", \"keras.ops.numpy.lcm\"])\n+def lcm(x1, x2):\n+    \"\"\"Least common multiple of `x1` and `x2`, element-wise.\n+\n+    Args:\n+        x1: First input tensor (integer type).\n+        x2: Second input tensor (integer type).\n+\n+    Returns:\n+        Output tensor, element-wise least common multiple of `x1` and `x2`.\n+    \"\"\"",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe docstring for the lcm function is missing a usage example. according to the keras api design guidelines, all docstrings should include code examples to demonstrate how to use the api.[^1]\n\nplease add an example to illustrate its usage.\n\npython\n    \"\"\"least common multiple of x1 and x2, element-wise.\n\n    args:\n        x1: first input tensor (integer type).\n        x2: second input tensor (integer type).\n\n    returns:\n        output tensor, element-wise least common multiple of x1 and x2.\n\n    examples:\n\n    >>> x1 = keras.ops.array([2, 3, 4])\n    >>> x2 = keras.ops.array([5, 6, 7])\n    >>> keras.ops.lcm(x1, x2)\n    array([10,  6, 28], dtype=int32)\n    \"\"\"\n\n\n#### style guide references\n[^1]: the style guide mandates that all docstrings should include code examples to show how to use the api.",
    "line_number": 3916,
    "enriched": "File: keras/src/ops/numpy.py\nCode: @@ -3888,6 +3888,37 @@ def kron(x1, x2):\n     return backend.numpy.kron(x1, x2)\n \n \n+class Lcm(Operation):\n+    def call(self, x1, x2):\n+        return backend.numpy.lcm(x1, x2)\n+\n+    def compute_output_spec(self, x1, x2):\n+        x1_shape = getattr(x1, \"shape\", [])\n+        x2_shape = getattr(x2, \"shape\", [])\n+        output_shape = broadcast_shapes(x1_shape, x2_shape)\n+\n+        x1_type = backend.standardize_dtype(getattr(x1, \"dtype\", type(x1)))\n+        x2_type = backend.standardize_dtype(getattr(x2, \"dtype\", type(x2)))\n+        dtype = dtypes.result_type(x1_type, x2_type)\n+        return KerasTensor(output_shape, dtype=dtype)\n+\n+\n+@keras_export([\"keras.ops.lcm\", \"keras.ops.numpy.lcm\"])\n+def lcm(x1, x2):\n+    \"\"\"Least common multiple of `x1` and `x2`, element-wise.\n+\n+    Args:\n+        x1: First input tensor (integer type).\n+        x2: Second input tensor (integer type).\n+\n+    Returns:\n+        Output tensor, element-wise least common multiple of `x1` and `x2`.\n+    \"\"\"\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe docstring for the `lcm` function is missing a usage example. According to the Keras API design guidelines, all docstrings should include code examples to demonstrate how to use the API.[^1]\n\nPlease add an example to illustrate its usage.\n\n```python\n    \"\"\"Least common multiple of `x1` and `x2`, element-wise.\n\n    Args:\n        x1: First input tensor (integer type).\n        x2: Second input tensor (integer type).\n\n    Returns:\n        Output tensor, element-wise least common multiple of `x1` and `x2`.\n\n    Examples:\n\n    >>> x1 = keras.ops.array([2, 3, 4])\n    >>> x2 = keras.ops.array([5, 6, 7])\n    >>> keras.ops.lcm(x1, x2)\n    array([10,  6, 28], dtype=int32)\n    \"\"\"\n```\n\n#### Style Guide References\n[^1]: The style guide mandates that all docstrings should include code examples to show how to use the API.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/ops/numpy.py",
    "pr_number": 21636,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2328625282,
    "comment_created_at": "2025-09-07T10:44:57Z"
  },
  {
    "code": "@@ -1711,6 +1711,53 @@ def isposinf(x):\n     return tf.math.equal(x, tf.constant(float(\"inf\"), dtype=x.dtype))\n \n \n+def kron(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    ndim_x1 = tf.rank(x1)\n+    ndim_x2 = tf.rank(x2)\n+\n+    def expand_front(x, num):\n+        for _ in range(num):\n+            x = tf.expand_dims(x, axis=0)\n+        return x\n+\n+    x1 = tf.cond(\n+        ndim_x1 < ndim_x2,\n+        lambda: expand_front(x1, ndim_x2 - ndim_x1),\n+        lambda: x1,\n+    )\n+    x2 = tf.cond(\n+        ndim_x2 < ndim_x1,\n+        lambda: expand_front(x2, ndim_x1 - ndim_x2),\n+        lambda: x2,\n+    )\n+\n+    x1_reshaped = tf.reshape(\n+        x1,\n+        tf.reshape(\n+            tf.stack([tf.shape(x1), tf.ones_like(tf.shape(x1))], axis=1), [-1]\n+        ),\n+    )\n+    x2_reshaped = tf.reshape(\n+        x2,\n+        tf.reshape(\n+            tf.stack([tf.ones_like(tf.shape(x2)), tf.shape(x2)], axis=1), [-1]\n+        ),\n+    )\n+\n+    out = tf.multiply(x1_reshaped, x2_reshaped)\n+\n+    out_shape = tf.shape(x1) * tf.shape(x2)\n+    out = tf.reshape(out, out_shape)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nin the tensorflow backend, the shape of the output tensor is calculated using standard python multiplication (*). however, tensorflow's tf.shape returns a tensorflow tensor, not a python tuple. this could lead to errors during graph construction, as tensorflow needs to evaluate the tensor to determine its value. it's safer to use tf.shape to ensure that the output shape is also a tensorflow tensor.\n\nconsider using tf.concat to combine the shapes instead of the * operator to ensure compatibility with tensorflow's graph execution.\n\npython\nout_shape = tf.concat([tf.shape(x1), tf.shape(x2)], axis=0)\n\n\npython\nout_shape = tf.concat([tf.shape(x1), tf.shape(x2)], axis=0)\n",
    "line_number": 1757,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1711,6 +1711,53 @@ def isposinf(x):\n     return tf.math.equal(x, tf.constant(float(\"inf\"), dtype=x.dtype))\n \n \n+def kron(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    ndim_x1 = tf.rank(x1)\n+    ndim_x2 = tf.rank(x2)\n+\n+    def expand_front(x, num):\n+        for _ in range(num):\n+            x = tf.expand_dims(x, axis=0)\n+        return x\n+\n+    x1 = tf.cond(\n+        ndim_x1 < ndim_x2,\n+        lambda: expand_front(x1, ndim_x2 - ndim_x1),\n+        lambda: x1,\n+    )\n+    x2 = tf.cond(\n+        ndim_x2 < ndim_x1,\n+        lambda: expand_front(x2, ndim_x1 - ndim_x2),\n+        lambda: x2,\n+    )\n+\n+    x1_reshaped = tf.reshape(\n+        x1,\n+        tf.reshape(\n+            tf.stack([tf.shape(x1), tf.ones_like(tf.shape(x1))], axis=1), [-1]\n+        ),\n+    )\n+    x2_reshaped = tf.reshape(\n+        x2,\n+        tf.reshape(\n+            tf.stack([tf.ones_like(tf.shape(x2)), tf.shape(x2)], axis=1), [-1]\n+        ),\n+    )\n+\n+    out = tf.multiply(x1_reshaped, x2_reshaped)\n+\n+    out_shape = tf.shape(x1) * tf.shape(x2)\n+    out = tf.reshape(out, out_shape)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nIn the TensorFlow backend, the shape of the output tensor is calculated using standard Python multiplication (`*`). However, TensorFlow's `tf.shape` returns a TensorFlow tensor, not a Python tuple. This could lead to errors during graph construction, as TensorFlow needs to evaluate the tensor to determine its value. It's safer to use `tf.shape` to ensure that the output shape is also a TensorFlow tensor.\n\nConsider using `tf.concat` to combine the shapes instead of the `*` operator to ensure compatibility with TensorFlow's graph execution.\n\n```python\nout_shape = tf.concat([tf.shape(x1), tf.shape(x2)], axis=0)\n```\n\n```python\nout_shape = tf.concat([tf.shape(x1), tf.shape(x2)], axis=0)\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21633,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2318970325,
    "comment_created_at": "2025-09-03T13:23:09Z"
  },
  {
    "code": "@@ -3,7 +3,9 @@\n if is_nnx_enabled():\n     from flax import nnx\n \n-    BaseLayer = nnx.Module\n+    class BaseLayer(nnx.Module):\n+        def __init_subclass__(cls, **kwargs):\n+            super().__init_subclass__(pytree=False, **kwargs)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis new baselayer class is a good change to control how keras layers interact with jax's pytree system when nnx is enabled. to improve maintainability and help future developers understand the reasoning behind this implementation, consider adding a docstring to the baselayer class explaining why pytree=false is necessary. this aligns with the keras api design guidelines regarding documentation.[^1]\n\n\n\n#### style guide references\n[^1]: documentation is an integral part of the api and user experience. while this is an internal class, a docstring explaining the purpose of this class and the pytree=false setting would be very helpful for maintainability. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#l137-l137)",
    "line_number": 8,
    "enriched": "File: keras/src/backend/jax/layer.py\nCode: @@ -3,7 +3,9 @@\n if is_nnx_enabled():\n     from flax import nnx\n \n-    BaseLayer = nnx.Module\n+    class BaseLayer(nnx.Module):\n+        def __init_subclass__(cls, **kwargs):\n+            super().__init_subclass__(pytree=False, **kwargs)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis new `BaseLayer` class is a good change to control how Keras layers interact with JAX's PyTree system when NNX is enabled. To improve maintainability and help future developers understand the reasoning behind this implementation, consider adding a docstring to the `BaseLayer` class explaining why `pytree=False` is necessary. This aligns with the Keras API design guidelines regarding documentation.[^1]\n\n```suggestion\n    class BaseLayer(nnx.Module):\n        \"\"\"Base layer for JAX backend with NNX.\n\n        This class ensures that Keras layers are not registered as PyTrees,\n        allowing Keras to manage its own state tracking without conflicting\n        with JAX's tree traversal mechanisms.\n        \"\"\"\n\n        def __init_subclass__(cls, **kwargs):\n            super().__init_subclass__(pytree=False, **kwargs)\n```\n\n#### Style Guide References\n[^1]: Documentation is an integral part of the API and user experience. While this is an internal class, a docstring explaining the purpose of this class and the `pytree=False` setting would be very helpful for maintainability. [(link)](https://github.com/keras-team/keras/blob/master/.gemini/styleguide.md?plain=1#L137-L137)",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/backend/jax/layer.py",
    "pr_number": 21632,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2317242270,
    "comment_created_at": "2025-09-02T21:47:04Z"
  },
  {
    "code": "@@ -68,7 +68,9 @@ def train_step(self, data):\n             )\n             self._loss_tracker.update_state(\n                 loss_module.unscale_loss_for_distribution(loss),\n-                sample_weight=tf.shape(tree.flatten(x)[0])[0],\n+                sample_weight=tf.shape(\n+                    next(i for i in tree.flatten(x) if i is not None)\n+                )[0],",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis change correctly handles cases where the first input is none. however, it introduces a risk of a stopiteration error if all inputs in x are none. this can be difficult to debug, especially inside a tf.function.\n\na more robust approach would be to handle this edge case explicitly, for example by raising a valueerror with a clear message.\n\nalso, this logic is duplicated in test_step. consider extracting it into a private helper method to improve maintainability and ensure consistency.",
    "line_number": 73,
    "enriched": "File: keras/src/backend/tensorflow/trainer.py\nCode: @@ -68,7 +68,9 @@ def train_step(self, data):\n             )\n             self._loss_tracker.update_state(\n                 loss_module.unscale_loss_for_distribution(loss),\n-                sample_weight=tf.shape(tree.flatten(x)[0])[0],\n+                sample_weight=tf.shape(\n+                    next(i for i in tree.flatten(x) if i is not None)\n+                )[0],\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis change correctly handles cases where the first input is `None`. However, it introduces a risk of a `StopIteration` error if all inputs in `x` are `None`. This can be difficult to debug, especially inside a `tf.function`.\n\nA more robust approach would be to handle this edge case explicitly, for example by raising a `ValueError` with a clear message.\n\nAlso, this logic is duplicated in `test_step`. Consider extracting it into a private helper method to improve maintainability and ensure consistency.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/trainer.py",
    "pr_number": 21630,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2312665471,
    "comment_created_at": "2025-08-31T23:05:13Z"
  },
  {
    "code": "@@ -9,15 +10,109 @@\n from keras.src.layers import EinsumDense\n from keras.src.layers import Embedding\n from keras.src.quantizers.gptq import GPTQ\n-from keras.src.quantizers.gptq_quant import GPTQQuantization\n \n \n-def get_dataloader(tokenizer, sequence_length, dataset, num_samples=128):\n+@contextmanager\n+def stream_hessians(layers_map, gptq_objects):\n     \"\"\"\n-    Prepares and chunks the calibration dataloader, repeating short datasets.\n+    Temporarily monkey-patch each target layer's `call` method so\n+    that input activations are streamed into the GPTQ instance\n+    running Hessian estimate at capture time.\n+\n+    On `__enter__`: For every (name, layer) in `layers_map`, replaces\n+     `layer.call` with a wrapper that:\n+     1) extracts the layer input from `*args`/`**kwargs`,\n+     2) reshapes it to 2D `[-1, rows]` where\n+      `rows = gptq_objects[name].rows`,\n+     3) calls `gptq_objects[name].update_hessian_with_batch(x2d)`\n+     4) delegates to the original `layer.call` and returns its\n+      output.\n+\n+    On `__exit__`: All original `layer.call` methods are restored even if an\n+     exception occurs.\n+\n+    * Space complexity: O(d**2) per layer (for the Hessian).\n+    * No weights are modified; only GPTQ statistics are updated.\n+\n+    Args:\n+        layers_map: Dict[str, Layer]. Mapping from logical layer names to\n+         the Keras layers that should be patched during calibration. Keys must\n+         match `gptq_objects`.\n+        gptq_objects: Dict[str, GPTQ]. Mapping from names to GPTQ instances.\n+\n+    Yields:\n+        None: The patched state is active only within the `with` block. After\n+         exit, all layers are unpatched and safe to use normally.\n+\n+    Example:\n+    ```python\n+    >>> with stream_hessians(layers_map, gptq_objects):\n+    ...     for sample in calibration_inputs:\n+    ...         if len(sample.shape) == 2:\n+    ...             sample = ops.expand_dims(sample, 0)\n+    ...         _ = block(sample)   # hooks update Hessians on-the-fly\n+    # <- original layer.call methods restored here",
    "comment": "don't you need >>> here ?",
    "line_number": 54,
    "enriched": "File: keras/src/quantizers/gptq_core.py\nCode: @@ -9,15 +10,109 @@\n from keras.src.layers import EinsumDense\n from keras.src.layers import Embedding\n from keras.src.quantizers.gptq import GPTQ\n-from keras.src.quantizers.gptq_quant import GPTQQuantization\n \n \n-def get_dataloader(tokenizer, sequence_length, dataset, num_samples=128):\n+@contextmanager\n+def stream_hessians(layers_map, gptq_objects):\n     \"\"\"\n-    Prepares and chunks the calibration dataloader, repeating short datasets.\n+    Temporarily monkey-patch each target layer's `call` method so\n+    that input activations are streamed into the GPTQ instance\n+    running Hessian estimate at capture time.\n+\n+    On `__enter__`: For every (name, layer) in `layers_map`, replaces\n+     `layer.call` with a wrapper that:\n+     1) extracts the layer input from `*args`/`**kwargs`,\n+     2) reshapes it to 2D `[-1, rows]` where\n+      `rows = gptq_objects[name].rows`,\n+     3) calls `gptq_objects[name].update_hessian_with_batch(x2d)`\n+     4) delegates to the original `layer.call` and returns its\n+      output.\n+\n+    On `__exit__`: All original `layer.call` methods are restored even if an\n+     exception occurs.\n+\n+    * Space complexity: O(d**2) per layer (for the Hessian).\n+    * No weights are modified; only GPTQ statistics are updated.\n+\n+    Args:\n+        layers_map: Dict[str, Layer]. Mapping from logical layer names to\n+         the Keras layers that should be patched during calibration. Keys must\n+         match `gptq_objects`.\n+        gptq_objects: Dict[str, GPTQ]. Mapping from names to GPTQ instances.\n+\n+    Yields:\n+        None: The patched state is active only within the `with` block. After\n+         exit, all layers are unpatched and safe to use normally.\n+\n+    Example:\n+    ```python\n+    >>> with stream_hessians(layers_map, gptq_objects):\n+    ...     for sample in calibration_inputs:\n+    ...         if len(sample.shape) == 2:\n+    ...             sample = ops.expand_dims(sample, 0)\n+    ...         _ = block(sample)   # hooks update Hessians on-the-fly\n+    # <- original layer.call methods restored here\nComment: Don't you need `>>>` here ?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "keras/src/quantizers/gptq_core.py",
    "pr_number": 21628,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2317538904,
    "comment_created_at": "2025-09-03T01:25:01Z"
  },
  {
    "code": "@@ -104,3 +104,52 @@ def test_tf_data_compatibility(self):\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(4).map(layer)\n         output = next(iter(ds)).numpy()\n         self.assertAllClose(output, np.array([2, 3, 4, 0]))\n+\n+    def test_one_hot_output_with_higher_rank_input(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        layer = layers.IntegerLookup(\n+            vocabulary=vocabulary, output_mode=\"one_hot\"\n+        )\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 2, 4))\n+        expected_output = np.array(\n+            [\n+                [[0, 1, 0, 0], [0, 0, 1, 0]],\n+                [[0, 0, 0, 1], [1, 0, 0, 0]],\n+            ]\n+        )\n+        self.assertAllClose(output_data, expected_output)\n+        output_data_3d = layer(np.expand_dims(input_data, axis=0))\n+        self.assertEqual(output_data_3d.shape, (1, 2, 2, 4))\n+        self.assertAllClose(\n+            output_data_3d, np.expand_dims(expected_output, axis=0)\n+        )\n+\n+    def test_multi_hot_output_shape(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        layer = layers.IntegerLookup(\n+            vocabulary=vocabulary, output_mode=\"multi_hot\"\n+        )\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 4))\n+\n+    def test_count_output_shape(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        layer = layers.IntegerLookup(vocabulary=vocabulary, output_mode=\"count\")\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 4))\n+\n+    def test_tf_idf_output_shape(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        idf_weights = [1.0, 1.0, 1.0]\n+        layer = layers.IntegerLookup(\n+            vocabulary=vocabulary,\n+            idf_weights=idf_weights,\n+            output_mode=\"tf_idf\",\n+        )\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 4))",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nto improve maintainability and reduce code duplication, you can refactor these three tests (test_multi_hot_output_shape, test_count_output_shape, test_tf_idf_output_shape) into a single private helper method that is called by the individual test cases. this consolidates the common setup and assertion logic.\n\n",
    "line_number": 155,
    "enriched": "File: keras/src/layers/preprocessing/integer_lookup_test.py\nCode: @@ -104,3 +104,52 @@ def test_tf_data_compatibility(self):\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(4).map(layer)\n         output = next(iter(ds)).numpy()\n         self.assertAllClose(output, np.array([2, 3, 4, 0]))\n+\n+    def test_one_hot_output_with_higher_rank_input(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        layer = layers.IntegerLookup(\n+            vocabulary=vocabulary, output_mode=\"one_hot\"\n+        )\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 2, 4))\n+        expected_output = np.array(\n+            [\n+                [[0, 1, 0, 0], [0, 0, 1, 0]],\n+                [[0, 0, 0, 1], [1, 0, 0, 0]],\n+            ]\n+        )\n+        self.assertAllClose(output_data, expected_output)\n+        output_data_3d = layer(np.expand_dims(input_data, axis=0))\n+        self.assertEqual(output_data_3d.shape, (1, 2, 2, 4))\n+        self.assertAllClose(\n+            output_data_3d, np.expand_dims(expected_output, axis=0)\n+        )\n+\n+    def test_multi_hot_output_shape(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        layer = layers.IntegerLookup(\n+            vocabulary=vocabulary, output_mode=\"multi_hot\"\n+        )\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 4))\n+\n+    def test_count_output_shape(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        layer = layers.IntegerLookup(vocabulary=vocabulary, output_mode=\"count\")\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 4))\n+\n+    def test_tf_idf_output_shape(self):\n+        input_data = np.array([[1, 2], [3, 0]])\n+        vocabulary = [1, 2, 3]\n+        idf_weights = [1.0, 1.0, 1.0]\n+        layer = layers.IntegerLookup(\n+            vocabulary=vocabulary,\n+            idf_weights=idf_weights,\n+            output_mode=\"tf_idf\",\n+        )\n+        output_data = layer(input_data)\n+        self.assertEqual(output_data.shape, (2, 4))\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nTo improve maintainability and reduce code duplication, you can refactor these three tests (`test_multi_hot_output_shape`, `test_count_output_shape`, `test_tf_idf_output_shape`) into a single private helper method that is called by the individual test cases. This consolidates the common setup and assertion logic.\n\n```suggestion\n    def _test_output_shape_for_mode(self, output_mode):\n        input_data = np.array([[1, 2], [3, 0]])\n        vocabulary = [1, 2, 3]\n        kwargs = {\"vocabulary\": vocabulary, \"output_mode\": output_mode}\n        if output_mode == \"tf_idf\":\n            kwargs[\"idf_weights\"] = [1.0, 1.0, 1.0]\n        layer = layers.IntegerLookup(**kwargs)\n        output_data = layer(input_data)\n        self.assertEqual(output_data.shape, (2, 4))\n\n    def test_multi_hot_output_shape(self):\n        self._test_output_shape_for_mode(\"multi_hot\")\n\n    def test_count_output_shape(self):\n        self._test_output_shape_for_mode(\"count\")\n\n    def test_tf_idf_output_shape(self):\n        self._test_output_shape_for_mode(\"tf_idf\")\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/layers/preprocessing/integer_lookup_test.py",
    "pr_number": 21625,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2309280219,
    "comment_created_at": "2025-08-29T06:30:29Z"
  },
  {
    "code": "@@ -1531,6 +1531,44 @@ def full_like(x, fill_value, dtype=None):\n     return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n+def gcd(x1, x2):\n+    x1 = tf.convert_to_tensor(x1)\n+    x2 = tf.convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    if not x1.dtype.is_integer:\n+        raise TypeError(\"Arguments to gcd must be integers.\")\n+\n+    target_shape = tf.broadcast_static_shape(x1.shape, x2.shape)\n+    x1 = tf.broadcast_to(x1, target_shape)\n+    x2 = tf.broadcast_to(x2, target_shape)\n+\n+    def cond(a, b):\n+        return tf.reduce_any(b != 0)\n+\n+    def body(a, b):\n+        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n+        a, b = (\n+            tf.where(tf.not_equal(b, 0), b, a),\n+            tf.where(\n+                tf.not_equal(b, 0),\n+                tf.math.floormod(a, b_safe),\n+                tf.zeros_like(b),\n+            ),\n+        )\n+        return (tf.where(a < b, b, a), tf.where(a < b, a, b))",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe sorting of a and b at the end of the body function is redundant. the euclidean algorithm, which this function implements, works correctly without requiring a >= b at each step, especially when using floormod. if a < b, the first iteration of a, b = b, a % b effectively swaps them. removing this sorting step simplifies the code and improves performance by avoiding two tf.where operations in each loop iteration.\n\n",
    "line_number": 1562,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1531,6 +1531,44 @@ def full_like(x, fill_value, dtype=None):\n     return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n+def gcd(x1, x2):\n+    x1 = tf.convert_to_tensor(x1)\n+    x2 = tf.convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    if not x1.dtype.is_integer:\n+        raise TypeError(\"Arguments to gcd must be integers.\")\n+\n+    target_shape = tf.broadcast_static_shape(x1.shape, x2.shape)\n+    x1 = tf.broadcast_to(x1, target_shape)\n+    x2 = tf.broadcast_to(x2, target_shape)\n+\n+    def cond(a, b):\n+        return tf.reduce_any(b != 0)\n+\n+    def body(a, b):\n+        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n+        a, b = (\n+            tf.where(tf.not_equal(b, 0), b, a),\n+            tf.where(\n+                tf.not_equal(b, 0),\n+                tf.math.floormod(a, b_safe),\n+                tf.zeros_like(b),\n+            ),\n+        )\n+        return (tf.where(a < b, b, a), tf.where(a < b, a, b))\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe sorting of `a` and `b` at the end of the `body` function is redundant. The Euclidean algorithm, which this function implements, works correctly without requiring `a >= b` at each step, especially when using `floormod`. If `a < b`, the first iteration of `a, b = b, a % b` effectively swaps them. Removing this sorting step simplifies the code and improves performance by avoiding two `tf.where` operations in each loop iteration.\n\n```suggestion\n    def body(a, b):\n        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n        a, b = (\n            tf.where(tf.not_equal(b, 0), b, a),\n            tf.where(\n                tf.not_equal(b, 0),\n                tf.math.floormod(a, b_safe),\n                tf.zeros_like(b),\n            ),\n        )\n        return a, b\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21623,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2307550199,
    "comment_created_at": "2025-08-28T14:13:56Z"
  },
  {
    "code": "@@ -1531,6 +1531,44 @@ def full_like(x, fill_value, dtype=None):\n     return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n+def gcd(x1, x2):\n+    x1 = tf.convert_to_tensor(x1)\n+    x2 = tf.convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    if not x1.dtype.is_integer:\n+        raise TypeError(\"Arguments to gcd must be integers.\")\n+\n+    target_shape = tf.broadcast_static_shape(x1.shape, x2.shape)\n+    x1 = tf.broadcast_to(x1, target_shape)\n+    x2 = tf.broadcast_to(x2, target_shape)\n+\n+    def cond(a, b):\n+        return tf.reduce_any(b != 0)\n+\n+    def body(a, b):\n+        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n+        a, b = (\n+            tf.where(tf.not_equal(b, 0), b, a),\n+            tf.where(\n+                tf.not_equal(b, 0),\n+                tf.math.floormod(a, b_safe),\n+                tf.zeros_like(b),\n+            ),\n+        )\n+        return tf.maximum(a, b), tf.minimum(a, b)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe tf.maximum and tf.minimum calls here are redundant. the a and b variables have already been updated with the next values for the euclidean algorithm on the preceding lines. the algorithm works correctly without requiring a >= b. you can simplify this by directly returning the updated a and b, which improves readability and may offer a minor performance improvement.\n\n",
    "line_number": 1562,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1531,6 +1531,44 @@ def full_like(x, fill_value, dtype=None):\n     return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n+def gcd(x1, x2):\n+    x1 = tf.convert_to_tensor(x1)\n+    x2 = tf.convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    if not x1.dtype.is_integer:\n+        raise TypeError(\"Arguments to gcd must be integers.\")\n+\n+    target_shape = tf.broadcast_static_shape(x1.shape, x2.shape)\n+    x1 = tf.broadcast_to(x1, target_shape)\n+    x2 = tf.broadcast_to(x2, target_shape)\n+\n+    def cond(a, b):\n+        return tf.reduce_any(b != 0)\n+\n+    def body(a, b):\n+        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n+        a, b = (\n+            tf.where(tf.not_equal(b, 0), b, a),\n+            tf.where(\n+                tf.not_equal(b, 0),\n+                tf.math.floormod(a, b_safe),\n+                tf.zeros_like(b),\n+            ),\n+        )\n+        return tf.maximum(a, b), tf.minimum(a, b)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe `tf.maximum` and `tf.minimum` calls here are redundant. The `a` and `b` variables have already been updated with the next values for the Euclidean algorithm on the preceding lines. The algorithm works correctly without requiring `a >= b`. You can simplify this by directly returning the updated `a` and `b`, which improves readability and may offer a minor performance improvement.\n\n```suggestion\n        return a, b\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21622,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2307504359,
    "comment_created_at": "2025-08-28T13:57:32Z"
  },
  {
    "code": "@@ -950,10 +958,14 @@ def map_coordinates(\n         )\n     else:\n         padded = np.pad(inputs, padding, mode=pad_mode)\n+\n+    # `scipy.ndimage.map_coordinates` lacks support for float16 and bfloat16.\n+    if backend.is_float_dtype(padded.dtype):\n+        padded = padded.astype(\"float32\")",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe current implementation for handling dtypes in map_coordinates unconditionally casts any float type to float32. this could lead to a loss of precision if the input is float64. other functions in this file, like affine_transform, were updated to use backend.result_type(input_dtype, \"float32\") to determine the computation dtype. this correctly preserves float64 while upcasting lower precision floats. for consistency and to avoid potential precision loss, map_coordinates should adopt the same approach.\n\npython\n    if backend.is_float_dtype(padded.dtype):\n        compute_dtype = backend.result_type(padded.dtype, \"float32\")\n        padded = padded.astype(compute_dtype)\n",
    "line_number": 964,
    "enriched": "File: keras/src/backend/numpy/image.py\nCode: @@ -950,10 +958,14 @@ def map_coordinates(\n         )\n     else:\n         padded = np.pad(inputs, padding, mode=pad_mode)\n+\n+    # `scipy.ndimage.map_coordinates` lacks support for float16 and bfloat16.\n+    if backend.is_float_dtype(padded.dtype):\n+        padded = padded.astype(\"float32\")\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe current implementation for handling dtypes in `map_coordinates` unconditionally casts any float type to `float32`. This could lead to a loss of precision if the input is `float64`. Other functions in this file, like `affine_transform`, were updated to use `backend.result_type(input_dtype, \"float32\")` to determine the computation dtype. This correctly preserves `float64` while upcasting lower precision floats. For consistency and to avoid potential precision loss, `map_coordinates` should adopt the same approach.\n\n```python\n    if backend.is_float_dtype(padded.dtype):\n        compute_dtype = backend.result_type(padded.dtype, \"float32\")\n        padded = padded.astype(compute_dtype)\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/numpy/image.py",
    "pr_number": 21621,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2307490737,
    "comment_created_at": "2025-08-28T13:52:40Z"
  },
  {
    "code": "@@ -1551,9 +1550,22 @@ def cond(a, b):\n         return tf.reduce_any(b != 0)\n \n     def body(a, b):\n-        return b, tf.math.floormod(a, b)\n+        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n+        a, b = (\n+            tf.where(tf.not_equal(b, 0), b, a),\n+            tf.where(\n+                tf.not_equal(b, 0),\n+                tf.math.floormod(a, b_safe),\n+                tf.zeros_like(b),\n+            ),\n+        )\n+        return (tf.where(a < b, b, a), tf.where(a < b, a, b))",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nfor improved readability and to make the intent of sorting the pair (a, b) more explicit, you could use tf.maximum and tf.minimum here. the current implementation is correct, but this change would make the code slightly more self-documenting.\n\n",
    "line_number": 1562,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1551,9 +1550,22 @@ def cond(a, b):\n         return tf.reduce_any(b != 0)\n \n     def body(a, b):\n-        return b, tf.math.floormod(a, b)\n+        b_safe = tf.where(tf.equal(b, 0), tf.ones_like(b), b)\n+        a, b = (\n+            tf.where(tf.not_equal(b, 0), b, a),\n+            tf.where(\n+                tf.not_equal(b, 0),\n+                tf.math.floormod(a, b_safe),\n+                tf.zeros_like(b),\n+            ),\n+        )\n+        return (tf.where(a < b, b, a), tf.where(a < b, a, b))\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nFor improved readability and to make the intent of sorting the pair `(a, b)` more explicit, you could use `tf.maximum` and `tf.minimum` here. The current implementation is correct, but this change would make the code slightly more self-documenting.\n\n```suggestion\n        return (tf.maximum(a, b), tf.minimum(a, b))\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21620,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2307460007,
    "comment_created_at": "2025-08-28T13:41:40Z"
  },
  {
    "code": "@@ -0,0 +1,402 @@\n+# Style Guide\n+\n+## General Principles\n+\n+APIs added should be:\n+- Scalable\n+- Robust\n+- Backwards compatible\n+- Future-proof\n+- Interoperable\n+- Must work across all backends\n+\n+---\n+\n+# Keras API design guidelines\n+\n+These guidelines are meant to help focus design discussions and help us create d\n+elightful developer experiences.",
    "comment": "word is cut \"delightful\"",
    "line_number": 18,
    "enriched": "File: .gemini/styleguide.md\nCode: @@ -0,0 +1,402 @@\n+# Style Guide\n+\n+## General Principles\n+\n+APIs added should be:\n+- Scalable\n+- Robust\n+- Backwards compatible\n+- Future-proof\n+- Interoperable\n+- Must work across all backends\n+\n+---\n+\n+# Keras API design guidelines\n+\n+These guidelines are meant to help focus design discussions and help us create d\n+elightful developer experiences.\nComment: word is cut \"delightful\"",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": ".gemini/styleguide.md",
    "pr_number": 21619,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2305328453,
    "comment_created_at": "2025-08-27T21:24:42Z"
  },
  {
    "code": "@@ -1531,6 +1531,32 @@ def full_like(x, fill_value, dtype=None):\n     return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n+def gcd(x1, x2):\n+    x1 = tf.convert_to_tensor(x1)\n+    x2 = tf.convert_to_tensor(x2)\n+\n+    if x1.dtype.size < x2.dtype.size:\n+        x1 = tf.cast(x1, x2.dtype)\n+    elif x2.dtype.size < x1.dtype.size:\n+        x2 = tf.cast(x2, x1.dtype)\n+\n+    if not x1.dtype.is_integer:\n+        raise TypeError(\"Arguments to gcd must be integers.\")\n+\n+    target_shape = tf.broadcast_static_shape(x1.shape, x2.shape)\n+    x1 = tf.broadcast_to(x1, target_shape)\n+    x2 = tf.broadcast_to(x2, target_shape)\n+\n+    def cond(a, b):\n+        return tf.reduce_any(b != 0)\n+\n+    def body(a, b):\n+        return b, tf.math.floormod(a, b)\n+\n+    gcd_val, _ = tf.while_loop(cond, body, [tf.abs(x1), tf.abs(x2)])\n+    return gcd_val",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe custom implementation of gcd using tf.while_loop is susceptible to division-by-zero errors and is overly complex. the tf.math.floormod(a, b) call within the loop body will fail if any element in b becomes zero during the execution of the euclidean algorithm, which can happen for certain inputs. a simpler and more robust approach is to use the native tf.math.gcd function, which correctly handles edge cases like gcd(x, 0) and is optimized for performance. this also makes the implementation consistent with other backends that use their native gcd functions. the suggested implementation also aligns with the type promotion logic used in other binary operations in this file.\n\n",
    "line_number": 1557,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1531,6 +1531,32 @@ def full_like(x, fill_value, dtype=None):\n     return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n+def gcd(x1, x2):\n+    x1 = tf.convert_to_tensor(x1)\n+    x2 = tf.convert_to_tensor(x2)\n+\n+    if x1.dtype.size < x2.dtype.size:\n+        x1 = tf.cast(x1, x2.dtype)\n+    elif x2.dtype.size < x1.dtype.size:\n+        x2 = tf.cast(x2, x1.dtype)\n+\n+    if not x1.dtype.is_integer:\n+        raise TypeError(\"Arguments to gcd must be integers.\")\n+\n+    target_shape = tf.broadcast_static_shape(x1.shape, x2.shape)\n+    x1 = tf.broadcast_to(x1, target_shape)\n+    x2 = tf.broadcast_to(x2, target_shape)\n+\n+    def cond(a, b):\n+        return tf.reduce_any(b != 0)\n+\n+    def body(a, b):\n+        return b, tf.math.floormod(a, b)\n+\n+    gcd_val, _ = tf.while_loop(cond, body, [tf.abs(x1), tf.abs(x2)])\n+    return gcd_val\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe custom implementation of `gcd` using `tf.while_loop` is susceptible to division-by-zero errors and is overly complex. The `tf.math.floormod(a, b)` call within the loop body will fail if any element in `b` becomes zero during the execution of the Euclidean algorithm, which can happen for certain inputs. A simpler and more robust approach is to use the native `tf.math.gcd` function, which correctly handles edge cases like `gcd(x, 0)` and is optimized for performance. This also makes the implementation consistent with other backends that use their native `gcd` functions. The suggested implementation also aligns with the type promotion logic used in other binary operations in this file.\n\n```suggestion\ndef gcd(x1, x2):\n    x1 = convert_to_tensor(x1)\n    x2 = convert_to_tensor(x2)\n    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n    if \"int\" not in standardize_dtype(dtype):\n        raise TypeError(\"Arguments to gcd must be integers.\")\n\n    # tf.math.gcd does not support int8\n    promoted_dtype = dtype\n    if standardize_dtype(promoted_dtype) == \"int8\":\n        promoted_dtype = \"int16\"\n\n    x1 = cast(x1, promoted_dtype)\n    x2 = cast(x2, promoted_dtype)\n    return tf.math.gcd(x1, x2)\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21616,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2303560883,
    "comment_created_at": "2025-08-27T10:48:04Z"
  },
  {
    "code": "@@ -130,8 +130,33 @@ def __new__(cls, *args, **kwargs):\n                 vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n \n         # Generate a config to be returned by default by `get_config()`.\n-        arg_names = inspect.getfullargspec(cls.__init__).args\n-        kwargs.update(dict(zip(arg_names[1 : len(args) + 1], args)))\n+        signature = inspect.signature(cls.__init__)\n+        argspec = inspect.getfullargspec(cls.__init__)\n+\n+        try:\n+            bound_parameters = signature.bind(None, *args, **kwargs)\n+            # Include default values in the config.\n+            bound_parameters.apply_defaults()\n+            # Extract all arguments as a dictionary.\n+            kwargs = bound_parameters.arguments\n+            # Expand variable kwargs argument.\n+            kwargs |= kwargs.pop(argspec.varkw, {})",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nif cls.__init__ does not have a variable keyword argument (**kwargs), argspec.varkw will be none. in this case, kwargs.pop(none, {}) will raise a keyerror, causing a crash. you should guard this operation with a check to ensure argspec.varkw is not none.\n\n",
    "line_number": 143,
    "enriched": "File: keras/src/ops/operation.py\nCode: @@ -130,8 +130,33 @@ def __new__(cls, *args, **kwargs):\n                 vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n \n         # Generate a config to be returned by default by `get_config()`.\n-        arg_names = inspect.getfullargspec(cls.__init__).args\n-        kwargs.update(dict(zip(arg_names[1 : len(args) + 1], args)))\n+        signature = inspect.signature(cls.__init__)\n+        argspec = inspect.getfullargspec(cls.__init__)\n+\n+        try:\n+            bound_parameters = signature.bind(None, *args, **kwargs)\n+            # Include default values in the config.\n+            bound_parameters.apply_defaults()\n+            # Extract all arguments as a dictionary.\n+            kwargs = bound_parameters.arguments\n+            # Expand variable kwargs argument.\n+            kwargs |= kwargs.pop(argspec.varkw, {})\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nIf `cls.__init__` does not have a variable keyword argument (`**kwargs`), `argspec.varkw` will be `None`. In this case, `kwargs.pop(None, {})` will raise a `KeyError`, causing a crash. You should guard this operation with a check to ensure `argspec.varkw` is not `None`.\n\n```suggestion\n            if argspec.varkw is not None:\n                kwargs |= kwargs.pop(argspec.varkw, {})\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/ops/operation.py",
    "pr_number": 21615,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2301689611,
    "comment_created_at": "2025-08-26T17:41:33Z"
  },
  {
    "code": "@@ -382,13 +384,24 @@ def get_config(self):\n         return {**base_config, **config}\n \n     def call(self, inputs):\n-        if isinstance(inputs, (tf.Tensor, tf.RaggedTensor, tf.SparseTensor)):\n-            tf_inputs = True\n+        original_torch = False\n+        if(backend.backend() == \"torch\" and isinstance(inputs, torch.Tensor)):\n+            original_torch = True\n+            numpy_inputs = inputs.detach().cpu().numpy()\n+            inputs_for_processing = tf.convert_to_tensor(numpy_inputs)\n+        elif isinstance(inputs, (np.ndarray, list, tuple)):\n+            if backend.backend() == \"torch\":\n+                original_torch = True\n+            inputs_for_processing = tf.convert_to_tensor(inputs)\n         else:\n-            tf_inputs = False\n-            if not isinstance(inputs, (np.ndarray, list, tuple)):\n-                inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = super().call(inputs)\n-        if not tf_inputs:\n-            outputs = backend_utils.convert_tf_tensor(outputs)\n-        return outputs\n+            inputs_for_processing = inputs\n+        output = super().call(inputs_for_processing)\n+        if not original_torch:\n+            return output\n+        numpy_outputs = output.numpy()\n+        if self.invert:\n+            return [n.decode(self.encoding) for n in numpy_outputs]\n+            # This returns a list[str] to make it equivalent to the torch implementation of this.\n+            # See : https://docs.pytorch.org/text/stable/_modules/torchtext/vocab/vocab.html#Vocab.lookup_tokens\n+        else:\n+            return torch.from_numpy(numpy_outputs)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe logic in the call method can be refactored for better readability and to fix an unreachable comment. the current implementation has complex conditional logic for handling different input types and backends. a clearer structure would be to first handle input conversion, then call the parent method, and finally handle output conversion if necessary. this also resolves the issue of having a comment after a return statement, which is unreachable code.\n\nhere is a suggested refactoring that improves clarity while preserving the intended logic:\n\npython\n    def call(self, inputs):\n        is_torch_backend = backend.backend() == \"torch\"\n\n        inputs_for_processing = inputs\n        if is_torch_backend and isinstance(inputs, torch.tensor):\n            inputs_for_processing = tf.convert_to_tensor(\n                inputs.detach().cpu().numpy()\n            )\n        elif isinstance(inputs, (np.ndarray, list, tuple)):\n            inputs_for_processing = tf.convert_to_tensor(inputs)\n\n        output = super().call(inputs_for_processing)\n\n        if is_torch_backend and isinstance(\n            inputs, (torch.tensor, np.ndarray, list, tuple)\n        ):\n            numpy_outputs = output.numpy()\n            if self.invert:\n                # this returns a list[str] to make it equivalent to the torch\n                # implementation of this.\n                # see : https://docs.pytorch.org/text/stable/_modules/torchtext/vocab/vocab.html#vocab.lookup_tokens\n                return [n.decode(self.encoding) for n in numpy_outputs]\n            else:\n                return torch.from_numpy(numpy_outputs)\n        return output\n",
    "line_number": 407,
    "enriched": "File: keras/src/layers/preprocessing/string_lookup.py\nCode: @@ -382,13 +384,24 @@ def get_config(self):\n         return {**base_config, **config}\n \n     def call(self, inputs):\n-        if isinstance(inputs, (tf.Tensor, tf.RaggedTensor, tf.SparseTensor)):\n-            tf_inputs = True\n+        original_torch = False\n+        if(backend.backend() == \"torch\" and isinstance(inputs, torch.Tensor)):\n+            original_torch = True\n+            numpy_inputs = inputs.detach().cpu().numpy()\n+            inputs_for_processing = tf.convert_to_tensor(numpy_inputs)\n+        elif isinstance(inputs, (np.ndarray, list, tuple)):\n+            if backend.backend() == \"torch\":\n+                original_torch = True\n+            inputs_for_processing = tf.convert_to_tensor(inputs)\n         else:\n-            tf_inputs = False\n-            if not isinstance(inputs, (np.ndarray, list, tuple)):\n-                inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = super().call(inputs)\n-        if not tf_inputs:\n-            outputs = backend_utils.convert_tf_tensor(outputs)\n-        return outputs\n+            inputs_for_processing = inputs\n+        output = super().call(inputs_for_processing)\n+        if not original_torch:\n+            return output\n+        numpy_outputs = output.numpy()\n+        if self.invert:\n+            return [n.decode(self.encoding) for n in numpy_outputs]\n+            # This returns a list[str] to make it equivalent to the torch implementation of this.\n+            # See : https://docs.pytorch.org/text/stable/_modules/torchtext/vocab/vocab.html#Vocab.lookup_tokens\n+        else:\n+            return torch.from_numpy(numpy_outputs)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe logic in the `call` method can be refactored for better readability and to fix an unreachable comment. The current implementation has complex conditional logic for handling different input types and backends. A clearer structure would be to first handle input conversion, then call the parent method, and finally handle output conversion if necessary. This also resolves the issue of having a comment after a `return` statement, which is unreachable code.\n\nHere is a suggested refactoring that improves clarity while preserving the intended logic:\n\n```python\n    def call(self, inputs):\n        is_torch_backend = backend.backend() == \"torch\"\n\n        inputs_for_processing = inputs\n        if is_torch_backend and isinstance(inputs, torch.Tensor):\n            inputs_for_processing = tf.convert_to_tensor(\n                inputs.detach().cpu().numpy()\n            )\n        elif isinstance(inputs, (np.ndarray, list, tuple)):\n            inputs_for_processing = tf.convert_to_tensor(inputs)\n\n        output = super().call(inputs_for_processing)\n\n        if is_torch_backend and isinstance(\n            inputs, (torch.Tensor, np.ndarray, list, tuple)\n        ):\n            numpy_outputs = output.numpy()\n            if self.invert:\n                # This returns a list[str] to make it equivalent to the torch\n                # implementation of this.\n                # See : https://docs.pytorch.org/text/stable/_modules/torchtext/vocab/vocab.html#Vocab.lookup_tokens\n                return [n.decode(self.encoding) for n in numpy_outputs]\n            else:\n                return torch.from_numpy(numpy_outputs)\n        return output\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/layers/preprocessing/string_lookup.py",
    "pr_number": 21614,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2298651756,
    "comment_created_at": "2025-08-25T17:09:11Z"
  },
  {
    "code": "@@ -997,9 +997,153 @@ def less_equal(x1, x2):\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n-    raise NotImplementedError(\n-        \"`linspace` is not supported with openvino backend\"\n+    start = get_ov_output(start)\n+    stop = get_ov_output(stop)\n+\n+    if hasattr(num, \"output\") or isinstance(num, OpenVINOKerasTensor):\n+        num_tensor = get_ov_output(num)\n+        try:\n+            if num_tensor.get_node().get_type_name() == \"Constant\":\n+                num_value = num_tensor.get_node().get_vector()[0]\n+                num = int(num_value)\n+            else:\n+                raise NotImplementedError(\n+                    \"Dynamic num values not fully supported\"\n+                )\n+        except:\n+            raise NotImplementedError(\"Could not extract num value from tensor\")\n+    else:\n+        num = int(num)\n+    if dtype is None:\n+        start_type = start.get_element_type()\n+        stop_type = stop.get_element_type()\n+        if start_type.is_real() or stop_type.is_real():\n+            output_type = OPENVINO_DTYPES[config.floatx()]\n+        else:\n+            output_type = OPENVINO_DTYPES[config.floatx()]\n+    else:\n+        output_type = OPENVINO_DTYPES[dtype]\n+\n+    start = ov_opset.convert(start, output_type).output(0)\n+    stop = ov_opset.convert(stop, output_type).output(0)\n+    if num <= 0:\n+        empty_shape = ov_opset.constant([0], Type.i32).output(0)\n+        result = ov_opset.broadcast(\n+            ov_opset.constant(0.0, output_type).output(0), empty_shape\n+        ).output(0)\n+        if retstep:\n+            return OpenVINOKerasTensor(result), None\n+        return OpenVINOKerasTensor(result)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthis implementation deviates from numpy's linspace behavior for num <= 0.\n1. numpy raises a valueerror for num < 0, but this implementation returns an empty tensor.\n2. for num = 0 with retstep=true, numpy returns nan for the step, whereas this returns none.\n\nto align with numpy's behavior, you should add a check for negative num and handle the retstep case for num=0 correctly.\n\npython\n    if num < 0:\n        raise valueerror(\"number of samples, num, must be non-negative.\")\n    if num == 0:\n        empty_shape = ov_opset.constant([0], type.i32).output(0)\n        result = ov_opset.broadcast(\n            ov_opset.constant(0.0, output_type).output(0), empty_shape\n        ).output(0)\n        if retstep:\n            nan_step = ov_opset.constant(np.nan, output_type).output(0)\n            return openvinokerastensor(result), openvinokerastensor(nan_step)\n        return openvinokerastensor(result)\n",
    "line_number": 1036,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -997,9 +997,153 @@ def less_equal(x1, x2):\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n-    raise NotImplementedError(\n-        \"`linspace` is not supported with openvino backend\"\n+    start = get_ov_output(start)\n+    stop = get_ov_output(stop)\n+\n+    if hasattr(num, \"output\") or isinstance(num, OpenVINOKerasTensor):\n+        num_tensor = get_ov_output(num)\n+        try:\n+            if num_tensor.get_node().get_type_name() == \"Constant\":\n+                num_value = num_tensor.get_node().get_vector()[0]\n+                num = int(num_value)\n+            else:\n+                raise NotImplementedError(\n+                    \"Dynamic num values not fully supported\"\n+                )\n+        except:\n+            raise NotImplementedError(\"Could not extract num value from tensor\")\n+    else:\n+        num = int(num)\n+    if dtype is None:\n+        start_type = start.get_element_type()\n+        stop_type = stop.get_element_type()\n+        if start_type.is_real() or stop_type.is_real():\n+            output_type = OPENVINO_DTYPES[config.floatx()]\n+        else:\n+            output_type = OPENVINO_DTYPES[config.floatx()]\n+    else:\n+        output_type = OPENVINO_DTYPES[dtype]\n+\n+    start = ov_opset.convert(start, output_type).output(0)\n+    stop = ov_opset.convert(stop, output_type).output(0)\n+    if num <= 0:\n+        empty_shape = ov_opset.constant([0], Type.i32).output(0)\n+        result = ov_opset.broadcast(\n+            ov_opset.constant(0.0, output_type).output(0), empty_shape\n+        ).output(0)\n+        if retstep:\n+            return OpenVINOKerasTensor(result), None\n+        return OpenVINOKerasTensor(result)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThis implementation deviates from NumPy's `linspace` behavior for `num <= 0`.\n1. NumPy raises a `ValueError` for `num < 0`, but this implementation returns an empty tensor.\n2. For `num = 0` with `retstep=True`, NumPy returns `nan` for the step, whereas this returns `None`.\n\nTo align with NumPy's behavior, you should add a check for negative `num` and handle the `retstep` case for `num=0` correctly.\n\n```python\n    if num < 0:\n        raise ValueError(\"Number of samples, `num`, must be non-negative.\")\n    if num == 0:\n        empty_shape = ov_opset.constant([0], Type.i32).output(0)\n        result = ov_opset.broadcast(\n            ov_opset.constant(0.0, output_type).output(0), empty_shape\n        ).output(0)\n        if retstep:\n            nan_step = ov_opset.constant(np.nan, output_type).output(0)\n            return OpenVINOKerasTensor(result), OpenVINOKerasTensor(nan_step)\n        return OpenVINOKerasTensor(result)\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21613,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2296798847,
    "comment_created_at": "2025-08-24T20:37:35Z"
  },
  {
    "code": "@@ -892,10 +900,11 @@ def compute_homography_matrix(start_points, end_points):\n         axis=-1,\n     )\n     target_vector = np.expand_dims(target_vector, axis=-1)\n-\n+    coefficient_matrix = coefficient_matrix.astype(compute_dtype)\n+    target_vector = target_vector.astype(compute_dtype)\n     homography_matrix = np.linalg.solve(coefficient_matrix, target_vector)\n     homography_matrix = np.reshape(homography_matrix, [-1, 8])\n-\n+    homography_matrix.astype(dtype)\n     return homography_matrix",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe result of homography_matrix.astype(dtype) is not assigned to any variable, so this line has no effect. the function will return the homography_matrix with compute_dtype instead of the intended original dtype. you should return the result of the cast directly.\n\n",
    "line_number": 908,
    "enriched": "File: keras/src/backend/numpy/image.py\nCode: @@ -892,10 +900,11 @@ def compute_homography_matrix(start_points, end_points):\n         axis=-1,\n     )\n     target_vector = np.expand_dims(target_vector, axis=-1)\n-\n+    coefficient_matrix = coefficient_matrix.astype(compute_dtype)\n+    target_vector = target_vector.astype(compute_dtype)\n     homography_matrix = np.linalg.solve(coefficient_matrix, target_vector)\n     homography_matrix = np.reshape(homography_matrix, [-1, 8])\n-\n+    homography_matrix.astype(dtype)\n     return homography_matrix\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe result of `homography_matrix.astype(dtype)` is not assigned to any variable, so this line has no effect. The function will return the `homography_matrix` with `compute_dtype` instead of the intended original `dtype`. You should return the result of the cast directly.\n\n```suggestion\n    return homography_matrix.astype(dtype)\n```",
    "subcategory": "false positive",
    "category": "false positive",
    "file_path": "keras/src/backend/numpy/image.py",
    "pr_number": 21612,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2296581654,
    "comment_created_at": "2025-08-24T09:12:55Z"
  },
  {
    "code": "@@ -146,11 +146,14 @@ def compute_mask(self, inputs, mask=None):\n                 masks.append(ops.ones_like(input_i, dtype=\"bool\"))\n             elif mask_i.ndim < input_i.ndim:\n                 # Mask is smaller than the input, expand it\n-                masks.append(\n-                    ops.broadcast_to(\n-                        ops.expand_dims(mask_i, axis=-1), ops.shape(input_i)\n-                    )\n-                )\n+                mask_i = ops.expand_dims(mask_i, axis=-1)\n+                # Broadcast mask shape to match in a way where we capture the\n+                # input as a symbolic input in the op graph.\n+                mask_dtype = mask_i.dtype\n+                mask_i = ops.cast(mask_i, \"int32\")\n+                mask_i = mask_i + ops.zeros_like(input_i, \"int32\")\n+                mask_i = ops.cast(mask_i, dtype=mask_dtype)\n+                masks.append(mask_i)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe current implementation for broadcasting the mask works, but it's a bit verbose with the casting to int32 and back. a more direct and idiomatic way to achieve this for boolean tensors is to use ops.logical_or with a tensor of zeros. this avoids the type casting and makes the intent clearer.\n\n",
    "line_number": 154,
    "enriched": "File: keras/src/layers/merging/concatenate.py\nCode: @@ -146,11 +146,14 @@ def compute_mask(self, inputs, mask=None):\n                 masks.append(ops.ones_like(input_i, dtype=\"bool\"))\n             elif mask_i.ndim < input_i.ndim:\n                 # Mask is smaller than the input, expand it\n-                masks.append(\n-                    ops.broadcast_to(\n-                        ops.expand_dims(mask_i, axis=-1), ops.shape(input_i)\n-                    )\n-                )\n+                mask_i = ops.expand_dims(mask_i, axis=-1)\n+                # Broadcast mask shape to match in a way where we capture the\n+                # input as a symbolic input in the op graph.\n+                mask_dtype = mask_i.dtype\n+                mask_i = ops.cast(mask_i, \"int32\")\n+                mask_i = mask_i + ops.zeros_like(input_i, \"int32\")\n+                mask_i = ops.cast(mask_i, dtype=mask_dtype)\n+                masks.append(mask_i)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe current implementation for broadcasting the mask works, but it's a bit verbose with the casting to `int32` and back. A more direct and idiomatic way to achieve this for boolean tensors is to use `ops.logical_or` with a tensor of zeros. This avoids the type casting and makes the intent clearer.\n\n```suggestion\n                mask_i = ops.expand_dims(mask_i, axis=-1)\n                # Broadcast mask to the same shape as the input by using\n                # `ops.logical_or` with a zero tensor of the target shape. This\n                # correctly handles symbolic tensors.\n                mask_i = ops.logical_or(mask_i, ops.zeros_like(input_i, dtype=\"bool\"))\n                masks.append(mask_i)\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/layers/merging/concatenate.py",
    "pr_number": 21611,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2296371945,
    "comment_created_at": "2025-08-23T23:20:09Z"
  },
  {
    "code": "@@ -98,35 +98,64 @@ def quantization_mode(self):\n         return self.default_policy.quantization_mode\n \n     def __getitem__(self, key):\n-        \"\"\"Retrieves the corresponding `DTypePolicy` by the string key.\n+        \"\"\"Retrieves a `DTypePolicy` by its key, with regex fallback logic.\n \n-        When there isn't an exact match, all the existing keys in the map\n-        will be treated as a regex and map against the input key again. When\n-        there are multiple matches for the regex, an `ValueError` will be\n-        raised. Returns `self.default_policy` if there isn't any match found.\n+        This method first attempts an exact key match. If no exact match is\n+        found, it treats the keys stored in the map as regular expression\n+        patterns and searches for a match against the input `key`.\n+\n+        A regex match is only considered valid if it meets two conditions:\n+        - It must match exactly one pattern in the policy map.\n+        - The policy associated with that pattern must not be for quantization.\n \n         Args:\n-            key: String key to query a `DTypePolicy`.\n+            key: The string key to query for a `DTypePolicy`.\n \n         Returns:\n-            Corresponding `DTypePolicy` based on the query.\n+            The corresponding `DTypePolicy`. If no valid match is found\n+            (either exact or regex), this method returns `self.default_policy`.\n+\n+        Raises:\n+            ValueError: If the `key` matches more than one regex pattern.\n         \"\"\"\n+        # 1. Check for an exact match.\n         if key in self._policy_map:\n             return self._policy_map[key]\n \n-        matching_keys = []\n-        for k in self._policy_map:\n-            if re.search(k, key):\n-                matching_keys.append(k)\n+        # 2. If no exact match is found, treat the keys in the map as regular\n+        # expression patterns and search for a match against the input `key`.\n+        matching_keys = [\n+            pattern for pattern in self._policy_map if re.search(pattern, key)\n+        ]\n+\n+        # 3. Handle cases based on the number of matches found.\n         if len(matching_keys) > 1:\n             raise ValueError(\n                 f\"Path '{key}' matches multiple dtype policy \"\n                 f\"specification keys: {matching_keys}. Please make \"\n                 \"sure each path only matches at most \"\n                 \"one dtype policy specification key in the DTypePolicyMap.\"\n             )\n-        elif len(matching_keys) == 1:\n-            return self._policy_map[matching_keys[0]]\n+\n+        if len(matching_keys) == 1:\n+            policy = self._policy_map[matching_keys[0]]\n+\n+            # A single regex match is only considered valid if the resulting\n+            # policy is non-quantized.\n+            # This logic specifically handles how KerasHub serializes quantized\n+            # models. In that workflow, every quantized layer is stored in the\n+            # policy map with its exact path, ensuring it's found by the direct\n+            # lookup at the start of this method.\n+            #\n+            #  For example, it stops a general key like \"attention/query\" which\n+            # represents a quantization-compatible layer from incorrectly\n+            # matching a different, incompatible layer like\n+            # \"attention/query_norm\"\n+            if not policy.quantization_mode:",
    "comment": "i'm not sure i'm following the exact bug here, but this seems like we are losing a lot of flexibility with the api here. the hope was this could also be used as a more granular dtype setting api for non quantized cases, say running at different precisions for different layers.\r\n\r\n\r\ndtype_policy_map = dtypepolicymap()\r\ndtype_policy_map[\"layer/dense_0\"] = dtypepolicy(\"float32\")\r\ndtype_policy_map[\"layer/dense_1\"] = dtypepolicy(\"bfloat16\")\r\n\r\n\r\nwon't we lose that ability with this change? is there another way we can fix whatever the issue is here?",
    "line_number": 154,
    "enriched": "File: keras/src/dtype_policies/dtype_policy_map.py\nCode: @@ -98,35 +98,64 @@ def quantization_mode(self):\n         return self.default_policy.quantization_mode\n \n     def __getitem__(self, key):\n-        \"\"\"Retrieves the corresponding `DTypePolicy` by the string key.\n+        \"\"\"Retrieves a `DTypePolicy` by its key, with regex fallback logic.\n \n-        When there isn't an exact match, all the existing keys in the map\n-        will be treated as a regex and map against the input key again. When\n-        there are multiple matches for the regex, an `ValueError` will be\n-        raised. Returns `self.default_policy` if there isn't any match found.\n+        This method first attempts an exact key match. If no exact match is\n+        found, it treats the keys stored in the map as regular expression\n+        patterns and searches for a match against the input `key`.\n+\n+        A regex match is only considered valid if it meets two conditions:\n+        - It must match exactly one pattern in the policy map.\n+        - The policy associated with that pattern must not be for quantization.\n \n         Args:\n-            key: String key to query a `DTypePolicy`.\n+            key: The string key to query for a `DTypePolicy`.\n \n         Returns:\n-            Corresponding `DTypePolicy` based on the query.\n+            The corresponding `DTypePolicy`. If no valid match is found\n+            (either exact or regex), this method returns `self.default_policy`.\n+\n+        Raises:\n+            ValueError: If the `key` matches more than one regex pattern.\n         \"\"\"\n+        # 1. Check for an exact match.\n         if key in self._policy_map:\n             return self._policy_map[key]\n \n-        matching_keys = []\n-        for k in self._policy_map:\n-            if re.search(k, key):\n-                matching_keys.append(k)\n+        # 2. If no exact match is found, treat the keys in the map as regular\n+        # expression patterns and search for a match against the input `key`.\n+        matching_keys = [\n+            pattern for pattern in self._policy_map if re.search(pattern, key)\n+        ]\n+\n+        # 3. Handle cases based on the number of matches found.\n         if len(matching_keys) > 1:\n             raise ValueError(\n                 f\"Path '{key}' matches multiple dtype policy \"\n                 f\"specification keys: {matching_keys}. Please make \"\n                 \"sure each path only matches at most \"\n                 \"one dtype policy specification key in the DTypePolicyMap.\"\n             )\n-        elif len(matching_keys) == 1:\n-            return self._policy_map[matching_keys[0]]\n+\n+        if len(matching_keys) == 1:\n+            policy = self._policy_map[matching_keys[0]]\n+\n+            # A single regex match is only considered valid if the resulting\n+            # policy is non-quantized.\n+            # This logic specifically handles how KerasHub serializes quantized\n+            # models. In that workflow, every quantized layer is stored in the\n+            # policy map with its exact path, ensuring it's found by the direct\n+            # lookup at the start of this method.\n+            #\n+            #  For example, it stops a general key like \"attention/query\" which\n+            # represents a quantization-compatible layer from incorrectly\n+            # matching a different, incompatible layer like\n+            # \"attention/query_norm\"\n+            if not policy.quantization_mode:\nComment: I'm not sure I'm following the exact bug here, but this seems like we are losing a lot of flexibility with the API here. The hope was this could also be used as a more granular dtype setting API for non quantized cases, say running at different precisions for different layers.\r\n\r\n```\r\ndtype_policy_map = DTypePolicyMap()\r\ndtype_policy_map[\"layer/dense_0\"] = DTypePolicy(\"float32\")\r\ndtype_policy_map[\"layer/dense_1\"] = DTypePolicy(\"bfloat16\")\r\n```\r\n\r\nWon't we lose that ability with this change? Is there another way we can fix whatever the issue is here?",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "keras/src/dtype_policies/dtype_policy_map.py",
    "pr_number": 21608,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2294432443,
    "comment_created_at": "2025-08-22T18:48:05Z"
  },
  {
    "code": "@@ -1559,6 +1559,22 @@ def hstack(xs):\n     return tf.concat(xs, axis=1)\n \n \n+def hypot(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    if dtype in [\"int8\", \"int16\", \"int32\", \"uint8\", \"uint16\", \"uint32\"]:\n+        dtype = config.floatx()\n+    elif dtype in [\"int64\"]:\n+        dtype = \"float64\"\n+\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    return tf.math.sqrt(tf.math.square(x1) + tf.math.square(x2))",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe direct implementation tf.math.sqrt(tf.math.square(x1) + tf.math.square(x2)) is numerically unstable. it can lead to overflow for large input values or underflow for very small values, resulting in inf or 0 respectively.\n\nfor example, hypot(1e20, 1) should be 1e20, but this implementation would compute sqrt((1e20)**2 + 1**2) = sqrt(inf + 1) = inf.\n\na more stable implementation should be used to avoid this. a common way to implement hypot stably is to factor out the maximum absolute value to prevent intermediate overflow.\n\n",
    "line_number": 1575,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1559,6 +1559,22 @@ def hstack(xs):\n     return tf.concat(xs, axis=1)\n \n \n+def hypot(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    if dtype in [\"int8\", \"int16\", \"int32\", \"uint8\", \"uint16\", \"uint32\"]:\n+        dtype = config.floatx()\n+    elif dtype in [\"int64\"]:\n+        dtype = \"float64\"\n+\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n+\n+    return tf.math.sqrt(tf.math.square(x1) + tf.math.square(x2))\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe direct implementation `tf.math.sqrt(tf.math.square(x1) + tf.math.square(x2))` is numerically unstable. It can lead to overflow for large input values or underflow for very small values, resulting in `inf` or `0` respectively.\n\nFor example, `hypot(1e20, 1)` should be `1e20`, but this implementation would compute `sqrt((1e20)**2 + 1**2) = sqrt(inf + 1) = inf`.\n\nA more stable implementation should be used to avoid this. A common way to implement `hypot` stably is to factor out the maximum absolute value to prevent intermediate overflow.\n\n```suggestion\n    x1_abs = tf.abs(x1)\n    x2_abs = tf.abs(x2)\n    max_val = tf.maximum(x1_abs, x2_abs)\n    min_val = tf.minimum(x1_abs, x2_abs)\n    # Using divide_no_nan to handle the case where max_val is 0.\n    ratio = tf.math.divide_no_nan(min_val, max_val)\n    return max_val * tf.sqrt(1.0 + tf.square(ratio))\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21606,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2290885940,
    "comment_created_at": "2025-08-21T12:21:31Z"
  },
  {
    "code": "@@ -205,6 +205,23 @@ def test_pass_initial_state(self):\n             output,\n         )\n \n+    def test_pass_return_state(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        initial_state = np.arange(4).reshape((2, 2)).astype(\"float32\")\n+        layer = layers.GRU(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            return_state=True,\n+        )\n+        output, state = layer(sequence, initial_state=initial_state)\n+        self.assertAllClose(\n+            np.array([[0.23774096, 0.33508456], [0.83659905, 1.0227708]]),\n+            output,\n+        )\n+        self.assertEqual(state.shape, (2, 2))",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis is a good test case for return_state=true. to make it more comprehensive, consider a few enhancements:\n\n1.  **assert output is state**: when return_sequences=false, the final output of a gru layer is its final state. you can add an assertion to verify this: self.assertallclose(output, state). this is a stronger check than just verifying the shape.\n\n2.  **test go_backwards=true**: the go_backwards argument can affect state handling. it would be beneficial to add a test case for go_backwards=true to ensure correctness in that scenario as well.\n\nhere is a suggested implementation combining these points:\n\npython\n    def test_pass_return_state(self):\n        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n        initial_state = np.arange(4).reshape((2, 2)).astype(\"float32\")\n\n        # test with go_backwards=false\n        layer = layers.gru(\n            2,\n            kernel_initializer=initializers.constant(0.01),\n            recurrent_initializer=initializers.constant(0.02),\n            bias_initializer=initializers.constant(0.03),\n            return_state=true,\n        )\n        output, state = layer(sequence, initial_state=initial_state)\n        self.assertallclose(\n            np.array([[0.23774096, 0.33508456], [0.83659905, 1.0227708]]),\n            output,\n        )\n        self.assertallclose(output, state)\n\n        # test with go_backwards=true\n        layer = layers.gru(\n            2,\n            kernel_initializer=initializers.constant(0.01),\n            recurrent_initializer=initializers.constant(0.02),\n            bias_initializer=initializers.constant(0.03),\n            return_state=true,\n            go_backwards=true,\n        )\n        output, state = layer(sequence, initial_state=initial_state)\n        self.assertallclose(\n            np.array([[0.13486053, 0.23261218], [0.78257304, 0.9691353]]),\n            output,\n        )\n        self.assertallclose(output, state)\n",
    "line_number": 223,
    "enriched": "File: keras/src/layers/rnn/gru_test.py\nCode: @@ -205,6 +205,23 @@ def test_pass_initial_state(self):\n             output,\n         )\n \n+    def test_pass_return_state(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        initial_state = np.arange(4).reshape((2, 2)).astype(\"float32\")\n+        layer = layers.GRU(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            return_state=True,\n+        )\n+        output, state = layer(sequence, initial_state=initial_state)\n+        self.assertAllClose(\n+            np.array([[0.23774096, 0.33508456], [0.83659905, 1.0227708]]),\n+            output,\n+        )\n+        self.assertEqual(state.shape, (2, 2))\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis is a good test case for `return_state=True`. To make it more comprehensive, consider a few enhancements:\n\n1.  **Assert output is state**: When `return_sequences=False`, the final output of a GRU layer is its final state. You can add an assertion to verify this: `self.assertAllClose(output, state)`. This is a stronger check than just verifying the shape.\n\n2.  **Test `go_backwards=True`**: The `go_backwards` argument can affect state handling. It would be beneficial to add a test case for `go_backwards=True` to ensure correctness in that scenario as well.\n\nHere is a suggested implementation combining these points:\n\n```python\n    def test_pass_return_state(self):\n        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n        initial_state = np.arange(4).reshape((2, 2)).astype(\"float32\")\n\n        # Test with go_backwards=False\n        layer = layers.GRU(\n            2,\n            kernel_initializer=initializers.Constant(0.01),\n            recurrent_initializer=initializers.Constant(0.02),\n            bias_initializer=initializers.Constant(0.03),\n            return_state=True,\n        )\n        output, state = layer(sequence, initial_state=initial_state)\n        self.assertAllClose(\n            np.array([[0.23774096, 0.33508456], [0.83659905, 1.0227708]]),\n            output,\n        )\n        self.assertAllClose(output, state)\n\n        # Test with go_backwards=True\n        layer = layers.GRU(\n            2,\n            kernel_initializer=initializers.Constant(0.01),\n            recurrent_initializer=initializers.Constant(0.02),\n            bias_initializer=initializers.Constant(0.03),\n            return_state=True,\n            go_backwards=True,\n        )\n        output, state = layer(sequence, initial_state=initial_state)\n        self.assertAllClose(\n            np.array([[0.13486053, 0.23261218], [0.78257304, 0.9691353]]),\n            output,\n        )\n        self.assertAllClose(output, state)\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/layers/rnn/gru_test.py",
    "pr_number": 21603,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2289565597,
    "comment_created_at": "2025-08-21T00:09:11Z"
  },
  {
    "code": "@@ -158,8 +158,13 @@ def test_saving_lambda(self):\n \n         temp_filepath = os.path.join(self.get_temp_dir(), \"lambda_model.h5\")\n         legacy_h5_format.save_model_to_hdf5(model, temp_filepath)\n-        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\n \n+        with self.assertRaisesRegex(ValueError, \"Deserializing it is unsafe\"):\n+            legacy_h5_format.load_model_from_hdf5(temp_filepath)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe regex \"deserializing it is unsafe\" does not match the valueerror raised when deserializing a lambda in safe mode. the actual error message is \"requested the deserialization of a lambda object. this carries a potential risk of arbitrary code execution...\". a more appropriate regex, like \"arbitrary code execution\", should be used to correctly test for this exception.\n\n",
    "line_number": 163,
    "enriched": "File: keras/src/legacy/saving/legacy_h5_format_test.py\nCode: @@ -158,8 +158,13 @@ def test_saving_lambda(self):\n \n         temp_filepath = os.path.join(self.get_temp_dir(), \"lambda_model.h5\")\n         legacy_h5_format.save_model_to_hdf5(model, temp_filepath)\n-        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\n \n+        with self.assertRaisesRegex(ValueError, \"Deserializing it is unsafe\"):\n+            legacy_h5_format.load_model_from_hdf5(temp_filepath)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe regex `\"Deserializing it is unsafe\"` does not match the `ValueError` raised when deserializing a lambda in safe mode. The actual error message is \"Requested the deserialization of a `lambda` object. This carries a potential risk of arbitrary code execution...\". A more appropriate regex, like `\"arbitrary code execution\"`, should be used to correctly test for this exception.\n\n```suggestion\n        with self.assertRaisesRegex(ValueError, \"arbitrary code execution\"):\n            legacy_h5_format.load_model_from_hdf5(temp_filepath)\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/legacy/saving/legacy_h5_format_test.py",
    "pr_number": 21602,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2289196194,
    "comment_created_at": "2025-08-20T20:12:47Z"
  },
  {
    "code": "@@ -1097,9 +1097,52 @@ def median(x, axis=None, keepdims=False):\n \n \n def meshgrid(*x, indexing=\"xy\"):\n-    raise NotImplementedError(\n-        \"`meshgrid` is not supported with openvino backend\"\n-    )\n+    if len(x) < 2:\n+        raise ValueError(\"meshgrid requires at least 2 input arrays\")\n+    if indexing not in (\"xy\", \"ij\"):\n+        raise ValueError(\"indexing must be either 'xy' or 'ij'\")\n+\n+    tensors = [get_ov_output(xi) for xi in x]\n+    n = len(tensors)\n+\n+    shapes = [ov_opset.shape_of(t, Type.i64).output(0) for t in tensors]   # each is [Ni]\n+    one = ov_opset.constant([1], Type.i64).output(0)\n+\n+    if indexing == \"xy\" and n >= 2:\n+        out_shape = ov_opset.concat([shapes[1], shapes[0]] + shapes[2:], axis=0).output(0)\n+    else:\n+        out_shape = ov_opset.concat(shapes, axis=0).output(0)\n+\n+    outputs = []\n+    for i, t in enumerate(tensors):\n+        parts = []\n+        for axis in range(n):\n+            if indexing == \"xy\" and n >= 2:\n+                if i == 0:\n+                    if axis == 0:\n+                        parts.append(one)\n+                    elif axis == 1:\n+                        parts.append(shapes[0])\n+                    else:\n+                        parts.append(one if axis != i else shapes[i])  \n+                elif i == 1:\n+                    if axis == 0:\n+                        parts.append(shapes[1])\n+                    elif axis == 1:\n+                        parts.append(one)\n+                    else:\n+                        parts.append(one if axis != i else shapes[i])  \n+                else:\n+                    parts.append(shapes[i] if axis == i else one)\n+            else:\n+                parts.append(shapes[i] if axis == i else one)\n+\n+        reshape_shape = ov_opset.concat(parts, axis=0).output(0)\n+        reshaped = ov_opset.reshape(t, reshape_shape, False).output(0)\n+        broadcasted = ov_opset.broadcast(reshaped, out_shape).output(0)\n+        outputs.append(OpenVINOKerasTensor(broadcasted))\n+",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe logic for constructing the reshape shapes is quite complex and can be simplified for better readability and maintainability.\n\nyou can determine the reshape_parts directly without the nested for loop over axes. also, the and n >= 2 check is redundant since it's already handled at the beginning of the function.\n\nhere's a suggested refactoring:\n\npython\n    if indexing == \"xy\":\n        # for 'xy' indexing, the first two dimensions of the output grid are swapped.\n        shape_list = [shapes[1], shapes[0]] + shapes[2:]\n        out_shape = ov_opset.concat(shape_list, axis=0).output(0)\n    else:  # 'ij' indexing\n        out_shape = ov_opset.concat(shapes, axis=0).output(0)\n\n    outputs = []\n    for i, t in enumerate(tensors):\n        # reshape each tensor to a 1d tensor of its own size along one axis\n        # and size 1 along all other axes.\n        reshape_parts = [one] * n\n        if indexing == \"xy\":\n            if i == 0:\n                # first output varies along second axis\n                reshape_parts[1] = shapes[0]\n            elif i == 1:\n                # second output varies along first axis\n                reshape_parts[0] = shapes[1]\n            else:\n                reshape_parts[i] = shapes[i]\n        else:  # 'ij' indexing\n            reshape_parts[i] = shapes[i]\n\n        reshape_shape = ov_opset.concat(reshape_parts, axis=0).output(0)\n        reshaped = ov_opset.reshape(t, reshape_shape, false).output(0)\n        broadcasted = ov_opset.broadcast(reshaped, out_shape).output(0)\n        outputs.append(openvinokerastensor(broadcasted))\n\n",
    "line_number": 1144,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -1097,9 +1097,52 @@ def median(x, axis=None, keepdims=False):\n \n \n def meshgrid(*x, indexing=\"xy\"):\n-    raise NotImplementedError(\n-        \"`meshgrid` is not supported with openvino backend\"\n-    )\n+    if len(x) < 2:\n+        raise ValueError(\"meshgrid requires at least 2 input arrays\")\n+    if indexing not in (\"xy\", \"ij\"):\n+        raise ValueError(\"indexing must be either 'xy' or 'ij'\")\n+\n+    tensors = [get_ov_output(xi) for xi in x]\n+    n = len(tensors)\n+\n+    shapes = [ov_opset.shape_of(t, Type.i64).output(0) for t in tensors]   # each is [Ni]\n+    one = ov_opset.constant([1], Type.i64).output(0)\n+\n+    if indexing == \"xy\" and n >= 2:\n+        out_shape = ov_opset.concat([shapes[1], shapes[0]] + shapes[2:], axis=0).output(0)\n+    else:\n+        out_shape = ov_opset.concat(shapes, axis=0).output(0)\n+\n+    outputs = []\n+    for i, t in enumerate(tensors):\n+        parts = []\n+        for axis in range(n):\n+            if indexing == \"xy\" and n >= 2:\n+                if i == 0:\n+                    if axis == 0:\n+                        parts.append(one)\n+                    elif axis == 1:\n+                        parts.append(shapes[0])\n+                    else:\n+                        parts.append(one if axis != i else shapes[i])  \n+                elif i == 1:\n+                    if axis == 0:\n+                        parts.append(shapes[1])\n+                    elif axis == 1:\n+                        parts.append(one)\n+                    else:\n+                        parts.append(one if axis != i else shapes[i])  \n+                else:\n+                    parts.append(shapes[i] if axis == i else one)\n+            else:\n+                parts.append(shapes[i] if axis == i else one)\n+\n+        reshape_shape = ov_opset.concat(parts, axis=0).output(0)\n+        reshaped = ov_opset.reshape(t, reshape_shape, False).output(0)\n+        broadcasted = ov_opset.broadcast(reshaped, out_shape).output(0)\n+        outputs.append(OpenVINOKerasTensor(broadcasted))\n+\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe logic for constructing the reshape shapes is quite complex and can be simplified for better readability and maintainability.\n\nYou can determine the `reshape_parts` directly without the nested `for` loop over axes. Also, the `and n >= 2` check is redundant since it's already handled at the beginning of the function.\n\nHere's a suggested refactoring:\n\n```python\n    if indexing == \"xy\":\n        # For 'xy' indexing, the first two dimensions of the output grid are swapped.\n        shape_list = [shapes[1], shapes[0]] + shapes[2:]\n        out_shape = ov_opset.concat(shape_list, axis=0).output(0)\n    else:  # 'ij' indexing\n        out_shape = ov_opset.concat(shapes, axis=0).output(0)\n\n    outputs = []\n    for i, t in enumerate(tensors):\n        # Reshape each tensor to a 1D tensor of its own size along one axis\n        # and size 1 along all other axes.\n        reshape_parts = [one] * n\n        if indexing == \"xy\":\n            if i == 0:\n                # First output varies along second axis\n                reshape_parts[1] = shapes[0]\n            elif i == 1:\n                # Second output varies along first axis\n                reshape_parts[0] = shapes[1]\n            else:\n                reshape_parts[i] = shapes[i]\n        else:  # 'ij' indexing\n            reshape_parts[i] = shapes[i]\n\n        reshape_shape = ov_opset.concat(reshape_parts, axis=0).output(0)\n        reshaped = ov_opset.reshape(t, reshape_shape, False).output(0)\n        broadcasted = ov_opset.broadcast(reshaped, out_shape).output(0)\n        outputs.append(OpenVINOKerasTensor(broadcasted))\n\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21600,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2286972804,
    "comment_created_at": "2025-08-20T04:49:49Z"
  },
  {
    "code": "@@ -0,0 +1,159 @@\n+import keras.src.backend\n+from keras.src import tree\n+from keras.src.layers.layer import Layer\n+from keras.src.random.seed_generator import SeedGenerator\n+from keras.src.utils import backend_utils\n+from keras.src.utils import jax_utils\n+from keras.src.utils import tracking\n+\n+\n+class DataLayer(Layer):\n+    \"\"\"Layer designed for safe use in `tf.data` or `grain` pipeline.\n+\n+    This layer overrides the `__call__` method to ensure that the correct\n+    backend is used and that computation is performed on the CPU.\n+\n+    The `call()` method in subclasses should use `self.backend` ops. If\n+    randomness is needed, define both `seed` and `generator` in `__init__` and\n+    retrieve the running seed using `self._get_seed_generator()`. If the layer\n+    has weights in `__init__` or `build()`, use `convert_weight()` to ensure\n+    they are in the correct backend.\n+\n+    **Note:** This layer and its subclasses only support a single input tensor.\n+\n+    Examples:\n+\n+    **Custom `DataLayer` subclass:**\n+\n+    ```python\n+    from keras.src.layers.preprocessing.data_layer import DataLayer\n+    from keras.src.random import SeedGenerator",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe example code in the docstring uses ops on line 36 without importing it. to make the example runnable, please add from keras.src import ops.\n\n",
    "line_number": 30,
    "enriched": "File: keras/src/layers/preprocessing/data_layer.py\nCode: @@ -0,0 +1,159 @@\n+import keras.src.backend\n+from keras.src import tree\n+from keras.src.layers.layer import Layer\n+from keras.src.random.seed_generator import SeedGenerator\n+from keras.src.utils import backend_utils\n+from keras.src.utils import jax_utils\n+from keras.src.utils import tracking\n+\n+\n+class DataLayer(Layer):\n+    \"\"\"Layer designed for safe use in `tf.data` or `grain` pipeline.\n+\n+    This layer overrides the `__call__` method to ensure that the correct\n+    backend is used and that computation is performed on the CPU.\n+\n+    The `call()` method in subclasses should use `self.backend` ops. If\n+    randomness is needed, define both `seed` and `generator` in `__init__` and\n+    retrieve the running seed using `self._get_seed_generator()`. If the layer\n+    has weights in `__init__` or `build()`, use `convert_weight()` to ensure\n+    they are in the correct backend.\n+\n+    **Note:** This layer and its subclasses only support a single input tensor.\n+\n+    Examples:\n+\n+    **Custom `DataLayer` subclass:**\n+\n+    ```python\n+    from keras.src.layers.preprocessing.data_layer import DataLayer\n+    from keras.src.random import SeedGenerator\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe example code in the docstring uses `ops` on line 36 without importing it. To make the example runnable, please add `from keras.src import ops`.\n\n```suggestion\n    from keras.src import ops\n    from keras.src.layers.preprocessing.data_layer import DataLayer\n    from keras.src.random import SeedGenerator\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "keras/src/layers/preprocessing/data_layer.py",
    "pr_number": 21598,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2285505805,
    "comment_created_at": "2025-08-19T14:44:07Z"
  },
  {
    "code": "@@ -1105,6 +1106,18 @@ def append_batch_axis(t):\n         y = tree.map_structure(append_batch_axis, y)\n         return y\n \n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"function\": self.function})\n+        return config\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        config[\"function\"] = serialization_lib.deserialize_keras_object(\n+            config[\"function\"]\n+        )\n+        return cls(**config)",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nto prevent potential side effects for callers of this method, it's a good practice to avoid modifying the config dictionary in-place. creating a copy ensures that the original dictionary passed to this method remains unchanged. this pattern is also used elsewhere in the codebase, for instance in operation.from_config.\n\n",
    "line_number": 1120,
    "enriched": "File: keras/src/ops/core.py\nCode: @@ -1105,6 +1106,18 @@ def append_batch_axis(t):\n         y = tree.map_structure(append_batch_axis, y)\n         return y\n \n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"function\": self.function})\n+        return config\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        config[\"function\"] = serialization_lib.deserialize_keras_object(\n+            config[\"function\"]\n+        )\n+        return cls(**config)\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nTo prevent potential side effects for callers of this method, it's a good practice to avoid modifying the `config` dictionary in-place. Creating a copy ensures that the original dictionary passed to this method remains unchanged. This pattern is also used elsewhere in the codebase, for instance in `Operation.from_config`.\n\n```suggestion\n    @classmethod\n    def from_config(cls, config):\n        config = config.copy()\n        config[\"function\"] = serialization_lib.deserialize_keras_object(\n            config[\"function\"]\n        )\n        return cls(**config)\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/ops/core.py",
    "pr_number": 21597,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2283532208,
    "comment_created_at": "2025-08-18T21:25:08Z"
  },
  {
    "code": "@@ -9110,7 +9100,7 @@ def test_angle(self, dtype):\n         x = knp.ones((1,), dtype=dtype)\n         x_jax = jnp.ones((1,), dtype=dtype)\n         expected_dtype = standardize_dtype(jnp.angle(x_jax).dtype)\n-        if dtype == \"int64\":\n+        if dtype == \"bool\" or is_int_dtype(dtype):\n             expected_dtype = backend.floatx()",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthis change correctly identifies that knp.angle should return backend.floatx() for all integer and boolean dtypes to be consistent across backends and jax configurations. however, the current implementation of knp.angle in the jax backend only handles int64 specifically.\n\nthe implementation in keras/src/backend/jax/numpy.py is:\npython\ndef angle(x):\n    x = convert_to_tensor(x)\n    if standardize_dtype(x.dtype) == \"int64\":\n        dtype = config.floatx()\n    else:\n        dtype = dtypes.result_type(x.dtype, float)\n    x = cast(x, dtype)\n    return jnp.angle(x)\n\nthis will cause this test to fail for other integer types (e.g., int32) when backend.floatx() is not float32.\n\nto fix this, the implementation of angle in keras/src/backend/jax/numpy.py should be updated to handle all integer and boolean types, similar to your change here. for example:\npython\nfrom keras.src.backend.common import is_int_dtype\n\ndef angle(x):\n    x = convert_to_tensor(x)\n    x_dtype = standardize_dtype(x.dtype)\n    if is_int_dtype(x_dtype) or x_dtype == \"bool\":\n        dtype = config.floatx()\n    else:\n        dtype = dtypes.result_type(x.dtype, float)\n    x = cast(x, dtype)\n    return jnp.angle(x)\n\nadditionally, the compute_output_spec for keras.ops.numpy.angle should be updated accordingly to reflect this change for symbolic execution.",
    "line_number": 9104,
    "enriched": "File: keras/src/ops/numpy_test.py\nCode: @@ -9110,7 +9100,7 @@ def test_angle(self, dtype):\n         x = knp.ones((1,), dtype=dtype)\n         x_jax = jnp.ones((1,), dtype=dtype)\n         expected_dtype = standardize_dtype(jnp.angle(x_jax).dtype)\n-        if dtype == \"int64\":\n+        if dtype == \"bool\" or is_int_dtype(dtype):\n             expected_dtype = backend.floatx()\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThis change correctly identifies that `knp.angle` should return `backend.floatx()` for all integer and boolean dtypes to be consistent across backends and JAX configurations. However, the current implementation of `knp.angle` in the JAX backend only handles `int64` specifically.\n\nThe implementation in `keras/src/backend/jax/numpy.py` is:\n```python\ndef angle(x):\n    x = convert_to_tensor(x)\n    if standardize_dtype(x.dtype) == \"int64\":\n        dtype = config.floatx()\n    else:\n        dtype = dtypes.result_type(x.dtype, float)\n    x = cast(x, dtype)\n    return jnp.angle(x)\n```\nThis will cause this test to fail for other integer types (e.g., `int32`) when `backend.floatx()` is not `float32`.\n\nTo fix this, the implementation of `angle` in `keras/src/backend/jax/numpy.py` should be updated to handle all integer and boolean types, similar to your change here. For example:\n```python\nfrom keras.src.backend.common import is_int_dtype\n\ndef angle(x):\n    x = convert_to_tensor(x)\n    x_dtype = standardize_dtype(x.dtype)\n    if is_int_dtype(x_dtype) or x_dtype == \"bool\":\n        dtype = config.floatx()\n    else:\n        dtype = dtypes.result_type(x.dtype, float)\n    x = cast(x, dtype)\n    return jnp.angle(x)\n```\nAdditionally, the `compute_output_spec` for `keras.ops.numpy.Angle` should be updated accordingly to reflect this change for symbolic execution.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/ops/numpy_test.py",
    "pr_number": 21596,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2283063982,
    "comment_created_at": "2025-08-18T17:47:28Z"
  },
  {
    "code": "@@ -86,6 +86,21 @@ def test_attention_with_mask(self):\n         self.assertAllClose(output, [[[1.0, 1.0], [0.0, 0.0]]])\n         self.assertAllClose(scores, [[[1.0, 0.0], [1.0, 0.0]]])\n \n+    def test_attention_2D_mask_shape_mismatch(self):\n+        layer = layers.Attention()\n+        batch_size, Tq, Tv, dim = 2, 3, 3, 4",
    "comment": "please use different values for tq and tv so that the tests check that there is no mixup.",
    "line_number": 91,
    "enriched": "File: keras/src/layers/attention/attention_test.py\nCode: @@ -86,6 +86,21 @@ def test_attention_with_mask(self):\n         self.assertAllClose(output, [[[1.0, 1.0], [0.0, 0.0]]])\n         self.assertAllClose(scores, [[[1.0, 0.0], [1.0, 0.0]]])\n \n+    def test_attention_2D_mask_shape_mismatch(self):\n+        layer = layers.Attention()\n+        batch_size, Tq, Tv, dim = 2, 3, 3, 4\nComment: Please use different values for `Tq` and `Tv` so that the tests check that there is no mixup.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "keras/src/layers/attention/attention_test.py",
    "pr_number": 21595,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2285724166,
    "comment_created_at": "2025-08-19T16:08:47Z"
  },
  {
    "code": "@@ -97,31 +107,39 @@ def test_image_dataset_from_directory_no_labels(self):\n         self.assertEqual(batch_count, 2)\n         self.assertEqual(sample_count, 10)\n \n-    def test_image_dataset_from_directory_binary(self):\n+    @parameterized.named_parameters(\n+        (\"tf\", \"tf\"),\n+        (\"grain\", \"grain\"),\n+    )\n+    def test_image_dataset_from_directory_binary(self, format):\n         directory = self._prepare_directory(num_classes=2)\n         dataset = image_dataset_utils.image_dataset_from_directory(\n-            directory, batch_size=8, image_size=(18, 18), label_mode=\"int\"\n+            directory,\n+            batch_size=8,\n+            image_size=(18, 18),\n+            label_mode=\"int\",\n+            format=format,\n         )\n         if backend.config.image_data_format() == \"channels_last\":\n             output_shape = [8, 18, 18, 3]\n         else:\n             output_shape = [8, 3, 18, 18]\n         batch = next(iter(dataset))\n         self.assertLen(batch, 2)\n-        self.assertEqual(batch[0].shape, output_shape)\n-        self.assertEqual(batch[0].dtype.name, \"float32\")\n-        self.assertEqual(batch[1].shape, (8,))\n-        self.assertEqual(batch[1].dtype.name, \"int32\")\n+        self.assertEqual(list(batch[0].shape), output_shape)\n+        self.assertDType(batch[0], \"float32\")\n+        self.assertEqual(list(batch[1].shape), [8])\n+        self.assertDType(batch[1], \"int32\")\n \n         dataset = image_dataset_utils.image_dataset_from_directory(\n             directory, batch_size=8, image_size=(18, 18), label_mode=\"binary\"\n         )",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe format argument is missing in this call to image_dataset_from_directory. when running with format='grain', this will default to format='tf', so the grain path for label_mode='binary' is not being tested.\n\n",
    "line_number": 140,
    "enriched": "File: keras/src/utils/image_dataset_utils_test.py\nCode: @@ -97,31 +107,39 @@ def test_image_dataset_from_directory_no_labels(self):\n         self.assertEqual(batch_count, 2)\n         self.assertEqual(sample_count, 10)\n \n-    def test_image_dataset_from_directory_binary(self):\n+    @parameterized.named_parameters(\n+        (\"tf\", \"tf\"),\n+        (\"grain\", \"grain\"),\n+    )\n+    def test_image_dataset_from_directory_binary(self, format):\n         directory = self._prepare_directory(num_classes=2)\n         dataset = image_dataset_utils.image_dataset_from_directory(\n-            directory, batch_size=8, image_size=(18, 18), label_mode=\"int\"\n+            directory,\n+            batch_size=8,\n+            image_size=(18, 18),\n+            label_mode=\"int\",\n+            format=format,\n         )\n         if backend.config.image_data_format() == \"channels_last\":\n             output_shape = [8, 18, 18, 3]\n         else:\n             output_shape = [8, 3, 18, 18]\n         batch = next(iter(dataset))\n         self.assertLen(batch, 2)\n-        self.assertEqual(batch[0].shape, output_shape)\n-        self.assertEqual(batch[0].dtype.name, \"float32\")\n-        self.assertEqual(batch[1].shape, (8,))\n-        self.assertEqual(batch[1].dtype.name, \"int32\")\n+        self.assertEqual(list(batch[0].shape), output_shape)\n+        self.assertDType(batch[0], \"float32\")\n+        self.assertEqual(list(batch[1].shape), [8])\n+        self.assertDType(batch[1], \"int32\")\n \n         dataset = image_dataset_utils.image_dataset_from_directory(\n             directory, batch_size=8, image_size=(18, 18), label_mode=\"binary\"\n         )\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe `format` argument is missing in this call to `image_dataset_from_directory`. When running with `format='grain'`, this will default to `format='tf'`, so the grain path for `label_mode='binary'` is not being tested.\n\n```suggestion\n        dataset = image_dataset_utils.image_dataset_from_directory(\n            directory, batch_size=8, image_size=(18, 18), label_mode=\"binary\", format=format\n        )\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/utils/image_dataset_utils_test.py",
    "pr_number": 21593,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2280794580,
    "comment_created_at": "2025-08-17T09:07:04Z"
  },
  {
    "code": "@@ -834,8 +834,9 @@ def _retrieve_class_or_fn(\n                 )\n \n     raise TypeError(\n-        f\"Could not locate {obj_type} '{name}'. \"\n-        \"Make sure custom classes are decorated with \"\n-        \"`@keras.saving.register_keras_serializable()`. \"\n-        f\"Full object config: {full_config}\"\n+        f\"Could not locate {obj_type} '{name}'. Make sure custom classes and \"\n+        \"functions are decorated with \"\n+        \"`@keras.saving.register_keras_serializable()`. If they are already \"\n+        \"decorated, make sure they are all imported so that the decorator is \"\n+        f\"run before trying to load them. Full object config: {full_config}\"",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthis multi-line string construction is invalid and will cause a syntaxerror. when implicitly concatenating string literals, you cannot have an f-string following a regular string in the sequence. a simple way to fix this is to make each part of the concatenated string an f-string by adding the f prefix to each line.\n\n",
    "line_number": 841,
    "enriched": "File: keras/src/saving/serialization_lib.py\nCode: @@ -834,8 +834,9 @@ def _retrieve_class_or_fn(\n                 )\n \n     raise TypeError(\n-        f\"Could not locate {obj_type} '{name}'. \"\n-        \"Make sure custom classes are decorated with \"\n-        \"`@keras.saving.register_keras_serializable()`. \"\n-        f\"Full object config: {full_config}\"\n+        f\"Could not locate {obj_type} '{name}'. Make sure custom classes and \"\n+        \"functions are decorated with \"\n+        \"`@keras.saving.register_keras_serializable()`. If they are already \"\n+        \"decorated, make sure they are all imported so that the decorator is \"\n+        f\"run before trying to load them. Full object config: {full_config}\"\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThis multi-line string construction is invalid and will cause a `SyntaxError`. When implicitly concatenating string literals, you cannot have an f-string following a regular string in the sequence. A simple way to fix this is to make each part of the concatenated string an f-string by adding the `f` prefix to each line.\n\n```suggestion\n        f\"Could not locate {obj_type} '{name}'. Make sure custom classes and \"\n        f\"functions are decorated with \"\n        f\"`@keras.saving.register_keras_serializable()`. If they are already \"\n        f\"decorated, make sure they are all imported so that the decorator is \"\n        f\"run before trying to load them. Full object config: {full_config}\"\n```",
    "subcategory": "false positive",
    "category": "false positive",
    "file_path": "keras/src/saving/serialization_lib.py",
    "pr_number": 21590,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2279927461,
    "comment_created_at": "2025-08-15T21:35:10Z"
  },
  {
    "code": "@@ -76,8 +76,9 @@ class IntegerLookup(IndexLookup):\n             If passing a file path, the file should contain one line per term\n             in the vocabulary. If this argument is set,\n             there is no need to `adapt()` the layer.\n-        vocabulary_dtype: The dtype of the vocabulary terms, for example\n-            `\"int64\"` or `\"int32\"`. Defaults to `\"int64\"`.\n+        vocabulary_dtype: The dtype of the vocabulary terms.\n+            Only `vocabulary_dtype='int64'` is supported at this time.\n+            Defaults to `\"int64\"`.",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nwhile updating the docstring is a good step to inform users about the current limitation, it would be more robust to enforce this constraint in the code. this would prevent users from accidentally using an unsupported vocabulary_dtype and encountering errors later.\n\nconsider adding a check in the __init__ method of the integerlookup layer to raise a valueerror if vocabulary_dtype is not 'int64'. for example:\n\npython\n# in integerlookup.__init__\nif vocabulary_dtype != \"int64\":\n    raise valueerror(\n        \"only vocabulary_dtype='int64' is supported at this time for \"\n        f\"integerlookup. received: {vocabulary_dtype}\"\n    )\n\n\nthis would provide immediate feedback to the user and make the layer's behavior more explicit.",
    "line_number": 81,
    "enriched": "File: keras/src/layers/preprocessing/integer_lookup.py\nCode: @@ -76,8 +76,9 @@ class IntegerLookup(IndexLookup):\n             If passing a file path, the file should contain one line per term\n             in the vocabulary. If this argument is set,\n             there is no need to `adapt()` the layer.\n-        vocabulary_dtype: The dtype of the vocabulary terms, for example\n-            `\"int64\"` or `\"int32\"`. Defaults to `\"int64\"`.\n+        vocabulary_dtype: The dtype of the vocabulary terms.\n+            Only `vocabulary_dtype='int64'` is supported at this time.\n+            Defaults to `\"int64\"`.\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nWhile updating the docstring is a good step to inform users about the current limitation, it would be more robust to enforce this constraint in the code. This would prevent users from accidentally using an unsupported `vocabulary_dtype` and encountering errors later.\n\nConsider adding a check in the `__init__` method of the `IntegerLookup` layer to raise a `ValueError` if `vocabulary_dtype` is not `'int64'`. For example:\n\n```python\n# In IntegerLookup.__init__\nif vocabulary_dtype != \"int64\":\n    raise ValueError(\n        \"Only `vocabulary_dtype='int64'` is supported at this time for \"\n        f\"IntegerLookup. Received: {vocabulary_dtype}\"\n    )\n```\n\nThis would provide immediate feedback to the user and make the layer's behavior more explicit.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "keras/src/layers/preprocessing/integer_lookup.py",
    "pr_number": 21587,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2278918456,
    "comment_created_at": "2025-08-15T12:38:39Z"
  },
  {
    "code": "@@ -17,6 +18,109 @@\n from keras.src.backend.openvino.core import ov_to_keras_type\n \n \n+def diagonal(x, offset=0, axis1=0, axis2=1):\n+    x_node = ov.constant(x)  # -> ov.Node\n+    offset_const = ov_opset.constant(int(offset), dtype=\"i64\")\n+\n+    # rank & normalize axes\n+    shape = ov_opset.shape_of(x_node)  # i64 vector\n+    rank = ov_opset.shape_of(shape)  # scalar i64 (len of shape)\n+    rank_val = ov_opset.squeeze(rank)  # [] -> scalar\n+    axis1_node = ov_opset.floor_mod(\n+        ov_opset.add(ov_opset.constant(int(axis1), dtype=\"i64\"), rank_val),\n+        rank_val,\n+    )\n+    axis2_node = ov_opset.floor_mod(\n+        ov_opset.add(ov_opset.constant(int(axis2), dtype=\"i64\"), rank_val),\n+        rank_val,\n+    )",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe implementation of diagonal doesn't correctly handle tensor inputs for x, offset, axis1, and axis2. it assumes they are python scalars or numpy arrays by using ov.constant and int(), which will lead to errors when openvinokerastensor objects are passed.\n\nto fix this and align with the style of other functions in this backend, please use the get_ov_output utility to convert inputs to openvino tensors and ov_opset.convert for type casting. this will also make the new import of opset13 on line 4 unnecessary.\n\n",
    "line_number": 36,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -17,6 +18,109 @@\n from keras.src.backend.openvino.core import ov_to_keras_type\n \n \n+def diagonal(x, offset=0, axis1=0, axis2=1):\n+    x_node = ov.constant(x)  # -> ov.Node\n+    offset_const = ov_opset.constant(int(offset), dtype=\"i64\")\n+\n+    # rank & normalize axes\n+    shape = ov_opset.shape_of(x_node)  # i64 vector\n+    rank = ov_opset.shape_of(shape)  # scalar i64 (len of shape)\n+    rank_val = ov_opset.squeeze(rank)  # [] -> scalar\n+    axis1_node = ov_opset.floor_mod(\n+        ov_opset.add(ov_opset.constant(int(axis1), dtype=\"i64\"), rank_val),\n+        rank_val,\n+    )\n+    axis2_node = ov_opset.floor_mod(\n+        ov_opset.add(ov_opset.constant(int(axis2), dtype=\"i64\"), rank_val),\n+        rank_val,\n+    )\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe implementation of `diagonal` doesn't correctly handle tensor inputs for `x`, `offset`, `axis1`, and `axis2`. It assumes they are Python scalars or numpy arrays by using `ov.constant` and `int()`, which will lead to errors when `OpenVINOKerasTensor` objects are passed.\n\nTo fix this and align with the style of other functions in this backend, please use the `get_ov_output` utility to convert inputs to OpenVINO tensors and `ov_opset.convert` for type casting. This will also make the new import of `opset13` on line 4 unnecessary.\n\n```suggestion\n    x_node = get_ov_output(x)\n    offset_const = ov_opset.convert(get_ov_output(offset), Type.i64)\n\n    # rank & normalize axes\n    shape = ov_opset.shape_of(x_node)  # i64 vector\n    rank = ov_opset.shape_of(shape)  # scalar i64 (len of shape)\n    rank_val = ov_opset.squeeze(rank)  # [] -> scalar\n    axis1_node = ov_opset.floor_mod(\n        ov_opset.add(ov_opset.convert(get_ov_output(axis1), Type.i64), rank_val),\n        rank_val,\n    )\n    axis2_node = ov_opset.floor_mod(\n        ov_opset.add(ov_opset.convert(get_ov_output(axis2), Type.i64), rank_val),\n        rank_val,\n    )\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21585,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2278892607,
    "comment_created_at": "2025-08-15T12:19:23Z"
  },
  {
    "code": "@@ -242,3 +245,311 @@ def _to_backend_layout(tensor_layout):\n     partition_spec = jax.sharding.PartitionSpec(*tensor_layout.axes)\n     jax_mesh = tensor_layout.device_mesh.backend_mesh\n     return jax.sharding.NamedSharding(jax_mesh, partition_spec)\n+\n+\n+_JAX_CLASSES_DEFINED = False\n+JaxGraph = None\n+JaxShardingPlanner = None\n+JaxShardApplier = None\n+\n+\n+def _define_and_register_jax_classes():\n+    global _JAX_CLASSES_DEFINED, JaxGraph, JaxShardingPlanner, JaxShardApplier\n+    if _JAX_CLASSES_DEFINED:\n+        return\n+\n+    from keras.src.distribution.autoshard_utils import MergeableGraph\n+\n+    def parse_jaxpr(jaxpr) -> MergeableGraph:\n+        graph = MergeableGraph()\n+\n+        def same_axis(node1, node2):\n+            var1, axis1 = node1\n+            var2, axis2 = node2\n+            if var1.aval.shape[axis1] != var2.aval.shape[axis2]:\n+                return\n+            graph.merge_nodes(node1, node2)\n+\n+        def parse_dot_general(eqn):\n+            lhs, rhs = eqn.invars\n+            out = eqn.outvars[0]\n+            (lc, rc), (lb, rb) = eqn.params[\"dimension_numbers\"]\n+            for l, r in zip(lc, rc):\n+                same_axis((lhs, l), (rhs, r))\n+            o_offset = 0\n+            for l, r in zip(lb, rb):\n+                same_axis((lhs, l), (rhs, r))\n+                same_axis((lhs, l), (out, o_offset))\n+                o_offset += 1\n+            for i in range(lhs.aval.ndim):\n+                if i not in lb and i not in lc:\n+                    same_axis((lhs, i), (out, o_offset))\n+                    o_offset += 1\n+            for j in range(rhs.aval.ndim):\n+                if j not in rb and j not in rc:\n+                    same_axis((rhs, j), (out, o_offset))\n+                    o_offset += 1\n+\n+        def parse_reshape(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            in_idx, out_idx = 0, 0\n+            in_prod, out_prod = 1, 1\n+            while in_idx < invar.aval.ndim and out_idx < out.aval.ndim:\n+                if (\n+                    in_prod == out_prod\n+                    and invar.aval.shape[in_idx] == out.aval.shape[out_idx]\n+                ):\n+                    if invar.aval.shape[in_idx] > 1:\n+                        same_axis((invar, in_idx), (out, out_idx))\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    out_prod *= out.aval.shape[out_idx]\n+                    in_idx += 1\n+                    out_idx += 1\n+                elif in_prod < out_prod:\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    in_idx += 1\n+                else:\n+                    out_prod *= out.aval.shape[out_idx]\n+                    out_idx += 1\n+\n+        def parse_transpose(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            for i, j in enumerate(eqn.params[\"permutation\"]):\n+                same_axis((invar, j), (out, i))\n+\n+        def parse_elementwise_with_broadcast(eqn):\n+            out = eqn.outvars[0]\n+            for invar in eqn.invars:\n+                if invar.aval.ndim == 0:\n+                    continue\n+                for i in range(1, min(invar.aval.ndim, out.aval.ndim) + 1):\n+                    in_axis, out_axis = -i, -i\n+                    if invar.aval.shape[in_axis] == out.aval.shape[out_axis]:\n+                        same_axis(\n+                            (invar, invar.aval.ndim + in_axis),\n+                            (out, out.aval.ndim + out_axis),\n+                        )\n+\n+        for var in jaxpr.jaxpr.invars:\n+            for i, j in itertools.combinations(range(var.aval.ndim), 2):\n+                graph.add_edge((var, i), (var, j))\n+\n+        for eqn in jaxpr.eqns:\n+            for outvar in eqn.outvars:\n+                for i, j in itertools.combinations(range(outvar.aval.ndim), 2):\n+                    graph.add_edge((outvar, i), (outvar, j))\n+\n+            primitive_parsers = {\n+                \"dot_general\": parse_dot_general,\n+                \"reshape\": parse_reshape,\n+                \"transpose\": parse_transpose,\n+            }\n+            parser = primitive_parsers.get(\n+                eqn.primitive.name, parse_elementwise_with_broadcast\n+            )\n+            parser(eqn)\n+        return graph\n+\n+    def shard_model(\n+        fn,\n+        params,\n+        *inputs,\n+        min_shard_size=1,\n+        data_axis_name=\"data\",\n+        model_axis_name=\"model\",\n+    ):\n+        \"\"\"Analyzes a function via jaxpr and returns sharding assignments.\"\"\"\n+        jaxpr, abs_ret = jax.make_jaxpr(fn, return_shape=True)(params, *inputs)\n+        graph = parse_jaxpr(jaxpr)\n+\n+        params_flat, params_treedef = jax.tree.flatten(params)\n+        _, inputs_treedef = jax.tree.flatten(inputs)\n+        _, outputs_treedef = jax.tree.flatten(abs_ret)\n+\n+        seen = collections.Counter()\n+        for var in jaxpr.jaxpr.invars[: len(params_flat)]:\n+            for i in range(var.aval.ndim):\n+                if var.aval.shape[i] >= min_shard_size:\n+                    seen.update([graph.get_root((var, i))])\n+\n+        model_axis_root = max(seen, key=seen.get) if seen else None\n+\n+        data_axes_roots = []\n+        for var in jaxpr.jaxpr.invars[len(params_flat) :]:\n+            for i in range(var.aval.ndim):\n+                root = graph.get_root((var, i))\n+                if root not in seen and root not in data_axes_roots:\n+                    data_axes_roots.append(root)\n+\n+        def assign_layouts(vars_flat, is_params=False):\n+            assignments = []\n+            for var in vars_flat:\n+                layout = [None] * var.aval.ndim\n+                for i in range(var.aval.ndim):\n+                    if var.aval.shape[i] < min_shard_size:\n+                        continue\n+                    root = graph.get_root((var, i))\n+                    if (\n+                        is_params\n+                        and model_axis_root\n+                        and root == model_axis_root\n+                    ):\n+                        layout[i] = model_axis_name\n+                    elif not is_params and root in data_axes_roots:\n+                        name = data_axis_name\n+                        if len(data_axes_roots) > 1:\n+                            name += str(data_axes_roots.index(root))\n+                        layout[i] = name\n+                assignments.append(layout)\n+            return assignments\n+\n+        params_assignments = params_treedef.unflatten(\n+            assign_layouts(\n+                jaxpr.jaxpr.invars[: len(params_flat)], is_params=True\n+            )\n+        )\n+        inputs_assignments = inputs_treedef.unflatten(\n+            assign_layouts(jaxpr.jaxpr.invars[len(params_flat) :])\n+        )\n+        output_assignments = outputs_treedef.unflatten(\n+            assign_layouts(jaxpr.jaxpr.outvars)\n+        )\n+\n+        return (params_assignments, *inputs_assignments), output_assignments\n+\n+    class _JaxGraph:\n+        \"\"\"A wrapper for a JAX computation graph (jaxpr) of a Keras model.\"\"\"\n+\n+        def __init__(\n+            self,\n+            jaxpr,\n+            trainable_variables,\n+            non_trainable_variables,\n+            in_treedefs,\n+        ):\n+            self.jaxpr = jaxpr\n+            self.trainable_variables = trainable_variables\n+            self.non_trainable_variables = non_trainable_variables\n+            self.in_treedefs = in_treedefs\n+\n+        @classmethod\n+        def from_model(cls, model, *args, **kwargs):\n+            \"\"\"Creates a _JaxGraph instance by tracing the model.\"\"\"\n+\n+            def stateless_fn(\n+                trainable_vars, non_trainable_vars, f_args, f_kwargs\n+            ):\n+                return model.stateless_call(\n+                    trainable_vars, non_trainable_vars, *f_args, **f_kwargs\n+                )\n+\n+            trainable_vars = model.trainable_variables\n+            non_trainable_vars = model.non_trainable_variables\n+\n+            _, t_vars_treedef = jax.tree.flatten(trainable_vars)\n+            _, nt_vars_treedef = jax.tree.flatten(non_trainable_vars)\n+            _, args_treedef = jax.tree.flatten(args)\n+            _, kwargs_treedef = jax.tree.flatten(kwargs)\n+            in_treedefs = (\n+                t_vars_treedef,\n+                nt_vars_treedef,\n+                args_treedef,\n+                kwargs_treedef,\n+            )\n+\n+            closed_jaxpr, _ = jax.make_jaxpr(stateless_fn, return_shape=True)(\n+                trainable_vars, non_trainable_vars, args, kwargs\n+            )\n+            return cls(\n+                closed_jaxpr, trainable_vars, non_trainable_vars, in_treedefs\n+            )\n+\n+    class _JaxShardingPlanner:\n+        \"\"\"\n+        Determines the optimal sharding layout for model variables using\n+        the embedded graph-parsing engine.\n+        \"\"\"\n+\n+        def plan(self, graph, device_mesh):\n+            t_vars = graph.trainable_variables\n+            nt_vars = graph.non_trainable_variables\n+\n+            all_in_avals = [var.aval for var in graph.jaxpr.jaxpr.invars]\n+            t_vars_leaves, _ = jax.tree.flatten(t_vars)\n+            nt_vars_leaves, _ = jax.tree.flatten(nt_vars)\n+\n+            pos = len(t_vars_leaves) + len(nt_vars_leaves)\n+\n+            args_treedef = graph.in_treedefs[2]\n+            kwargs_treedef = graph.in_treedefs[3]\n+\n+            num_args_leaves = args_treedef.num_leaves\n+            args_avals = all_in_avals[pos : pos + num_args_leaves]\n+            kwargs_avals = all_in_avals[pos + num_args_leaves :]\n+\n+            args_aval_tree = jax.tree.unflatten(args_treedef, args_avals)\n+            kwargs_aval_tree = jax.tree.unflatten(kwargs_treedef, kwargs_avals)\n+\n+            dummy_args = jax.tree.map(\n+                lambda x: np.zeros(x.shape, x.dtype), args_aval_tree\n+            )\n+            dummy_kwargs = jax.tree.map(\n+                lambda x: np.zeros(x.shape, x.dtype), kwargs_aval_tree\n+            )\n+\n+            def fn_to_trace(trainable_params, *fn_args, **fn_kwargs):\n+                \"\"\"A function that executes the original jaxpr computation.\"\"\"\n+                all_leaves = (\n+                    jax.tree.leaves(trainable_params)\n+                    + jax.tree.leaves(nt_vars)\n+                    + jax.tree.leaves(fn_args)\n+                    + jax.tree.leaves(fn_kwargs)\n+                )\n+                return jax.core.eval_jaxpr(\n+                    graph.jaxpr.jaxpr, graph.jaxpr.consts, *all_leaves\n+                )\n+\n+            (param_assignments, *_) = shard_model(\n+                fn_to_trace, t_vars, dummy_args, **dummy_kwargs\n+            )",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe call to shard_model here is incorrect as it passes **dummy_kwargs, but the shard_model function signature does not accept arbitrary keyword arguments. this will raise a typeerror at runtime if dummy_kwargs is not empty. \n\nfurthermore, the jax.make_jaxpr call inside shard_model is invoked only with positional arguments (*inputs), so any keyword arguments would be ignored even if the signature was corrected. this would lead to an incorrect sharding analysis for models that use keyword arguments in their call method.\n\nto fix this, the shard_model function needs to be updated to correctly handle both *args and **kwargs throughout its implementation, including passing them to jax.make_jaxpr and correctly processing the input treedefs.",
    "line_number": 514,
    "enriched": "File: keras/src/backend/jax/distribution_lib.py\nCode: @@ -242,3 +245,311 @@ def _to_backend_layout(tensor_layout):\n     partition_spec = jax.sharding.PartitionSpec(*tensor_layout.axes)\n     jax_mesh = tensor_layout.device_mesh.backend_mesh\n     return jax.sharding.NamedSharding(jax_mesh, partition_spec)\n+\n+\n+_JAX_CLASSES_DEFINED = False\n+JaxGraph = None\n+JaxShardingPlanner = None\n+JaxShardApplier = None\n+\n+\n+def _define_and_register_jax_classes():\n+    global _JAX_CLASSES_DEFINED, JaxGraph, JaxShardingPlanner, JaxShardApplier\n+    if _JAX_CLASSES_DEFINED:\n+        return\n+\n+    from keras.src.distribution.autoshard_utils import MergeableGraph\n+\n+    def parse_jaxpr(jaxpr) -> MergeableGraph:\n+        graph = MergeableGraph()\n+\n+        def same_axis(node1, node2):\n+            var1, axis1 = node1\n+            var2, axis2 = node2\n+            if var1.aval.shape[axis1] != var2.aval.shape[axis2]:\n+                return\n+            graph.merge_nodes(node1, node2)\n+\n+        def parse_dot_general(eqn):\n+            lhs, rhs = eqn.invars\n+            out = eqn.outvars[0]\n+            (lc, rc), (lb, rb) = eqn.params[\"dimension_numbers\"]\n+            for l, r in zip(lc, rc):\n+                same_axis((lhs, l), (rhs, r))\n+            o_offset = 0\n+            for l, r in zip(lb, rb):\n+                same_axis((lhs, l), (rhs, r))\n+                same_axis((lhs, l), (out, o_offset))\n+                o_offset += 1\n+            for i in range(lhs.aval.ndim):\n+                if i not in lb and i not in lc:\n+                    same_axis((lhs, i), (out, o_offset))\n+                    o_offset += 1\n+            for j in range(rhs.aval.ndim):\n+                if j not in rb and j not in rc:\n+                    same_axis((rhs, j), (out, o_offset))\n+                    o_offset += 1\n+\n+        def parse_reshape(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            in_idx, out_idx = 0, 0\n+            in_prod, out_prod = 1, 1\n+            while in_idx < invar.aval.ndim and out_idx < out.aval.ndim:\n+                if (\n+                    in_prod == out_prod\n+                    and invar.aval.shape[in_idx] == out.aval.shape[out_idx]\n+                ):\n+                    if invar.aval.shape[in_idx] > 1:\n+                        same_axis((invar, in_idx), (out, out_idx))\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    out_prod *= out.aval.shape[out_idx]\n+                    in_idx += 1\n+                    out_idx += 1\n+                elif in_prod < out_prod:\n+                    in_prod *= invar.aval.shape[in_idx]\n+                    in_idx += 1\n+                else:\n+                    out_prod *= out.aval.shape[out_idx]\n+                    out_idx += 1\n+\n+        def parse_transpose(eqn):\n+            invar, out = eqn.invars[0], eqn.outvars[0]\n+            for i, j in enumerate(eqn.params[\"permutation\"]):\n+                same_axis((invar, j), (out, i))\n+\n+        def parse_elementwise_with_broadcast(eqn):\n+            out = eqn.outvars[0]\n+            for invar in eqn.invars:\n+                if invar.aval.ndim == 0:\n+                    continue\n+                for i in range(1, min(invar.aval.ndim, out.aval.ndim) + 1):\n+                    in_axis, out_axis = -i, -i\n+                    if invar.aval.shape[in_axis] == out.aval.shape[out_axis]:\n+                        same_axis(\n+                            (invar, invar.aval.ndim + in_axis),\n+                            (out, out.aval.ndim + out_axis),\n+                        )\n+\n+        for var in jaxpr.jaxpr.invars:\n+            for i, j in itertools.combinations(range(var.aval.ndim), 2):\n+                graph.add_edge((var, i), (var, j))\n+\n+        for eqn in jaxpr.eqns:\n+            for outvar in eqn.outvars:\n+                for i, j in itertools.combinations(range(outvar.aval.ndim), 2):\n+                    graph.add_edge((outvar, i), (outvar, j))\n+\n+            primitive_parsers = {\n+                \"dot_general\": parse_dot_general,\n+                \"reshape\": parse_reshape,\n+                \"transpose\": parse_transpose,\n+            }\n+            parser = primitive_parsers.get(\n+                eqn.primitive.name, parse_elementwise_with_broadcast\n+            )\n+            parser(eqn)\n+        return graph\n+\n+    def shard_model(\n+        fn,\n+        params,\n+        *inputs,\n+        min_shard_size=1,\n+        data_axis_name=\"data\",\n+        model_axis_name=\"model\",\n+    ):\n+        \"\"\"Analyzes a function via jaxpr and returns sharding assignments.\"\"\"\n+        jaxpr, abs_ret = jax.make_jaxpr(fn, return_shape=True)(params, *inputs)\n+        graph = parse_jaxpr(jaxpr)\n+\n+        params_flat, params_treedef = jax.tree.flatten(params)\n+        _, inputs_treedef = jax.tree.flatten(inputs)\n+        _, outputs_treedef = jax.tree.flatten(abs_ret)\n+\n+        seen = collections.Counter()\n+        for var in jaxpr.jaxpr.invars[: len(params_flat)]:\n+            for i in range(var.aval.ndim):\n+                if var.aval.shape[i] >= min_shard_size:\n+                    seen.update([graph.get_root((var, i))])\n+\n+        model_axis_root = max(seen, key=seen.get) if seen else None\n+\n+        data_axes_roots = []\n+        for var in jaxpr.jaxpr.invars[len(params_flat) :]:\n+            for i in range(var.aval.ndim):\n+                root = graph.get_root((var, i))\n+                if root not in seen and root not in data_axes_roots:\n+                    data_axes_roots.append(root)\n+\n+        def assign_layouts(vars_flat, is_params=False):\n+            assignments = []\n+            for var in vars_flat:\n+                layout = [None] * var.aval.ndim\n+                for i in range(var.aval.ndim):\n+                    if var.aval.shape[i] < min_shard_size:\n+                        continue\n+                    root = graph.get_root((var, i))\n+                    if (\n+                        is_params\n+                        and model_axis_root\n+                        and root == model_axis_root\n+                    ):\n+                        layout[i] = model_axis_name\n+                    elif not is_params and root in data_axes_roots:\n+                        name = data_axis_name\n+                        if len(data_axes_roots) > 1:\n+                            name += str(data_axes_roots.index(root))\n+                        layout[i] = name\n+                assignments.append(layout)\n+            return assignments\n+\n+        params_assignments = params_treedef.unflatten(\n+            assign_layouts(\n+                jaxpr.jaxpr.invars[: len(params_flat)], is_params=True\n+            )\n+        )\n+        inputs_assignments = inputs_treedef.unflatten(\n+            assign_layouts(jaxpr.jaxpr.invars[len(params_flat) :])\n+        )\n+        output_assignments = outputs_treedef.unflatten(\n+            assign_layouts(jaxpr.jaxpr.outvars)\n+        )\n+\n+        return (params_assignments, *inputs_assignments), output_assignments\n+\n+    class _JaxGraph:\n+        \"\"\"A wrapper for a JAX computation graph (jaxpr) of a Keras model.\"\"\"\n+\n+        def __init__(\n+            self,\n+            jaxpr,\n+            trainable_variables,\n+            non_trainable_variables,\n+            in_treedefs,\n+        ):\n+            self.jaxpr = jaxpr\n+            self.trainable_variables = trainable_variables\n+            self.non_trainable_variables = non_trainable_variables\n+            self.in_treedefs = in_treedefs\n+\n+        @classmethod\n+        def from_model(cls, model, *args, **kwargs):\n+            \"\"\"Creates a _JaxGraph instance by tracing the model.\"\"\"\n+\n+            def stateless_fn(\n+                trainable_vars, non_trainable_vars, f_args, f_kwargs\n+            ):\n+                return model.stateless_call(\n+                    trainable_vars, non_trainable_vars, *f_args, **f_kwargs\n+                )\n+\n+            trainable_vars = model.trainable_variables\n+            non_trainable_vars = model.non_trainable_variables\n+\n+            _, t_vars_treedef = jax.tree.flatten(trainable_vars)\n+            _, nt_vars_treedef = jax.tree.flatten(non_trainable_vars)\n+            _, args_treedef = jax.tree.flatten(args)\n+            _, kwargs_treedef = jax.tree.flatten(kwargs)\n+            in_treedefs = (\n+                t_vars_treedef,\n+                nt_vars_treedef,\n+                args_treedef,\n+                kwargs_treedef,\n+            )\n+\n+            closed_jaxpr, _ = jax.make_jaxpr(stateless_fn, return_shape=True)(\n+                trainable_vars, non_trainable_vars, args, kwargs\n+            )\n+            return cls(\n+                closed_jaxpr, trainable_vars, non_trainable_vars, in_treedefs\n+            )\n+\n+    class _JaxShardingPlanner:\n+        \"\"\"\n+        Determines the optimal sharding layout for model variables using\n+        the embedded graph-parsing engine.\n+        \"\"\"\n+\n+        def plan(self, graph, device_mesh):\n+            t_vars = graph.trainable_variables\n+            nt_vars = graph.non_trainable_variables\n+\n+            all_in_avals = [var.aval for var in graph.jaxpr.jaxpr.invars]\n+            t_vars_leaves, _ = jax.tree.flatten(t_vars)\n+            nt_vars_leaves, _ = jax.tree.flatten(nt_vars)\n+\n+            pos = len(t_vars_leaves) + len(nt_vars_leaves)\n+\n+            args_treedef = graph.in_treedefs[2]\n+            kwargs_treedef = graph.in_treedefs[3]\n+\n+            num_args_leaves = args_treedef.num_leaves\n+            args_avals = all_in_avals[pos : pos + num_args_leaves]\n+            kwargs_avals = all_in_avals[pos + num_args_leaves :]\n+\n+            args_aval_tree = jax.tree.unflatten(args_treedef, args_avals)\n+            kwargs_aval_tree = jax.tree.unflatten(kwargs_treedef, kwargs_avals)\n+\n+            dummy_args = jax.tree.map(\n+                lambda x: np.zeros(x.shape, x.dtype), args_aval_tree\n+            )\n+            dummy_kwargs = jax.tree.map(\n+                lambda x: np.zeros(x.shape, x.dtype), kwargs_aval_tree\n+            )\n+\n+            def fn_to_trace(trainable_params, *fn_args, **fn_kwargs):\n+                \"\"\"A function that executes the original jaxpr computation.\"\"\"\n+                all_leaves = (\n+                    jax.tree.leaves(trainable_params)\n+                    + jax.tree.leaves(nt_vars)\n+                    + jax.tree.leaves(fn_args)\n+                    + jax.tree.leaves(fn_kwargs)\n+                )\n+                return jax.core.eval_jaxpr(\n+                    graph.jaxpr.jaxpr, graph.jaxpr.consts, *all_leaves\n+                )\n+\n+            (param_assignments, *_) = shard_model(\n+                fn_to_trace, t_vars, dummy_args, **dummy_kwargs\n+            )\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe call to `shard_model` here is incorrect as it passes `**dummy_kwargs`, but the `shard_model` function signature does not accept arbitrary keyword arguments. This will raise a `TypeError` at runtime if `dummy_kwargs` is not empty. \n\nFurthermore, the `jax.make_jaxpr` call inside `shard_model` is invoked only with positional arguments (`*inputs`), so any keyword arguments would be ignored even if the signature was corrected. This would lead to an incorrect sharding analysis for models that use keyword arguments in their `call` method.\n\nTo fix this, the `shard_model` function needs to be updated to correctly handle both `*args` and `**kwargs` throughout its implementation, including passing them to `jax.make_jaxpr` and correctly processing the input treedefs.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/jax/distribution_lib.py",
    "pr_number": 21583,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2277143735,
    "comment_created_at": "2025-08-14T16:38:30Z"
  },
  {
    "code": "@@ -1638,6 +1638,14 @@ def isneginf(x):\n     return tf.math.equal(x, -tf.constant(float(\"inf\"), dtype=x.dtype))\n \n \n+def isposinf(x):\n+    x = convert_to_tensor(x)\n+    dtype_as_dtype = tf.as_dtype(x.dtype)\n+    if dtype_as_dtype.is_integer or not dtype_as_dtype.is_numeric:\n+        return tf.zeros(x.shape, tf.bool)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nusing x.shape with tf.zeros can cause an error when x has a dynamic shape (i.e., contains none in its shape tuple), as tf.zeros does not support none in the shape argument. using tf.zeros_like(x, dtype=tf.bool) is a safer alternative that correctly handles both static and dynamic shapes.\n\n",
    "line_number": 1645,
    "enriched": "File: keras/src/backend/tensorflow/numpy.py\nCode: @@ -1638,6 +1638,14 @@ def isneginf(x):\n     return tf.math.equal(x, -tf.constant(float(\"inf\"), dtype=x.dtype))\n \n \n+def isposinf(x):\n+    x = convert_to_tensor(x)\n+    dtype_as_dtype = tf.as_dtype(x.dtype)\n+    if dtype_as_dtype.is_integer or not dtype_as_dtype.is_numeric:\n+        return tf.zeros(x.shape, tf.bool)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nUsing `x.shape` with `tf.zeros` can cause an error when `x` has a dynamic shape (i.e., contains `None` in its shape tuple), as `tf.zeros` does not support `None` in the shape argument. Using `tf.zeros_like(x, dtype=tf.bool)` is a safer alternative that correctly handles both static and dynamic shapes.\n\n```suggestion\n        return tf.zeros_like(x, dtype=tf.bool)\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/backend/tensorflow/numpy.py",
    "pr_number": 21582,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2276121577,
    "comment_created_at": "2025-08-14T09:40:59Z"
  },
  {
    "code": "@@ -843,9 +843,16 @@ def prepare_slice_index(val):\n     start = ov_opset.concat(start, axis=0).output(0)\n     stop = ov_opset.concat(stop, axis=0).output(0)\n     axes = ov_opset.constant(axes, Type.i32).output(0)\n-    return OpenVINOKerasTensor(\n-        ov_opset.slice(inputs, start, stop, step, axes).output(0)\n-    )\n+    result = ov_opset.slice(inputs, start, stop, step, axes).output(0)\n+\n+    # Apply reshape to ensure output matches expected shape\n+    # Convert None (dynamic) dimensions to -1 for OpenVINO compatibility\n+    if all(dim is None or (isinstance(dim, int) and dim >= 0) for dim in shape):\n+        reshape_pattern = [(-1 if dim is None else dim) for dim in shape]\n+        target_shape = ov_opset.constant(reshape_pattern, Type.i32).output(0)\n+        result = ov_opset.reshape(result, target_shape, False).output(0)",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe condition to apply the reshape is too restrictive as it only handles cases where the shape argument is a tuple of static integers or none. this means the fix for preserving shape information doesn't apply when shape contains dynamic dimensions (i.e., tensors), which is a key use case. this makes the fix incomplete because it won't work for dynamically shaped slices.\n\na more general approach is needed to construct the target_shape for the reshape operation that handles both static and dynamic values in the shape tuple.\n\npython\n    target_shape_nodes = []\n    for dim in shape:\n        if dim is none:\n            # use -1 to represent a dynamic dimension for reshape.\n            node = ov_opset.constant([-1], type.i32).output(0)\n        else:\n            # convert dim to an openvino tensor if it's not already one.\n            node = convert_to_tensor(dim, dtype=\"int32\").output\n            # reshape's shape input must be 1d, so unsqueeze scalars.\n            if len(node.get_partial_shape()) == 0:\n                node = ov_opset.unsqueeze(\n                    node, ov_opset.constant(0, type.i32)\n                ).output(0)\n        target_shape_nodes.append(node)\n\n    # concatenate all parts to form the final shape tensor.\n    target_shape = ov_opset.concat(target_shape_nodes, axis=0).output(0)\n    result = ov_opset.reshape(result, target_shape, false).output(0)\n",
    "line_number": 853,
    "enriched": "File: keras/src/backend/openvino/core.py\nCode: @@ -843,9 +843,16 @@ def prepare_slice_index(val):\n     start = ov_opset.concat(start, axis=0).output(0)\n     stop = ov_opset.concat(stop, axis=0).output(0)\n     axes = ov_opset.constant(axes, Type.i32).output(0)\n-    return OpenVINOKerasTensor(\n-        ov_opset.slice(inputs, start, stop, step, axes).output(0)\n-    )\n+    result = ov_opset.slice(inputs, start, stop, step, axes).output(0)\n+\n+    # Apply reshape to ensure output matches expected shape\n+    # Convert None (dynamic) dimensions to -1 for OpenVINO compatibility\n+    if all(dim is None or (isinstance(dim, int) and dim >= 0) for dim in shape):\n+        reshape_pattern = [(-1 if dim is None else dim) for dim in shape]\n+        target_shape = ov_opset.constant(reshape_pattern, Type.i32).output(0)\n+        result = ov_opset.reshape(result, target_shape, False).output(0)\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe condition to apply the reshape is too restrictive as it only handles cases where the `shape` argument is a tuple of static integers or `None`. This means the fix for preserving shape information doesn't apply when `shape` contains dynamic dimensions (i.e., tensors), which is a key use case. This makes the fix incomplete because it won't work for dynamically shaped slices.\n\nA more general approach is needed to construct the `target_shape` for the `reshape` operation that handles both static and dynamic values in the `shape` tuple.\n\n```python\n    target_shape_nodes = []\n    for dim in shape:\n        if dim is None:\n            # Use -1 to represent a dynamic dimension for reshape.\n            node = ov_opset.constant([-1], Type.i32).output(0)\n        else:\n            # Convert dim to an OpenVINO tensor if it's not already one.\n            node = convert_to_tensor(dim, dtype=\"int32\").output\n            # Reshape's shape input must be 1D, so unsqueeze scalars.\n            if len(node.get_partial_shape()) == 0:\n                node = ov_opset.unsqueeze(\n                    node, ov_opset.constant(0, Type.i32)\n                ).output(0)\n        target_shape_nodes.append(node)\n\n    # Concatenate all parts to form the final shape tensor.\n    target_shape = ov_opset.concat(target_shape_nodes, axis=0).output(0)\n    result = ov_opset.reshape(result, target_shape, False).output(0)\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/core.py",
    "pr_number": 21580,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2275163397,
    "comment_created_at": "2025-08-14T02:18:12Z"
  },
  {
    "code": "@@ -410,32 +438,11 @@ def compute_weight_mat(\n \n \n def _resize(image, shape, method, antialias):\n-    def _fill_triangle_kernel(x):\n-        return np.maximum(0, 1 - np.abs(x))\n-\n-    def _fill_keys_cubic_kernel(x):\n-        out = ((1.5 * x - 2.5) * x) * x + 1.0\n-        out = np.where(x >= 1.0, ((-0.5 * x + 2.5) * x - 4.0) * x + 2.0, out)\n-        return np.where(x >= 2.0, 0.0, out)\n-\n-    def _fill_lanczos_kernel(radius, x):\n-        y = radius * np.sin(np.pi * x) * np.sin(np.pi * x / radius)\n-        out = np.where(\n-            x > 1e-3, np.divide(y, np.where(x != 0, np.pi**2 * x**2, 1)), 1\n-        )\n-        return np.where(x > radius, 0.0, out)\n-\n     if method == \"nearest\":\n         return _resize_nearest(image, shape)\n-    elif method == \"bilinear\":\n-        kernel = _fill_triangle_kernel\n-    elif method == \"lanczos3\":\n-        kernel = lambda x: _fill_lanczos_kernel(3.0, x)\n-    elif method == \"lanczos5\":\n-        kernel = lambda x: _fill_lanczos_kernel(5.0, x)\n-    elif method == \"bicubic\":\n-        kernel = _fill_keys_cubic_kernel\n     else:\n+        kernel = _kernels.get(method, None)\n+    if kernel is None:\n         raise ValueError(\"Unknown resize method\")",
    "comment": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nthe refactoring of _resize introduces a bug. the resize operation supports interpolation methods like \"bilinear\" and \"bicubic\". after this change, _resize is called with these method names, but the _kernels dictionary uses \"linear\" and \"cubic\" as keys. this will cause _kernels.get(method, none) to return none, raising a valueerror.\n\nto fix this, you should map the interpolation method names similarly to how it's done in the new scale_and_translate function.\n\n",
    "line_number": 446,
    "enriched": "File: keras/src/backend/numpy/image.py\nCode: @@ -410,32 +438,11 @@ def compute_weight_mat(\n \n \n def _resize(image, shape, method, antialias):\n-    def _fill_triangle_kernel(x):\n-        return np.maximum(0, 1 - np.abs(x))\n-\n-    def _fill_keys_cubic_kernel(x):\n-        out = ((1.5 * x - 2.5) * x) * x + 1.0\n-        out = np.where(x >= 1.0, ((-0.5 * x + 2.5) * x - 4.0) * x + 2.0, out)\n-        return np.where(x >= 2.0, 0.0, out)\n-\n-    def _fill_lanczos_kernel(radius, x):\n-        y = radius * np.sin(np.pi * x) * np.sin(np.pi * x / radius)\n-        out = np.where(\n-            x > 1e-3, np.divide(y, np.where(x != 0, np.pi**2 * x**2, 1)), 1\n-        )\n-        return np.where(x > radius, 0.0, out)\n-\n     if method == \"nearest\":\n         return _resize_nearest(image, shape)\n-    elif method == \"bilinear\":\n-        kernel = _fill_triangle_kernel\n-    elif method == \"lanczos3\":\n-        kernel = lambda x: _fill_lanczos_kernel(3.0, x)\n-    elif method == \"lanczos5\":\n-        kernel = lambda x: _fill_lanczos_kernel(5.0, x)\n-    elif method == \"bicubic\":\n-        kernel = _fill_keys_cubic_kernel\n     else:\n+        kernel = _kernels.get(method, None)\n+    if kernel is None:\n         raise ValueError(\"Unknown resize method\")\nComment: ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe refactoring of `_resize` introduces a bug. The `resize` operation supports `interpolation` methods like `\"bilinear\"` and `\"bicubic\"`. After this change, `_resize` is called with these method names, but the `_kernels` dictionary uses `\"linear\"` and `\"cubic\"` as keys. This will cause `_kernels.get(method, None)` to return `None`, raising a `ValueError`.\n\nTo fix this, you should map the interpolation method names similarly to how it's done in the new `scale_and_translate` function.\n\n```suggestion\n    else:\n        if method == \"bilinear\":\n            method = \"linear\"\n        elif method == \"bicubic\":\n            method = \"cubic\"\n        kernel = _kernels.get(method, None)\n    if kernel is None:\n        raise ValueError(f\"Unknown resize method: {method}\")\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "keras/src/backend/numpy/image.py",
    "pr_number": 21577,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2272747210,
    "comment_created_at": "2025-08-13T09:45:06Z"
  },
  {
    "code": "@@ -99,13 +99,11 @@ exclude_lines = [\n ]\n omit = [\n     \"*/*_test.py\",\n-    \"keras/src/legacy/*\",\n ]",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nto improve maintainability and reduce redundancy, you can remove this omit configuration. according to the coverage.py documentation, if an option like omit is specified under [tool.coverage.run] but not under [tool.coverage.report], the [run] configuration will be used for both execution and reporting. since the omit list here is identical to the one in [tool.coverage.run], this block is redundant and can be removed.",
    "line_number": 103,
    "enriched": "File: pyproject.toml\nCode: @@ -99,13 +99,11 @@ exclude_lines = [\n ]\n omit = [\n     \"*/*_test.py\",\n-    \"keras/src/legacy/*\",\n ]\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nTo improve maintainability and reduce redundancy, you can remove this `omit` configuration. According to the `coverage.py` documentation, if an option like `omit` is specified under `[tool.coverage.run]` but not under `[tool.coverage.report]`, the `[run]` configuration will be used for both execution and reporting. Since the `omit` list here is identical to the one in `[tool.coverage.run]`, this block is redundant and can be removed.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "pyproject.toml",
    "pr_number": 21576,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2271853894,
    "comment_created_at": "2025-08-13T01:15:12Z"
  },
  {
    "code": "@@ -123,7 +123,11 @@ def __new__(cls, *args, **kwargs):\n         if backend.backend() == \"jax\" and is_nnx_enabled():\n             from flax import nnx\n \n-            vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n+            try:\n+                vars(instance)[\"_pytree__state\"] = nnx.pytreelib.PytreeState()\n+            except AttributeError:\n+                vars(instance)[\"_object__state\"] = nnx.object.ObjectState()",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe use of a broad try...except attributeerror block can be brittle. it might catch an attributeerror from an unexpected source within the pytreestate() call, potentially masking other issues. a more explicit check for the existence of nnx.pytreelib.pytreestate using hasattr would be more robust and clearer about the intent, which is to handle different versions of the flax library.\n\n",
    "line_number": 129,
    "enriched": "File: keras/src/ops/operation.py\nCode: @@ -123,7 +123,11 @@ def __new__(cls, *args, **kwargs):\n         if backend.backend() == \"jax\" and is_nnx_enabled():\n             from flax import nnx\n \n-            vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n+            try:\n+                vars(instance)[\"_pytree__state\"] = nnx.pytreelib.PytreeState()\n+            except AttributeError:\n+                vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe use of a broad `try...except AttributeError` block can be brittle. It might catch an `AttributeError` from an unexpected source within the `PytreeState()` call, potentially masking other issues. A more explicit check for the existence of `nnx.pytreelib.PytreeState` using `hasattr` would be more robust and clearer about the intent, which is to handle different versions of the `flax` library.\n\n```suggestion\n            if hasattr(nnx, \"pytreelib\") and hasattr(nnx.pytreelib, \"PytreeState\"):\n                vars(instance)[\"_pytree__state\"] = nnx.pytreelib.PytreeState()\n            else:\n                vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/ops/operation.py",
    "pr_number": 21570,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2267694130,
    "comment_created_at": "2025-08-11T18:50:47Z"
  },
  {
    "code": "@@ -53,19 +65,23 @@ def compute_output_spec(self, inputs):\n             shape=output_shape, dtype=inputs.dtype, sparse=inputs.sparse\n         )\n \n-    def build(self, input_shape):\n-        sample_output_shape = operation_utils.compute_reshape_output_shape(\n-            input_shape[1:], self.target_shape, \"target_shape\"\n-        )\n-        self._resolved_target_shape = tuple(\n-            -1 if d is None else d for d in sample_output_shape\n-        )\n-\n     def call(self, inputs):\n+        target_shape = self.target_shape\n+        if self.need_explicit_shape_for_batch_size_None and (inputs.shape[0] is None):\n+            input_nonbatch_shape = tuple(inputs.shape[1:])\n+            if input_nonbatch_shape.count(None) == 0:\n+                # If the input shape is fully defined, we can compute the desired target_shape\n+                if True:\n+                    inp_nonbatch_size = math.prod(inputs.shape[1:])\n+                else:\n+                    inp_nonbatch_size = ops.prod(ops.shape(inputs)[1:])",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthis if true: construct with an else branch appears to be leftover debugging code. the else branch is unreachable, creating dead code. given the check on line 72, input_nonbatch_shape.count(none) == 0, the non-batch dimensions are guaranteed to be fully known. therefore, the conditional is unnecessary and can be simplified for better readability and maintainability.\n\npython\n                inp_nonbatch_size = math.prod(inputs.shape[1:])\n",
    "line_number": 77,
    "enriched": "File: keras/src/layers/reshaping/reshape.py\nCode: @@ -53,19 +65,23 @@ def compute_output_spec(self, inputs):\n             shape=output_shape, dtype=inputs.dtype, sparse=inputs.sparse\n         )\n \n-    def build(self, input_shape):\n-        sample_output_shape = operation_utils.compute_reshape_output_shape(\n-            input_shape[1:], self.target_shape, \"target_shape\"\n-        )\n-        self._resolved_target_shape = tuple(\n-            -1 if d is None else d for d in sample_output_shape\n-        )\n-\n     def call(self, inputs):\n+        target_shape = self.target_shape\n+        if self.need_explicit_shape_for_batch_size_None and (inputs.shape[0] is None):\n+            input_nonbatch_shape = tuple(inputs.shape[1:])\n+            if input_nonbatch_shape.count(None) == 0:\n+                # If the input shape is fully defined, we can compute the desired target_shape\n+                if True:\n+                    inp_nonbatch_size = math.prod(inputs.shape[1:])\n+                else:\n+                    inp_nonbatch_size = ops.prod(ops.shape(inputs)[1:])\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis `if True:` construct with an `else` branch appears to be leftover debugging code. The `else` branch is unreachable, creating dead code. Given the check on line 72, `input_nonbatch_shape.count(None) == 0`, the non-batch dimensions are guaranteed to be fully known. Therefore, the conditional is unnecessary and can be simplified for better readability and maintainability.\n\n```python\n                inp_nonbatch_size = math.prod(inputs.shape[1:])\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "keras/src/layers/reshaping/reshape.py",
    "pr_number": 21568,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2264846641,
    "comment_created_at": "2025-08-09T16:29:52Z"
  },
  {
    "code": "@@ -1303,7 +1303,43 @@ def pad(x, pad_width, mode=\"constant\", constant_values=None):\n \n \n def prod(x, axis=None, keepdims=False, dtype=None):\n-    raise NotImplementedError(\"`prod` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    \n+    # Convert to appropriate type for reduction\n+    if x_type == Type.boolean:\n+        # Convert boolean to int32 for reduction\n+        x = ov_opset.convert(x, Type.i32).output(0)\n+    \n+    if axis is None:\n+        flatten_shape = ov_opset.constant([-1], Type.i32).output(0)\n+        x = ov_opset.reshape(x, flatten_shape, False).output(0)\n+        axis = 0\n+    \n+    if isinstance(axis, tuple):\n+        axis = list(axis)\n+    axis = ov_opset.constant(axis, Type.i32).output(0)\n+    \n+    # Compute the product\n+    result = ov_opset.reduce_prod(x, axis, keepdims).output(0)\n+    \n+    # Convert to the specified dtype if provided\n+    if dtype is not None:\n+        ov_dtype = _convert_to_ov_type(dtype)\n+        result = ov_opset.convert(result, ov_dtype).output(0)\n+    else:\n+        # Handle dtype promotion rules\n+        if x_type == Type.boolean:\n+            result = ov_opset.convert(result, Type.i32).output(0)\n+        elif x_type == Type.i64:\n+            result = ov_opset.convert(result, Type.i64).output(0)\n+        elif x_type in (Type.u8, Type.u16, Type.u32):\n+            result = ov_opset.convert(result, Type.u32).output(0)\n+        elif x_type.is_integral():\n+            result = ov_opset.convert(result, Type.i32).output(0)\n+    \n+    return OpenVINOKerasTensor(result)",
    "comment": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nthe current implementation has a couple of issues:\n1.  _convert_to_ov_type is not defined, which will cause a nameerror at runtime.\n2.  the dtype promotion logic for uint64 is incorrect, as it gets converted to int32, which can lead to data loss.\n3.  there's a redundant type conversion for boolean inputs, which is inefficient.\n\ni suggest refactoring the function to handle dtype promotion before the reduction. this makes the logic cleaner, more efficient, and corrects the bugs.\n\npython\n    x = get_ov_output(x)\n\n    # if a specific dtype is requested, cast the input to that dtype.\n    if dtype is not none:\n        ov_dtype = openvino_dtypes[standardize_dtype(dtype)]\n        x = ov_opset.convert(x, ov_dtype).output(0)\n    # otherwise, apply dtype promotion rules before reduction.\n    else:\n        x_type = x.get_element_type()\n        if x_type == type.boolean:\n            x = ov_opset.convert(x, type.i32).output(0)\n        elif x_type in (type.i8, type.i16):\n            x = ov_opset.convert(x, type.i32).output(0)\n        elif x_type in (type.u8, type.u16):\n            x = ov_opset.convert(x, type.u32).output(0)\n\n    if axis is none:\n        flatten_shape = ov_opset.constant([-1], type.i32).output(0)\n        x = ov_opset.reshape(x, flatten_shape, false).output(0)\n        axis = 0\n\n    if isinstance(axis, tuple):\n        axis = list(axis)\n    axis = ov_opset.constant(axis, type.i32).output(0)\n\n    # compute the product\n    result = ov_opset.reduce_prod(x, axis, keepdims).output(0)\n\n    return openvinokerastensor(result)\n",
    "line_number": 1341,
    "enriched": "File: keras/src/backend/openvino/numpy.py\nCode: @@ -1303,7 +1303,43 @@ def pad(x, pad_width, mode=\"constant\", constant_values=None):\n \n \n def prod(x, axis=None, keepdims=False, dtype=None):\n-    raise NotImplementedError(\"`prod` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    \n+    # Convert to appropriate type for reduction\n+    if x_type == Type.boolean:\n+        # Convert boolean to int32 for reduction\n+        x = ov_opset.convert(x, Type.i32).output(0)\n+    \n+    if axis is None:\n+        flatten_shape = ov_opset.constant([-1], Type.i32).output(0)\n+        x = ov_opset.reshape(x, flatten_shape, False).output(0)\n+        axis = 0\n+    \n+    if isinstance(axis, tuple):\n+        axis = list(axis)\n+    axis = ov_opset.constant(axis, Type.i32).output(0)\n+    \n+    # Compute the product\n+    result = ov_opset.reduce_prod(x, axis, keepdims).output(0)\n+    \n+    # Convert to the specified dtype if provided\n+    if dtype is not None:\n+        ov_dtype = _convert_to_ov_type(dtype)\n+        result = ov_opset.convert(result, ov_dtype).output(0)\n+    else:\n+        # Handle dtype promotion rules\n+        if x_type == Type.boolean:\n+            result = ov_opset.convert(result, Type.i32).output(0)\n+        elif x_type == Type.i64:\n+            result = ov_opset.convert(result, Type.i64).output(0)\n+        elif x_type in (Type.u8, Type.u16, Type.u32):\n+            result = ov_opset.convert(result, Type.u32).output(0)\n+        elif x_type.is_integral():\n+            result = ov_opset.convert(result, Type.i32).output(0)\n+    \n+    return OpenVINOKerasTensor(result)\nComment: ![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe current implementation has a couple of issues:\n1.  `_convert_to_ov_type` is not defined, which will cause a `NameError` at runtime.\n2.  The dtype promotion logic for `uint64` is incorrect, as it gets converted to `int32`, which can lead to data loss.\n3.  There's a redundant type conversion for boolean inputs, which is inefficient.\n\nI suggest refactoring the function to handle dtype promotion before the reduction. This makes the logic cleaner, more efficient, and corrects the bugs.\n\n```python\n    x = get_ov_output(x)\n\n    # If a specific dtype is requested, cast the input to that dtype.\n    if dtype is not None:\n        ov_dtype = OPENVINO_DTYPES[standardize_dtype(dtype)]\n        x = ov_opset.convert(x, ov_dtype).output(0)\n    # Otherwise, apply dtype promotion rules before reduction.\n    else:\n        x_type = x.get_element_type()\n        if x_type == Type.boolean:\n            x = ov_opset.convert(x, Type.i32).output(0)\n        elif x_type in (Type.i8, Type.i16):\n            x = ov_opset.convert(x, Type.i32).output(0)\n        elif x_type in (Type.u8, Type.u16):\n            x = ov_opset.convert(x, Type.u32).output(0)\n\n    if axis is None:\n        flatten_shape = ov_opset.constant([-1], Type.i32).output(0)\n        x = ov_opset.reshape(x, flatten_shape, False).output(0)\n        axis = 0\n\n    if isinstance(axis, tuple):\n        axis = list(axis)\n    axis = ov_opset.constant(axis, Type.i32).output(0)\n\n    # Compute the product\n    result = ov_opset.reduce_prod(x, axis, keepdims).output(0)\n\n    return OpenVINOKerasTensor(result)\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "keras/src/backend/openvino/numpy.py",
    "pr_number": 21567,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2264557873,
    "comment_created_at": "2025-08-09T06:42:29Z"
  },
  {
    "code": "@@ -13,9 +13,9 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        python-version: ['3.10']\n+        python-version: [\"3.10\"]\n         backend: [tensorflow, jax, torch, numpy]\n-    name: Run tests\n+    name: Run tests (Python 3.10)",
    "comment": "does it work if you put ${{ matrix.python-version }} here?\n\nthe goal is to minimize the number of things to change when we upgrade python.",
    "line_number": 18,
    "enriched": "File: .github/workflows/nightly.yml\nCode: @@ -13,9 +13,9 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        python-version: ['3.10']\n+        python-version: [\"3.10\"]\n         backend: [tensorflow, jax, torch, numpy]\n-    name: Run tests\n+    name: Run tests (Python 3.10)\nComment: Does it work if you put `${{ matrix.python-version }}` here?\n\nThe goal is to minimize the number of things to change when we upgrade Python.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".github/workflows/nightly.yml",
    "pr_number": 21566,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2407545918,
    "comment_created_at": "2025-10-06T17:16:51Z"
  },
  {
    "code": "@@ -123,7 +123,7 @@ def __new__(cls, *args, **kwargs):\n         if backend.backend() == \"jax\" and is_nnx_enabled():\n             from flax import nnx\n \n-            vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n+            vars(instance)[\"_pytree__state\"] = nnx.pytreelib.PytreeState()",
    "comment": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nthe nnx.pytreelib module appears to be an internal api of flax.nnx. relying on internal apis can make the code brittle and lead to breakages when flax is updated. to improve long-term maintainability, it's preferable to use a public api for making an object a pytree, if one exists. for example, flax.nnx might provide a registration function or a base class for this purpose.",
    "line_number": 126,
    "enriched": "File: keras/src/ops/operation.py\nCode: @@ -123,7 +123,7 @@ def __new__(cls, *args, **kwargs):\n         if backend.backend() == \"jax\" and is_nnx_enabled():\n             from flax import nnx\n \n-            vars(instance)[\"_object__state\"] = nnx.object.ObjectState()\n+            vars(instance)[\"_pytree__state\"] = nnx.pytreelib.PytreeState()\nComment: ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe `nnx.pytreelib` module appears to be an internal API of `flax.nnx`. Relying on internal APIs can make the code brittle and lead to breakages when `flax` is updated. To improve long-term maintainability, it's preferable to use a public API for making an object a Pytree, if one exists. For example, `flax.nnx` might provide a registration function or a base class for this purpose.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "keras/src/ops/operation.py",
    "pr_number": 21565,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 2263792509,
    "comment_created_at": "2025-08-08T18:57:31Z"
  },
  {
    "code": "@@ -281,6 +281,17 @@\n cb.set_ticks([-500, 0, 1000, 2000, 3000, 4000])\n plt.show()\n \n+# %%\n+#\n+# .. note::",
    "comment": "i think this note is going to be pretty mysterious to folks who don't know what the previous behaviour was.  if i were doing this, i'd just add another example to this doc that shows manipulating the colorbar scale, either with this example or another one.",
    "line_number": 286,
    "enriched": "File: galleries/users_explain/colors/colormapnorms.py\nCode: @@ -281,6 +281,17 @@\n cb.set_ticks([-500, 0, 1000, 2000, 3000, 4000])\n plt.show()\n \n+# %%\n+#\n+# .. note::\nComment: I think this note is going to be pretty mysterious to folks who don't know what the previous behaviour was.  If I were doing this, I'd just add another example to this doc that shows manipulating the colorbar scale, either with this example or another one.  ",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "galleries/users_explain/colors/colormapnorms.py",
    "pr_number": 30639,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2407999639,
    "comment_created_at": "2025-10-06T18:56:41Z"
  },
  {
    "code": "@@ -7772,13 +7772,10 @@ def hist2d(self, x, y, bins=10, range=None, density=False, weights=None,\n \n         Notes\n         -----\n-        - Currently ``hist2d`` calculates its own axis limits, and any limits",
    "comment": "i think we should leave a .. versionchanged:: notice here.",
    "line_number": 7775,
    "enriched": "File: lib/matplotlib/axes/_axes.py\nCode: @@ -7772,13 +7772,10 @@ def hist2d(self, x, y, bins=10, range=None, density=False, weights=None,\n \n         Notes\n         -----\n-        - Currently ``hist2d`` calculates its own axis limits, and any limits\nComment: I think we should leave a `.. versionchanged::` notice here.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "lib/matplotlib/axes/_axes.py",
    "pr_number": 30634,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2404371042,
    "comment_created_at": "2025-10-05T08:59:38Z"
  },
  {
    "code": "@@ -1981,7 +1981,7 @@ def test_mult_norm_call_types():\n     mn.vmin = (-2, -2)\n     mn.vmax = (2, 2)\n \n-    vals = np.arange(6).reshape((3,2))\n+    vals = np.arange(6, dtype='int64').reshape((3, 2))",
    "comment": "i assume what fails is the asserion below\r\n\r\n\r\nassert no_norm_out[1].dtype == np.dtype('int64')\r\n\r\n\r\nif so, wouldn't it be slightly better to leave vals as is and change the assert to\r\n\r\nassert no_norm_out[1].dtype == vals.dtype\r\n\r\n?",
    "line_number": 1984,
    "enriched": "File: lib/matplotlib/tests/test_colors.py\nCode: @@ -1981,7 +1981,7 @@ def test_mult_norm_call_types():\n     mn.vmin = (-2, -2)\n     mn.vmax = (2, 2)\n \n-    vals = np.arange(6).reshape((3,2))\n+    vals = np.arange(6, dtype='int64').reshape((3, 2))\nComment: I assume what fails is the asserion below\r\n\r\n```\r\nassert no_norm_out[1].dtype == np.dtype('int64')\r\n```\r\n\r\nIf so, wouldn't it be slightly better to leave `vals` as is and change the assert to\r\n```\r\nassert no_norm_out[1].dtype == vals.dtype\r\n```\r\n?",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "lib/matplotlib/tests/test_colors.py",
    "pr_number": 30629,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2402369635,
    "comment_created_at": "2025-10-03T15:20:04Z"
  },
  {
    "code": "@@ -157,33 +166,23 @@ def track(self, font: FT2Font, s: str) -> list[tuple[int, CharacterCodeType]]:\n             whole). If *subset_size* is not specified, then the subset will always be 0\n             and the character codes will be returned from the string unchanged.\n         \"\"\"\n-        font_glyphs = []\n-        char_to_font = font._get_fontmap(s)\n-        for _c, _f in char_to_font.items():\n-            charcode = ord(_c)\n-            glyph_index = _f.get_char_index(charcode)\n-            if self.subset_size != 0:\n-                subset = charcode // self.subset_size\n-                subset_charcode = charcode % self.subset_size\n-            else:\n-                subset = 0\n-                subset_charcode = charcode\n-            self.used.setdefault((_f.fname, subset), {})[subset_charcode] = glyph_index\n-            font_glyphs.append((subset, subset_charcode))\n-        return font_glyphs\n-\n-    def track_glyph(\n-            self, font: FT2Font, charcode: CharacterCodeType,\n-            glyph: GlyphIndexType) -> tuple[int, CharacterCodeType]:\n+        return [\n+            self.track_glyph(_f, ord(_c), _f.get_char_index(ord(_c)))",
    "comment": "you don't need the leading underscores here?",
    "line_number": 170,
    "enriched": "File: lib/matplotlib/backends/_backend_pdf_ps.py\nCode: @@ -157,33 +166,23 @@ def track(self, font: FT2Font, s: str) -> list[tuple[int, CharacterCodeType]]:\n             whole). If *subset_size* is not specified, then the subset will always be 0\n             and the character codes will be returned from the string unchanged.\n         \"\"\"\n-        font_glyphs = []\n-        char_to_font = font._get_fontmap(s)\n-        for _c, _f in char_to_font.items():\n-            charcode = ord(_c)\n-            glyph_index = _f.get_char_index(charcode)\n-            if self.subset_size != 0:\n-                subset = charcode // self.subset_size\n-                subset_charcode = charcode % self.subset_size\n-            else:\n-                subset = 0\n-                subset_charcode = charcode\n-            self.used.setdefault((_f.fname, subset), {})[subset_charcode] = glyph_index\n-            font_glyphs.append((subset, subset_charcode))\n-        return font_glyphs\n-\n-    def track_glyph(\n-            self, font: FT2Font, charcode: CharacterCodeType,\n-            glyph: GlyphIndexType) -> tuple[int, CharacterCodeType]:\n+        return [\n+            self.track_glyph(_f, ord(_c), _f.get_char_index(ord(_c)))\nComment: You don't need the leading underscores here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/matplotlib/backends/_backend_pdf_ps.py",
    "pr_number": 30608,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2387487798,
    "comment_created_at": "2025-09-29T10:40:55Z"
  },
  {
    "code": "@@ -190,7 +190,7 @@ def _get_col_row(self, n):\n         return col, row\n \n     n_axes = property(lambda self: len(self.axes_all))\n-    ngrids = _api.deprecated(property(lambda self: len(self.axes_all)))\n+    ngrids = _api.deprecated('3.11')(property(lambda self: len(self.axes_all)))",
    "comment": "we should check the input parameter to prevent something like this in the future.\r\n\r\ninterestingly, why did type-checking not catch this?",
    "line_number": 193,
    "enriched": "File: lib/mpl_toolkits/axes_grid1/axes_grid.py\nCode: @@ -190,7 +190,7 @@ def _get_col_row(self, n):\n         return col, row\n \n     n_axes = property(lambda self: len(self.axes_all))\n-    ngrids = _api.deprecated(property(lambda self: len(self.axes_all)))\n+    ngrids = _api.deprecated('3.11')(property(lambda self: len(self.axes_all)))\nComment: We should check the input parameter to prevent something like this in the future.\r\n\r\nInterestingly, why did type-checking not catch this?",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "lib/mpl_toolkits/axes_grid1/axes_grid.py",
    "pr_number": 30603,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2381120369,
    "comment_created_at": "2025-09-26T07:13:23Z"
  },
  {
    "code": "@@ -61,6 +61,7 @@ def convert_limits(lim, converter):\n             )\n             for name, axis in axis_map.items()\n         ]),\n+        ('Legend visible', axes.legend_ is not None),",
    "comment": "this looks unrelated?",
    "line_number": 64,
    "enriched": "File: lib/matplotlib/backends/qt_editor/figureoptions.py\nCode: @@ -61,6 +61,7 @@ def convert_limits(lim, converter):\n             )\n             for name, axis in axis_map.items()\n         ]),\n+        ('Legend visible', axes.legend_ is not None),\nComment: This looks unrelated?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/matplotlib/backends/qt_editor/figureoptions.py",
    "pr_number": 30590,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2376942175,
    "comment_created_at": "2025-09-24T20:08:59Z"
  },
  {
    "code": "@@ -285,18 +285,11 @@ def test_contourf3d_extend(fig_test, fig_ref, extend, levels):\n     # Z is in the range [0, 8]\n     Z = X**2 + Y**2\n \n-    # Manually set the over/under colors to be the end of the colormap\n-    cmap = mpl.colormaps['viridis'].copy()\n-    cmap.set_under(cmap(0))\n-    cmap.set_over(cmap(255))\n-    # Set vmin/max to be the min/max values plotted on the reference image\n-    kwargs = {'vmin': 1, 'vmax': 7, 'cmap': cmap}",
    "comment": "this cmap acrobatics is actually unneeded. if the colormap does not have extremes, as in the case of viridis, the values used to render over/under are actually the ends of the colormap. so setting them explicitly does not have an impact on the rendered image.",
    "line_number": 293,
    "enriched": "File: lib/mpl_toolkits/mplot3d/tests/test_axes3d.py\nCode: @@ -285,18 +285,11 @@ def test_contourf3d_extend(fig_test, fig_ref, extend, levels):\n     # Z is in the range [0, 8]\n     Z = X**2 + Y**2\n \n-    # Manually set the over/under colors to be the end of the colormap\n-    cmap = mpl.colormaps['viridis'].copy()\n-    cmap.set_under(cmap(0))\n-    cmap.set_over(cmap(255))\n-    # Set vmin/max to be the min/max values plotted on the reference image\n-    kwargs = {'vmin': 1, 'vmax': 7, 'cmap': cmap}\nComment: This cmap acrobatics is actually unneeded. If the colormap does not have extremes, as in the case of viridis, the values used to render over/under are actually the ends of the colormap. So setting them explicitly does not have an impact on the rendered image.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "lib/mpl_toolkits/mplot3d/tests/test_axes3d.py",
    "pr_number": 30582,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2357704841,
    "comment_created_at": "2025-09-18T06:47:28Z"
  },
  {
    "code": "@@ -148,11 +174,6 @@ def test_find_invalid(tmp_path):\n     with pytest.raises(FileNotFoundError):\n         get_font(bytes(tmp_path / 'non-existent-font-name.ttf'))\n \n-    # Not really public, but get_font doesn't expose non-filename constructor.",
    "comment": "note to future reviewers, this was removed as redundent with other test",
    "line_number": 151,
    "enriched": "File: lib/matplotlib/tests/test_font_manager.py\nCode: @@ -148,11 +174,6 @@ def test_find_invalid(tmp_path):\n     with pytest.raises(FileNotFoundError):\n         get_font(bytes(tmp_path / 'non-existent-font-name.ttf'))\n \n-    # Not really public, but get_font doesn't expose non-filename constructor.\nComment: note to future reviewers, this was removed as redundent with other test",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "lib/matplotlib/tests/test_font_manager.py",
    "pr_number": 30573,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2379935970,
    "comment_created_at": "2025-09-25T18:06:37Z"
  },
  {
    "code": "@@ -77,14 +77,13 @@ jobs:\n             pygobject-ver: '<3.52.0'\n           - os: ubuntu-24.04\n             python-version: '3.12'\n-          - os: macos-13  # This runner is on Intel chips.\n-            # merge numpy and pandas install in nighties test when this runner is dropped\n+          - os: macos-14  # This runner is on M1 (arm64) chips.\n             python-version: '3.11'\n           - os: macos-14  # This runner is on M1 (arm64) chips.\n             python-version: '3.12'\n             # https://github.com/matplotlib/matplotlib/issues/29732\n             pygobject-ver: '<3.52.0'",
    "comment": "possibly we need this limitation also on the py3.11 case to fix the test error.",
    "line_number": 85,
    "enriched": "File: .github/workflows/tests.yml\nCode: @@ -77,14 +77,13 @@ jobs:\n             pygobject-ver: '<3.52.0'\n           - os: ubuntu-24.04\n             python-version: '3.12'\n-          - os: macos-13  # This runner is on Intel chips.\n-            # merge numpy and pandas install in nighties test when this runner is dropped\n+          - os: macos-14  # This runner is on M1 (arm64) chips.\n             python-version: '3.11'\n           - os: macos-14  # This runner is on M1 (arm64) chips.\n             python-version: '3.12'\n             # https://github.com/matplotlib/matplotlib/issues/29732\n             pygobject-ver: '<3.52.0'\nComment: Possibly we need this limitation also on the py3.11 case to fix the test error.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".github/workflows/tests.yml",
    "pr_number": 30571,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2353809834,
    "comment_created_at": "2025-09-16T22:43:36Z"
  },
  {
    "code": "@@ -962,9 +960,9 @@ def writeFonts(self):\n             else:\n                 # a normal TrueType font\n                 _log.debug('Writing TrueType font.')\n-                glyphs = self._character_tracker.used.get(filename)\n-                if glyphs:\n-                    fonts[Fx] = self.embedTTF(filename, glyphs)\n+                charmap = self._character_tracker.used.get((filename, 0))",
    "comment": "do we need to loop and add all the partitions?",
    "line_number": 963,
    "enriched": "File: lib/matplotlib/backends/backend_pdf.py\nCode: @@ -962,9 +960,9 @@ def writeFonts(self):\n             else:\n                 # a normal TrueType font\n                 _log.debug('Writing TrueType font.')\n-                glyphs = self._character_tracker.used.get(filename)\n-                if glyphs:\n-                    fonts[Fx] = self.embedTTF(filename, glyphs)\n+                charmap = self._character_tracker.used.get((filename, 0))\nComment: do we need to loop and add all the partitions?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/matplotlib/backends/backend_pdf.py",
    "pr_number": 30566,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2353607870,
    "comment_created_at": "2025-09-16T20:41:05Z"
  },
  {
    "code": "@@ -2592,7 +2592,7 @@ def scroll_handler(event, canvas=None, toolbar=None):\n         # is required for interactive navigation.\n         return\n \n-    if event.key == \"control\":  # zoom towards the mouse position\n+    if event.key in {\"control\", \"x\", \"X\", \"y\", \"Y\"}:  # zoom towards the mouse position",
    "comment": "\r\nwe should only have the lower-case versions here. \"x\" would be generated by \"shift+x\", which i'd consider a separate modifier that we don't want to bind to zoom.",
    "line_number": 2595,
    "enriched": "File: lib/matplotlib/backend_bases.py\nCode: @@ -2592,7 +2592,7 @@ def scroll_handler(event, canvas=None, toolbar=None):\n         # is required for interactive navigation.\n         return\n \n-    if event.key == \"control\":  # zoom towards the mouse position\n+    if event.key in {\"control\", \"x\", \"X\", \"y\", \"Y\"}:  # zoom towards the mouse position\nComment: ```suggestion\r\n    if event.key in {\"control\", \"x\", \"y\"}:  # zoom towards the mouse position\r\n```\r\nWe should only have the lower-case versions here. \"X\" would be generated by \"Shift+x\", which I'd consider a separate modifier that we don't want to bind to zoom.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "lib/matplotlib/backend_bases.py",
    "pr_number": 30543,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2337794299,
    "comment_created_at": "2025-09-10T20:14:05Z"
  },
  {
    "code": "@@ -127,7 +128,13 @@ def glyph_name_or_index(self):\n     def _as_unicode_or_name(self):\n         if self.font.subfont:\n             raise NotImplementedError(\"Indexing TTC fonts is not supported yet\")\n-        face = font_manager.get_font(self.font.resolve_path())\n+        path = self.font.resolve_path()\n+        if path.name.lower().endswith(\"pk\"):\n+            # PK fonts have no encoding information; report glyphs as ASCII but\n+            # with a \"?\" to indicate that this is just a guess.\n+            return (f\"{chr(self.glyph)}?\" if chr(self.glyph).isprintable() else\n+                    \"??\")",
    "comment": "should we fall back to something like f'pk{self.glyph:x}' so that they are all unique? (the pk prefix is something _like_ the uni prefix on unicode-encoded glyphs)",
    "line_number": 136,
    "enriched": "File: lib/matplotlib/dviread.py\nCode: @@ -127,7 +128,13 @@ def glyph_name_or_index(self):\n     def _as_unicode_or_name(self):\n         if self.font.subfont:\n             raise NotImplementedError(\"Indexing TTC fonts is not supported yet\")\n-        face = font_manager.get_font(self.font.resolve_path())\n+        path = self.font.resolve_path()\n+        if path.name.lower().endswith(\"pk\"):\n+            # PK fonts have no encoding information; report glyphs as ASCII but\n+            # with a \"?\" to indicate that this is just a guess.\n+            return (f\"{chr(self.glyph)}?\" if chr(self.glyph).isprintable() else\n+                    \"??\")\nComment: Should we fall back to something like `f'pk{self.glyph:x}'` so that they are all unique? (the `pk` prefix is something _like_ the `uni` prefix on Unicode-encoded glyphs)",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "lib/matplotlib/dviread.py",
    "pr_number": 30514,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2326574668,
    "comment_created_at": "2025-09-06T06:22:12Z"
  },
  {
    "code": "@@ -107,23 +107,98 @@ class CharacterTracker:\n     \"\"\"\n     Helper for font subsetting by the PDF and PS backends.\n \n-    Maintains a mapping of font paths to the set of glyphs that are being used from that\n-    font.\n-    \"\"\"\n+    Maintains a mapping of font paths to the set of characters and glyphs that are being\n+    used from that font.\n \n-    def __init__(self) -> None:\n-        self.used: dict[str, set[GlyphIndexType]] = {}\n+    Attributes\n+    ----------\n+    subset_size : int\n+        The size at which characters are grouped into subsets.\n+    used : dict[tuple[str, int], dict[CharacterCodeType, GlyphIndexType]]\n+        A dictionary of font files to character maps. The key is a font filename and\n+        subset within that font. The value is a dictionary mapping a character code to a\n+        glyph index. If *subset_size* is not set, then there will only be one subset per\n+        font filename.\n+    \"\"\"\n \n-    def track(self, font: FT2Font, s: str) -> None:\n-        \"\"\"Record that string *s* is being typeset using font *font*.\"\"\"\n+    def __init__(self, subset_size: int = 0):\n+        \"\"\"\n+        Parameters\n+        ----------\n+        subset_size : int, optional\n+            The maximum size that is supported for an embedded font. If provided, then\n+            characters will be grouped into these sized subsets.\n+        \"\"\"\n+        self.used: dict[tuple[str, int], dict[CharacterCodeType, GlyphIndexType]] = {}\n+        self.subset_size = subset_size\n+\n+    def track(self, font: FT2Font, s: str) -> list[tuple[int, CharacterCodeType]]:\n+        \"\"\"\n+        Record that string *s* is being typeset using font *font*.\n+\n+        Parameters\n+        ----------\n+        font : FT2Font\n+            A font that is being used for the provided string.\n+        s : str\n+            The string that should be marked as tracked by the provided font.\n+\n+        Returns\n+        -------\n+        list[tuple[int, CharacterCodeType]]\n+            A list of subset and character code pairs corresponding to the input string.\n+            If a *subset_size* is specified on this instance, then the character code\n+            will correspond with the given subset (and not necessarily the string as a\n+            whole). If *subset_size* is not specified, then the subset will always be 0\n+            and the character codes will be returned from the string unchanged.\n+        \"\"\"\n+        font_glyphs = []\n         char_to_font = font._get_fontmap(s)\n         for _c, _f in char_to_font.items():\n-            glyph_index = _f.get_char_index(ord(_c))\n-            self.used.setdefault(_f.fname, set()).add(glyph_index)\n-\n-    def track_glyph(self, font: FT2Font, glyph_index: GlyphIndexType) -> None:\n-        \"\"\"Record that glyph index *glyph_index* is being typeset using font *font*.\"\"\"\n-        self.used.setdefault(font.fname, set()).add(glyph_index)\n+            charcode = ord(_c)\n+            glyph_index = _f.get_char_index(charcode)\n+            if self.subset_size != 0:\n+                subset = charcode // self.subset_size\n+                subset_charcode = charcode % self.subset_size\n+            else:\n+                subset = 0\n+                subset_charcode = charcode\n+            self.used.setdefault((_f.fname, subset), {})[subset_charcode] = glyph_index\n+            font_glyphs.append((subset, subset_charcode))\n+        return font_glyphs\n+\n+    def track_glyph(\n+            self, font: FT2Font, charcode: CharacterCodeType,\n+            glyph: GlyphIndexType) -> tuple[int, CharacterCodeType]:\n+        \"\"\"\n+        Record character code *charcode* at glyph index *glyph* as using font *font*.\n+\n+        Parameters\n+        ----------\n+        font : FT2Font\n+            A font that is being used for the provided string.\n+        charcode : CharacterCodeType, optional",
    "comment": "i don't think this is \"optional\"?",
    "line_number": 180,
    "enriched": "File: lib/matplotlib/backends/_backend_pdf_ps.py\nCode: @@ -107,23 +107,98 @@ class CharacterTracker:\n     \"\"\"\n     Helper for font subsetting by the PDF and PS backends.\n \n-    Maintains a mapping of font paths to the set of glyphs that are being used from that\n-    font.\n-    \"\"\"\n+    Maintains a mapping of font paths to the set of characters and glyphs that are being\n+    used from that font.\n \n-    def __init__(self) -> None:\n-        self.used: dict[str, set[GlyphIndexType]] = {}\n+    Attributes\n+    ----------\n+    subset_size : int\n+        The size at which characters are grouped into subsets.\n+    used : dict[tuple[str, int], dict[CharacterCodeType, GlyphIndexType]]\n+        A dictionary of font files to character maps. The key is a font filename and\n+        subset within that font. The value is a dictionary mapping a character code to a\n+        glyph index. If *subset_size* is not set, then there will only be one subset per\n+        font filename.\n+    \"\"\"\n \n-    def track(self, font: FT2Font, s: str) -> None:\n-        \"\"\"Record that string *s* is being typeset using font *font*.\"\"\"\n+    def __init__(self, subset_size: int = 0):\n+        \"\"\"\n+        Parameters\n+        ----------\n+        subset_size : int, optional\n+            The maximum size that is supported for an embedded font. If provided, then\n+            characters will be grouped into these sized subsets.\n+        \"\"\"\n+        self.used: dict[tuple[str, int], dict[CharacterCodeType, GlyphIndexType]] = {}\n+        self.subset_size = subset_size\n+\n+    def track(self, font: FT2Font, s: str) -> list[tuple[int, CharacterCodeType]]:\n+        \"\"\"\n+        Record that string *s* is being typeset using font *font*.\n+\n+        Parameters\n+        ----------\n+        font : FT2Font\n+            A font that is being used for the provided string.\n+        s : str\n+            The string that should be marked as tracked by the provided font.\n+\n+        Returns\n+        -------\n+        list[tuple[int, CharacterCodeType]]\n+            A list of subset and character code pairs corresponding to the input string.\n+            If a *subset_size* is specified on this instance, then the character code\n+            will correspond with the given subset (and not necessarily the string as a\n+            whole). If *subset_size* is not specified, then the subset will always be 0\n+            and the character codes will be returned from the string unchanged.\n+        \"\"\"\n+        font_glyphs = []\n         char_to_font = font._get_fontmap(s)\n         for _c, _f in char_to_font.items():\n-            glyph_index = _f.get_char_index(ord(_c))\n-            self.used.setdefault(_f.fname, set()).add(glyph_index)\n-\n-    def track_glyph(self, font: FT2Font, glyph_index: GlyphIndexType) -> None:\n-        \"\"\"Record that glyph index *glyph_index* is being typeset using font *font*.\"\"\"\n-        self.used.setdefault(font.fname, set()).add(glyph_index)\n+            charcode = ord(_c)\n+            glyph_index = _f.get_char_index(charcode)\n+            if self.subset_size != 0:\n+                subset = charcode // self.subset_size\n+                subset_charcode = charcode % self.subset_size\n+            else:\n+                subset = 0\n+                subset_charcode = charcode\n+            self.used.setdefault((_f.fname, subset), {})[subset_charcode] = glyph_index\n+            font_glyphs.append((subset, subset_charcode))\n+        return font_glyphs\n+\n+    def track_glyph(\n+            self, font: FT2Font, charcode: CharacterCodeType,\n+            glyph: GlyphIndexType) -> tuple[int, CharacterCodeType]:\n+        \"\"\"\n+        Record character code *charcode* at glyph index *glyph* as using font *font*.\n+\n+        Parameters\n+        ----------\n+        font : FT2Font\n+            A font that is being used for the provided string.\n+        charcode : CharacterCodeType, optional\nComment: I don't think this is \"optional\"?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/matplotlib/backends/_backend_pdf_ps.py",
    "pr_number": 30512,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2350949995,
    "comment_created_at": "2025-09-16T06:04:00Z"
  },
  {
    "code": "@@ -3325,9 +3343,23 @@ def _release(self, event):\n         self.update()\n         self._active_handle = None\n         self._extents_on_press = None\n-\n         return False\n \n+    def _get_action(self):\n+        \"\"\"\n+        Return one of \"rotate\", \"move\", \"resize\", \"create\"",
    "comment": "let's make this an enum instead. an enum is safer and easier to reason about compared to a bunch of strings.",
    "line_number": 3350,
    "enriched": "File: lib/matplotlib/widgets.py\nCode: @@ -3325,9 +3343,23 @@ def _release(self, event):\n         self.update()\n         self._active_handle = None\n         self._extents_on_press = None\n-\n         return False\n \n+    def _get_action(self):\n+        \"\"\"\n+        Return one of \"rotate\", \"move\", \"resize\", \"create\"\nComment: Let's make this an Enum instead. An enum is safer and easier to reason about compared to a bunch of strings.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "lib/matplotlib/widgets.py",
    "pr_number": 30499,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2312374400,
    "comment_created_at": "2025-08-31T09:48:22Z"
  },
  {
    "code": "@@ -215,14 +215,15 @@ def test_figureoptions():\n \n \n @pytest.mark.backend('QtAgg', skip_on_importerror=True)\n-def test_save_figure_return():\n+def test_save_figure_return(tmp_path):\n+    os.chdir(tmp_path)\n     fig, ax = plt.subplots()\n     ax.imshow([[1]])\n     prop = \"matplotlib.backends.qt_compat.QtWidgets.QFileDialog.getSaveFileName\"\n     with mock.patch(prop, return_value=(\"foobar.png\", None)):",
    "comment": "perhaps it would be even safer to set the return value to an absolute directory instead of having a chdir call at the top of the test?",
    "line_number": 223,
    "enriched": "File: lib/matplotlib/tests/test_backend_qt.py\nCode: @@ -215,14 +215,15 @@ def test_figureoptions():\n \n \n @pytest.mark.backend('QtAgg', skip_on_importerror=True)\n-def test_save_figure_return():\n+def test_save_figure_return(tmp_path):\n+    os.chdir(tmp_path)\n     fig, ax = plt.subplots()\n     ax.imshow([[1]])\n     prop = \"matplotlib.backends.qt_compat.QtWidgets.QFileDialog.getSaveFileName\"\n     with mock.patch(prop, return_value=(\"foobar.png\", None)):\nComment: Perhaps it would be even safer to set the return value to an absolute directory instead of having a `chdir` call at the top of the test?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/matplotlib/tests/test_backend_qt.py",
    "pr_number": 30497,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2321923140,
    "comment_created_at": "2025-09-04T12:25:26Z"
  },
  {
    "code": "@@ -12,8 +12,8 @@\n     },\n     {\n         \"name\": \"3.9\",\n-        \"version\": \"3.9.3\",\n-        \"url\": \"https://matplotlib.org/3.9.3/\"\n+        \"version\": \"3.9.4\",\n+        \"url\": \"https://matplotlib.org/3.9.4/\"",
    "comment": "this url didn't exist, not sure why it got bumped on v3.10.x.",
    "line_number": 16,
    "enriched": "File: doc/_static/switcher.json\nCode: @@ -12,8 +12,8 @@\n     },\n     {\n         \"name\": \"3.9\",\n-        \"version\": \"3.9.3\",\n-        \"url\": \"https://matplotlib.org/3.9.3/\"\n+        \"version\": \"3.9.4\",\n+        \"url\": \"https://matplotlib.org/3.9.4/\"\nComment: This URL didn't exist, not sure why it got bumped on `v3.10.x`.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "doc/_static/switcher.json",
    "pr_number": 30491,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2311780527,
    "comment_created_at": "2025-08-30T04:44:31Z"
  },
  {
    "code": "@@ -2172,6 +2172,11 @@ def update_background(self, event):\n         # `release` can call a draw event even when `ignore` is True.\n         if not self.useblit:\n             return\n+        # Skip blitting if we are saving to disk or if the backend doesn\u2019t support it\n+        if getattr(self.canvas, \"_is_saving\", False):\n+            return",
    "comment": "\r\nthis is defined in figurecanvasbase and thus available in all figurecanvas subclasses.",
    "line_number": 2177,
    "enriched": "File: lib/matplotlib/widgets.py\nCode: @@ -2172,6 +2172,11 @@ def update_background(self, event):\n         # `release` can call a draw event even when `ignore` is True.\n         if not self.useblit:\n             return\n+        # Skip blitting if we are saving to disk or if the backend doesn\u2019t support it\n+        if getattr(self.canvas, \"_is_saving\", False):\n+            return\nComment: ```suggestion\r\n        if self.canvas.is_saving():\r\n            return\r\n```\r\nThis is defined in `FigureCanvasBase` and thus available in all FigureCanvas subclasses.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "lib/matplotlib/widgets.py",
    "pr_number": 30490,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2312629143,
    "comment_created_at": "2025-08-31T21:11:23Z"
  },
  {
    "code": "@@ -517,7 +517,12 @@ def _draw_idle(self):\n             if not self._draw_pending:\n                 return\n             self._draw_pending = False\n-            if self.height() <= 0 or self.width() <= 0:\n+            try:\n+                if self.height() <= 0 or self.width() <= 0:\n+                    return\n+            except RuntimeError:\n+                # This can happen if the c++ object is already cleaned up",
    "comment": "can we check the error message to ensure we\u2019re not swallowing other potential runtimeerrors?\r\n\r\nalso, are we sure this is the only place that may access the c++ object?",
    "line_number": 524,
    "enriched": "File: lib/matplotlib/backends/backend_qt.py\nCode: @@ -517,7 +517,12 @@ def _draw_idle(self):\n             if not self._draw_pending:\n                 return\n             self._draw_pending = False\n-            if self.height() <= 0 or self.width() <= 0:\n+            try:\n+                if self.height() <= 0 or self.width() <= 0:\n+                    return\n+            except RuntimeError:\n+                # This can happen if the c++ object is already cleaned up\nComment: Can we check the error message to ensure we\u2019re not swallowing other potential RuntimeErrors?\r\n\r\nAlso, are we sure this is the only place that may access the C++ object?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/matplotlib/backends/backend_qt.py",
    "pr_number": 30484,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2309199200,
    "comment_created_at": "2025-08-29T05:42:04Z"
  },
  {
    "code": "@@ -1418,24 +1418,27 @@ def contains_branch(self, other):\n                 return True\n         return False\n \n-    def contains_branch_seperately(self, other_transform):\n+    def contains_branch_separately(self, other_transform):\n         \"\"\"\n         Return whether the given branch is a sub-tree of this transform on\n         each separate dimension.\n \n         A common use for this method is to identify if a transform is a blended\n         transform containing an Axes' data transform. e.g.::\n \n-            x_isdata, y_isdata = trans.contains_branch_seperately(ax.transData)\n+            x_isdata, y_isdata = trans.contains_branch_separately(ax.transData)\n \n         \"\"\"\n         if self.output_dims != 2:\n-            raise ValueError('contains_branch_seperately only supports '\n+            raise ValueError('contains_branch_separately only supports '\n                              'transforms with 2 output dimensions')\n         # for a non-blended transform each separate dimension is the same, so\n         # just return the appropriate shape.\n         return (self.contains_branch(other_transform), ) * 2\n \n+    # Permanent alias for backwards compatibility (historical typo)\n+    contains_branch_seperately = contains_branch_separately\n+",
    "comment": "if you make this an actual method, then the call it makes will go to the subclass automatically, and there's no need for duplicate aliases on the other classes.",
    "line_number": 1443,
    "enriched": "File: lib/matplotlib/transforms.py\nCode: @@ -1418,24 +1418,27 @@ def contains_branch(self, other):\n                 return True\n         return False\n \n-    def contains_branch_seperately(self, other_transform):\n+    def contains_branch_separately(self, other_transform):\n         \"\"\"\n         Return whether the given branch is a sub-tree of this transform on\n         each separate dimension.\n \n         A common use for this method is to identify if a transform is a blended\n         transform containing an Axes' data transform. e.g.::\n \n-            x_isdata, y_isdata = trans.contains_branch_seperately(ax.transData)\n+            x_isdata, y_isdata = trans.contains_branch_separately(ax.transData)\n \n         \"\"\"\n         if self.output_dims != 2:\n-            raise ValueError('contains_branch_seperately only supports '\n+            raise ValueError('contains_branch_separately only supports '\n                              'transforms with 2 output dimensions')\n         # for a non-blended transform each separate dimension is the same, so\n         # just return the appropriate shape.\n         return (self.contains_branch(other_transform), ) * 2\n \n+    # Permanent alias for backwards compatibility (historical typo)\n+    contains_branch_seperately = contains_branch_separately\n+\nComment: If you make this an actual method, then the call it makes will go to the subclass automatically, and there's no need for duplicate aliases on the other classes.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "lib/matplotlib/transforms.py",
    "pr_number": 30475,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2306258089,
    "comment_created_at": "2025-08-28T05:55:02Z"
  },
  {
    "code": "@@ -3,8 +3,8 @@\n \n .. _imshow_extent:\n \n-*origin* and *extent* in `~.Axes.imshow`\n-========================================\n+The *origin* and *extent* in `~.Axes.imshow`\n+============================================",
    "comment": "i wonder if origin and extent are really important to place in the title here; maybe it should have been something like \"positioning and orientation of ~.axes.imshow images\"",
    "line_number": 7,
    "enriched": "File: galleries/users_explain/artists/imshow_extent.py\nCode: @@ -3,8 +3,8 @@\n \n .. _imshow_extent:\n \n-*origin* and *extent* in `~.Axes.imshow`\n-========================================\n+The *origin* and *extent* in `~.Axes.imshow`\n+============================================\nComment: I wonder if `origin` and `extent` are really important to place in the title here; maybe it should have been something like \"Positioning and orientation of `~.Axes.imshow` images\"",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "galleries/users_explain/artists/imshow_extent.py",
    "pr_number": 30471,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2299509649,
    "comment_created_at": "2025-08-26T01:39:54Z"
  },
  {
    "code": "@@ -29,35 +29,32 @@ def hat_graph(ax, xlabels, values, group_labels):\n         The group labels displayed in the legend.\n     \"\"\"\n \n-    def label_bars(heights, rects):\n-        \"\"\"Attach a text label on top of each bar.\"\"\"\n-        for height, rect in zip(heights, rects):\n-            ax.annotate(f'{height}',\n-                        xy=(rect.get_x() + rect.get_width() / 2, height),\n-                        xytext=(0, 4),  # 4 points vertical offset.\n-                        textcoords='offset points',\n-                        ha='center', va='bottom')\n-\n     values = np.asarray(values)\n     x = np.arange(values.shape[1])\n     ax.set_xticks(x, labels=xlabels)",
    "comment": "can we remove this? i believe grouped_bar(..., tick_labels=...) already cares for this.",
    "line_number": 34,
    "enriched": "File: galleries/examples/lines_bars_and_markers/hat_graph.py\nCode: @@ -29,35 +29,32 @@ def hat_graph(ax, xlabels, values, group_labels):\n         The group labels displayed in the legend.\n     \"\"\"\n \n-    def label_bars(heights, rects):\n-        \"\"\"Attach a text label on top of each bar.\"\"\"\n-        for height, rect in zip(heights, rects):\n-            ax.annotate(f'{height}',\n-                        xy=(rect.get_x() + rect.get_width() / 2, height),\n-                        xytext=(0, 4),  # 4 points vertical offset.\n-                        textcoords='offset points',\n-                        ha='center', va='bottom')\n-\n     values = np.asarray(values)\n     x = np.arange(values.shape[1])\n     ax.set_xticks(x, labels=xlabels)\nComment: Can we remove this? I believe `grouped_bar(..., tick_labels=...)` already cares for this.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "galleries/examples/lines_bars_and_markers/hat_graph.py",
    "pr_number": 30459,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2296384900,
    "comment_created_at": "2025-08-24T00:06:41Z"
  },
  {
    "code": "@@ -7,6 +7,8 @@\n Installation\n ************\n \n+.. include:: quick_install.inc.rst\n+   :end-line: 45",
    "comment": "can we do something like:\r\n\r\nso that we don't hardcode the line?",
    "line_number": 11,
    "enriched": "File: doc/install/index.rst\nCode: @@ -7,6 +7,8 @@\n Installation\n ************\n \n+.. include:: quick_install.inc.rst\n+   :end-line: 45\nComment: Can we do something like:\r\n```suggestion\r\n   :end-before: tab-item:: other\r\n```\r\nso that we don't hardcode the line?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "doc/install/index.rst",
    "pr_number": 30451,
    "repo": "matplotlib",
    "owner": "matplotlib",
    "comment_id": 2292179402,
    "comment_created_at": "2025-08-21T21:32:06Z"
  },
  {
    "code": "@@ -37,12 +38,12 @@ class AbstractProvider:  # type: ignore[no-redef]\n \n \n # TODO: add python requirements to ansible-test's ansible-core distribution info and remove the hardcoded lowerbound/upperbound fallback\n-RESOLVELIB_LOWERBOUND = SemanticVersion(\"0.5.3\")\n+RESOLVELIB_LOWERBOUND = SemanticVersion(\"0.8.0\")\n RESOLVELIB_UPPERBOUND = SemanticVersion(\"2.0.0\")\n RESOLVELIB_VERSION = SemanticVersion.from_loose_version(LooseVersion(resolvelib_version))\n \n \n-class CollectionDependencyProviderBase(AbstractProvider):\n+class CollectionDependencyProvider(AbstractProvider):",
    "comment": "the # type: collectiondependencyproviderbase annotation below should be removed, since that type no longer exists and the annotation is redundant.",
    "line_number": 46,
    "enriched": "File: lib/ansible/galaxy/dependency_resolution/providers.py\nCode: @@ -37,12 +38,12 @@ class AbstractProvider:  # type: ignore[no-redef]\n \n \n # TODO: add python requirements to ansible-test's ansible-core distribution info and remove the hardcoded lowerbound/upperbound fallback\n-RESOLVELIB_LOWERBOUND = SemanticVersion(\"0.5.3\")\n+RESOLVELIB_LOWERBOUND = SemanticVersion(\"0.8.0\")\n RESOLVELIB_UPPERBOUND = SemanticVersion(\"2.0.0\")\n RESOLVELIB_VERSION = SemanticVersion.from_loose_version(LooseVersion(resolvelib_version))\n \n \n-class CollectionDependencyProviderBase(AbstractProvider):\n+class CollectionDependencyProvider(AbstractProvider):\nComment: The `# type: CollectionDependencyProviderBase` annotation below should be removed, since that type no longer exists and the annotation is redundant.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "lib/ansible/galaxy/dependency_resolution/providers.py",
    "pr_number": 85936,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2395603310,
    "comment_created_at": "2025-10-01T19:09:43Z"
  },
  {
    "code": "@@ -136,6 +133,18 @@ class AnsibleUnwantedChecker(BaseChecker):\n                                         modules_only=True),\n     }\n \n+    def __init__(self, *args, **kwargs) -> None:\n+        super().__init__(*args, **kwargs)\n+        if self.is_ansible_core:",
    "comment": "why limit this check to core?",
    "line_number": 138,
    "enriched": "File: test/lib/ansible_test/_util/controller/sanity/pylint/plugins/unwanted.py\nCode: @@ -136,6 +133,18 @@ class AnsibleUnwantedChecker(BaseChecker):\n                                         modules_only=True),\n     }\n \n+    def __init__(self, *args, **kwargs) -> None:\n+        super().__init__(*args, **kwargs)\n+        if self.is_ansible_core:\nComment: Why limit this check to core?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "test/lib/ansible_test/_util/controller/sanity/pylint/plugins/unwanted.py",
    "pr_number": 85934,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2399310267,
    "comment_created_at": "2025-10-02T15:57:56Z"
  },
  {
    "code": "@@ -165,17 +165,40 @@ def load_list_of_tasks(ds, play, block=None, role=None, task_include=None, use_h\n                     subdir = 'tasks'\n                     if use_handlers:\n                         subdir = 'handlers'\n+                    try:\n+                        include_target = templar.template(task.args['_raw_params'])\n+                    except AnsibleUndefinedVariable as ex:\n+                        raise AnsibleParserError(\n+                            message=f\"Error when evaluating variable in import path {task.args['_raw_params']!r}.\",\n+                            help_text=\"When using static imports, ensure that any variables used in their names are defined in vars/vars_files\\n\"\n+                                      \"or extra-vars passed in from the command line. Static imports cannot use variables from facts or inventory\\n\"\n+                                      \"sources like group or host vars.\",\n+                            obj=task_ds,\n+                        ) from ex\n+                    # FIXME this appears to be (almost?) duplicate code as in IncludedFile for include_tasks\n                     while parent_include is not None:\n                         if not isinstance(parent_include, TaskInclude):\n                             parent_include = parent_include._parent\n                             continue\n-                        parent_include.post_validate(templar=templar)\n-                        parent_include_dir = os.path.dirname(parent_include.args.get('_raw_params'))\n+                        if isinstance(parent_include, IncludeRole):\n+                            parent_include_dir = parent_include._role_path\n+                        else:\n+                            try:\n+                                parent_include_dir = os.path.dirname(templar.template(parent_include.args.get('_raw_params')))\n+                            except AnsibleUndefinedVariable as ex:\n+                                if not parent_include.statically_loaded:",
    "comment": "does _raw_params need templating here if it wasn't statically loaded? i'm wondering if the error handling for non-static parent includes is reachable.",
    "line_number": 189,
    "enriched": "File: lib/ansible/playbook/helpers.py\nCode: @@ -165,17 +165,40 @@ def load_list_of_tasks(ds, play, block=None, role=None, task_include=None, use_h\n                     subdir = 'tasks'\n                     if use_handlers:\n                         subdir = 'handlers'\n+                    try:\n+                        include_target = templar.template(task.args['_raw_params'])\n+                    except AnsibleUndefinedVariable as ex:\n+                        raise AnsibleParserError(\n+                            message=f\"Error when evaluating variable in import path {task.args['_raw_params']!r}.\",\n+                            help_text=\"When using static imports, ensure that any variables used in their names are defined in vars/vars_files\\n\"\n+                                      \"or extra-vars passed in from the command line. Static imports cannot use variables from facts or inventory\\n\"\n+                                      \"sources like group or host vars.\",\n+                            obj=task_ds,\n+                        ) from ex\n+                    # FIXME this appears to be (almost?) duplicate code as in IncludedFile for include_tasks\n                     while parent_include is not None:\n                         if not isinstance(parent_include, TaskInclude):\n                             parent_include = parent_include._parent\n                             continue\n-                        parent_include.post_validate(templar=templar)\n-                        parent_include_dir = os.path.dirname(parent_include.args.get('_raw_params'))\n+                        if isinstance(parent_include, IncludeRole):\n+                            parent_include_dir = parent_include._role_path\n+                        else:\n+                            try:\n+                                parent_include_dir = os.path.dirname(templar.template(parent_include.args.get('_raw_params')))\n+                            except AnsibleUndefinedVariable as ex:\n+                                if not parent_include.statically_loaded:\nComment: Does _raw_params need templating here if it wasn't statically loaded? I'm wondering if the error handling for non-static parent includes is reachable.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "lib/ansible/playbook/helpers.py",
    "pr_number": 85877,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2376712144,
    "comment_created_at": "2025-09-24T18:32:15Z"
  },
  {
    "code": "@@ -673,6 +673,12 @@ def _fixup_perms2(self, remote_paths, remote_user=None, execute=True):\n \n         if res['rc'] == 0:\n             return remote_paths\n+        if res['rc'] == 127:",
    "comment": "most of these the user cannot do much about, this would be better as a debug:\r\n\r\nself._display.debug(f'failed to set facl {setfacl_mode}, got:{res!r}')\r\n",
    "line_number": 676,
    "enriched": "File: lib/ansible/plugins/action/__init__.py\nCode: @@ -673,6 +673,12 @@ def _fixup_perms2(self, remote_paths, remote_user=None, execute=True):\n \n         if res['rc'] == 0:\n             return remote_paths\n+        if res['rc'] == 127:\nComment: most of these the user cannot do much about, this would be better as a debug:\r\n```\r\nself._display.debug(f'Failed to set facl {setfacl_mode}, got:{res!r}')\r\n```",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "lib/ansible/plugins/action/__init__.py",
    "pr_number": 85839,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2337779248,
    "comment_created_at": "2025-09-10T20:05:43Z"
  },
  {
    "code": "@@ -162,6 +162,7 @@ def set(self, key, value):\n             except OSError as ex:\n                 display.error_as_warning(f\"Error in {self.plugin_name!r} cache plugin while trying to write to {tmpfile_path!r}.\", exception=ex)\n             try:\n+                os.close(tmpfile_handle)",
    "comment": "please create a [changelog fragment](https://docs.ansible.com/ansible/latest/community/development_process.html#creating-a-changelog-fragment). see [this fragment](https://github.com/ansible/ansible/blob/c2f528b7686c9573aa8493357219e0bbe4416dd3/changelogs/fragments/reboot-change-default-boot-command.yaml) as an example. ~also, please add integration tests for this change so that we don't regress in future?~",
    "line_number": 165,
    "enriched": "File: lib/ansible/plugins/cache/__init__.py\nCode: @@ -162,6 +162,7 @@ def set(self, key, value):\n             except OSError as ex:\n                 display.error_as_warning(f\"Error in {self.plugin_name!r} cache plugin while trying to write to {tmpfile_path!r}.\", exception=ex)\n             try:\n+                os.close(tmpfile_handle)\nComment: Please create a [changelog fragment](https://docs.ansible.com/ansible/latest/community/development_process.html#creating-a-changelog-fragment). See [this fragment](https://github.com/ansible/ansible/blob/c2f528b7686c9573aa8493357219e0bbe4416dd3/changelogs/fragments/reboot-change-default-boot-command.yaml) as an example. ~Also, please add integration tests for this change so that we don't regress in future?~",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "lib/ansible/plugins/cache/__init__.py",
    "pr_number": 85816,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2330684332,
    "comment_created_at": "2025-09-08T15:57:22Z"
  },
  {
    "code": "@@ -0,0 +1,7 @@\n+minor_changes:\n+- >-\n+  Add tech preview play argument spec validation, which can be\n+  enabled by setting the play keyword ``validate_argspec`` to ``True``.\n+  This will load a file matching the pattern <playbook_name>.meta.yml,\n+  which should contain a top dictionary \"argument_specs\", containing\n+  the play name, containing an \"options\" dictionary.",
    "comment": "iirc, for role argspec validation, the existence of an argspec file means we automatically do the validation. i think include_role and import_role have options to disable. perhaps we follow the same pattern for consistency sake? if an argspec exists and a play name matches, always do the validation. the new option could then be used to explicitly disable.",
    "line_number": 7,
    "enriched": "File: changelogs/fragments/play-argument-spec-validation.yml\nCode: @@ -0,0 +1,7 @@\n+minor_changes:\n+- >-\n+  Add tech preview play argument spec validation, which can be\n+  enabled by setting the play keyword ``validate_argspec`` to ``True``.\n+  This will load a file matching the pattern <playbook_name>.meta.yml,\n+  which should contain a top dictionary \"argument_specs\", containing\n+  the play name, containing an \"options\" dictionary.\nComment: IIRC, for role argspec validation, the existence of an argspec file means we automatically do the validation. I think `include_role` and `import_role` have options to disable. Perhaps we follow the same pattern for consistency sake? If an argspec exists and a play name matches, always do the validation. The new option could then be used to explicitly disable.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "changelogs/fragments/play-argument-spec-validation.yml",
    "pr_number": 85763,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2310544378,
    "comment_created_at": "2025-08-29T16:02:35Z"
  },
  {
    "code": "@@ -537,6 +537,8 @@ def _configure_base(self, base, conf_file, disable_gpg_check, installroot='/', s\n         conf.sslverify = sslverify\n \n         # Set installroot\n+        if os.path.isfile(installroot):",
    "comment": "\r\n\r\nwe need to check specifically for a directory. we can't assume that if the path is not a regular file that it is a directory. the path can be non-existing, broken symlink, ... same below.",
    "line_number": 540,
    "enriched": "File: lib/ansible/modules/dnf.py\nCode: @@ -537,6 +537,8 @@ def _configure_base(self, base, conf_file, disable_gpg_check, installroot='/', s\n         conf.sslverify = sslverify\n \n         # Set installroot\n+        if os.path.isfile(installroot):\nComment: ```suggestion\r\n        if not os.path.isdir(installroot):\r\n```\r\n\r\nWe need to check specifically for a directory. We can't assume that if the path is not a regular file that it is a directory. The path can be non-existing, broken symlink, ... Same below.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "lib/ansible/modules/dnf.py",
    "pr_number": 85748,
    "repo": "ansible",
    "owner": "ansible",
    "comment_id": 2303047941,
    "comment_created_at": "2025-08-27T06:56:05Z"
  },
  {
    "code": "@@ -2568,7 +2568,7 @@ HTTPie plays exceptionally well with the following tools:\n \n Helpers to convert from other client tools:\n \n-- [CurliPie](https://curlipie.now.sh/) help convert cURL command line to HTTPie command line\n+- [CurliPie](https://pypi.org/project/curlipie/) help convert cURL command line to HTTPie command line",
    "comment": "would the web frontend be a better replacement?\r\n\r\n",
    "line_number": 2571,
    "enriched": "File: docs/README.md\nCode: @@ -2568,7 +2568,7 @@ HTTPie plays exceptionally well with the following tools:\n \n Helpers to convert from other client tools:\n \n-- [CurliPie](https://curlipie.now.sh/) help convert cURL command line to HTTPie command line\n+- [CurliPie](https://pypi.org/project/curlipie/) help convert cURL command line to HTTPie command line\nComment: would the web frontend be a better replacement?\r\n\r\n```suggestion\r\n- [CurliPie](https://curlipie.open-api.vn) \u2014 library to convert cURL commands to HTTPie\r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/README.md",
    "pr_number": 1582,
    "repo": "httpie",
    "owner": "httpie",
    "comment_id": 1723894773,
    "comment_created_at": "2024-08-20T19:44:03Z"
  },
  {
    "code": "@@ -39,7 +39,7 @@ tools:\n       install:\n         - curl -SsL https://packages.httpie.io/deb/KEY.gpg | sudo gpg --dearmor -o /usr/share/keyrings/httpie.gpg\n       # - curl -SsL -o /etc/apt/sources.list.d/httpie.list https://packages.httpie.io/deb/httpie.list\n-        - sudo echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/httpie.gpg] https://packages.httpie.io/deb ./\" > /etc/apt/sources.list.d/httpie.list\n+        - sudo sh -c 'echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/httpie.gpg] https://packages.httpie.io/deb ./\" > /etc/apt/sources.list.d/httpie.list'",
    "comment": "hi, there is also another way to do it.\r\nsh\r\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/httpie.gpg] https://packages.httpie.io/deb ./\" | sudo tee -a /etc/apt/sources.list.d/httpie.list\r\n",
    "line_number": 42,
    "enriched": "File: docs/installation/methods.yml\nCode: @@ -39,7 +39,7 @@ tools:\n       install:\n         - curl -SsL https://packages.httpie.io/deb/KEY.gpg | sudo gpg --dearmor -o /usr/share/keyrings/httpie.gpg\n       # - curl -SsL -o /etc/apt/sources.list.d/httpie.list https://packages.httpie.io/deb/httpie.list\n-        - sudo echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/httpie.gpg] https://packages.httpie.io/deb ./\" > /etc/apt/sources.list.d/httpie.list\n+        - sudo sh -c 'echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/httpie.gpg] https://packages.httpie.io/deb ./\" > /etc/apt/sources.list.d/httpie.list'\nComment: Hi, there is also another way to do it.\r\n```sh\r\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/httpie.gpg] https://packages.httpie.io/deb ./\" | sudo tee -a /etc/apt/sources.list.d/httpie.list\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "docs/installation/methods.yml",
    "pr_number": 1534,
    "repo": "httpie",
    "owner": "httpie",
    "comment_id": 1370479613,
    "comment_created_at": "2023-10-24T16:16:22Z"
  },
  {
    "code": "@@ -334,23 +334,24 @@ def test_complex_json_arguments_with_non_json(httpbin, request_type, value):\n                 },\n             },\n         ),\n-        (\n-            [\n-                r'foo\\[key\\]:=1',\n-                r'bar\\[1\\]:=2',\n-                r'baz\\[\\]:3',\n-                r'quux[key\\[escape\\]]:=4',\n-                r'quux[key 2][\\\\][\\\\\\\\][\\\\\\[\\]\\\\\\]\\\\\\[\\n\\\\]:=5',\n-            ],\n-            {\n-                'foo[key]': 1,\n-                'bar[1]': 2,\n-                'quux': {\n-                    'key[escape]': 4,\n-                    'key 2': {'\\\\': {'\\\\\\\\': {'\\\\[]\\\\]\\\\[\\\\n\\\\': 5}}},\n-                },\n-            },\n-        ),\n+        # todo: this expression leads to an invalid header name! this is a bug.",
    "comment": "fyi this test case had to be disabled. it produces an invalid header name.\r\nrequests did not say anything alarming due to the too-permissive underlying urllib3 x http.client.",
    "line_number": 337,
    "enriched": "File: tests/test_json.py\nCode: @@ -334,23 +334,24 @@ def test_complex_json_arguments_with_non_json(httpbin, request_type, value):\n                 },\n             },\n         ),\n-        (\n-            [\n-                r'foo\\[key\\]:=1',\n-                r'bar\\[1\\]:=2',\n-                r'baz\\[\\]:3',\n-                r'quux[key\\[escape\\]]:=4',\n-                r'quux[key 2][\\\\][\\\\\\\\][\\\\\\[\\]\\\\\\]\\\\\\[\\n\\\\]:=5',\n-            ],\n-            {\n-                'foo[key]': 1,\n-                'bar[1]': 2,\n-                'quux': {\n-                    'key[escape]': 4,\n-                    'key 2': {'\\\\': {'\\\\\\\\': {'\\\\[]\\\\]\\\\[\\\\n\\\\': 5}}},\n-                },\n-            },\n-        ),\n+        # todo: this expression leads to an invalid header name! this is a bug.\nComment: FYI this test case had to be disabled. It produces an invalid header name.\r\nrequests did not say anything alarming due to the too-permissive underlying urllib3 x http.client. ",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "tests/test_json.py",
    "pr_number": 1531,
    "repo": "httpie",
    "owner": "httpie",
    "comment_id": 1355616302,
    "comment_created_at": "2023-10-11T18:58:44Z"
  },
  {
    "code": "@@ -215,14 +215,14 @@ $ pacman -Syu httpie\n $ pacman -Syu\n ```\n \n-#### Single Binary Executables\n+#### Single binary executables\n \n-Have a standalone HTTPie executable, when you don't want to go through the full installatin process.\n+Have a standalone HTTPie executable when you don't want to go through the full installation process",
    "comment": "@isidentical i'm still trying to wrap my head around the phrasing here, is there something that escapes me from the structure of the methods above it, or the call-to-action should simply be anything like:\r\n\"get the standalone httpie linux executables when\u2026\" or\r\n\"use the standalone httpie linux executables when\u2026\"?",
    "line_number": 220,
    "enriched": "File: docs/README.md\nCode: @@ -215,14 +215,14 @@ $ pacman -Syu httpie\n $ pacman -Syu\n ```\n \n-#### Single Binary Executables\n+#### Single binary executables\n \n-Have a standalone HTTPie executable, when you don't want to go through the full installatin process.\n+Have a standalone HTTPie executable when you don't want to go through the full installation process\nComment: @isidentical I'm still trying to wrap my head around the phrasing here, is there something that escapes me from the structure of the methods above it, or the call-to-action should simply be anything like:\r\n`\"Get the standalone HTTPie Linux executables when\u2026\"` or\r\n`\"Use the standalone HTTPie Linux executables when\u2026\"`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/README.md",
    "pr_number": 1396,
    "repo": "httpie",
    "owner": "httpie",
    "comment_id": 869142229,
    "comment_created_at": "2022-05-10T11:52:01Z"
  },
  {
    "code": "@@ -163,7 +164,10 @@ def get_stream_type_and_kwargs(\n     if not is_stream and message_type is HTTPResponse:\n         # If this is a response, then check the headers for determining\n         # auto-streaming.\n-        is_stream = headers.get('Content-Type') == 'text/event-stream'\n+        ct_raw = headers.get('Content-Type', None)\n+        if ct_raw:\n+            ct, _ = parse_content_type_header(ct_raw)\n+            is_stream = ct == 'text/event-stream'",
    "comment": "i find expliciter names much more suitable (with the rest of the code base).\r\n",
    "line_number": 170,
    "enriched": "File: httpie/output/writer.py\nCode: @@ -163,7 +164,10 @@ def get_stream_type_and_kwargs(\n     if not is_stream and message_type is HTTPResponse:\n         # If this is a response, then check the headers for determining\n         # auto-streaming.\n-        is_stream = headers.get('Content-Type') == 'text/event-stream'\n+        ct_raw = headers.get('Content-Type', None)\n+        if ct_raw:\n+            ct, _ = parse_content_type_header(ct_raw)\n+            is_stream = ct == 'text/event-stream'\nComment: I find expliciter names much more suitable (with the rest of the code base).\r\n```suggestion\r\n        raw_content_type_header = headers.get('Content-Type', None)\r\n        if raw_content_type_header:\r\n            content_type_header, _ = parse_content_type_header(raw_content_type_header)\r\n            is_stream = (content_type_header == 'text/event-stream')\r\n ```",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "httpie/output/writer.py",
    "pr_number": 1383,
    "repo": "httpie",
    "owner": "httpie",
    "comment_id": 864224248,
    "comment_created_at": "2022-05-03T20:38:50Z"
  },
  {
    "code": "@@ -0,0 +1,41 @@\n+\"\"\"\n+This is a pure implementation of simple swap sort algorithm in Python.\n+It counts the number of smaller values and then swap the element\n+with the matching digit. It uses minimum number of swaps: \n+https://geeksforgeeks.org/minimum-number-swaps-required-sort-array/?ref=lbp\n+Each element may only occur ones.\n+\n+For doctests run following command:\n+python3 -m doctest -v swap_sort.py\n+For manual testing run:\n+python3 swap_sort.py\n+\"\"\"\n+\n+\n+def swap_sort(collection):",
    "comment": "please provide return type hint for the function: swap_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: collection",
    "line_number": 15,
    "enriched": "File: sorts/swap_sort.py\nCode: @@ -0,0 +1,41 @@\n+\"\"\"\n+This is a pure implementation of simple swap sort algorithm in Python.\n+It counts the number of smaller values and then swap the element\n+with the matching digit. It uses minimum number of swaps: \n+https://geeksforgeeks.org/minimum-number-swaps-required-sort-array/?ref=lbp\n+Each element may only occur ones.\n+\n+For doctests run following command:\n+python3 -m doctest -v swap_sort.py\n+For manual testing run:\n+python3 swap_sort.py\n+\"\"\"\n+\n+\n+def swap_sort(collection):\nComment: Please provide return type hint for the function: `swap_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `collection`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "sorts/swap_sort.py",
    "pr_number": 7356,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 997155145,
    "comment_created_at": "2022-10-17T14:44:49Z"
  },
  {
    "code": "@@ -0,0 +1,107 @@\n+class FibonacciNode:\n+    def __init__(self, key):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: key",
    "line_number": 2,
    "enriched": "File: data_structures/heap/fibonacci_heap.py\nCode: @@ -0,0 +1,107 @@\n+class FibonacciNode:\n+    def __init__(self, key):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `key`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/heap/fibonacci_heap.py",
    "pr_number": 9321,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342167956,
    "comment_created_at": "2023-10-01T17:59:13Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+import random\n+\n+\"\"\"\n+* Guess Money is a simple fun math game.\n+* In this game, you have to think of any amount.\n+* It will return the total amount that is left in your pocket.\n+* Have Fun :)\n+\"\"\"\n+\n+# Example\n+\"\"\"\n+1. Let suppose you have 100 \u20b9 in your pocket.\n+2. Borrow 100\u20b9 from your friend..\n+3. Your 100 + Friend 100 = 200\u20b9.\n+5. Let the third amount be 50\u20b9 so, add 50\u20b9 in 200\u20b9 and it become 250\u20b9.\n+6. Spend 100\u20b9 of your friend to purchase some cookies.\n+7. You have 100\u20b9 initially so, donate 100 from left amount.\n+8. You have 50\u20b9 left in your pocket .\n+\"\"\"\n+\n+# Rules\n+\"\"\"\n+1. Do not enter any amount that you are thinking.\n+2. The player just have to press enter and that's it.\n+3. If you are weak in simple calculations then take a calculator and then play.\n+\"\"\"\n+\n+\n+class Guess:\n+    def calculate_number(self) -> int:\n+        # Generate a random number between 1 and 1000\n+        return random.randint(1, 10000)\n+\n+    def print_statement(self):",
    "comment": "please provide return type hint for the function: print_statement. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 34,
    "enriched": "File: other/guess_money.py\nCode: @@ -0,0 +1,58 @@\n+import random\n+\n+\"\"\"\n+* Guess Money is a simple fun math game.\n+* In this game, you have to think of any amount.\n+* It will return the total amount that is left in your pocket.\n+* Have Fun :)\n+\"\"\"\n+\n+# Example\n+\"\"\"\n+1. Let suppose you have 100 \u20b9 in your pocket.\n+2. Borrow 100\u20b9 from your friend..\n+3. Your 100 + Friend 100 = 200\u20b9.\n+5. Let the third amount be 50\u20b9 so, add 50\u20b9 in 200\u20b9 and it become 250\u20b9.\n+6. Spend 100\u20b9 of your friend to purchase some cookies.\n+7. You have 100\u20b9 initially so, donate 100 from left amount.\n+8. You have 50\u20b9 left in your pocket .\n+\"\"\"\n+\n+# Rules\n+\"\"\"\n+1. Do not enter any amount that you are thinking.\n+2. The player just have to press enter and that's it.\n+3. If you are weak in simple calculations then take a calculator and then play.\n+\"\"\"\n+\n+\n+class Guess:\n+    def calculate_number(self) -> int:\n+        # Generate a random number between 1 and 1000\n+        return random.randint(1, 10000)\n+\n+    def print_statement(self):\nComment: Please provide return type hint for the function: `print_statement`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "other/guess_money.py",
    "pr_number": 10410,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359152222,
    "comment_created_at": "2023-10-14T05:08:59Z"
  },
  {
    "code": "@@ -0,0 +1,67 @@\n+import json\n+import socket\n+\n+\n+def request(sock: socket.socket, req_type: str, endpoint: str, headers: list[str], body: str):",
    "comment": "please provide return type hint for the function: request. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 5,
    "enriched": "File: web_programming/http_request_and_response.py\nCode: @@ -0,0 +1,67 @@\n+import json\n+import socket\n+\n+\n+def request(sock: socket.socket, req_type: str, endpoint: str, headers: list[str], body: str):\nComment: Please provide return type hint for the function: `request`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "web_programming/http_request_and_response.py",
    "pr_number": 8018,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1040905202,
    "comment_created_at": "2022-12-06T12:22:08Z"
  },
  {
    "code": "@@ -1,37 +1,26 @@\n \"\"\"\n Greatest Common Divisor.\n \n-Wikipedia reference: https://en.wikipedia.org/wiki/Greatest_common_divisor\n+Wikipedia reference: https://en.wikipedia.org/wiki/greatest_common_divisor",
    "comment": "the original url is correct: it's capitalized",
    "line_number": 4,
    "enriched": "File: maths/greatest_common_divisor.py\nCode: @@ -1,37 +1,26 @@\n \"\"\"\n Greatest Common Divisor.\n \n-Wikipedia reference: https://en.wikipedia.org/wiki/Greatest_common_divisor\n+Wikipedia reference: https://en.wikipedia.org/wiki/greatest_common_divisor\nComment: ```suggestion\r\nWikipedia reference: https://en.wikipedia.org/wiki/Greatest_common_divisor\r\n```\r\nThe original URL is correct: it's capitalized",
    "subcategory": "false positive",
    "category": "false positive",
    "file_path": "maths/greatest_common_divisor.py",
    "pr_number": 9358,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349619563,
    "comment_created_at": "2023-10-08T04:14:53Z"
  },
  {
    "code": "@@ -0,0 +1,133 @@\n+\"\"\"\n+RC4 (Rivest Cipher 4) Algorithm:\n+The 'RC4' class below implements the RC4 stream cipher algorithm, also known as the Rivest Cipher 4.\n+RC4 generates a pseudorandom stream of bits (keystream) based on an initial secret key.\n+It is used for both encryption and decryption by XORing the plaintext with the keystream.\n+\n+Algorithm:\n+1. Key Scheduling: Initialize the S-box (state array) based on the secret key.\n+2. Stream Generation: Generate the keystream based on the S-box.\n+3. Encryption: XOR the plaintext with the keystream to produce the ciphertext.\n+4. Decryption: Decryption is the same as encryption, as RC4 is a symmetric cipher.\n+\n+References:\n+https://en.wikipedia.org/wiki/RC4\n+https://tools.ietf.org/html/rfc6229\n+\"\"\"\n+\n+class RC4:\n+    def __init__(self, key: bytes):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 19,
    "enriched": "File: ciphers/rivest_cipher.py\nCode: @@ -0,0 +1,133 @@\n+\"\"\"\n+RC4 (Rivest Cipher 4) Algorithm:\n+The 'RC4' class below implements the RC4 stream cipher algorithm, also known as the Rivest Cipher 4.\n+RC4 generates a pseudorandom stream of bits (keystream) based on an initial secret key.\n+It is used for both encryption and decryption by XORing the plaintext with the keystream.\n+\n+Algorithm:\n+1. Key Scheduling: Initialize the S-box (state array) based on the secret key.\n+2. Stream Generation: Generate the keystream based on the S-box.\n+3. Encryption: XOR the plaintext with the keystream to produce the ciphertext.\n+4. Decryption: Decryption is the same as encryption, as RC4 is a symmetric cipher.\n+\n+References:\n+https://en.wikipedia.org/wiki/RC4\n+https://tools.ietf.org/html/rfc6229\n+\"\"\"\n+\n+class RC4:\n+    def __init__(self, key: bytes):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "ciphers/rivest_cipher.py",
    "pr_number": 10698,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1365704151,
    "comment_created_at": "2023-10-19T15:04:00Z"
  },
  {
    "code": "@@ -0,0 +1,68 @@\n+class TreeNode:\n+    def __init__(self, val=0, left=None, right=None):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: val\n\nplease provide type hint for the parameter: left\n\nplease provide type hint for the parameter: right",
    "line_number": 2,
    "enriched": "File: data_structures/binary_tree/max_sum_BST.py\nCode: @@ -0,0 +1,68 @@\n+class TreeNode:\n+    def __init__(self, val=0, left=None, right=None):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `val`\n\nPlease provide type hint for the parameter: `left`\n\nPlease provide type hint for the parameter: `right`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/binary_tree/max_sum_BST.py",
    "pr_number": 11832,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1789141902,
    "comment_created_at": "2024-10-06T15:48:11Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+class Node:\n+    def __init__(self, info):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: info",
    "line_number": 2,
    "enriched": "File: height_of_tree/tree.py\nCode: @@ -0,0 +1,53 @@\n+class Node:\n+    def __init__(self, info):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `info`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "height_of_tree/tree.py",
    "pr_number": 13170,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2403937102,
    "comment_created_at": "2025-10-04T12:00:11Z"
  },
  {
    "code": "@@ -72,7 +72,7 @@ def __len__(self) -> int:\n         >>> len(linked_list)\n         0\n         \"\"\"\n-        return len(tuple(iter(self)))\n+        return sum(1 for _ in self)",
    "comment": "the sum will call iter(self) inside,  so this code is still o(n).",
    "line_number": 75,
    "enriched": "File: data_structures/linked_list/singly_linked_list.py\nCode: @@ -72,7 +72,7 @@ def __len__(self) -> int:\n         >>> len(linked_list)\n         0\n         \"\"\"\n-        return len(tuple(iter(self)))\n+        return sum(1 for _ in self)\nComment: The sum will call iter(self) inside,  so this code is still O(n).    ",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "data_structures/linked_list/singly_linked_list.py",
    "pr_number": 8183,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1140565997,
    "comment_created_at": "2023-03-17T18:16:50Z"
  },
  {
    "code": "@@ -1,33 +1,32 @@\n-def permute(nums: list[int]) -> list[list[int]]:\n+def permute_iterative(nums: list[int]) -> list[list[int]]:\n     \"\"\"\n     Return all permutations.\n-    >>> from itertools import permutations\n-    >>> numbers= [1,2,3]\n-    >>> all(list(nums) in permute(numbers) for nums in permutations(numbers))\n-    True\n+\n+    >>> permute_backtrack([1, 2, 3])\n+    [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 2, 1], [3, 1, 2]]\n     \"\"\"\n-    result = []\n+    result: list[list[int]] = []\n     if len(nums) == 1:\n         return [nums.copy()]",
    "comment": "i think we should make the base case 0 rather than 1. this slightly extends the function to work on an empty list input, and this matches the behavior of itertools.permutations.",
    "line_number": 10,
    "enriched": "File: data_structures/arrays/permutations.py\nCode: @@ -1,33 +1,32 @@\n-def permute(nums: list[int]) -> list[list[int]]:\n+def permute_iterative(nums: list[int]) -> list[list[int]]:\n     \"\"\"\n     Return all permutations.\n-    >>> from itertools import permutations\n-    >>> numbers= [1,2,3]\n-    >>> all(list(nums) in permute(numbers) for nums in permutations(numbers))\n-    True\n+\n+    >>> permute_backtrack([1, 2, 3])\n+    [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 2, 1], [3, 1, 2]]\n     \"\"\"\n-    result = []\n+    result: list[list[int]] = []\n     if len(nums) == 1:\n         return [nums.copy()]\nComment: ```suggestion\r\n    if len(nums) == 0:\r\n        return [[]]\r\n```\r\nI think we should make the base case 0 rather than 1. This slightly extends the function to work on an empty list input, and this matches the behavior of `itertools.permutations`.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/arrays/permutations.py",
    "pr_number": 9007,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1301227239,
    "comment_created_at": "2023-08-22T08:14:07Z"
  },
  {
    "code": "@@ -0,0 +1,116 @@\n+import numpy as np\n+\n+def outcome_of_rolling_n_sided_dice_k_time(n_side: int, k_time: int):",
    "comment": "please provide return type hint for the function: outcome_of_rolling_n_sided_dice_k_time. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 3,
    "enriched": "File: maths/sum_of_outcomes_for_rolling_n_sided_dice_k_time.py\nCode: @@ -0,0 +1,116 @@\n+import numpy as np\n+\n+def outcome_of_rolling_n_sided_dice_k_time(n_side: int, k_time: int):\nComment: Please provide return type hint for the function: `outcome_of_rolling_n_sided_dice_k_time`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/sum_of_outcomes_for_rolling_n_sided_dice_k_time.py",
    "pr_number": 11452,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1639717574,
    "comment_created_at": "2024-06-14T12:00:35Z"
  },
  {
    "code": "@@ -0,0 +1,28 @@\n+def octal_to_binary(octal_number):",
    "comment": "please provide return type hint for the function: octal_to_binary. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: octal_number",
    "line_number": 1,
    "enriched": "File: conversions/octal_to_binary.py\nCode: @@ -0,0 +1,28 @@\n+def octal_to_binary(octal_number):\nComment: Please provide return type hint for the function: `octal_to_binary`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `octal_number`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "conversions/octal_to_binary.py",
    "pr_number": 8949,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1291241973,
    "comment_created_at": "2023-08-11T11:43:21Z"
  },
  {
    "code": "@@ -5,7 +5,7 @@\n # Wikipedia reference: https://en.wikipedia.org/wiki/Fermat%27s_little_theorem\n \n \n-def binary_exponentiation(a, n, mod):\n+def binary_exponentiation(a: int, n: float | int, mod: int) -> int:",
    "comment": "per the ruff error, just typehint it as float rather than float | int",
    "line_number": 8,
    "enriched": "File: maths/fermat_little_theorem.py\nCode: @@ -5,7 +5,7 @@\n # Wikipedia reference: https://en.wikipedia.org/wiki/Fermat%27s_little_theorem\n \n \n-def binary_exponentiation(a, n, mod):\n+def binary_exponentiation(a: int, n: float | int, mod: int) -> int:\nComment: ```suggestion\r\ndef binary_exponentiation(a: int, n: float, mod: int) -> int:\r\n```\r\nPer the ruff error, just typehint it as `float` rather than `float | int`",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "maths/fermat_little_theorem.py",
    "pr_number": 9794,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347264559,
    "comment_created_at": "2023-10-05T11:26:55Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+\"\"\"\n+LeetCode 133. Clone Graph\n+https://leetcode.com/problems/clone-graph/\n+\n+Given a reference of a node in a connected undirected graph.\n+\n+Return a deep copy (clone) of the graph.\n+\n+Each node in the graph contains a value (int) and a list (List[Node]) of its\n+neighbors.\n+\"\"\"\n+\n+\n+class Node:\n+    def __init__(self, value=0, neighbors=None):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: value\n\nplease provide type hint for the parameter: neighbors",
    "line_number": 15,
    "enriched": "File: graphs/deep-clone-graph.py\nCode: @@ -0,0 +1,60 @@\n+\"\"\"\n+LeetCode 133. Clone Graph\n+https://leetcode.com/problems/clone-graph/\n+\n+Given a reference of a node in a connected undirected graph.\n+\n+Return a deep copy (clone) of the graph.\n+\n+Each node in the graph contains a value (int) and a list (List[Node]) of its\n+neighbors.\n+\"\"\"\n+\n+\n+class Node:\n+    def __init__(self, value=0, neighbors=None):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `value`\n\nPlease provide type hint for the parameter: `neighbors`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "graphs/deep-clone-graph.py",
    "pr_number": 9765,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346678516,
    "comment_created_at": "2023-10-05T01:54:42Z"
  },
  {
    "code": "@@ -0,0 +1,63 @@\n+\"\"\"\n+This is a pure Python implementation of the binary insertion sort algorithm\n+\n+For doctests run following command:\n+python -m doctest -v binary_insertion_sort.py\n+or\n+python3 -m doctest -v binary_insertion_sort.py\n+\n+For manual testing run:\n+python binary_insertion_sort.py\n+\"\"\"\n+\n+\n+def binary_insertion_sort(collection):",
    "comment": "please provide return type hint for the function: binary_insertion_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: collection",
    "line_number": 14,
    "enriched": "File: sorts/binary_insertion_sort.py\nCode: @@ -0,0 +1,63 @@\n+\"\"\"\n+This is a pure Python implementation of the binary insertion sort algorithm\n+\n+For doctests run following command:\n+python -m doctest -v binary_insertion_sort.py\n+or\n+python3 -m doctest -v binary_insertion_sort.py\n+\n+For manual testing run:\n+python binary_insertion_sort.py\n+\"\"\"\n+\n+\n+def binary_insertion_sort(collection):\nComment: Please provide return type hint for the function: `binary_insertion_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `collection`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "sorts/binary_insertion_sort.py",
    "pr_number": 8024,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1042834811,
    "comment_created_at": "2022-12-08T01:32:41Z"
  },
  {
    "code": "@@ -0,0 +1,168 @@\n+\"\"\"\n+This program is a MNIST classifier using AlexNet. It accepts three parameters provided as a command line input. \n+The first two inputs are two digits between 0-9 which are used to train and test the classifier and the third \n+parameter controls the number of training epochs.\n+Syntax: python program.py <number> <number> <number>\n+\n+For example, to train and test AlexNet with 1 and 2 MNIST samples with 4 training epochs, the command line input should be:\n+python program.py 1 2 4\n+\n+\"\"\"\n+\n+\n+import sys\n+import torch\n+import torch.nn as nn\n+import torchvision.datasets as dset\n+import torchvision.transforms as transforms\n+from torch.autograd import Variable\n+import torch.nn.functional as F\n+import torch.optim as optim\n+\n+\n+class AlexNet(nn.Module):   \n+    def __init__(self, num=10):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: num",
    "line_number": 24,
    "enriched": "File: computer_vision/mnist_classifier.py\nCode: @@ -0,0 +1,168 @@\n+\"\"\"\n+This program is a MNIST classifier using AlexNet. It accepts three parameters provided as a command line input. \n+The first two inputs are two digits between 0-9 which are used to train and test the classifier and the third \n+parameter controls the number of training epochs.\n+Syntax: python program.py <number> <number> <number>\n+\n+For example, to train and test AlexNet with 1 and 2 MNIST samples with 4 training epochs, the command line input should be:\n+python program.py 1 2 4\n+\n+\"\"\"\n+\n+\n+import sys\n+import torch\n+import torch.nn as nn\n+import torchvision.datasets as dset\n+import torchvision.transforms as transforms\n+from torch.autograd import Variable\n+import torch.nn.functional as F\n+import torch.optim as optim\n+\n+\n+class AlexNet(nn.Module):   \n+    def __init__(self, num=10):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `num`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "computer_vision/mnist_classifier.py",
    "pr_number": 12325,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1821881978,
    "comment_created_at": "2024-10-30T05:05:17Z"
  },
  {
    "code": "@@ -4,8 +4,9 @@\n def get_index_of_rightmost_set_bit(number: int) -> int:\n     \"\"\"\n     Take in a positive integer 'number'.\n-    Returns the zero-based index of first set bit in that 'number' from right.\n-    Returns -1, If no set bit found.\n+    Return the zero-based index of the first set bit in this 'number'\n+    starting from right.\n+    Return -1, if the set bit is not found.",
    "comment": "minor formatting",
    "line_number": 9,
    "enriched": "File: bit_manipulation/index_of_rightmost_set_bit.py\nCode: @@ -4,8 +4,9 @@\n def get_index_of_rightmost_set_bit(number: int) -> int:\n     \"\"\"\n     Take in a positive integer 'number'.\n-    Returns the zero-based index of first set bit in that 'number' from right.\n-    Returns -1, If no set bit found.\n+    Return the zero-based index of the first set bit in this 'number'\n+    starting from right.\n+    Return -1, if the set bit is not found.\nComment: ```suggestion\r\n    Return the zero-based index of the first set bit in 'number' starting from right.\r\n    Return -1 if the set bit is not found.\r\n```\r\nMinor formatting",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "bit_manipulation/index_of_rightmost_set_bit.py",
    "pr_number": 9074,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1335106012,
    "comment_created_at": "2023-09-24T05:16:24Z"
  },
  {
    "code": "@@ -20,6 +77,12 @@ def __repr__(self) -> str:\n             return str(self.value)\n         return pformat({f\"{self.value}\": (self.left, self.right)}, indent=1)\n \n+    @property\n+    def is_right(self) -> bool:\n+        if self.parent and self.parent.right:\n+            return self == self.parent.right\n+        return False",
    "comment": "i think this function can be simplified like this:\r\n\r\n\r\n\r\n1) == checks if two variables have the same value, while is checks if they refer to the same object, so i think is is more appropriate here.\r\n\r\n2) we don't need to check if self.parent.right is not none: if it _is_ none, then self is self.parent.right would fail and the function would return false anyway.",
    "line_number": 84,
    "enriched": "File: data_structures/binary_tree/binary_search_tree.py\nCode: @@ -20,6 +77,12 @@ def __repr__(self) -> str:\n             return str(self.value)\n         return pformat({f\"{self.value}\": (self.left, self.right)}, indent=1)\n \n+    @property\n+    def is_right(self) -> bool:\n+        if self.parent and self.parent.right:\n+            return self == self.parent.right\n+        return False\nComment: I think this function can be simplified like this:\r\n\r\n```suggestion\r\n        return self.parent and self is self.parent.right\r\n```\r\n\r\n1) `==` checks if two variables have the same value, while `is` checks if they refer to the same object, so I think `is` is more appropriate here.\r\n\r\n2) We don't need to check if `self.parent.right` is not None: if it _is_ None, then `self is self.parent.right` would fail and the function would return False anyway.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "data_structures/binary_tree/binary_search_tree.py",
    "pr_number": 8693,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1292912236,
    "comment_created_at": "2023-08-14T02:08:35Z"
  },
  {
    "code": "@@ -0,0 +1,149 @@\n+import numpy as np\n+\n+class DecisionTree:\n+    \"\"\"\n+    Decision Tree classifier.\n+\n+    Parameters:\n+        max_depth (int): Maximum depth of the tree. If None, the tree grows until pure nodes or min_samples_split is reached.\n+\n+    Attributes:\n+        tree (tuple): The decision tree structure.\n+\n+    Examples:\n+        >>> np.random.seed(42)\n+        >>> X = np.random.rand(100, 2)\n+        >>> y = (X[:, 0] + X[:, 1] > 1).astype(int)\n+        >>> tree = DecisionTree(max_depth=3)\n+        >>> tree.fit(X, y)\n+        >>> predictions = tree.predict(np.array([[0.7, 0.3], [0.2, 0.8]]))\n+    \"\"\"\n+\n+    def __init__(self, max_depth=None):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: max_depth",
    "line_number": 22,
    "enriched": "File: machine_learning/random_forest_classifier.py\nCode: @@ -0,0 +1,149 @@\n+import numpy as np\n+\n+class DecisionTree:\n+    \"\"\"\n+    Decision Tree classifier.\n+\n+    Parameters:\n+        max_depth (int): Maximum depth of the tree. If None, the tree grows until pure nodes or min_samples_split is reached.\n+\n+    Attributes:\n+        tree (tuple): The decision tree structure.\n+\n+    Examples:\n+        >>> np.random.seed(42)\n+        >>> X = np.random.rand(100, 2)\n+        >>> y = (X[:, 0] + X[:, 1] > 1).astype(int)\n+        >>> tree = DecisionTree(max_depth=3)\n+        >>> tree.fit(X, y)\n+        >>> predictions = tree.predict(np.array([[0.7, 0.3], [0.2, 0.8]]))\n+    \"\"\"\n+\n+    def __init__(self, max_depth=None):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `max_depth`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/random_forest_classifier.py",
    "pr_number": 9770,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346725855,
    "comment_created_at": "2023-10-05T03:14:56Z"
  },
  {
    "code": "@@ -0,0 +1,83 @@\n+import doctest\n+\n+class Polynomial:\n+    def __init__(self, coefficients):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: coefficients",
    "line_number": 4,
    "enriched": "File: maths/polynomials/polynomial_manipulation.py\nCode: @@ -0,0 +1,83 @@\n+import doctest\n+\n+class Polynomial:\n+    def __init__(self, coefficients):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `coefficients`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/polynomials/polynomial_manipulation.py",
    "pr_number": 9424,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342423495,
    "comment_created_at": "2023-10-02T08:43:12Z"
  },
  {
    "code": "@@ -90,6 +111,66 @@ def calculate_current_element(\n     current_row[current_col_idx] = above_to_left_elt + above_to_right_elt\n \n \n+def generate_pascal_triangle_optimized(num_rows: int) -> list[list[int]]:\n+    \"\"\"\n+    This function returns a matrix representing the corresponding pascal's triangle\n+    according to the given input of number of rows of Pascal's triangle to be generated.\n+    It reduces the operations done to generate a row by half\n+    by eliminating redundant calculations.\n+\n+    :param num_rows: Integer specifying the number of rows in the Pascal's triangle\n+    :return: 2-D List (matrix) representing the Pascal's triangle\n+\n+    Return the Pascal's triangle of given rows\n+    >>> generate_pascal_triangle_optimized(3)\n+    [[1], [1, 1], [1, 2, 1]]\n+    >>> generate_pascal_triangle_optimized(1)\n+    [[1]]\n+    >>> generate_pascal_triangle_optimized(0)\n+    []\n+    >>> generate_pascal_triangle_optimized(-5)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The input value of 'num_rows' should be greater than or equal to 0\n+    >>> generate_pascal_triangle_optimized(7.89)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: The input value of 'num_rows' should be 'int'\n+    \"\"\"\n+\n+    if not isinstance(num_rows, int):\n+        raise TypeError(\"The input value of 'num_rows' should be 'int'\")\n+\n+    if num_rows == 0:\n+        return []\n+    elif num_rows < 0:\n+        raise ValueError(\n+            \"The input value of 'num_rows' should be greater than or equal to 0\"\n+        )\n+\n+    result: list[list[int]] = [[1]]\n+\n+    for row_index in range(1, num_rows):\n+        temp_row = [0] + result[-1] + [0]\n+        row_length = row_index + 1\n+\n+        # It calculates the number of distinct elements in a row\n+        distinct_elements = (\n+            row_length // 2 if row_length % 2 == 0 else row_length // 2 + 1\n+        )",
    "comment": "leverage the [python builtin functions](https://docs.python.org/3/library/functions.html):",
    "line_number": 160,
    "enriched": "File: other/pascal_triangle.py\nCode: @@ -90,6 +111,66 @@ def calculate_current_element(\n     current_row[current_col_idx] = above_to_left_elt + above_to_right_elt\n \n \n+def generate_pascal_triangle_optimized(num_rows: int) -> list[list[int]]:\n+    \"\"\"\n+    This function returns a matrix representing the corresponding pascal's triangle\n+    according to the given input of number of rows of Pascal's triangle to be generated.\n+    It reduces the operations done to generate a row by half\n+    by eliminating redundant calculations.\n+\n+    :param num_rows: Integer specifying the number of rows in the Pascal's triangle\n+    :return: 2-D List (matrix) representing the Pascal's triangle\n+\n+    Return the Pascal's triangle of given rows\n+    >>> generate_pascal_triangle_optimized(3)\n+    [[1], [1, 1], [1, 2, 1]]\n+    >>> generate_pascal_triangle_optimized(1)\n+    [[1]]\n+    >>> generate_pascal_triangle_optimized(0)\n+    []\n+    >>> generate_pascal_triangle_optimized(-5)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The input value of 'num_rows' should be greater than or equal to 0\n+    >>> generate_pascal_triangle_optimized(7.89)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: The input value of 'num_rows' should be 'int'\n+    \"\"\"\n+\n+    if not isinstance(num_rows, int):\n+        raise TypeError(\"The input value of 'num_rows' should be 'int'\")\n+\n+    if num_rows == 0:\n+        return []\n+    elif num_rows < 0:\n+        raise ValueError(\n+            \"The input value of 'num_rows' should be greater than or equal to 0\"\n+        )\n+\n+    result: list[list[int]] = [[1]]\n+\n+    for row_index in range(1, num_rows):\n+        temp_row = [0] + result[-1] + [0]\n+        row_length = row_index + 1\n+\n+        # It calculates the number of distinct elements in a row\n+        distinct_elements = (\n+            row_length // 2 if row_length % 2 == 0 else row_length // 2 + 1\n+        )\nComment: Leverage the [Python builtin functions](https://docs.python.org/3/library/functions.html):\r\n```suggestion\r\n        distinct_elements = sum(divmod(row_length, 2))\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "other/pascal_triangle.py",
    "pr_number": 7901,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1009141034,
    "comment_created_at": "2022-10-31T08:24:51Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+# Huffman Coding in python\n+\n+string = 'DADBOBHEHHOOEEDDDD'\n+\n+\n+# Creating tree nodes\n+class NodeTree(object):\n+\n+    def __init__(self, left=None, right=None):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: left\n\nplease provide type hint for the parameter: right",
    "line_number": 9,
    "enriched": "File: greedy_methods/huffman_coding.py\nCode: @@ -0,0 +1,60 @@\n+# Huffman Coding in python\n+\n+string = 'DADBOBHEHHOOEEDDDD'\n+\n+\n+# Creating tree nodes\n+class NodeTree(object):\n+\n+    def __init__(self, left=None, right=None):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `left`\n\nPlease provide type hint for the parameter: `right`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "greedy_methods/huffman_coding.py",
    "pr_number": 7584,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002757625,
    "comment_created_at": "2022-10-23T19:29:14Z"
  },
  {
    "code": "@@ -0,0 +1,128 @@\n+from flask import Flask, render_template_string\n+import requests\n+\n+app = Flask(__name__)\n+\n+# Public APIs for real-time global data (you can replace or add more as needed)\n+COVID_API_URL = \"https://disease.sh/v3/covid-19/countries\"  # Get COVID-19 stats by country\n+NEWS_API_URL = \"https://gnews.io/api/v4/top-headlines?token=YOUR_API_KEY&lang=en\"  # Replace with your API key\n+\n+# Base HTML template\n+BASE_TEMPLATE = \"\"\"\n+<!DOCTYPE html>\n+<html lang=\"en\">\n+<head>\n+    <meta charset=\"UTF-8\">\n+    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n+    <title>Global Events Tracker</title>\n+    <style>\n+        body { font-family: Arial, sans-serif; margin: 0; padding: 0; }\n+        header { background-color: #007bff; color: white; padding: 15px; text-align: center; }\n+        nav a { margin: 0 15px; color: white; text-decoration: none; }\n+        nav { margin-bottom: 20px; }\n+        main { padding: 20px; }\n+        ul { list-style-type: none; padding: 0; }\n+        li { margin: 15px 0; padding: 10px; border: 1px solid #ddd; background-color: #f9f9f9; }\n+        .event-title { font-size: 1.2em; font-weight: bold; }\n+        .event-meta { font-size: 0.9em; color: #555; }\n+        .description { margin-top: 10px; }\n+    </style>\n+</head>\n+<body>\n+    <header>\n+        <h1>Global Events Tracker</h1>\n+        <nav>\n+            <a href=\"/\">Home</a>\n+            <a href=\"/covid-stats\">COVID-19 Stats</a>\n+            <a href=\"/news\">Global News</a>\n+        </nav>\n+    </header>\n+    <main>\n+        {% block content %}{% endblock %}\n+    </main>\n+</body>\n+</html>\n+\"\"\"\n+\n+# Index (Home) Template\n+INDEX_TEMPLATE = \"\"\"\n+{% extends \"base.html\" %}\n+{% block content %}\n+<h2>Welcome to the Global Events Tracker</h2>\n+<p>Track live global events like COVID-19 stats and global news headlines from reliable sources.</p>\n+<ul>\n+    <li><a href=\"/covid-stats\">View COVID-19 Global Stats</a></li>\n+    <li><a href=\"/news\">View Latest Global News</a></li>\n+</ul>\n+{% endblock %}\n+\"\"\"\n+\n+# COVID-19 Stats Template\n+COVID_TEMPLATE = \"\"\"\n+{% extends \"base.html\" %}\n+{% block content %}\n+<h2>Global COVID-19 Stats</h2>\n+<p>Real-time data from the disease.sh API</p>\n+<ul>\n+    {% for country in covid_data %}\n+        <li>\n+            <div class=\"event-title\">{{ country['country'] }}</div>\n+            <div class=\"event-meta\">Cases: {{ country['cases'] }} | Deaths: {{ country['deaths'] }} | Recovered: {{ country['recovered'] }}</div>\n+        </li>\n+    {% else %}\n+        <li>No data available</li>\n+    {% endfor %}\n+</ul>\n+{% endblock %}\n+\"\"\"\n+\n+# News Template\n+NEWS_TEMPLATE = \"\"\"\n+{% extends \"base.html\" %}\n+{% block content %}\n+<h2>Latest Global News</h2>\n+<p>Real-time news fetched from the GNews API</p>\n+<ul>\n+    {% for article in news_data %}\n+        <li>\n+            <div class=\"event-title\"><a href=\"{{ article['url'] }}\" target=\"_blank\">{{ article['title'] }}</a></div>\n+            <div class=\"event-meta\">Source: {{ article['source']['name'] }} | Published: {{ article['publishedAt'][:10] }}</div>\n+            <p class=\"description\">{{ article['description'] }}</p>\n+        </li>\n+    {% else %}\n+        <li>No news articles found</li>\n+    {% endfor %}\n+</ul>\n+{% endblock %}\n+\"\"\"\n+\n+@app.route('/')\n+def index():",
    "comment": "please provide return type hint for the function: index. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 100,
    "enriched": "File: web_programming/Global Events Tracker.py\nCode: @@ -0,0 +1,128 @@\n+from flask import Flask, render_template_string\n+import requests\n+\n+app = Flask(__name__)\n+\n+# Public APIs for real-time global data (you can replace or add more as needed)\n+COVID_API_URL = \"https://disease.sh/v3/covid-19/countries\"  # Get COVID-19 stats by country\n+NEWS_API_URL = \"https://gnews.io/api/v4/top-headlines?token=YOUR_API_KEY&lang=en\"  # Replace with your API key\n+\n+# Base HTML template\n+BASE_TEMPLATE = \"\"\"\n+<!DOCTYPE html>\n+<html lang=\"en\">\n+<head>\n+    <meta charset=\"UTF-8\">\n+    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n+    <title>Global Events Tracker</title>\n+    <style>\n+        body { font-family: Arial, sans-serif; margin: 0; padding: 0; }\n+        header { background-color: #007bff; color: white; padding: 15px; text-align: center; }\n+        nav a { margin: 0 15px; color: white; text-decoration: none; }\n+        nav { margin-bottom: 20px; }\n+        main { padding: 20px; }\n+        ul { list-style-type: none; padding: 0; }\n+        li { margin: 15px 0; padding: 10px; border: 1px solid #ddd; background-color: #f9f9f9; }\n+        .event-title { font-size: 1.2em; font-weight: bold; }\n+        .event-meta { font-size: 0.9em; color: #555; }\n+        .description { margin-top: 10px; }\n+    </style>\n+</head>\n+<body>\n+    <header>\n+        <h1>Global Events Tracker</h1>\n+        <nav>\n+            <a href=\"/\">Home</a>\n+            <a href=\"/covid-stats\">COVID-19 Stats</a>\n+            <a href=\"/news\">Global News</a>\n+        </nav>\n+    </header>\n+    <main>\n+        {% block content %}{% endblock %}\n+    </main>\n+</body>\n+</html>\n+\"\"\"\n+\n+# Index (Home) Template\n+INDEX_TEMPLATE = \"\"\"\n+{% extends \"base.html\" %}\n+{% block content %}\n+<h2>Welcome to the Global Events Tracker</h2>\n+<p>Track live global events like COVID-19 stats and global news headlines from reliable sources.</p>\n+<ul>\n+    <li><a href=\"/covid-stats\">View COVID-19 Global Stats</a></li>\n+    <li><a href=\"/news\">View Latest Global News</a></li>\n+</ul>\n+{% endblock %}\n+\"\"\"\n+\n+# COVID-19 Stats Template\n+COVID_TEMPLATE = \"\"\"\n+{% extends \"base.html\" %}\n+{% block content %}\n+<h2>Global COVID-19 Stats</h2>\n+<p>Real-time data from the disease.sh API</p>\n+<ul>\n+    {% for country in covid_data %}\n+        <li>\n+            <div class=\"event-title\">{{ country['country'] }}</div>\n+            <div class=\"event-meta\">Cases: {{ country['cases'] }} | Deaths: {{ country['deaths'] }} | Recovered: {{ country['recovered'] }}</div>\n+        </li>\n+    {% else %}\n+        <li>No data available</li>\n+    {% endfor %}\n+</ul>\n+{% endblock %}\n+\"\"\"\n+\n+# News Template\n+NEWS_TEMPLATE = \"\"\"\n+{% extends \"base.html\" %}\n+{% block content %}\n+<h2>Latest Global News</h2>\n+<p>Real-time news fetched from the GNews API</p>\n+<ul>\n+    {% for article in news_data %}\n+        <li>\n+            <div class=\"event-title\"><a href=\"{{ article['url'] }}\" target=\"_blank\">{{ article['title'] }}</a></div>\n+            <div class=\"event-meta\">Source: {{ article['source']['name'] }} | Published: {{ article['publishedAt'][:10] }}</div>\n+            <p class=\"description\">{{ article['description'] }}</p>\n+        </li>\n+    {% else %}\n+        <li>No news articles found</li>\n+    {% endfor %}\n+</ul>\n+{% endblock %}\n+\"\"\"\n+\n+@app.route('/')\n+def index():\nComment: Please provide return type hint for the function: `index`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "web_programming/Global Events Tracker.py",
    "pr_number": 11722,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1786849815,
    "comment_created_at": "2024-10-03T21:12:13Z"
  },
  {
    "code": "@@ -0,0 +1,52 @@\n+''' This file provides a function which will take  \n+a product name as input from the user,and fetch the necessary  \n+information about that kind of products from Amazon like the product \n+title,link to that product,price of the product,the ratings of \n+the product and the discount available on the product  \n+in the form of a csv file,this will help the users by improving searchability  \n+and navigability and find the right product easily and in a short period of time, \n+it will also be beneficial for performing better analysis on products'''         \n+\n+\n+\n+\n+import pandas as pd \n+from bs4 import BeautifulSoup as bs \n+import requests \n+import itertools as it    \n+\n+\n+\n+\n+def get_product_info(product = 'laptop'):                 #function that will take the product as input and return the product details as output in the form of a csv file,if no input is given,it will fetch the details of laptop by default    ",
    "comment": "please provide return type hint for the function: get_product_info. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: product",
    "line_number": 21,
    "enriched": "File: web_programming/fetch_amazon_product_data.py\nCode: @@ -0,0 +1,52 @@\n+''' This file provides a function which will take  \n+a product name as input from the user,and fetch the necessary  \n+information about that kind of products from Amazon like the product \n+title,link to that product,price of the product,the ratings of \n+the product and the discount available on the product  \n+in the form of a csv file,this will help the users by improving searchability  \n+and navigability and find the right product easily and in a short period of time, \n+it will also be beneficial for performing better analysis on products'''         \n+\n+\n+\n+\n+import pandas as pd \n+from bs4 import BeautifulSoup as bs \n+import requests \n+import itertools as it    \n+\n+\n+\n+\n+def get_product_info(product = 'laptop'):                 #function that will take the product as input and return the product details as output in the form of a csv file,if no input is given,it will fetch the details of laptop by default    \nComment: Please provide return type hint for the function: `get_product_info`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `product`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "web_programming/fetch_amazon_product_data.py",
    "pr_number": 7585,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002880769,
    "comment_created_at": "2022-10-24T04:51:45Z"
  },
  {
    "code": "@@ -0,0 +1,46 @@\n+# Adjascency List representation in Python\n+\n+\n+class AdjNode:\n+    def __init__(self, value):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: value",
    "line_number": 5,
    "enriched": "File: data_structures/linked_list/adjascency_list.py\nCode: @@ -0,0 +1,46 @@\n+# Adjascency List representation in Python\n+\n+\n+class AdjNode:\n+    def __init__(self, value):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `value`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/linked_list/adjascency_list.py",
    "pr_number": 7562,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002719001,
    "comment_created_at": "2022-10-23T14:24:31Z"
  },
  {
    "code": "@@ -0,0 +1,66 @@\n+\"\"\"\n+This is a pure Python implementation of the sleep algorithm,\n+In general, sleep sort works by starting a separate task for \n+each item to be sorted, where each task sleeps for an interval \n+corresponding to the item's sort key, then emits the item. \n+Items are then collected sequentially in time. \n+\n+More info on: https://rosettacode.org/wiki/Sorting_algorithms/Sleep_sort\n+\n+For doctests run following command:\n+python -m doctest -v sleep_sort.py\n+or\n+python3 -m doctest -v sleep_sort.py\n+For manual testing run:\n+python sleep_sort.py\n+\"\"\"\n+\n+from time import sleep\n+from threading import Timer\n+\n+def sleep_sort(collection):",
    "comment": "please provide return type hint for the function: sleep_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: collection",
    "line_number": 21,
    "enriched": "File: sorts/sleep_sort.py\nCode: @@ -0,0 +1,66 @@\n+\"\"\"\n+This is a pure Python implementation of the sleep algorithm,\n+In general, sleep sort works by starting a separate task for \n+each item to be sorted, where each task sleeps for an interval \n+corresponding to the item's sort key, then emits the item. \n+Items are then collected sequentially in time. \n+\n+More info on: https://rosettacode.org/wiki/Sorting_algorithms/Sleep_sort\n+\n+For doctests run following command:\n+python -m doctest -v sleep_sort.py\n+or\n+python3 -m doctest -v sleep_sort.py\n+For manual testing run:\n+python sleep_sort.py\n+\"\"\"\n+\n+from time import sleep\n+from threading import Timer\n+\n+def sleep_sort(collection):\nComment: Please provide return type hint for the function: `sleep_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `collection`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "sorts/sleep_sort.py",
    "pr_number": 6951,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 991633950,
    "comment_created_at": "2022-10-10T20:55:57Z"
  },
  {
    "code": "@@ -0,0 +1,138 @@\n+import random\n+import sys\n+from sympy import isprime, mod_inverse\n+\n+def generate_prime_candidate(length):",
    "comment": "please provide return type hint for the function: generate_prime_candidate. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: length",
    "line_number": 5,
    "enriched": "File: other/RSA_Algo/rsa_algo.py\nCode: @@ -0,0 +1,138 @@\n+import random\n+import sys\n+from sympy import isprime, mod_inverse\n+\n+def generate_prime_candidate(length):\nComment: Please provide return type hint for the function: `generate_prime_candidate`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `length`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "other/RSA_Algo/rsa_algo.py",
    "pr_number": 11869,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1790742856,
    "comment_created_at": "2024-10-07T19:09:51Z"
  },
  {
    "code": "@@ -0,0 +1,71 @@\n+class Vertex:\n+    def __init__(self, name):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: name",
    "line_number": 2,
    "enriched": "File: graphs/find_minimal_spanning_tree.py\nCode: @@ -0,0 +1,71 @@\n+class Vertex:\n+    def __init__(self, name):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `name`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "graphs/find_minimal_spanning_tree.py",
    "pr_number": 9873,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1348008558,
    "comment_created_at": "2023-10-05T21:35:30Z"
  },
  {
    "code": "@@ -53,7 +53,20 @@ def __repr__(self) -> str:\n         return str(self.h)\n \n     def parent_index(self, child_idx: int) -> int | None:\n-        \"\"\"return the parent index of given child\"\"\"\n+        \"\"\"\n+        return the parent index of given child\n+\n+        >>> h = Heap()\n+        >>> h.build_max_heap([103, 9, 1, 7, 11, 15, 25, 201, 209, 107, 5])",
    "comment": "let's test with zero, negative integers, floating point numbers, and strings.\r\n\r\nlet's test h.parent_index(12) which is not in the heap.\r\n\r\nwe want to test how the algorithm works but also how it fails when given bad input.",
    "line_number": 60,
    "enriched": "File: data_structures/heap/heap.py\nCode: @@ -53,7 +53,20 @@ def __repr__(self) -> str:\n         return str(self.h)\n \n     def parent_index(self, child_idx: int) -> int | None:\n-        \"\"\"return the parent index of given child\"\"\"\n+        \"\"\"\n+        return the parent index of given child\n+\n+        >>> h = Heap()\n+        >>> h.build_max_heap([103, 9, 1, 7, 11, 15, 25, 201, 209, 107, 5])\nComment: Let's test with zero, negative integers, floating point numbers, and strings.\r\n\r\nLet's test `h.parent_index(12)` which is NOT in the heap.\r\n\r\nWe want to test how the algorithm works but also how it fails when given bad input.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/heap/heap.py",
    "pr_number": 11129,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1382504684,
    "comment_created_at": "2023-11-05T04:31:18Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+\"\"\"\n+Calculate the Simple Moving Average (SMA) for a time series data.\n+https://en.wikipedia.org/wiki/Moving_average\n+\"\"\"\n+\n+\n+def simple_moving_average(data, window_size):",
    "comment": "please provide return type hint for the function: simple_moving_average. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: data\n\nplease provide type hint for the parameter: window_size",
    "line_number": 7,
    "enriched": "File: financial/simple_moving_average.py\nCode: @@ -0,0 +1,58 @@\n+\"\"\"\n+Calculate the Simple Moving Average (SMA) for a time series data.\n+https://en.wikipedia.org/wiki/Moving_average\n+\"\"\"\n+\n+\n+def simple_moving_average(data, window_size):\nComment: Please provide return type hint for the function: `simple_moving_average`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `data`\n\nPlease provide type hint for the parameter: `window_size`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "financial/simple_moving_average.py",
    "pr_number": 9291,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342133552,
    "comment_created_at": "2023-10-01T13:04:48Z"
  },
  {
    "code": "@@ -0,0 +1,62 @@\n+#Atomical Length Unit Conversion\n+\n+# REFRENCE  = https://en.wikipedia.org/wiki/Orders_of_magnitude_(length)\n+\n+def convert_to_meters(value, unit):",
    "comment": "please provide return type hint for the function: convert_to_meters. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: value\n\nplease provide type hint for the parameter: unit",
    "line_number": 5,
    "enriched": "File: conversions/Atomical_Length_conversion.py\nCode: @@ -0,0 +1,62 @@\n+#Atomical Length Unit Conversion\n+\n+# REFRENCE  = https://en.wikipedia.org/wiki/Orders_of_magnitude_(length)\n+\n+def convert_to_meters(value, unit):\nComment: Please provide return type hint for the function: `convert_to_meters`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `value`\n\nPlease provide type hint for the parameter: `unit`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "conversions/Atomical_Length_conversion.py",
    "pr_number": 10329,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1357084114,
    "comment_created_at": "2023-10-12T16:34:43Z"
  },
  {
    "code": "@@ -0,0 +1,113 @@\n+\"\"\"\n+\u2014\u2014\u2192 Introduction to Pytorch Autograd (TORCH.AUTOGRAD) :-\n+\n+>>> [torch.autograd] is Pytorch's automatic differential engine that powers neural network training.\n+\n+\u2014\u2192 Background :-\n+\n+>>> Neural network (NN's) are a collection of nested function that are executed on some input data.\n+    These functions are defined by parameters (consist of weights & biases).\n+\n+        Training a NN happens in two steps:\n+\n+        \u2192 Forward Propagation :- In forward prop, NN makes its best guess about the correct output.\n+                                It runs the input data through each of its functions to make this guess.\n+\n+        \u2192 Backward Propagation :- In backward, NN adjusts its parameters proportionate to the error in its guess.\n+                                  It does this by traveling backwards from the input, collecting derivative of the\n+                                  error with respect to the parameters of the functions(gradients), and optimizing\n+                                  the parameters using gradient descent.\n+\n+\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\n+\n+\u2014\u2014\u2192 Defining New Autograd Functions\n+\n+>>> In this example we define our own model as y = a + b * P\u2083(c + dx) instead of y = a + b*x + c*x\u00b2 + d*x\u00b3, where\n+    P\u2083(x) = (1/2)*(5*x\u00b3 - 3*x) is the legendre polynomial of degree three.\n+\"\"\"\n+\n+\n+import torch\n+import math\n+\n+\n+class LegendrePolynomial3(torch.autograd.Function):\n+    \"\"\"\n+    We can implement our own custom autograd Functions by subclassing\n+    torch.autograd.Function and implementing the forward and backward passes\n+    which operate on Tensors.\n+    \"\"\"\n+\n+    @staticmethod\n+    def forward(ctx, input):",
    "comment": "please provide return type hint for the function: forward. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: ctx\n\nplease provide type hint for the parameter: input",
    "line_number": 42,
    "enriched": "File: neural_network/legendre_function.py\nCode: @@ -0,0 +1,113 @@\n+\"\"\"\n+\u2014\u2014\u2192 Introduction to Pytorch Autograd (TORCH.AUTOGRAD) :-\n+\n+>>> [torch.autograd] is Pytorch's automatic differential engine that powers neural network training.\n+\n+\u2014\u2192 Background :-\n+\n+>>> Neural network (NN's) are a collection of nested function that are executed on some input data.\n+    These functions are defined by parameters (consist of weights & biases).\n+\n+        Training a NN happens in two steps:\n+\n+        \u2192 Forward Propagation :- In forward prop, NN makes its best guess about the correct output.\n+                                It runs the input data through each of its functions to make this guess.\n+\n+        \u2192 Backward Propagation :- In backward, NN adjusts its parameters proportionate to the error in its guess.\n+                                  It does this by traveling backwards from the input, collecting derivative of the\n+                                  error with respect to the parameters of the functions(gradients), and optimizing\n+                                  the parameters using gradient descent.\n+\n+\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\u274c\n+\n+\u2014\u2014\u2192 Defining New Autograd Functions\n+\n+>>> In this example we define our own model as y = a + b * P\u2083(c + dx) instead of y = a + b*x + c*x\u00b2 + d*x\u00b3, where\n+    P\u2083(x) = (1/2)*(5*x\u00b3 - 3*x) is the legendre polynomial of degree three.\n+\"\"\"\n+\n+\n+import torch\n+import math\n+\n+\n+class LegendrePolynomial3(torch.autograd.Function):\n+    \"\"\"\n+    We can implement our own custom autograd Functions by subclassing\n+    torch.autograd.Function and implementing the forward and backward passes\n+    which operate on Tensors.\n+    \"\"\"\n+\n+    @staticmethod\n+    def forward(ctx, input):\nComment: Please provide return type hint for the function: `forward`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `ctx`\n\nPlease provide type hint for the parameter: `input`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "neural_network/legendre_function.py",
    "pr_number": 10588,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1360558650,
    "comment_created_at": "2023-10-16T12:08:38Z"
  },
  {
    "code": "@@ -0,0 +1,29 @@\n+# Definition for singly-linked list.\n+class ListNode:\n+     def __init__(self, val=0, next=None):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: val\n\nplease provide type hint for the parameter: next",
    "line_number": 3,
    "enriched": "File: data_structures/linked_list/sort_linked_list.py\nCode: @@ -0,0 +1,29 @@\n+# Definition for singly-linked list.\n+class ListNode:\n+     def __init__(self, val=0, next=None):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `val`\n\nPlease provide type hint for the parameter: `next`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/linked_list/sort_linked_list.py",
    "pr_number": 9910,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1348527673,
    "comment_created_at": "2023-10-06T10:18:21Z"
  },
  {
    "code": "@@ -0,0 +1,228 @@\n+from typing import List\n+import heapq\n+from copy import deepcopy\n+\n+\n+class UniformCostSearch:\n+    def __init__(self, current, final, grid):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: current\n\nplease provide type hint for the parameter: final\n\nplease provide type hint for the parameter: grid",
    "line_number": 7,
    "enriched": "File: graphs/uniform_search_cost.py\nCode: @@ -0,0 +1,228 @@\n+from typing import List\n+import heapq\n+from copy import deepcopy\n+\n+\n+class UniformCostSearch:\n+    def __init__(self, current, final, grid):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `current`\n\nPlease provide type hint for the parameter: `final`\n\nPlease provide type hint for the parameter: `grid`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "graphs/uniform_search_cost.py",
    "pr_number": 8929,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1285250921,
    "comment_created_at": "2023-08-06T18:47:26Z"
  },
  {
    "code": "@@ -0,0 +1,195 @@\n+import traceback\n+\n+\"\"\"\n+    author: Samarth Tankasali\n+    date: 29.10.2023\n+    class: Brainfuck Cipher\n+\n+    This class implements the BF-cipher algorithm and provides\n+    some useful methods for interpreting instructions\n+\n+    Description:\n+    Brainfuck is an esoteric programming language created in 1993 by Urban\n+    M\u00fcller.\n+\n+    Notable for its extreme minimalism, the language consists of only eight\n+    simple commands, a data pointer and an instruction pointer. While it is\n+    fully Turing complete, it is not intended for practical use, but to\n+    challenge and amuse programmers. Brainfuck requires one to break commands\n+    into microscopic steps.\n+\n+    The language's name is a reference to the slang term brainfuck, which refers\n+    to things so complicated or unusual that they exceed the limits of one's\n+    understanding, as it was not meant or made for designing actual software but\n+    to challenge the boundaries of computer programming.\n+    (https://en.wikipedia.org/wiki/Brainfuck)\n+\"\"\"\n+\n+\n+class BFCipher:\n+    \"\"\"\n+    Brainfuck Interpreter.\n+\n+    BFCipher class processes the brainfuck instruction and returns the output\n+    after compilation.\n+\n+    Args:\n+        instruction (str): A string of the BFCipher code.\n+\n+    Examples:\n+        >>> # Code to add 5 + 2\n+        >>> instruction = \"++>+++++[<+>-]++++++++[<++++++>-]<.\"\n+        >>> print(BFCipher(instruction).bf_compiler())\n+        7\n+        >>> # Code to print \"Hello World!\"\n+        >>> instruction = \"++++++++[>++++[>++>+++>+++>+<<<<-]>+>+>->>+[<]<-]>>\"\n+        >>> instruction += \".>---.+++++++..+++.>>.<-.<.+++.------.--------.>>+\"\n+        >>> instruction += \".>++.\"\n+        >>> print(BFCipher(instruction).bf_compiler(), end = \"\")\n+        Hello World!\n+    \"\"\"\n+\n+    def __init__(self, instruction: str) -> None:\n+        \"\"\"\n+        BFCipher constructor\n+\n+        Args:\n+            instruction (str): A string of the BFCipher code.\n+        \"\"\"\n+        self.main_arr: list = [0] * 30000\n+        self.instruction_ptr: int = 0\n+        self.data_ptr: int = 0\n+        self.user_input: list = []\n+        self.loop_table: dict = {}\n+        self.output: str = \"\"\n+        if not isinstance(instruction, str):\n+            raise TypeError(\"Instruction must be a string\")\n+        else:\n+            self.instruction: str = instruction\n+\n+    def __syntax_check(self) -> None:\n+        \"\"\"\n+        Performs a syntax check of the instruction and generates the\n+        `loop_table`.\n+        \"\"\"\n+        checker_ptr: int = self.instruction_ptr\n+        bracket_stack = []\n+        while checker_ptr < len(self.instruction):\n+            if self.instruction[checker_ptr] == \"[\":\n+                bracket_stack.append(checker_ptr)\n+            elif self.instruction[checker_ptr] == \"]\":\n+                if len(bracket_stack) == 0:\n+                    raise SyntaxError(\"Incomplete closure of bracket for instruction\")\n+                loop_beginning_index = bracket_stack.pop()\n+                self.loop_table[loop_beginning_index] = checker_ptr\n+                self.loop_table[checker_ptr] = loop_beginning_index\n+            checker_ptr += 1\n+        if len(bracket_stack) > 0:\n+            raise SyntaxError(\"Incomplete closure of bracket for instruction\")\n+\n+    def __increment_data_ptr(self) -> None:\n+        \"\"\"\n+        Increment the data pointer by one (to point to the next cell to the\n+        right).\n+        \"\"\"\n+        self.data_ptr += 1\n+        if self.data_ptr > 30000:\n+            raise ValueError(\"NullValue Reference: Data pointer referencing null.\")\n+\n+    def __decrement_data_ptr(self) -> None:\n+        \"\"\"\n+        Decrement the data pointer by one (to point to the next cell to the\n+        left).\n+        \"\"\"\n+        self.data_ptr -= 1\n+        if self.data_ptr < 0:\n+            raise ValueError(\"NullValue Reference: Data pointer referencing null.\")\n+\n+    def __increment_data_value(self) -> None:\n+        \"\"\"\n+        Increment the byte at the data pointer by one.\n+        \"\"\"\n+        self.main_arr[self.data_ptr] += 1\n+        self.main_arr[self.data_ptr] = self.main_arr[self.data_ptr] & 0xFF\n+\n+    def __decrement_data_value(self) -> None:\n+        \"\"\"\n+        Decrement the byte at the data pointer by one.\n+        \"\"\"\n+        self.main_arr[self.data_ptr] -= 1\n+        self.main_arr[self.data_ptr] = self.main_arr[self.data_ptr] & 0xFF\n+\n+    def __append_bracket(self) -> None:\n+        \"\"\"\n+        If the byte at the data pointer is zero, then instead of moving the\n+        instruction pointer forward to the next command, jump it forward to\n+        the command after the matching `]` command.\n+        \"\"\"\n+        if self.main_arr[self.data_ptr] == 0:\n+            self.instruction_ptr = self.loop_table[self.instruction_ptr]\n+\n+    def __pop_bracket(self) -> None:\n+        \"\"\"\n+        If the byte at the data pointer is nonzero, then instead of moving the\n+        instruction pointer forward to the next command, jump it back to the\n+        command after the matching `[` command.\n+        \"\"\"\n+        if self.main_arr[self.data_ptr] != 0:\n+            self.instruction_ptr = self.loop_table[self.instruction_ptr]\n+\n+    def __print_output(self) -> None:\n+        \"\"\"\n+        Output the byte at the data pointer.\n+        \"\"\"\n+        self.output += chr(self.main_arr[self.data_ptr])\n+\n+    def __take_input(self) -> None:\n+        \"\"\"\n+        Accept one byte of input, storing its value in the byte at the data\n+        pointer.\n+        \"\"\"\n+        if self.user_input == []:\n+            user_input = list(input() + \"\\n\")\n+        self.main_arr[self.data_ptr] = ord(user_input.pop(0))\n+\n+    def bf_compiler(self) -> str:\n+        \"\"\"\n+        Executes the brainfuck instructions and returns appropriate output after\n+        compilation.\n+\n+        Returns:\n+            `str`: A string representing the output of the BF instructions.\n+        \"\"\"\n+        try:\n+            self.__syntax_check()\n+            while self.instruction_ptr < len(self.instruction):\n+                if self.instruction[self.instruction_ptr] == \">\":\n+                    self.__increment_data_ptr()\n+                elif self.instruction[self.instruction_ptr] == \"<\":\n+                    self.__decrement_data_ptr()\n+                elif self.instruction[self.instruction_ptr] == \"+\":\n+                    self.__increment_data_value()\n+                elif self.instruction[self.instruction_ptr] == \"-\":\n+                    self.__decrement_data_value()\n+                elif self.instruction[self.instruction_ptr] == \"[\":\n+                    self.__append_bracket()\n+                elif self.instruction[self.instruction_ptr] == \"]\":\n+                    self.__pop_bracket()\n+                elif self.instruction[self.instruction_ptr] == \".\":\n+                    self.__print_output()\n+                elif self.instruction[self.instruction_ptr] == \",\":\n+                    self.__take_input()",
    "comment": "this needs timeit benchmarks because it would be a big performance gain to perform dict lookup of instructions.",
    "line_number": 181,
    "enriched": "File: ciphers/brainfuck.py\nCode: @@ -0,0 +1,195 @@\n+import traceback\n+\n+\"\"\"\n+    author: Samarth Tankasali\n+    date: 29.10.2023\n+    class: Brainfuck Cipher\n+\n+    This class implements the BF-cipher algorithm and provides\n+    some useful methods for interpreting instructions\n+\n+    Description:\n+    Brainfuck is an esoteric programming language created in 1993 by Urban\n+    M\u00fcller.\n+\n+    Notable for its extreme minimalism, the language consists of only eight\n+    simple commands, a data pointer and an instruction pointer. While it is\n+    fully Turing complete, it is not intended for practical use, but to\n+    challenge and amuse programmers. Brainfuck requires one to break commands\n+    into microscopic steps.\n+\n+    The language's name is a reference to the slang term brainfuck, which refers\n+    to things so complicated or unusual that they exceed the limits of one's\n+    understanding, as it was not meant or made for designing actual software but\n+    to challenge the boundaries of computer programming.\n+    (https://en.wikipedia.org/wiki/Brainfuck)\n+\"\"\"\n+\n+\n+class BFCipher:\n+    \"\"\"\n+    Brainfuck Interpreter.\n+\n+    BFCipher class processes the brainfuck instruction and returns the output\n+    after compilation.\n+\n+    Args:\n+        instruction (str): A string of the BFCipher code.\n+\n+    Examples:\n+        >>> # Code to add 5 + 2\n+        >>> instruction = \"++>+++++[<+>-]++++++++[<++++++>-]<.\"\n+        >>> print(BFCipher(instruction).bf_compiler())\n+        7\n+        >>> # Code to print \"Hello World!\"\n+        >>> instruction = \"++++++++[>++++[>++>+++>+++>+<<<<-]>+>+>->>+[<]<-]>>\"\n+        >>> instruction += \".>---.+++++++..+++.>>.<-.<.+++.------.--------.>>+\"\n+        >>> instruction += \".>++.\"\n+        >>> print(BFCipher(instruction).bf_compiler(), end = \"\")\n+        Hello World!\n+    \"\"\"\n+\n+    def __init__(self, instruction: str) -> None:\n+        \"\"\"\n+        BFCipher constructor\n+\n+        Args:\n+            instruction (str): A string of the BFCipher code.\n+        \"\"\"\n+        self.main_arr: list = [0] * 30000\n+        self.instruction_ptr: int = 0\n+        self.data_ptr: int = 0\n+        self.user_input: list = []\n+        self.loop_table: dict = {}\n+        self.output: str = \"\"\n+        if not isinstance(instruction, str):\n+            raise TypeError(\"Instruction must be a string\")\n+        else:\n+            self.instruction: str = instruction\n+\n+    def __syntax_check(self) -> None:\n+        \"\"\"\n+        Performs a syntax check of the instruction and generates the\n+        `loop_table`.\n+        \"\"\"\n+        checker_ptr: int = self.instruction_ptr\n+        bracket_stack = []\n+        while checker_ptr < len(self.instruction):\n+            if self.instruction[checker_ptr] == \"[\":\n+                bracket_stack.append(checker_ptr)\n+            elif self.instruction[checker_ptr] == \"]\":\n+                if len(bracket_stack) == 0:\n+                    raise SyntaxError(\"Incomplete closure of bracket for instruction\")\n+                loop_beginning_index = bracket_stack.pop()\n+                self.loop_table[loop_beginning_index] = checker_ptr\n+                self.loop_table[checker_ptr] = loop_beginning_index\n+            checker_ptr += 1\n+        if len(bracket_stack) > 0:\n+            raise SyntaxError(\"Incomplete closure of bracket for instruction\")\n+\n+    def __increment_data_ptr(self) -> None:\n+        \"\"\"\n+        Increment the data pointer by one (to point to the next cell to the\n+        right).\n+        \"\"\"\n+        self.data_ptr += 1\n+        if self.data_ptr > 30000:\n+            raise ValueError(\"NullValue Reference: Data pointer referencing null.\")\n+\n+    def __decrement_data_ptr(self) -> None:\n+        \"\"\"\n+        Decrement the data pointer by one (to point to the next cell to the\n+        left).\n+        \"\"\"\n+        self.data_ptr -= 1\n+        if self.data_ptr < 0:\n+            raise ValueError(\"NullValue Reference: Data pointer referencing null.\")\n+\n+    def __increment_data_value(self) -> None:\n+        \"\"\"\n+        Increment the byte at the data pointer by one.\n+        \"\"\"\n+        self.main_arr[self.data_ptr] += 1\n+        self.main_arr[self.data_ptr] = self.main_arr[self.data_ptr] & 0xFF\n+\n+    def __decrement_data_value(self) -> None:\n+        \"\"\"\n+        Decrement the byte at the data pointer by one.\n+        \"\"\"\n+        self.main_arr[self.data_ptr] -= 1\n+        self.main_arr[self.data_ptr] = self.main_arr[self.data_ptr] & 0xFF\n+\n+    def __append_bracket(self) -> None:\n+        \"\"\"\n+        If the byte at the data pointer is zero, then instead of moving the\n+        instruction pointer forward to the next command, jump it forward to\n+        the command after the matching `]` command.\n+        \"\"\"\n+        if self.main_arr[self.data_ptr] == 0:\n+            self.instruction_ptr = self.loop_table[self.instruction_ptr]\n+\n+    def __pop_bracket(self) -> None:\n+        \"\"\"\n+        If the byte at the data pointer is nonzero, then instead of moving the\n+        instruction pointer forward to the next command, jump it back to the\n+        command after the matching `[` command.\n+        \"\"\"\n+        if self.main_arr[self.data_ptr] != 0:\n+            self.instruction_ptr = self.loop_table[self.instruction_ptr]\n+\n+    def __print_output(self) -> None:\n+        \"\"\"\n+        Output the byte at the data pointer.\n+        \"\"\"\n+        self.output += chr(self.main_arr[self.data_ptr])\n+\n+    def __take_input(self) -> None:\n+        \"\"\"\n+        Accept one byte of input, storing its value in the byte at the data\n+        pointer.\n+        \"\"\"\n+        if self.user_input == []:\n+            user_input = list(input() + \"\\n\")\n+        self.main_arr[self.data_ptr] = ord(user_input.pop(0))\n+\n+    def bf_compiler(self) -> str:\n+        \"\"\"\n+        Executes the brainfuck instructions and returns appropriate output after\n+        compilation.\n+\n+        Returns:\n+            `str`: A string representing the output of the BF instructions.\n+        \"\"\"\n+        try:\n+            self.__syntax_check()\n+            while self.instruction_ptr < len(self.instruction):\n+                if self.instruction[self.instruction_ptr] == \">\":\n+                    self.__increment_data_ptr()\n+                elif self.instruction[self.instruction_ptr] == \"<\":\n+                    self.__decrement_data_ptr()\n+                elif self.instruction[self.instruction_ptr] == \"+\":\n+                    self.__increment_data_value()\n+                elif self.instruction[self.instruction_ptr] == \"-\":\n+                    self.__decrement_data_value()\n+                elif self.instruction[self.instruction_ptr] == \"[\":\n+                    self.__append_bracket()\n+                elif self.instruction[self.instruction_ptr] == \"]\":\n+                    self.__pop_bracket()\n+                elif self.instruction[self.instruction_ptr] == \".\":\n+                    self.__print_output()\n+                elif self.instruction[self.instruction_ptr] == \",\":\n+                    self.__take_input()\nComment: This needs timeit benchmarks because it would be a big performance gain to perform dict lookup of instructions.\r\n```suggestion\r\ninstructions = {\r\n    \"-\": self.__decrement_data_value,\r\n    \",\": self.__take_input,\r\n    \".\": self.__print_output,\r\n    \"[\": self.__append_bracket,\r\n    \"]\": self.__pop_bracket,\r\n    \"+\": self.__increment_data_value,\r\n    \"<\": self.__decrement_data_ptr,\r\n    \">\": self.__increment_data_ptr,\r\n}\r\nif method := instructions.get(self.instruction[self.instruction_ptr])\r\n    method()\r\nelse:\r\n    msg = f\"{self.instruction[self.instruction_ptr]} is an invalid instruction\"\r\n    raise ValueError(msg)\r\n```",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "ciphers/brainfuck.py",
    "pr_number": 11073,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375417674,
    "comment_created_at": "2023-10-29T11:22:56Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+class Node: \n+    def __init__(self,data): ",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: data",
    "line_number": 2,
    "enriched": "File: data_structures/linked_list/cdll.py\nCode: @@ -0,0 +1,94 @@\n+class Node: \n+    def __init__(self,data): \nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `data`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/linked_list/cdll.py",
    "pr_number": 7907,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1009267763,
    "comment_created_at": "2022-10-31T10:38:05Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+\"\"\"\n+Calculating Electric Power\n+Description : Electric power can be said to be the rate at which",
    "comment": "the provided code calculates the electric power for dc circuits based on the given input parameters, which include the potential difference (voltage), current, and resistance. \r\n\r\nyou should change pr name as dc electric power formula.",
    "line_number": 3,
    "enriched": "File: physics/electric_power.py\nCode: @@ -0,0 +1,60 @@\n+\"\"\"\n+Calculating Electric Power\n+Description : Electric power can be said to be the rate at which\nComment: The provided code calculates the electric power for DC circuits based on the given input parameters, which include the potential difference (voltage), current, and resistance. \r\n\r\nYou should change PR name as DC Electric Power formula.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "physics/electric_power.py",
    "pr_number": 8155,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1175654947,
    "comment_created_at": "2023-04-24T18:37:39Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+\"\"\"\n+Signum function\n+\n+Refer - https://en.wikipedia.org/wiki/Sign_function\n+\"\"\"\n+\n+def signum(num):",
    "comment": "please provide return type hint for the function: signum. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: num",
    "line_number": 7,
    "enriched": "File: maths/signum.py\nCode: @@ -0,0 +1,39 @@\n+\"\"\"\n+Signum function\n+\n+Refer - https://en.wikipedia.org/wiki/Sign_function\n+\"\"\"\n+\n+def signum(num):\nComment: Please provide return type hint for the function: `signum`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `num`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/signum.py",
    "pr_number": 7526,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002582399,
    "comment_created_at": "2022-10-22T21:06:45Z"
  },
  {
    "code": "@@ -0,0 +1,65 @@\n+\"\"\"\n+You are given a list of segments, where each segment is represented by its left and right endpoints. \n+The task is to find the minimum number of points needed to cover all the segments. \n+If you can cover all the segments with points, return the points' positions; otherwise, return -1.\n+\n+Implement the SegmentIntersection class with the following methods:\n+\n+ 1. __init__(self, segments): The constructor that initializes the SegmentIntersection object with a list of segments.\n+\n+ 2. find_min_points(self): A method that finds the minimum number of points needed to cover\n+all the segments and returns both the count of points and their positions as a list.\n+\n+Use the following implementation notes as a guide:\n+\n+1. Sort the segments by their right endpoints.\n+2. Initialize an empty list called points to store the positions of points.\n+3. While there are segments left in the list:\n+4. Take the segment with the smallest right endpoint, x.\n+5. Add the right endpoint of x to the points list.\n+6. Remove any segments in the list that intersect with x.\n+7. Return the count of points and the list of points.\n+\"\"\"\n+\n+class SegmentIntersection:\n+    def __init__(self, segments):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: segments",
    "line_number": 25,
    "enriched": "File: greedy_methods/minimum_segment_intersection.py\nCode: @@ -0,0 +1,65 @@\n+\"\"\"\n+You are given a list of segments, where each segment is represented by its left and right endpoints. \n+The task is to find the minimum number of points needed to cover all the segments. \n+If you can cover all the segments with points, return the points' positions; otherwise, return -1.\n+\n+Implement the SegmentIntersection class with the following methods:\n+\n+ 1. __init__(self, segments): The constructor that initializes the SegmentIntersection object with a list of segments.\n+\n+ 2. find_min_points(self): A method that finds the minimum number of points needed to cover\n+all the segments and returns both the count of points and their positions as a list.\n+\n+Use the following implementation notes as a guide:\n+\n+1. Sort the segments by their right endpoints.\n+2. Initialize an empty list called points to store the positions of points.\n+3. While there are segments left in the list:\n+4. Take the segment with the smallest right endpoint, x.\n+5. Add the right endpoint of x to the points list.\n+6. Remove any segments in the list that intersect with x.\n+7. Return the count of points and the list of points.\n+\"\"\"\n+\n+class SegmentIntersection:\n+    def __init__(self, segments):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `segments`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "greedy_methods/minimum_segment_intersection.py",
    "pr_number": 10647,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1362314345,
    "comment_created_at": "2023-10-17T15:21:55Z"
  },
  {
    "code": "@@ -0,0 +1,251 @@\n+\"\"\"\n+Quantum Logic Gates which are implemented mathematically\n+and can be used as functions to build complex calculations\n+and implement different operations. The input taken is a real value\n+and imaginary value of the number and the result is output after computation.\n+\n+References :\n+https://en.wikipedia.org/wiki/Quantum_logic_gate\n+\n+Book : Mathematics Of Quantum Computing An Introduction by Wolfgang Scherer\n+\n+Glossary ;\n+input_realvalue : the magnitude of the real part of the input complex number.\n+input_imaginaryvalue : the magnitude of the imaginary part of the input complex number.\n+In cases which require 2 inputs the input is named with a suffix of 1 and 2\n+(Eg. input_realvalue_1)\n+\n+alpha : angle of rotation as represented by the block sphere.\n+iota : The exponential complex of alpha value.\n+nx_value : value of vector in X axis as represented by Hilbert space.\n+nx_value : value of vector in Y axis as represented by Hilbert space.\n+nx_value : value of vector in Z axis as represented by Hilbert space.\n+\n+* The nx,ny and nz values can also be considered as values of vectors along\n+the respective axes on the bloch sphere.\n+\n+Usage :\n+>>> paulix_gate(2,3)\n+[3 2]\n+\n+>>> pauliy_gate(5,8)\n+[0.+8.j 0.-5.j]\n+\n+>>> pauliz_gate(4,1)\n+[ 4 -1]\n+\n+>>> identity_gate(7,2)\n+9\n+\n+>>> phasefactor_of_input(4,7,45)\n+[1.39737084e+20+0.j 2.44539897e+20+0.j]\n+\n+>>> phaseshift_of_input(3,9,30)\n+[3.00000000e+00+0.j 9.61782712e+13+0.j]\n+\n+>>> hadamard_gate(5,9)\n+[ 9.89949494 -2.82842712]\n+[1.+0.j 0.+0.j 0.+0.j 7.+0.j]\n+\n+>>> controlled_not_gate_in_0ket(1,7,4,8)\n+[7 1 4 8]\n+\n+>>> controlled_not_gate(6,3,7,5)\n+[6 3 5 7]\n+\n+>>> inverted_controlled_not_gate(8,4,9,6)\n+[8 6 9 4]\n+\n+>>> controlled_phase_multiplication(3,2,5,1,10)\n+[3.00000000e+00+0.j 2.00000000e+00+0.j 1.10132329e+05+0.j\n+ 2.20264658e+04+0.j]\n+\n+>>> swap_gate(5,1,3,7)\n+[5 3 1 7]\n+\n+>>> spin_of_input(6,3,45,1,8,3)\n+[-16.93201614+10.23066476j -50.61991392 -1.46152354j]\n+\n+\"\"\"\n+\n+import cmath\n+import math\n+\n+import numpy as np\n+\n+\n+def paulix_gate(input_realvalue, input_imaginaryvalue):",
    "comment": "please provide return type hint for the function: paulix_gate. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: input_realvalue\n\nplease provide type hint for the parameter: input_imaginaryvalue",
    "line_number": 77,
    "enriched": "File: quantum/qauntum_logic_gates.py\nCode: @@ -0,0 +1,251 @@\n+\"\"\"\n+Quantum Logic Gates which are implemented mathematically\n+and can be used as functions to build complex calculations\n+and implement different operations. The input taken is a real value\n+and imaginary value of the number and the result is output after computation.\n+\n+References :\n+https://en.wikipedia.org/wiki/Quantum_logic_gate\n+\n+Book : Mathematics Of Quantum Computing An Introduction by Wolfgang Scherer\n+\n+Glossary ;\n+input_realvalue : the magnitude of the real part of the input complex number.\n+input_imaginaryvalue : the magnitude of the imaginary part of the input complex number.\n+In cases which require 2 inputs the input is named with a suffix of 1 and 2\n+(Eg. input_realvalue_1)\n+\n+alpha : angle of rotation as represented by the block sphere.\n+iota : The exponential complex of alpha value.\n+nx_value : value of vector in X axis as represented by Hilbert space.\n+nx_value : value of vector in Y axis as represented by Hilbert space.\n+nx_value : value of vector in Z axis as represented by Hilbert space.\n+\n+* The nx,ny and nz values can also be considered as values of vectors along\n+the respective axes on the bloch sphere.\n+\n+Usage :\n+>>> paulix_gate(2,3)\n+[3 2]\n+\n+>>> pauliy_gate(5,8)\n+[0.+8.j 0.-5.j]\n+\n+>>> pauliz_gate(4,1)\n+[ 4 -1]\n+\n+>>> identity_gate(7,2)\n+9\n+\n+>>> phasefactor_of_input(4,7,45)\n+[1.39737084e+20+0.j 2.44539897e+20+0.j]\n+\n+>>> phaseshift_of_input(3,9,30)\n+[3.00000000e+00+0.j 9.61782712e+13+0.j]\n+\n+>>> hadamard_gate(5,9)\n+[ 9.89949494 -2.82842712]\n+[1.+0.j 0.+0.j 0.+0.j 7.+0.j]\n+\n+>>> controlled_not_gate_in_0ket(1,7,4,8)\n+[7 1 4 8]\n+\n+>>> controlled_not_gate(6,3,7,5)\n+[6 3 5 7]\n+\n+>>> inverted_controlled_not_gate(8,4,9,6)\n+[8 6 9 4]\n+\n+>>> controlled_phase_multiplication(3,2,5,1,10)\n+[3.00000000e+00+0.j 2.00000000e+00+0.j 1.10132329e+05+0.j\n+ 2.20264658e+04+0.j]\n+\n+>>> swap_gate(5,1,3,7)\n+[5 3 1 7]\n+\n+>>> spin_of_input(6,3,45,1,8,3)\n+[-16.93201614+10.23066476j -50.61991392 -1.46152354j]\n+\n+\"\"\"\n+\n+import cmath\n+import math\n+\n+import numpy as np\n+\n+\n+def paulix_gate(input_realvalue, input_imaginaryvalue):\nComment: Please provide return type hint for the function: `paulix_gate`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `input_realvalue`\n\nPlease provide type hint for the parameter: `input_imaginaryvalue`",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "quantum/qauntum_logic_gates.py",
    "pr_number": 8956,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1292927286,
    "comment_created_at": "2023-08-14T02:56:27Z"
  },
  {
    "code": "@@ -0,0 +1,74 @@\n+import doctest\n+\n+import numpy as np\n+from sklearn.datasets import load_iris\n+from sklearn.tree import DecisionTreeClassifier\n+\n+\"\"\"\"\n+Implementation of basic AdaBoost classifier on iris dataset.\n+The following classifier uses a pre-built DecisionTreeClassifier\n+from scikit-learn as a weak learner.\n+\n+AdaBoost (Adaptive Boosting) is an ensemble learning technique \n+used for classification problems. It combines multiple weak learners \n+(in this case, decision trees with maximum depth 1) to create a \n+strong classifier. The key idea behind AdaBoost is to give \n+more weight to the training instances thatare misclassified by \n+the previous weak learners, so that subsequent weak learners focus more on \n+these misclassified instances.\n+\"\"\"\n+\n+class AdaBoost:\n+    def __init__(self, n_estimators=50):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: n_estimators",
    "line_number": 22,
    "enriched": "File: machine_learning/adaboost_classifier.py\nCode: @@ -0,0 +1,74 @@\n+import doctest\n+\n+import numpy as np\n+from sklearn.datasets import load_iris\n+from sklearn.tree import DecisionTreeClassifier\n+\n+\"\"\"\"\n+Implementation of basic AdaBoost classifier on iris dataset.\n+The following classifier uses a pre-built DecisionTreeClassifier\n+from scikit-learn as a weak learner.\n+\n+AdaBoost (Adaptive Boosting) is an ensemble learning technique \n+used for classification problems. It combines multiple weak learners \n+(in this case, decision trees with maximum depth 1) to create a \n+strong classifier. The key idea behind AdaBoost is to give \n+more weight to the training instances thatare misclassified by \n+the previous weak learners, so that subsequent weak learners focus more on \n+these misclassified instances.\n+\"\"\"\n+\n+class AdaBoost:\n+    def __init__(self, n_estimators=50):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `n_estimators`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/adaboost_classifier.py",
    "pr_number": 10550,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359931617,
    "comment_created_at": "2023-10-15T19:17:44Z"
  },
  {
    "code": "@@ -0,0 +1,33 @@\n+def max_product_subarray(nums):",
    "comment": "please provide return type hint for the function: max_product_subarray. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: nums",
    "line_number": 1,
    "enriched": "File: Kadane_algorithm/max_product_subarray.py\nCode: @@ -0,0 +1,33 @@\n+def max_product_subarray(nums):\nComment: Please provide return type hint for the function: `max_product_subarray`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `nums`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "Kadane_algorithm/max_product_subarray.py",
    "pr_number": 8568,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1151407246,
    "comment_created_at": "2023-03-29T05:22:52Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+import numpy as np\n+\n+def matrix_factorization_svd(matrix):",
    "comment": "please provide return type hint for the function: matrix_factorization_svd. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: matrix",
    "line_number": 3,
    "enriched": "File: matrix/matrix_factorization.py\nCode: @@ -0,0 +1,48 @@\n+import numpy as np\n+\n+def matrix_factorization_svd(matrix):\nComment: Please provide return type hint for the function: `matrix_factorization_svd`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `matrix`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "matrix/matrix_factorization.py",
    "pr_number": 9496,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342915604,
    "comment_created_at": "2023-10-02T16:31:47Z"
  },
  {
    "code": "@@ -0,0 +1,73 @@\n+\"\"\"\n+Longest Common Substring Problem Statement: Given two sequences, find the length\n+of longest common substring present in both of them. A substring is\n+necessarily continuous.\n+Example:\"abcdef\", \"xabded\" have the length of longest common substring 2 (\"ab\" or \"de\").\n+\"\"\"\n+\n+\n+def longest_common_substring(t1: str, t2: str):",
    "comment": "please provide return type hint for the function: longest_common_substring. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 9,
    "enriched": "File: dynamic_programming/longest_common_substring.py\nCode: @@ -0,0 +1,73 @@\n+\"\"\"\n+Longest Common Substring Problem Statement: Given two sequences, find the length\n+of longest common substring present in both of them. A substring is\n+necessarily continuous.\n+Example:\"abcdef\", \"xabded\" have the length of longest common substring 2 (\"ab\" or \"de\").\n+\"\"\"\n+\n+\n+def longest_common_substring(t1: str, t2: str):\nComment: Please provide return type hint for the function: `longest_common_substring`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/longest_common_substring.py",
    "pr_number": 7488,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1001924134,
    "comment_created_at": "2022-10-21T15:28:07Z"
  },
  {
    "code": "@@ -1,18 +1,23 @@\n-\"\"\"\n-Description :\n-Newton's second law of motion pertains to the behavior of objects for which\n-all existing forces are not balanced.\n-The second law states that the acceleration of an object is dependent upon two variables\n-- the net force acting upon the object and the mass of the object.\n-The acceleration of an object depends directly\n-upon the net force acting upon the object,\n-and inversely upon the mass of the object.\n-As the force acting upon an object is increased,\n-the acceleration of the object is increased.\n-As the mass of an object is increased, the acceleration of the object is decreased.\n+r\"\"\"\n+Description:\n+    Newton's second law of motion pertains to the behavior of objects for which\n+    all existing forces are not balanced.\n+    The second law states that the acceleration of an object is dependent upon\n+    two variables - the net force acting upon the object and the mass of the object.\n+    The acceleration of an object depends directly\n+    upon the net force acting upon the object,\n+    and inversely upon the mass of the object.\n+    As the force acting upon an object is increased,\n+    the acceleration of the object is increased.\n+    As the mass of an object is increased, the acceleration of the object is decreased.\n+\n Source: https://www.physicsclassroom.com/class/newtlaws/Lesson-3/Newton-s-Second-Law\n-Formulation: Fnet = m \u2022 a\n-Diagrammatic Explanation:\n+\n+Formulation:\n+    .. math:: F_{net} = m \\cdot a",
    "comment": "this change helps computer readers but does hurt human readers of the code.",
    "line_number": 17,
    "enriched": "File: physics/newtons_second_law_of_motion.py\nCode: @@ -1,18 +1,23 @@\n-\"\"\"\n-Description :\n-Newton's second law of motion pertains to the behavior of objects for which\n-all existing forces are not balanced.\n-The second law states that the acceleration of an object is dependent upon two variables\n-- the net force acting upon the object and the mass of the object.\n-The acceleration of an object depends directly\n-upon the net force acting upon the object,\n-and inversely upon the mass of the object.\n-As the force acting upon an object is increased,\n-the acceleration of the object is increased.\n-As the mass of an object is increased, the acceleration of the object is decreased.\n+r\"\"\"\n+Description:\n+    Newton's second law of motion pertains to the behavior of objects for which\n+    all existing forces are not balanced.\n+    The second law states that the acceleration of an object is dependent upon\n+    two variables - the net force acting upon the object and the mass of the object.\n+    The acceleration of an object depends directly\n+    upon the net force acting upon the object,\n+    and inversely upon the mass of the object.\n+    As the force acting upon an object is increased,\n+    the acceleration of the object is increased.\n+    As the mass of an object is increased, the acceleration of the object is decreased.\n+\n Source: https://www.physicsclassroom.com/class/newtlaws/Lesson-3/Newton-s-Second-Law\n-Formulation: Fnet = m \u2022 a\n-Diagrammatic Explanation:\n+\n+Formulation:\n+    .. math:: F_{net} = m \\cdot a\nComment: This change helps computer readers but does hurt human readers of the code.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "physics/newtons_second_law_of_motion.py",
    "pr_number": 12480,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1899151777,
    "comment_created_at": "2024-12-29T15:33:17Z"
  },
  {
    "code": "@@ -0,0 +1,32 @@\n+def quick_sort(array):",
    "comment": "please provide return type hint for the function: quick_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: array",
    "line_number": 1,
    "enriched": "File: divide_and_conquer/quicksort.py\nCode: @@ -0,0 +1,32 @@\n+def quick_sort(array):\nComment: Please provide return type hint for the function: `quick_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `array`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "divide_and_conquer/quicksort.py",
    "pr_number": 11038,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375013248,
    "comment_created_at": "2023-10-27T20:23:56Z"
  },
  {
    "code": "@@ -0,0 +1,75 @@\n+\"\"\"\n+Contour Detection Using OpenCV\n+\n+This script reads an image, performs contour detection using OpenCV,\n+and saves the result with detected contours.\n+\n+Author: Anuj Mishra\n+Date: 05-10-2023\n+\"\"\"\n+\n+import cv2\n+import numpy as np\n+\n+class ContourDetector:\n+    def __init__(self, image_path):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: image_path",
    "line_number": 15,
    "enriched": "File: computer_vision/contour_detection_opencv.py\nCode: @@ -0,0 +1,75 @@\n+\"\"\"\n+Contour Detection Using OpenCV\n+\n+This script reads an image, performs contour detection using OpenCV,\n+and saves the result with detected contours.\n+\n+Author: Anuj Mishra\n+Date: 05-10-2023\n+\"\"\"\n+\n+import cv2\n+import numpy as np\n+\n+class ContourDetector:\n+    def __init__(self, image_path):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `image_path`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "computer_vision/contour_detection_opencv.py",
    "pr_number": 9781,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346895703,
    "comment_created_at": "2023-10-05T07:04:47Z"
  },
  {
    "code": "@@ -0,0 +1,77 @@\n+\"\"\"\n+Project Euler Problem 148 : https://projecteuler.net/problem=148\n+Author:\tSai Teja Manchi\n+Problem Statement:\n+We can easily verify that none of the entries in the\n+first seven rows of Pascal's triangle are divisible by 7:\n+                               1\n+                          1          1\n+                     1          2          1\n+                1          3          3          1\n+           1          4          6          4          1\n+      1          5         10         10          5          1\n+1          6         15         20         15          6          1\n+However, if we check the first one hundred rows, we will find that\n+only 2361 of the 5050 entries are not divisible by 7.\n+Find the number of entries which are not divisible by 7\n+in the first one billion (109) rows of Pascal's triangle.\n+\n+Solution:\n+We iteratively generate each row in the pascal triangle one-by-one.\n+Since Pascal's triangle is vertically-symmetric,\n+We only need to generate half of the values.\n+We then count the values which are not divisible by 7.\n+We only store the remainders(when divided by 7) in the list to reduce memory usage.\n+\n+Note: In the original problem, we need to calculate for 10^9 rows\n+      but we took 10^4 rows here by default.\n+\"\"\"\n+\n+\n+def solution(pascal_row_count: int = 10**4) -> int:",
    "comment": "it has a hardcoded value of pascal_row_count which is only 10^4. as a result, the function returns the result for the first 10^4 rows of pascal's triangle instead of the first billion rows, try this:\r\n\r\ndef solution(pascal_row_count: int = 10**9) -> int:",
    "line_number": 31,
    "enriched": "File: project_euler/problem_148/sol1.py\nCode: @@ -0,0 +1,77 @@\n+\"\"\"\n+Project Euler Problem 148 : https://projecteuler.net/problem=148\n+Author:\tSai Teja Manchi\n+Problem Statement:\n+We can easily verify that none of the entries in the\n+first seven rows of Pascal's triangle are divisible by 7:\n+                               1\n+                          1          1\n+                     1          2          1\n+                1          3          3          1\n+           1          4          6          4          1\n+      1          5         10         10          5          1\n+1          6         15         20         15          6          1\n+However, if we check the first one hundred rows, we will find that\n+only 2361 of the 5050 entries are not divisible by 7.\n+Find the number of entries which are not divisible by 7\n+in the first one billion (109) rows of Pascal's triangle.\n+\n+Solution:\n+We iteratively generate each row in the pascal triangle one-by-one.\n+Since Pascal's triangle is vertically-symmetric,\n+We only need to generate half of the values.\n+We then count the values which are not divisible by 7.\n+We only store the remainders(when divided by 7) in the list to reduce memory usage.\n+\n+Note: In the original problem, we need to calculate for 10^9 rows\n+      but we took 10^4 rows here by default.\n+\"\"\"\n+\n+\n+def solution(pascal_row_count: int = 10**4) -> int:\nComment: It has a hardcoded value of pascal_row_count which is only 10^4. As a result, the function returns the result for the first 10^4 rows of Pascal's triangle instead of the first billion rows, try this:\r\n\r\n`def solution(pascal_row_count: int = 10**9) -> int:`",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "project_euler/problem_148/sol1.py",
    "pr_number": 8662,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1174793342,
    "comment_created_at": "2023-04-24T05:32:00Z"
  },
  {
    "code": "@@ -0,0 +1,104 @@\n+\"\"\"\n+Title : Calculating the Hubble Parameter\n+\n+Description : The Hubble parameter H is the Universe expansion rate in any time.\n+In cosmology is customary to use the redshift redshift in place of time, because\n+the redshift is directily mensure in the light of galaxies moving away\n+from us.\n+\n+So, the general relation that we obtain is\n+\n+H = hubble_constant*(radiation_density*(redshift+1)**4\n+                     + matter_density*(redshift+1)**3\n+                     + curvature*(redshift+1)**2 + dark_energy)**(1/2)\n+\n+where radiation_density, matter_density, dark_energy are the relativity\n+(the percentage) energy densities that exist\n+in the Universe today. Here, matter_density is the\n+sum of the barion density and the\n+dark matter. Curvature is the curvature parameter and can be written in term\n+of the densities by the completeness\n+\n+\n+curvature = 1 - (matter_density + radiation_density + dark_energy)\n+\n+Source :\n+https://www.sciencedirect.com/topics/mathematics/hubble-parameter\n+\"\"\"\n+\n+\n+def hubble_parameter(",
    "comment": "please provide return type hint for the function: hubble_parameter. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 30,
    "enriched": "File: physics/hubble_parameter.py\nCode: @@ -0,0 +1,104 @@\n+\"\"\"\n+Title : Calculating the Hubble Parameter\n+\n+Description : The Hubble parameter H is the Universe expansion rate in any time.\n+In cosmology is customary to use the redshift redshift in place of time, because\n+the redshift is directily mensure in the light of galaxies moving away\n+from us.\n+\n+So, the general relation that we obtain is\n+\n+H = hubble_constant*(radiation_density*(redshift+1)**4\n+                     + matter_density*(redshift+1)**3\n+                     + curvature*(redshift+1)**2 + dark_energy)**(1/2)\n+\n+where radiation_density, matter_density, dark_energy are the relativity\n+(the percentage) energy densities that exist\n+in the Universe today. Here, matter_density is the\n+sum of the barion density and the\n+dark matter. Curvature is the curvature parameter and can be written in term\n+of the densities by the completeness\n+\n+\n+curvature = 1 - (matter_density + radiation_density + dark_energy)\n+\n+Source :\n+https://www.sciencedirect.com/topics/mathematics/hubble-parameter\n+\"\"\"\n+\n+\n+def hubble_parameter(\nComment: Please provide return type hint for the function: `hubble_parameter`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "physics/hubble_parameter.py",
    "pr_number": 7806,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008320634,
    "comment_created_at": "2022-10-28T18:02:47Z"
  },
  {
    "code": "@@ -0,0 +1,77 @@\n+from sympy import diff\n+from sympy.abc import x\n+",
    "comment": "i've checked your code and it seems to be free of syntax errors. however, i did notice that you have used the eval() function to convert the string function to an algebraic expression. while this approach works fine for simple expressions, it can be dangerous and lead to security vulnerabilities if the input string is not properly sanitized. it is generally recommended to use more secure methods like the ast module to parse and evaluate user input.",
    "line_number": 3,
    "enriched": "File: arithmetic_analysis/bisection_method.py\nCode: @@ -0,0 +1,77 @@\n+from sympy import diff\n+from sympy.abc import x\n+\nComment: I've checked your code and it seems to be free of syntax errors. However, I did notice that you have used the `eval()` function to convert the string function to an algebraic expression. While this approach works fine for simple expressions, it can be dangerous and lead to security vulnerabilities if the input string is not properly sanitized. It is generally recommended to use more secure methods like the `ast` module to parse and evaluate user input.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "arithmetic_analysis/bisection_method.py",
    "pr_number": 8143,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1174815655,
    "comment_created_at": "2023-04-24T05:56:30Z"
  },
  {
    "code": "@@ -0,0 +1,106 @@\n+from typing import Optional\n+\n+class ListNode:\n+    def __init__(self, val=0):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: val",
    "line_number": 4,
    "enriched": "File: data_structures/linked_list/reverse_k_group.py\nCode: @@ -0,0 +1,106 @@\n+from typing import Optional\n+\n+class ListNode:\n+    def __init__(self, val=0):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `val`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/linked_list/reverse_k_group.py",
    "pr_number": 9323,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342169363,
    "comment_created_at": "2023-10-01T18:13:23Z"
  },
  {
    "code": "@@ -0,0 +1,67 @@\n+from hashlib import sha256\n+\n+\n+def updatehash(*args):",
    "comment": "please provide return type hint for the function: updatehash. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: args",
    "line_number": 4,
    "enriched": "File: blockchain/block_description.py\nCode: @@ -0,0 +1,67 @@\n+from hashlib import sha256\n+\n+\n+def updatehash(*args):\nComment: Please provide return type hint for the function: `updatehash`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `args`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "blockchain/block_description.py",
    "pr_number": 10245,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1353070164,
    "comment_created_at": "2023-10-10T17:58:20Z"
  },
  {
    "code": "@@ -0,0 +1,66 @@\n+\n+\"\"\"\n+    Problem Statement : https://www.spoj.com/problems/HISTOGRA/\n+\"\"\"\n+\n+\n+histogram = [3, 5, 11, 7, 5, 9]\n+max_area = 25\n+\n+histogram2 = [3, 5, 1, 7, 5, 9]\n+max_area2 = 15\n+\n+def max_rectangle_area_histogram(histogram):",
    "comment": "please provide return type hint for the function: max_rectangle_area_histogram. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: histogram",
    "line_number": 13,
    "enriched": "File: data_structures/stacks/largest_rectangle_in_histogram.py\nCode: @@ -0,0 +1,66 @@\n+\n+\"\"\"\n+    Problem Statement : https://www.spoj.com/problems/HISTOGRA/\n+\"\"\"\n+\n+\n+histogram = [3, 5, 11, 7, 5, 9]\n+max_area = 25\n+\n+histogram2 = [3, 5, 1, 7, 5, 9]\n+max_area2 = 15\n+\n+def max_rectangle_area_histogram(histogram):\nComment: Please provide return type hint for the function: `max_rectangle_area_histogram`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `histogram`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/stacks/largest_rectangle_in_histogram.py",
    "pr_number": 7225,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996293684,
    "comment_created_at": "2022-10-15T11:33:26Z"
  },
  {
    "code": "@@ -0,0 +1,51 @@\n+import os\n+import requests\n+from bs4 import BeautifulSoup\n+\n+# Read the input file\n+url_list = list()\n+with open(\"url.txt\", \"r\") as f:\n+    url_list = f.read().split(\"\\n\")\n+    f.close()\n+\n+\n+# Create a directory to save the extracted articles\n+output_dir = \"extracted_articles\"\n+if not os.path.exists(output_dir):\n+    os.makedirs(output_dir)\n+\n+\n+# Function to extract article text\n+def extract_article_text(url):",
    "comment": "please provide return type hint for the function: extract_article_text. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: url",
    "line_number": 19,
    "enriched": "File: web_programming/webscraping.py\nCode: @@ -0,0 +1,51 @@\n+import os\n+import requests\n+from bs4 import BeautifulSoup\n+\n+# Read the input file\n+url_list = list()\n+with open(\"url.txt\", \"r\") as f:\n+    url_list = f.read().split(\"\\n\")\n+    f.close()\n+\n+\n+# Create a directory to save the extracted articles\n+output_dir = \"extracted_articles\"\n+if not os.path.exists(output_dir):\n+    os.makedirs(output_dir)\n+\n+\n+# Function to extract article text\n+def extract_article_text(url):\nComment: Please provide return type hint for the function: `extract_article_text`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `url`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "web_programming/webscraping.py",
    "pr_number": 10806,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367906211,
    "comment_created_at": "2023-10-22T13:26:44Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+def maxSubArraySum(arr, size):",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: maxsubarraysum\n\nplease provide return type hint for the function: maxsubarraysum. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: size",
    "line_number": 1,
    "enriched": "File: dynamic_programming/max_sub_array_sum.py\nCode: @@ -0,0 +1,42 @@\n+def maxSubArraySum(arr, size):\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `maxSubArraySum`\n\nPlease provide return type hint for the function: `maxSubArraySum`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `size`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/max_sub_array_sum.py",
    "pr_number": 8804,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1221988764,
    "comment_created_at": "2023-06-07T18:09:47Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+\"\"\"\n+This is a pure Python implementation for minimum waiting time problem using greedy\n+algorithm.\n+reference: https://www.youtube.com/watch?v=Sf3eiO12eJs\n+\n+For doctests run following command:\n+python -m doctest -v minimum_waiting_time.py\n+\n+The minimum_waiting_time function uses a greedy algorithm to calculate the minimum\n+time for queries to complete. It sorts the list in non-decreasing order, calculates\n+the waiting time for each query by multiplying its position in the list with the\n+sum of all remaining query times, and returns the total waiting time. A doctest\n+ensures that the function produces the correct output.\n+\"\"\"\n+\n+\n+def minimum_waiting_time(queries):",
    "comment": "please provide return type hint for the function: minimum_waiting_time. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: queries",
    "line_number": 17,
    "enriched": "File: greedy_methods/minimum_waiting_time.py\nCode: @@ -0,0 +1,48 @@\n+\"\"\"\n+This is a pure Python implementation for minimum waiting time problem using greedy\n+algorithm.\n+reference: https://www.youtube.com/watch?v=Sf3eiO12eJs\n+\n+For doctests run following command:\n+python -m doctest -v minimum_waiting_time.py\n+\n+The minimum_waiting_time function uses a greedy algorithm to calculate the minimum\n+time for queries to complete. It sorts the list in non-decreasing order, calculates\n+the waiting time for each query by multiplying its position in the list with the\n+sum of all remaining query times, and returns the total waiting time. A doctest\n+ensures that the function produces the correct output.\n+\"\"\"\n+\n+\n+def minimum_waiting_time(queries):\nComment: Please provide return type hint for the function: `minimum_waiting_time`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `queries`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "greedy_methods/minimum_waiting_time.py",
    "pr_number": 8701,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1181107349,
    "comment_created_at": "2023-04-29T17:41:23Z"
  },
  {
    "code": "@@ -13,6 +27,10 @@ def actual_power(a: int, b: int):\n \n def power(a: int, b: int) -> float:\n     \"\"\"\n+    :param a: The base (integer).\n+    :param b: The exponent (integer).\n+    :retuen: The result of a^b, as a float for negative exponents.",
    "comment": "fix typo",
    "line_number": 32,
    "enriched": "File: divide_and_conquer/power.py\nCode: @@ -13,6 +27,10 @@ def actual_power(a: int, b: int):\n \n def power(a: int, b: int) -> float:\n     \"\"\"\n+    :param a: The base (integer).\n+    :param b: The exponent (integer).\n+    :retuen: The result of a^b, as a float for negative exponents.\nComment: ```suggestion\r\n    :return: The result of a^b, as a float for negative exponents.\r\n```\r\nFix typo",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "divide_and_conquer/power.py",
    "pr_number": 11187,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1623185206,
    "comment_created_at": "2024-06-01T09:04:49Z"
  },
  {
    "code": "@@ -1,4 +1,4 @@\n-def bin_exp_mod(a, n, b):\n+def bin_exp_mod(a: int, n: float, b: int) -> int:",
    "comment": "n here is actually supposed to be an integer, not a float",
    "line_number": 1,
    "enriched": "File: maths/binary_exp_mod.py\nCode: @@ -1,4 +1,4 @@\n-def bin_exp_mod(a, n, b):\n+def bin_exp_mod(a: int, n: float, b: int) -> int:\nComment: ```suggestion\r\ndef bin_exp_mod(a: int, n: int, b: int) -> int:\r\n```\r\n`n` here is actually supposed to be an integer, not a float",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/binary_exp_mod.py",
    "pr_number": 9469,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342749445,
    "comment_created_at": "2023-10-02T14:13:50Z"
  },
  {
    "code": "@@ -0,0 +1,106 @@\n+\"\"\"\n+Doppler's effect\n+\n+The Doppler effect (also Doppler shift) is the change in the frequency of a wave\n+in relation to an observer who is moving relative to the source of the wave.\n+The Doppler effect is named after the physicist Christian Doppler.\n+A common example of Doppler shift is the change of pitch heard when\n+a vehicle sounding a horn approaches and recedes from an observer.\n+\n+The reason for the Doppler effect is that when the source of the waves\n+is moving towards the observer, each successive wave crest is emitted from a position\n+closer to the observer than the crest of the previous wave.\n+Therefore, each wave takes slightly less time to reach the observer\n+than the previous wave. Hence, the time between the arrivals of successive\n+wave crests at the observer is reduced, causing an increase in the frequency.\n+Similarly, if the source of waves is moving away from the observer,\n+each wave is emitted from a position farther from the observer than the previous wave,\n+so the arrival time between successive waves is increased, reducing the frequency.\n+\n+Now, if the source of waves is stationary but the observer is moving with respect\n+to the source, the transmission velocity of the waves changes\n+(ie the rate at which the observer receives waves) even if the wavelength\n+and frequency emitted from the source remain constant.\n+\n+All these results are summarized by the Doppler formula:\n+\n+    f = (f0 * (v + v0)) / (v - vs)\n+\n+where:\n+    f: frequency of the wave\n+    f0: frequency of the wave when the source is stationary\n+    v: velocity of the wave in the medium\n+    v0: velocity of the observer, positive if the observer is moving towards the source\n+    vs: velocity of the source, positive if the source is moving towards the observer\n+\n+Doppler's effect has many applications in physics and engineering,\n+such as radar, astronomy, medical imaging and seismology.\n+\n+References:\n+https://en.wikipedia.org/wiki/Doppler_effect\n+\n+Now, we will implement a function that calculates the frequency of a wave as a function\n+of the frequency of the wave when the source is stationary, the velocity of the wave\n+in the medium, the velocity of the observer and the velocity of the source.\n+\"\"\"\n+\n+\n+def doppler_effect(\n+    org_freq: float, wave_vel: float, obs_vel: float, src_vel: float\n+) -> float:\n+    \"\"\"\n+    Input Parameters:\n+    -----------------\n+    org_freq: frequency of the wave when the source is stationary\n+    wave_vel: velocity of the wave in the medium\n+    obs_vel: velocity of the observer, +ve if the observer is moving towards the source\n+    src_vel: velocity of the source, +ve if the source is moving towards the observer\n+\n+    Returns:\n+    --------\n+    f: frequency of the wave as perceived by the observer\n+\n+    Docstring Tests:\n+    >>> doppler_effect(100, 330, 10, 0) #observer moving towards the source\n+    103.03030303030303\n+    >>> doppler_effect(100, 330, -10, 0) #observer moving away from the source\n+    96.96969696969697\n+    >>> doppler_effect(100, 330, 0, 10) #source moving towards the observer\n+    103.125\n+    >>> doppler_effect(100, 330, 0, -10) #source moving away from the observer\n+    97.05882352941177\n+    >>> doppler_effect(100, 330, 10, 10) #observer and source moving towards each other\n+    106.25\n+    >>> doppler_effect(100, 330, -10, -10) #observer and source moving away\n+    94.11764705882354\n+    >>> doppler_effect(100, 330, 10, 330) #source moving at the same speed as the wave\n+    Error: Division by zero\n+    Infinite frequency implies vs = v and observer infront of the source\n+    0.0\n+    >>> doppler_effect(100, 330, 10, 340) #source moving faster than the wave\n+    Error: Non-positive frequency\n+    Non-positive frequency implies vs > v or v0 > v(in opposite direction)\n+    0.0\n+    >>> doppler_effect(100, 330, -340, 10) #observer moving faster than the wave\n+    Error: Non-positive frequency\n+    Non-positive frequency implies vs > v or v0 > v(in opposite direction)\n+    0.0\n+    \"\"\"\n+\n+    try:\n+        doppler_freq = (org_freq * (wave_vel + obs_vel)) / (wave_vel - src_vel)\n+    except ZeroDivisionError:\n+        print(\"Error: Division by zero\")\n+        print(\"Infinite frequency implies vs = v and observer infront of the source\")\n+        return 0.0",
    "comment": "contributing.md says that algorithmic functions should not print().  please let the zerodivisionerror be raised to the caller because the answer is not 0.",
    "line_number": 95,
    "enriched": "File: physics/doppler_frequency.py\nCode: @@ -0,0 +1,106 @@\n+\"\"\"\n+Doppler's effect\n+\n+The Doppler effect (also Doppler shift) is the change in the frequency of a wave\n+in relation to an observer who is moving relative to the source of the wave.\n+The Doppler effect is named after the physicist Christian Doppler.\n+A common example of Doppler shift is the change of pitch heard when\n+a vehicle sounding a horn approaches and recedes from an observer.\n+\n+The reason for the Doppler effect is that when the source of the waves\n+is moving towards the observer, each successive wave crest is emitted from a position\n+closer to the observer than the crest of the previous wave.\n+Therefore, each wave takes slightly less time to reach the observer\n+than the previous wave. Hence, the time between the arrivals of successive\n+wave crests at the observer is reduced, causing an increase in the frequency.\n+Similarly, if the source of waves is moving away from the observer,\n+each wave is emitted from a position farther from the observer than the previous wave,\n+so the arrival time between successive waves is increased, reducing the frequency.\n+\n+Now, if the source of waves is stationary but the observer is moving with respect\n+to the source, the transmission velocity of the waves changes\n+(ie the rate at which the observer receives waves) even if the wavelength\n+and frequency emitted from the source remain constant.\n+\n+All these results are summarized by the Doppler formula:\n+\n+    f = (f0 * (v + v0)) / (v - vs)\n+\n+where:\n+    f: frequency of the wave\n+    f0: frequency of the wave when the source is stationary\n+    v: velocity of the wave in the medium\n+    v0: velocity of the observer, positive if the observer is moving towards the source\n+    vs: velocity of the source, positive if the source is moving towards the observer\n+\n+Doppler's effect has many applications in physics and engineering,\n+such as radar, astronomy, medical imaging and seismology.\n+\n+References:\n+https://en.wikipedia.org/wiki/Doppler_effect\n+\n+Now, we will implement a function that calculates the frequency of a wave as a function\n+of the frequency of the wave when the source is stationary, the velocity of the wave\n+in the medium, the velocity of the observer and the velocity of the source.\n+\"\"\"\n+\n+\n+def doppler_effect(\n+    org_freq: float, wave_vel: float, obs_vel: float, src_vel: float\n+) -> float:\n+    \"\"\"\n+    Input Parameters:\n+    -----------------\n+    org_freq: frequency of the wave when the source is stationary\n+    wave_vel: velocity of the wave in the medium\n+    obs_vel: velocity of the observer, +ve if the observer is moving towards the source\n+    src_vel: velocity of the source, +ve if the source is moving towards the observer\n+\n+    Returns:\n+    --------\n+    f: frequency of the wave as perceived by the observer\n+\n+    Docstring Tests:\n+    >>> doppler_effect(100, 330, 10, 0) #observer moving towards the source\n+    103.03030303030303\n+    >>> doppler_effect(100, 330, -10, 0) #observer moving away from the source\n+    96.96969696969697\n+    >>> doppler_effect(100, 330, 0, 10) #source moving towards the observer\n+    103.125\n+    >>> doppler_effect(100, 330, 0, -10) #source moving away from the observer\n+    97.05882352941177\n+    >>> doppler_effect(100, 330, 10, 10) #observer and source moving towards each other\n+    106.25\n+    >>> doppler_effect(100, 330, -10, -10) #observer and source moving away\n+    94.11764705882354\n+    >>> doppler_effect(100, 330, 10, 330) #source moving at the same speed as the wave\n+    Error: Division by zero\n+    Infinite frequency implies vs = v and observer infront of the source\n+    0.0\n+    >>> doppler_effect(100, 330, 10, 340) #source moving faster than the wave\n+    Error: Non-positive frequency\n+    Non-positive frequency implies vs > v or v0 > v(in opposite direction)\n+    0.0\n+    >>> doppler_effect(100, 330, -340, 10) #observer moving faster than the wave\n+    Error: Non-positive frequency\n+    Non-positive frequency implies vs > v or v0 > v(in opposite direction)\n+    0.0\n+    \"\"\"\n+\n+    try:\n+        doppler_freq = (org_freq * (wave_vel + obs_vel)) / (wave_vel - src_vel)\n+    except ZeroDivisionError:\n+        print(\"Error: Division by zero\")\n+        print(\"Infinite frequency implies vs = v and observer infront of the source\")\n+        return 0.0\nComment: `CONTRIBUTING.md` says that algorithmic functions should not `print()`.  Please let the ZeroDivisionError be raised to the caller because the answer is NOT 0.\r\n```suggestion\r\n        print(\"Error: Division by zero\")\r\n        print(\"Infinite frequency implies vs = v and observer infront of the source\")\r\n        return 0.0\r\n```",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "physics/doppler_frequency.py",
    "pr_number": 10776,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367747924,
    "comment_created_at": "2023-10-21T15:33:49Z"
  },
  {
    "code": "@@ -0,0 +1,51 @@\n+import heapq\n+\n+def dijkstra(graph, start):",
    "comment": "please provide return type hint for the function: dijkstra. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: graph\n\nplease provide type hint for the parameter: start",
    "line_number": 3,
    "enriched": "File: greedy_methods/dijkstra_algorithm.py\nCode: @@ -0,0 +1,51 @@\n+import heapq\n+\n+def dijkstra(graph, start):\nComment: Please provide return type hint for the function: `dijkstra`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `graph`\n\nPlease provide type hint for the parameter: `start`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "greedy_methods/dijkstra_algorithm.py",
    "pr_number": 9517,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342984457,
    "comment_created_at": "2023-10-02T17:49:53Z"
  },
  {
    "code": "@@ -0,0 +1,79 @@\n+\"\"\"\n+A Telegram bot that can remove chat that contains offensive words provided to look after from a  Group or Channel the bot is part of.\n+\n+Prerequisite:\n+1. This script needs two 3rd party packages:  pyTelegramBotAPI and python-dotenv\n+\n+2. This script needs a BOT_TOKEN that can be generated following the simple steps provided in the official guide:\n+https://core.telegram.org/bots/features#creating-a-new-bot\n+\n+\n+Testing Steps:\n+1. Create the BOT using the above official doc\n+2. Add the BOT to a channel or Group\n+3. Install the required packages and run this Python file\n+4. Send any Offensive text that you have set to filter out in the group/channel and see the response\n+\n+\n+Author: Suman Mitra\n+https://youtube.com/@LetsCodeTogether\n+https://github.com/suman2023\n+\"\"\"\n+import os\n+\n+import telebot\n+from dotenv import load_dotenv\n+\n+\"\"\"\n+load_dotenv()\n+\n+Parse a .env file and then load all the variables found as environment variables.\n+\n+Add the BOT_TOKEN variable in the .env file\n+eg. BOT_TOKEN=<the token received from Bot Father>\n+\"\"\"\n+load_dotenv()\n+\n+\n+BOT_TOKEN = os.getenv(\"BOT_TOKEN\")\n+bot = telebot.TeleBot(BOT_TOKEN)\n+\n+# Add the words to be checked for in the set.\n+offensive_words = {\"stupid\", \"shit\", \"thug\"}\n+\n+\"\"\"\n+@bot.message_handler(func=lambda message: message != None)\n+\n+Handles New incoming message of any kind - text, photo, sticker, etc.\n+As a parameter to the decorator function, it passes :class:`telebot.types.Message` object.\n+Telegram Documentation: https://core.telegram.org/bots/api#message\n+\n+\n+bot.delete_message()\n+\n+Use this method to delete a message, including service messages.\n+- If the bot is an administrator of a group, it can delete any message there.\n+Returns True on success.\n+Telegram documentation: https://core.telegram.org/bots/api#deletemessage\n+\"\"\"\n+\n+\n+@bot.message_handler(func=lambda message: message != None)\n+def remove_offensive_chat(message):",
    "comment": "please provide return type hint for the function: remove_offensive_chat. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: message",
    "line_number": 62,
    "enriched": "File: web_programming/remove_offensive_chat_telegram_bot.py\nCode: @@ -0,0 +1,79 @@\n+\"\"\"\n+A Telegram bot that can remove chat that contains offensive words provided to look after from a  Group or Channel the bot is part of.\n+\n+Prerequisite:\n+1. This script needs two 3rd party packages:  pyTelegramBotAPI and python-dotenv\n+\n+2. This script needs a BOT_TOKEN that can be generated following the simple steps provided in the official guide:\n+https://core.telegram.org/bots/features#creating-a-new-bot\n+\n+\n+Testing Steps:\n+1. Create the BOT using the above official doc\n+2. Add the BOT to a channel or Group\n+3. Install the required packages and run this Python file\n+4. Send any Offensive text that you have set to filter out in the group/channel and see the response\n+\n+\n+Author: Suman Mitra\n+https://youtube.com/@LetsCodeTogether\n+https://github.com/suman2023\n+\"\"\"\n+import os\n+\n+import telebot\n+from dotenv import load_dotenv\n+\n+\"\"\"\n+load_dotenv()\n+\n+Parse a .env file and then load all the variables found as environment variables.\n+\n+Add the BOT_TOKEN variable in the .env file\n+eg. BOT_TOKEN=<the token received from Bot Father>\n+\"\"\"\n+load_dotenv()\n+\n+\n+BOT_TOKEN = os.getenv(\"BOT_TOKEN\")\n+bot = telebot.TeleBot(BOT_TOKEN)\n+\n+# Add the words to be checked for in the set.\n+offensive_words = {\"stupid\", \"shit\", \"thug\"}\n+\n+\"\"\"\n+@bot.message_handler(func=lambda message: message != None)\n+\n+Handles New incoming message of any kind - text, photo, sticker, etc.\n+As a parameter to the decorator function, it passes :class:`telebot.types.Message` object.\n+Telegram Documentation: https://core.telegram.org/bots/api#message\n+\n+\n+bot.delete_message()\n+\n+Use this method to delete a message, including service messages.\n+- If the bot is an administrator of a group, it can delete any message there.\n+Returns True on success.\n+Telegram documentation: https://core.telegram.org/bots/api#deletemessage\n+\"\"\"\n+\n+\n+@bot.message_handler(func=lambda message: message != None)\n+def remove_offensive_chat(message):\nComment: Please provide return type hint for the function: `remove_offensive_chat`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `message`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "web_programming/remove_offensive_chat_telegram_bot.py",
    "pr_number": 9876,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1348077986,
    "comment_created_at": "2023-10-05T22:49:00Z"
  },
  {
    "code": "@@ -0,0 +1,112 @@\n+# Author: Your Name\n+# Author email: your.email@example.com\n+# Coding date: Month Year\n+# Black: True\n+\n+\"\"\"\n+    * This code implements the xxHash algorithm:\n+        https://github.com/Cyan4973/xxHash\n+\n+    * xxHash is an extremely fast non-cryptographic hash algorithm that can be used\n+      to hash data for various applications like hash tables, checksums, and more.\n+\n+    * The implemented code consists of:\n+        * A function for calculating the xxHash hash value for given data.\n+\n+    * How to use:\n+        You can use the `xxhash` function to calculate the hash value for your data.\n+\n+    * Example:\n+        data = b'Hello, World!'\n+        seed = 0\n+        hash_value = xxhash(data, seed)\n+        print(f'Hash value: {hash_value}')\n+\n+\"\"\"\n+\n+# Constants\n+PRIME32_1 = 0x9E3779B1\n+PRIME32_2 = 0x85EBCA77\n+PRIME32_3 = 0xC2B2AE3D\n+PRIME32_4 = 0x27D4EB2F\n+PRIME32_5 = 0x165667B1\n+\n+\n+def xxhash(data, seed=0):",
    "comment": "please provide return type hint for the function: xxhash. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: data\n\nplease provide type hint for the parameter: seed",
    "line_number": 35,
    "enriched": "File: hashes/xxhash.py\nCode: @@ -0,0 +1,112 @@\n+# Author: Your Name\n+# Author email: your.email@example.com\n+# Coding date: Month Year\n+# Black: True\n+\n+\"\"\"\n+    * This code implements the xxHash algorithm:\n+        https://github.com/Cyan4973/xxHash\n+\n+    * xxHash is an extremely fast non-cryptographic hash algorithm that can be used\n+      to hash data for various applications like hash tables, checksums, and more.\n+\n+    * The implemented code consists of:\n+        * A function for calculating the xxHash hash value for given data.\n+\n+    * How to use:\n+        You can use the `xxhash` function to calculate the hash value for your data.\n+\n+    * Example:\n+        data = b'Hello, World!'\n+        seed = 0\n+        hash_value = xxhash(data, seed)\n+        print(f'Hash value: {hash_value}')\n+\n+\"\"\"\n+\n+# Constants\n+PRIME32_1 = 0x9E3779B1\n+PRIME32_2 = 0x85EBCA77\n+PRIME32_3 = 0xC2B2AE3D\n+PRIME32_4 = 0x27D4EB2F\n+PRIME32_5 = 0x165667B1\n+\n+\n+def xxhash(data, seed=0):\nComment: Please provide return type hint for the function: `xxhash`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `data`\n\nPlease provide type hint for the parameter: `seed`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "hashes/xxhash.py",
    "pr_number": 11123,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1382537973,
    "comment_created_at": "2023-11-05T09:20:26Z"
  },
  {
    "code": "@@ -0,0 +1,158 @@\n+\"\"\"\n+Implementation of gradient descent algorithm using momentum\n+for minimizing cost of a linear hypothesis function.\n+\"\"\"\n+\n+import numpy as np\n+\n+# List of input, output pairs\n+train_data = (\n+    ((5, 2, 3), 15),\n+    ((6, 5, 9), 25),\n+    ((11, 12, 13), 41),\n+    ((1, 1, 1), 8),\n+    ((11, 12, 13), 41),\n+)\n+test_data = (((515, 22, 13), 555), ((61, 35, 49), 150))\n+parameter_vector = [2, 4, 1, 5]\n+m = len(train_data)\n+LEARNING_RATE = 0.009\n+MOMENTUM = 0.9\n+\n+# Initialize velocity (for momentum)\n+velocity = [0] * len(parameter_vector)\n+\n+\n+def _error(example_no, data_set=\"train\"):",
    "comment": "please provide return type hint for the function: _error. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: example_no\n\nplease provide type hint for the parameter: data_set",
    "line_number": 26,
    "enriched": "File: machine_learning/gradient_descent_momentum.py\nCode: @@ -0,0 +1,158 @@\n+\"\"\"\n+Implementation of gradient descent algorithm using momentum\n+for minimizing cost of a linear hypothesis function.\n+\"\"\"\n+\n+import numpy as np\n+\n+# List of input, output pairs\n+train_data = (\n+    ((5, 2, 3), 15),\n+    ((6, 5, 9), 25),\n+    ((11, 12, 13), 41),\n+    ((1, 1, 1), 8),\n+    ((11, 12, 13), 41),\n+)\n+test_data = (((515, 22, 13), 555), ((61, 35, 49), 150))\n+parameter_vector = [2, 4, 1, 5]\n+m = len(train_data)\n+LEARNING_RATE = 0.009\n+MOMENTUM = 0.9\n+\n+# Initialize velocity (for momentum)\n+velocity = [0] * len(parameter_vector)\n+\n+\n+def _error(example_no, data_set=\"train\"):\nComment: Please provide return type hint for the function: `_error`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `example_no`\n\nPlease provide type hint for the parameter: `data_set`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/gradient_descent_momentum.py",
    "pr_number": 11884,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1791875802,
    "comment_created_at": "2024-10-08T13:26:16Z"
  },
  {
    "code": "@@ -0,0 +1,102 @@\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+\n+@dataclass\n+class Node:\n+    data: int\n+    next_node: Node | None = None\n+\n+def print_linked_list(head: Node | None) -> None:\n+    \"\"\"\n+    Print the entire linked list iteratively.\n+\n+    >>> head = insert_node(None, 0)\n+    >>> head = insert_node(head, 2)\n+    >>> head = insert_node(head, 1)\n+    >>> print_linked_list(head)\n+    0->2->1\n+    >>> head = insert_node(head, 4)\n+    >>> head = insert_node(head, 5)\n+    >>> print_linked_list(head)\n+    0->2->1->4->5\n+    \"\"\"\n+    if head is None:\n+        return\n+    while head.next_node is not None:\n+        print(head.data, end=\"->\")\n+        head = head.next_node\n+    print(head.data)\n+\n+def insert_node(head: Node | None, data: int) -> Node:\n+    \"\"\"\n+    Insert a new node at the end of a linked list\n+    and return the new head.\n+\n+    >>> head = insert_node(None, 10)\n+    >>> head = insert_node(head, 9)\n+    >>> head = insert_node(head, 8)\n+    >>> print_linked_list(head)\n+    10->9->8\n+    \"\"\"\n+    new_node = Node(data)\n+    if head is None:\n+        return new_node\n+\n+    temp_node = head\n+    while temp_node.next_node:\n+        temp_node = temp_node.next_node\n+    temp_node.next_node = new_node  \n+    return head\n+\n+def remove_duplicates(head:Node | None):",
    "comment": "please provide return type hint for the function: remove_duplicates. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 52,
    "enriched": "File: data_structures/linked_list/remove_duplicates.py\nCode: @@ -0,0 +1,102 @@\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+\n+@dataclass\n+class Node:\n+    data: int\n+    next_node: Node | None = None\n+\n+def print_linked_list(head: Node | None) -> None:\n+    \"\"\"\n+    Print the entire linked list iteratively.\n+\n+    >>> head = insert_node(None, 0)\n+    >>> head = insert_node(head, 2)\n+    >>> head = insert_node(head, 1)\n+    >>> print_linked_list(head)\n+    0->2->1\n+    >>> head = insert_node(head, 4)\n+    >>> head = insert_node(head, 5)\n+    >>> print_linked_list(head)\n+    0->2->1->4->5\n+    \"\"\"\n+    if head is None:\n+        return\n+    while head.next_node is not None:\n+        print(head.data, end=\"->\")\n+        head = head.next_node\n+    print(head.data)\n+\n+def insert_node(head: Node | None, data: int) -> Node:\n+    \"\"\"\n+    Insert a new node at the end of a linked list\n+    and return the new head.\n+\n+    >>> head = insert_node(None, 10)\n+    >>> head = insert_node(head, 9)\n+    >>> head = insert_node(head, 8)\n+    >>> print_linked_list(head)\n+    10->9->8\n+    \"\"\"\n+    new_node = Node(data)\n+    if head is None:\n+        return new_node\n+\n+    temp_node = head\n+    while temp_node.next_node:\n+        temp_node = temp_node.next_node\n+    temp_node.next_node = new_node  \n+    return head\n+\n+def remove_duplicates(head:Node | None):\nComment: Please provide return type hint for the function: `remove_duplicates`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/linked_list/remove_duplicates.py",
    "pr_number": 9395,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342330333,
    "comment_created_at": "2023-10-02T06:51:50Z"
  },
  {
    "code": "@@ -0,0 +1,163 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval=None):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: initval",
    "line_number": 21,
    "enriched": "File: sorts/tree_sort_2.py\nCode: @@ -0,0 +1,163 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval=None):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `initval`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "sorts/tree_sort_2.py",
    "pr_number": 7457,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000714984,
    "comment_created_at": "2022-10-20T14:29:17Z"
  },
  {
    "code": "@@ -0,0 +1,341 @@\n+\"\"\"\n+Binary Search Tree Implementation\n+\n+This implementation provides a binary search tree (BST) with basic operations\n+including insertion, search, deletion, and in-order traversal. Each operation\n+leverages recursive helper functions.\n+\n+Binary Search Tree Implementation with Doctest Examples\n+For more information on binary search trees, please see:\n+https://en.wikipedia.org/wiki/Binary_search_tree\n+\n+To run the doctests:\n+    python -m doctest -v binary_search_tree.py\n+\"\"\"\n+\n+\n+class BSTNode:\n+    \"\"\"\n+    A node in the binary search tree.\n+\n+    Attributes\n+    ----------\n+    key : int\n+        The key value stored in the node.\n+    left : BSTNode or None\n+        The left child node.\n+    right : BSTNode or None\n+        The right child node.\n+    \"\"\"\n+\n+    def __init__(self, key: int) -> None:\n+        \"\"\"\n+        Initializes a new BST node.\n+\n+        Parameters\n+        ----------\n+        key : int\n+            The key value for the new node.\n+        \"\"\"\n+        self.key = key\n+        self.left = None\n+        self.right = None\n+\n+\n+class BinarySearchTree:\n+    \"\"\"\n+    Binary Search Tree (BST) class that supports basic operations such as\n+    insertion, search, deletion, and in-order traversal.\n+\n+    For details on BSTs, see:\n+    https://en.wikipedia.org/wiki/Binary_search_tree\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        \"\"\"\n+        Initializes an empty Binary Search Tree.\n+        \"\"\"\n+        self.root = None\n+\n+    def insert(self, key: int) -> None:\n+        \"\"\"\n+        Inserts a new key into the BST.\n+\n+        Parameters\n+        ----------\n+        key : int\n+            The key to be inserted into the BST.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> bst.insert(10)\n+        >>> bst.insert(5)\n+        >>> bst.insert(15)\n+        >>> bst.inorder_traversal()\n+        [5, 10, 15]\n+        \"\"\"\n+        if self.root is None:\n+            self.root = BSTNode(key)\n+        else:\n+            self._insert_recursive(self.root, key)\n+\n+    def _insert_recursive(self, node: BSTNode, key: int) -> None:\n+        \"\"\"\n+        Recursively inserts a new key into the subtree rooted at the given node.\n+\n+        Parameters\n+        ----------\n+        node : BSTNode\n+            The current node in the BST.\n+        key : int\n+            The key to be inserted.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> bst.root = BSTNode(10)\n+        >>> bst._insert_recursive(bst.root, 5)\n+        >>> bst._insert_recursive(bst.root, 15)\n+        >>> bst.inorder_traversal()\n+        [5, 10, 15]\n+        \"\"\"\n+        if key < node.key:\n+            if node.left is None:\n+                node.left = BSTNode(key)\n+            else:\n+                self._insert_recursive(node.left, key)\n+        else:\n+            if node.right is None:\n+                node.right = BSTNode(key)\n+            else:\n+                self._insert_recursive(node.right, key)\n+\n+    def inorder_traversal(self) -> list:\n+        \"\"\"\n+        Performs an in-order traversal of the BST and returns the keys in sorted order.\n+\n+        Returns\n+        -------\n+        list\n+            A list of keys in increasing order.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> bst.inorder_traversal()  # For an empty BST.\n+        []\n+        >>> bst.insert(10)\n+        >>> bst.insert(5)\n+        >>> bst.insert(15)\n+        >>> bst.inorder_traversal()\n+        [5, 10, 15]\n+        >>> bst.insert(7)\n+        >>> bst.inorder_traversal()\n+        [5, 7, 10, 15]\n+        \"\"\"\n+        result = []\n+        self._inorder_recursive(self.root, result)\n+        return result\n+\n+    def _inorder_recursive(self, node: BSTNode, result: list) -> None:\n+        \"\"\"\n+        Helper function for recursively performing in-order traversal by\n+        accumulating the keys in the provided list.\n+\n+        Parameters\n+        ----------\n+        node : BSTNode or None\n+            The current node being visited.\n+        result : list\n+            The list to accumulate the keys.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> # Manually build a simple BST.\n+        >>> bst.root = BSTNode(20)\n+        >>> bst.root.left = BSTNode(10)\n+        >>> bst.root.right = BSTNode(30)\n+        >>> result = []\n+        >>> bst._inorder_recursive(bst.root, result)\n+        >>> result\n+        [10, 20, 30]\n+        >>> # If the subtree is empty, the result remains unchanged.\n+        >>> result = [1, 2, 3]\n+        >>> bst._inorder_recursive(None, result)\n+        >>> result\n+        [1, 2, 3]\n+        \"\"\"\n+        if node is not None:\n+            self._inorder_recursive(node.left, result)\n+            result.append(node.key)\n+            self._inorder_recursive(node.right, result)\n+\n+    def search(self, key: int):",
    "comment": "please provide return type hint for the function: search. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 175,
    "enriched": "File: tree/binary_search_tree.py\nCode: @@ -0,0 +1,341 @@\n+\"\"\"\n+Binary Search Tree Implementation\n+\n+This implementation provides a binary search tree (BST) with basic operations\n+including insertion, search, deletion, and in-order traversal. Each operation\n+leverages recursive helper functions.\n+\n+Binary Search Tree Implementation with Doctest Examples\n+For more information on binary search trees, please see:\n+https://en.wikipedia.org/wiki/Binary_search_tree\n+\n+To run the doctests:\n+    python -m doctest -v binary_search_tree.py\n+\"\"\"\n+\n+\n+class BSTNode:\n+    \"\"\"\n+    A node in the binary search tree.\n+\n+    Attributes\n+    ----------\n+    key : int\n+        The key value stored in the node.\n+    left : BSTNode or None\n+        The left child node.\n+    right : BSTNode or None\n+        The right child node.\n+    \"\"\"\n+\n+    def __init__(self, key: int) -> None:\n+        \"\"\"\n+        Initializes a new BST node.\n+\n+        Parameters\n+        ----------\n+        key : int\n+            The key value for the new node.\n+        \"\"\"\n+        self.key = key\n+        self.left = None\n+        self.right = None\n+\n+\n+class BinarySearchTree:\n+    \"\"\"\n+    Binary Search Tree (BST) class that supports basic operations such as\n+    insertion, search, deletion, and in-order traversal.\n+\n+    For details on BSTs, see:\n+    https://en.wikipedia.org/wiki/Binary_search_tree\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        \"\"\"\n+        Initializes an empty Binary Search Tree.\n+        \"\"\"\n+        self.root = None\n+\n+    def insert(self, key: int) -> None:\n+        \"\"\"\n+        Inserts a new key into the BST.\n+\n+        Parameters\n+        ----------\n+        key : int\n+            The key to be inserted into the BST.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> bst.insert(10)\n+        >>> bst.insert(5)\n+        >>> bst.insert(15)\n+        >>> bst.inorder_traversal()\n+        [5, 10, 15]\n+        \"\"\"\n+        if self.root is None:\n+            self.root = BSTNode(key)\n+        else:\n+            self._insert_recursive(self.root, key)\n+\n+    def _insert_recursive(self, node: BSTNode, key: int) -> None:\n+        \"\"\"\n+        Recursively inserts a new key into the subtree rooted at the given node.\n+\n+        Parameters\n+        ----------\n+        node : BSTNode\n+            The current node in the BST.\n+        key : int\n+            The key to be inserted.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> bst.root = BSTNode(10)\n+        >>> bst._insert_recursive(bst.root, 5)\n+        >>> bst._insert_recursive(bst.root, 15)\n+        >>> bst.inorder_traversal()\n+        [5, 10, 15]\n+        \"\"\"\n+        if key < node.key:\n+            if node.left is None:\n+                node.left = BSTNode(key)\n+            else:\n+                self._insert_recursive(node.left, key)\n+        else:\n+            if node.right is None:\n+                node.right = BSTNode(key)\n+            else:\n+                self._insert_recursive(node.right, key)\n+\n+    def inorder_traversal(self) -> list:\n+        \"\"\"\n+        Performs an in-order traversal of the BST and returns the keys in sorted order.\n+\n+        Returns\n+        -------\n+        list\n+            A list of keys in increasing order.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> bst.inorder_traversal()  # For an empty BST.\n+        []\n+        >>> bst.insert(10)\n+        >>> bst.insert(5)\n+        >>> bst.insert(15)\n+        >>> bst.inorder_traversal()\n+        [5, 10, 15]\n+        >>> bst.insert(7)\n+        >>> bst.inorder_traversal()\n+        [5, 7, 10, 15]\n+        \"\"\"\n+        result = []\n+        self._inorder_recursive(self.root, result)\n+        return result\n+\n+    def _inorder_recursive(self, node: BSTNode, result: list) -> None:\n+        \"\"\"\n+        Helper function for recursively performing in-order traversal by\n+        accumulating the keys in the provided list.\n+\n+        Parameters\n+        ----------\n+        node : BSTNode or None\n+            The current node being visited.\n+        result : list\n+            The list to accumulate the keys.\n+\n+        Examples\n+        --------\n+        >>> bst = BinarySearchTree()\n+        >>> # Manually build a simple BST.\n+        >>> bst.root = BSTNode(20)\n+        >>> bst.root.left = BSTNode(10)\n+        >>> bst.root.right = BSTNode(30)\n+        >>> result = []\n+        >>> bst._inorder_recursive(bst.root, result)\n+        >>> result\n+        [10, 20, 30]\n+        >>> # If the subtree is empty, the result remains unchanged.\n+        >>> result = [1, 2, 3]\n+        >>> bst._inorder_recursive(None, result)\n+        >>> result\n+        [1, 2, 3]\n+        \"\"\"\n+        if node is not None:\n+            self._inorder_recursive(node.left, result)\n+            result.append(node.key)\n+            self._inorder_recursive(node.right, result)\n+\n+    def search(self, key: int):\nComment: Please provide return type hint for the function: `search`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "tree/binary_search_tree.py",
    "pr_number": 12779,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2121759500,
    "comment_created_at": "2025-06-02T17:27:26Z"
  },
  {
    "code": "@@ -0,0 +1,145 @@\n+class Node():\n+    def __init__(self, start, end):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: start\n\nplease provide type hint for the parameter: end",
    "line_number": 2,
    "enriched": "File: data_structures/binary_tree/segment_tree_node.py\nCode: @@ -0,0 +1,145 @@\n+class Node():\n+    def __init__(self, start, end):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `start`\n\nPlease provide type hint for the parameter: `end`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/binary_tree/segment_tree_node.py",
    "pr_number": 12707,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2072877721,
    "comment_created_at": "2025-05-05T05:59:15Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+def find_median(nums):",
    "comment": "please provide return type hint for the function: find_median. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: nums",
    "line_number": 1,
    "enriched": "File: data_structures/arrays/median.py\nCode: @@ -0,0 +1,35 @@\n+def find_median(nums):\nComment: Please provide return type hint for the function: `find_median`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `nums`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/arrays/median.py",
    "pr_number": 11006,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1373553614,
    "comment_created_at": "2023-10-26T18:10:26Z"
  },
  {
    "code": "@@ -0,0 +1,32 @@\n+import requests\n+\n+# Function to get geolocation data for an IP address\n+def get_ip_geolocation(ip_address):",
    "comment": "please provide return type hint for the function: get_ip_geolocation. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: ip_address",
    "line_number": 4,
    "enriched": "File: web_programming/get_ip_geolocation.py\nCode: @@ -0,0 +1,32 @@\n+import requests\n+\n+# Function to get geolocation data for an IP address\n+def get_ip_geolocation(ip_address):\nComment: Please provide return type hint for the function: `get_ip_geolocation`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `ip_address`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "web_programming/get_ip_geolocation.py",
    "pr_number": 10902,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1370224543,
    "comment_created_at": "2023-10-24T14:01:01Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+\n+\"\"\"\n+\tEvery element represents the height of the block. The width of the block can be considered 1.\n+\tFind the volume of trapped rainwater between these block.\n+\"\"\"\n+\n+height = [34, 2, 5, 2, 3, 23, 18, 23, 45]\n+answer = 162\n+\n+def max_water(height):",
    "comment": "please provide return type hint for the function: max_water. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: height",
    "line_number": 10,
    "enriched": "File: data_structures/stacks/Trapping_Rain_Water.py\nCode: @@ -0,0 +1,53 @@\n+\n+\"\"\"\n+\tEvery element represents the height of the block. The width of the block can be considered 1.\n+\tFind the volume of trapped rainwater between these block.\n+\"\"\"\n+\n+height = [34, 2, 5, 2, 3, 23, 18, 23, 45]\n+answer = 162\n+\n+def max_water(height):\nComment: Please provide return type hint for the function: `max_water`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `height`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/stacks/Trapping_Rain_Water.py",
    "pr_number": 7223,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996288650,
    "comment_created_at": "2022-10-15T10:45:49Z"
  },
  {
    "code": "@@ -0,0 +1,126 @@\n+from __future__ import annotations\n+class Node:\n+    \"\"\"\n+prints the inorder Traversal of transformed tree\n+>>> sum = 0\n+>>> root = Node(11)\n+>>> root.left = Node(2)\n+>>> root.right = Node(29)\n+>>> root.left.left = Node(1)\n+>>> root.left.right = Node(7)\n+>>> root.right.left = Node(15)\n+>>> root.right.right = Node(40)\n+>>> root.right.right.left = Node(35)\n+>>> printInorder(root)\n+1 2 7 11 15 29 35 40 \n+\n+>>> transformTree(root)\n+\n+>>> printInorder(root)\n+139 137 130 119 104 75 40 0\n+\n+\"\"\"\n+\n+    def __init__(self, number:int) -> None:\n+        self.data = number\n+        self.left = None\n+        self.right = None\n+\n+# Recursive function to transform a BST to sum tree.\n+# This function traverses the tree in reverse inorder so\n+# that we have visited all greater key nodes of the currently\n+# visited node\n+def transform_tree_util(root:Node | None) -> None:\n+    \"\"\"\n+    Transform a binary tree into a sum tree.\n+    \n+    Example:\n+    >>> root = Node(11)\n+    >>> root.left = Node(2)\n+    >>> root.right = Node(29)\n+    >>> transformTree(root)\n+    >>> root.data\n+    60\n+    >>> root.left.data\n+    31\n+    >>> root.right.data\n+    29\n+    \"\"\"\n+\n+    # Base case\n+    if (root == None):\n+        return\n+\n+    # Recur for right subtree\n+    transform_tree_util(root.right)\n+\n+    # Update sum\n+    global sum\n+    sum = sum + root.data\n+\n+    # Store old sum in the current node\n+    root.data = sum - root.data\n+\n+    # Recur for left subtree\n+    transform_tree_util(root.left)\n+\n+# A wrapper over transformTreeUtil()\n+def transform_tree(root:Node | None) -> None:\n+    \"\"\"\n+    Transform a binary tree into a sum tree.\n+    \n+    Example:\n+    >>> root = Node(11)\n+    >>> root.left = Node(2)\n+    >>> root.right = Node(29)\n+    >>> transformTree(root)\n+    >>> root.data\n+    60\n+    >>> root.left.data\n+    31\n+    >>> root.right.data\n+    29\n+    \"\"\"\n+\n+    # sum = 0 #Initialize sum\n+    transform_tree_util(root)\n+\n+# A utility function to prindorder traversal of a\n+# binary tree\n+def print_inorder(root:Node | None):",
    "comment": "please provide return type hint for the function: print_inorder. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 90,
    "enriched": "File: data_structures/binary_tree/transform_bst_sum_tree.py\nCode: @@ -0,0 +1,126 @@\n+from __future__ import annotations\n+class Node:\n+    \"\"\"\n+prints the inorder Traversal of transformed tree\n+>>> sum = 0\n+>>> root = Node(11)\n+>>> root.left = Node(2)\n+>>> root.right = Node(29)\n+>>> root.left.left = Node(1)\n+>>> root.left.right = Node(7)\n+>>> root.right.left = Node(15)\n+>>> root.right.right = Node(40)\n+>>> root.right.right.left = Node(35)\n+>>> printInorder(root)\n+1 2 7 11 15 29 35 40 \n+\n+>>> transformTree(root)\n+\n+>>> printInorder(root)\n+139 137 130 119 104 75 40 0\n+\n+\"\"\"\n+\n+    def __init__(self, number:int) -> None:\n+        self.data = number\n+        self.left = None\n+        self.right = None\n+\n+# Recursive function to transform a BST to sum tree.\n+# This function traverses the tree in reverse inorder so\n+# that we have visited all greater key nodes of the currently\n+# visited node\n+def transform_tree_util(root:Node | None) -> None:\n+    \"\"\"\n+    Transform a binary tree into a sum tree.\n+    \n+    Example:\n+    >>> root = Node(11)\n+    >>> root.left = Node(2)\n+    >>> root.right = Node(29)\n+    >>> transformTree(root)\n+    >>> root.data\n+    60\n+    >>> root.left.data\n+    31\n+    >>> root.right.data\n+    29\n+    \"\"\"\n+\n+    # Base case\n+    if (root == None):\n+        return\n+\n+    # Recur for right subtree\n+    transform_tree_util(root.right)\n+\n+    # Update sum\n+    global sum\n+    sum = sum + root.data\n+\n+    # Store old sum in the current node\n+    root.data = sum - root.data\n+\n+    # Recur for left subtree\n+    transform_tree_util(root.left)\n+\n+# A wrapper over transformTreeUtil()\n+def transform_tree(root:Node | None) -> None:\n+    \"\"\"\n+    Transform a binary tree into a sum tree.\n+    \n+    Example:\n+    >>> root = Node(11)\n+    >>> root.left = Node(2)\n+    >>> root.right = Node(29)\n+    >>> transformTree(root)\n+    >>> root.data\n+    60\n+    >>> root.left.data\n+    31\n+    >>> root.right.data\n+    29\n+    \"\"\"\n+\n+    # sum = 0 #Initialize sum\n+    transform_tree_util(root)\n+\n+# A utility function to prindorder traversal of a\n+# binary tree\n+def print_inorder(root:Node | None):\nComment: Please provide return type hint for the function: `print_inorder`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/binary_tree/transform_bst_sum_tree.py",
    "pr_number": 9777,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346811388,
    "comment_created_at": "2023-10-05T05:39:04Z"
  },
  {
    "code": "@@ -0,0 +1,145 @@\n+# Python3 program to create target string, starting from\n+# random string using Genetic Algorithm\n+\n+import random\n+\n+# Number of individuals in each generation\n+POPULATION_SIZE = 100\n+\n+# Valid genes\n+GENES = '''abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOP\n+QRSTUVWXYZ 1234567890, .-;:_!\"#%&/()=?@${[]}'''\n+\n+# Target string to be generated\n+TARGET = \"I love GeeksforGeeks\"\n+\n+class Individual(object):\n+\t'''\n+\tClass representing individual in population\n+\t'''\n+\tdef __init__(self, chromosome):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: chromosome",
    "line_number": 20,
    "enriched": "File: genetic_algorithm/approach2.py\nCode: @@ -0,0 +1,145 @@\n+# Python3 program to create target string, starting from\n+# random string using Genetic Algorithm\n+\n+import random\n+\n+# Number of individuals in each generation\n+POPULATION_SIZE = 100\n+\n+# Valid genes\n+GENES = '''abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOP\n+QRSTUVWXYZ 1234567890, .-;:_!\"#%&/()=?@${[]}'''\n+\n+# Target string to be generated\n+TARGET = \"I love GeeksforGeeks\"\n+\n+class Individual(object):\n+\t'''\n+\tClass representing individual in population\n+\t'''\n+\tdef __init__(self, chromosome):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `chromosome`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "genetic_algorithm/approach2.py",
    "pr_number": 9624,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1344476187,
    "comment_created_at": "2023-10-03T17:26:49Z"
  },
  {
    "code": "@@ -0,0 +1,64 @@\n+import math\n+\n+from geometry.shapes.shape_types.closed_shapes import ClosedShape\n+\n+\n+class Circle(ClosedShape):\n+\n+    \"\"\"\n+    a structure which represents a\n+    geometrical circle on a 2D surface\n+\n+    >>> circle_one = Circle(5)\n+    >>> circle_one.get_diameter()\n+    10\n+    >>> circle_one.perimeter()\n+    31.41592653589793\n+    >>> circle_one.is_similar(None)\n+    Traceback (most recent call last):\n+    NotImplementedError: Not Implemented\n+    >>> circle_one.split()\n+    Traceback (most recent call last):\n+    NotImplementedError: Not Implemented\n+    >>> circle_one.max_parts(54)\n+    1486.0\n+    >>> circle_one.max_parts(7)\n+    29.0\n+    >>> circle_one.max_parts(22.5)\n+    265.375\n+    >>> circle_one.max_parts(-222)\n+    -1\n+    >>> circle_one.max_parts(\"-222\")\n+    Traceback (most recent call last):\n+    TypeError: num_cuts must be a numeric value.\n+    \"\"\"\n+\n+    def __init__(self, radius):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: radius",
    "line_number": 36,
    "enriched": "File: geometry/shapes/ellipses/circle.py\nCode: @@ -0,0 +1,64 @@\n+import math\n+\n+from geometry.shapes.shape_types.closed_shapes import ClosedShape\n+\n+\n+class Circle(ClosedShape):\n+\n+    \"\"\"\n+    a structure which represents a\n+    geometrical circle on a 2D surface\n+\n+    >>> circle_one = Circle(5)\n+    >>> circle_one.get_diameter()\n+    10\n+    >>> circle_one.perimeter()\n+    31.41592653589793\n+    >>> circle_one.is_similar(None)\n+    Traceback (most recent call last):\n+    NotImplementedError: Not Implemented\n+    >>> circle_one.split()\n+    Traceback (most recent call last):\n+    NotImplementedError: Not Implemented\n+    >>> circle_one.max_parts(54)\n+    1486.0\n+    >>> circle_one.max_parts(7)\n+    29.0\n+    >>> circle_one.max_parts(22.5)\n+    265.375\n+    >>> circle_one.max_parts(-222)\n+    -1\n+    >>> circle_one.max_parts(\"-222\")\n+    Traceback (most recent call last):\n+    TypeError: num_cuts must be a numeric value.\n+    \"\"\"\n+\n+    def __init__(self, radius):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `radius`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "geometry/shapes/ellipses/circle.py",
    "pr_number": 11138,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1389545836,
    "comment_created_at": "2023-11-10T15:29:14Z"
  },
  {
    "code": "@@ -0,0 +1,123 @@\n+import numpy as np\n+\n+def parse_function(user_input):",
    "comment": "please provide return type hint for the function: parse_function. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: user_input",
    "line_number": 3,
    "enriched": "File: genetic_algorithm/function_optimization.py\nCode: @@ -0,0 +1,123 @@\n+import numpy as np\n+\n+def parse_function(user_input):\nComment: Please provide return type hint for the function: `parse_function`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `user_input`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "genetic_algorithm/function_optimization.py",
    "pr_number": 11611,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1782531054,
    "comment_created_at": "2024-10-01T10:31:31Z"
  },
  {
    "code": "@@ -10,6 +10,24 @@\n \n \n def check_bipartite(graph):\n+    \"\"\"\n+    >>> check_bipartite(\n+    ... {0: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2]}\n+    ... )\n+    True\n+    >>> check_bipartite(\n+    ... {0: [1, 2, 3], 1: [0, 2], 2: [0, 1, 3], 3: [0, 2]}\n+    ... )\n+    False\n+    >>> check_bipartite(\n+    ... {0: [4], 1: [], 2: [4], 3: [4], 4: [0, 2, 3]}\n+    ... )\n+    True\n+    >>> check_bipartite(\n+    ... {0: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2], 4: [0]}\n+    ... )\n+    False",
    "comment": "please add some negative values, floating point values, and some strings.\r\nalso,\r\n{}\r\n{7: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2], 4: [0]}\r\n{0: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2], 9: [0]}",
    "line_number": 29,
    "enriched": "File: graphs/check_bipartite_graph_bfs.py\nCode: @@ -10,6 +10,24 @@\n \n \n def check_bipartite(graph):\n+    \"\"\"\n+    >>> check_bipartite(\n+    ... {0: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2]}\n+    ... )\n+    True\n+    >>> check_bipartite(\n+    ... {0: [1, 2, 3], 1: [0, 2], 2: [0, 1, 3], 3: [0, 2]}\n+    ... )\n+    False\n+    >>> check_bipartite(\n+    ... {0: [4], 1: [], 2: [4], 3: [4], 4: [0, 2, 3]}\n+    ... )\n+    True\n+    >>> check_bipartite(\n+    ... {0: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2], 4: [0]}\n+    ... )\n+    False\nComment: Please add some negative values, floating point values, and some strings.\r\nAlso,\r\n`{}`\r\n`{7: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2], 4: [0]}`\r\n`{0: [1, 3], 1: [0, 2], 2: [1, 3], 3: [0, 2], 9: [0]}`\r\n",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "graphs/check_bipartite_graph_bfs.py",
    "pr_number": 10688,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1365673281,
    "comment_created_at": "2023-10-19T14:45:05Z"
  },
  {
    "code": "@@ -0,0 +1,47 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Instalation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interval_start: The start of your internal scale.\n+            - interval_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Exemples:\n+    interval_start:0\n+    interval_end:100\n+    number:50\n+    output: 50.0\n+    interval_start:6\n+    interval_end:12\n+    number:9\n+    output:50.0\n+\n+    Link: https://en.wikipedia.org/wiki/Conversion_of_units\n+\"\"\"\n+\n+\n+def absolute_conversion(interval_start: float, interval_end: float, number: float):",
    "comment": "please provide return type hint for the function: absolute_conversion. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 26,
    "enriched": "File: conversions/absolute_conversion.py\nCode: @@ -0,0 +1,47 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Instalation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interval_start: The start of your internal scale.\n+            - interval_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Exemples:\n+    interval_start:0\n+    interval_end:100\n+    number:50\n+    output: 50.0\n+    interval_start:6\n+    interval_end:12\n+    number:9\n+    output:50.0\n+\n+    Link: https://en.wikipedia.org/wiki/Conversion_of_units\n+\"\"\"\n+\n+\n+def absolute_conversion(interval_start: float, interval_end: float, number: float):\nComment: Please provide return type hint for the function: `absolute_conversion`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "conversions/absolute_conversion.py",
    "pr_number": 7253,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996340231,
    "comment_created_at": "2022-10-15T18:33:59Z"
  },
  {
    "code": "@@ -53,6 +53,40 @@ def volume_of_gas_system(moles: float, kelvin: float, pressure: float) -> float:\n     return moles * kelvin * UNIVERSAL_GAS_CONSTANT / pressure\n \n \n+def temperature_of_gas_system(moles: float, volume: float, pressure: float) -> float:\n+    \"\"\"\n+    >>> temperature_of_gas_system(2, 100, 5)\n+    30.068090996146232\n+    >>> temperature_of_gas_system(11,5009,1000)\n+    54767.66101807144\n+    >>> temperature_of_gas_system(3, -0.46, 23.5)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Invalid inputs. Enter positive value.\n+    \"\"\"\n+    if moles < 0 or volume < 0 or pressure < 0:\n+        raise ValueError(\"Invalid inputs. Enter positive value.\")\n+\n+    return (pressure * volume) / (moles * UNIVERSAL_GAS_CONSTANT)\n+\n+\n+def num_moles_of_gas_in_system(kelvin: float, volume: float, pressure: float) -> float:",
    "comment": "more consistent name?",
    "line_number": 73,
    "enriched": "File: physics/ideal_gas_law.py\nCode: @@ -53,6 +53,40 @@ def volume_of_gas_system(moles: float, kelvin: float, pressure: float) -> float:\n     return moles * kelvin * UNIVERSAL_GAS_CONSTANT / pressure\n \n \n+def temperature_of_gas_system(moles: float, volume: float, pressure: float) -> float:\n+    \"\"\"\n+    >>> temperature_of_gas_system(2, 100, 5)\n+    30.068090996146232\n+    >>> temperature_of_gas_system(11,5009,1000)\n+    54767.66101807144\n+    >>> temperature_of_gas_system(3, -0.46, 23.5)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Invalid inputs. Enter positive value.\n+    \"\"\"\n+    if moles < 0 or volume < 0 or pressure < 0:\n+        raise ValueError(\"Invalid inputs. Enter positive value.\")\n+\n+    return (pressure * volume) / (moles * UNIVERSAL_GAS_CONSTANT)\n+\n+\n+def num_moles_of_gas_in_system(kelvin: float, volume: float, pressure: float) -> float:\nComment: ```suggestion\r\ndef moles_of_gas_system(kelvin: float, volume: float, pressure: float) -> float:\r\n```\r\nMore consistent name?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "physics/ideal_gas_law.py",
    "pr_number": 8919,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1282687570,
    "comment_created_at": "2023-08-03T06:07:19Z"
  },
  {
    "code": "@@ -0,0 +1,175 @@\n+\"\"\"\n+Python program for the Fractionated Morse Cipher.\n+\n+The Fractionated Morse cipher first converts the plaintext to morse code,\n+then enciphers fixed-size blocks of morse code back to letters.\n+This procedure means plaintext letters are mixed into the ciphertext letters,\n+making it more secure than substitution ciphers.\n+\n+For more information visit - http://practicalcryptography.com/ciphers/fractionated-morse-cipher/\n+\n+\"\"\"\n+\n+\n+import string\n+\n+# Define Morse code dictionary\n+MORSE_CODE_DICT = {\n+    \"A\": \".-\",\n+    \"B\": \"-...\",\n+    \"C\": \"-.-.\",\n+    \"D\": \"-..\",\n+    \"E\": \".\",\n+    \"F\": \"..-.\",\n+    \"G\": \"--.\",\n+    \"H\": \"....\",\n+    \"I\": \"..\",\n+    \"J\": \".---\",\n+    \"K\": \"-.-\",\n+    \"L\": \".-..\",\n+    \"M\": \"--\",\n+    \"N\": \"-.\",\n+    \"O\": \"---\",\n+    \"P\": \".--.\",\n+    \"Q\": \"--.-\",\n+    \"R\": \".-.\",\n+    \"S\": \"...\",\n+    \"T\": \"-\",\n+    \"U\": \"..-\",\n+    \"V\": \"...-\",\n+    \"W\": \".--\",\n+    \"X\": \"-..-\",\n+    \"Y\": \"-.--\",\n+    \"Z\": \"--..\",\n+    \" \": \"\",\n+}\n+\n+# Define possible trigrams of Morse code\n+MORSE_COMBINATIONS = [\n+    \"...\",\n+    \"..-\",\n+    \"..x\",\n+    \".-.\",\n+    \".--\",\n+    \".-x\",\n+    \".x.\",\n+    \".x-\",\n+    \".xx\",\n+    \"-..\",\n+    \"-.-\",\n+    \"-.x\",\n+    \"--.\",\n+    \"---\",\n+    \"--x\",\n+    \"-x.\",\n+    \"-x-\",\n+    \"-xx\",\n+    \"x..\",\n+    \"x.-\",\n+    \"x.x\",\n+    \"x-.\",\n+    \"x--\",\n+    \"x-x\",\n+    \"xx.\",\n+    \"xx-\",\n+    \"xxx\",\n+]\n+\n+# Create a reverse dictionary for Morse code\n+REVERSE_DICT = {value: key for key, value in MORSE_CODE_DICT.items()}\n+\n+\n+def encode_to_morse(plaintext: str) -> str:\n+    \"\"\"Encode a plaintext message into Morse code.\n+\n+    Args:\n+        plaintext (str): The plaintext message to encode.\n+\n+    Returns:\n+        str: The Morse code representation of the plaintext message.\n+\n+    Example:\n+        >>> encode_to_morse(\"defend the east\")\n+        '-..x.x..-.x.x-.x-..xx-x....x.xx.x.-x...x-'\n+    \"\"\"\n+    return \"x\".join([MORSE_CODE_DICT.get(letter.upper(), \"\") for letter in plaintext])\n+\n+\n+def encrypt_fractionated_morse(plaintext: str, key: str) -> str:\n+    \"\"\"Encrypt a plaintext message using Fractionated Morse Cipher.\n+\n+    Args:\n+        plaintext (str): The plaintext message to encrypt.\n+        key (str): The encryption key.\n+\n+    Returns:\n+        str: The encrypted ciphertext.\n+\n+    Example:\n+        >>> encrypt_fractionated_morse(\"defend the east\",\"Roundtable\")\n+        'ESOAVVLJRSSTRX'\n+\n+    \"\"\"\n+    morse_code = encode_to_morse(plaintext)\n+    key = key.upper() + string.ascii_uppercase\n+    key = \"\".join(sorted(set(key), key=key.find))\n+\n+    # Ensure morse_code length is a multiple of 3\n+    padding_length = 3 - (len(morse_code) % 3)\n+    morse_code += \"x\" * padding_length\n+\n+    fractionated_morse_dict = {\n+        value: key for key, value in zip(key, MORSE_COMBINATIONS)\n+    }\n+    fractionated_morse_dict[\"xxx\"] = \"\"\n+    encrypted_text = \"\".join(\n+        [\n+            fractionated_morse_dict[morse_code[i : i + 3]]\n+            for i in range(0, len(morse_code), 3)\n+        ]\n+    )\n+    return encrypted_text\n+\n+\n+def decrypt_fractionated_morse(ciphertext: str, key: str) -> str:\n+    \"\"\"Decrypt a ciphertext message encrypted with Fractionated Morse Cipher.\n+\n+    Args:\n+        ciphertext (str): The ciphertext message to decrypt.\n+        key (str): The decryption key.\n+\n+    Returns:\n+        str: The decrypted plaintext message.\n+\n+    Example:\n+        >>> decrypt_fractionated_morse(\"ESOAVVLJRSSTRX\",\"Roundtable\")\n+        'DEFEND THE EAST'\n+    \"\"\"\n+    key = key.upper() + string.ascii_uppercase\n+    key = \"\".join(sorted(set(key), key=key.find))\n+\n+    inverse_fractionated_morse_dict = dict(zip(key, MORSE_COMBINATIONS))\n+    morse_code = \"\".join(\n+        [inverse_fractionated_morse_dict.get(letter, \"\") for letter in ciphertext]\n+    )\n+    decrypted_text = \"\".join(\n+        [REVERSE_DICT[code] for code in morse_code.split(\"x\")]\n+    ).strip()\n+    return decrypted_text\n+\n+\n+def main():",
    "comment": "as there is no test file in this pull request nor any test function or class in the file ciphers/fractionated_morse_cipher.py, please provide doctest for the function main\n\nplease provide return type hint for the function: main. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 161,
    "enriched": "File: ciphers/fractionated_morse_cipher.py\nCode: @@ -0,0 +1,175 @@\n+\"\"\"\n+Python program for the Fractionated Morse Cipher.\n+\n+The Fractionated Morse cipher first converts the plaintext to morse code,\n+then enciphers fixed-size blocks of morse code back to letters.\n+This procedure means plaintext letters are mixed into the ciphertext letters,\n+making it more secure than substitution ciphers.\n+\n+For more information visit - http://practicalcryptography.com/ciphers/fractionated-morse-cipher/\n+\n+\"\"\"\n+\n+\n+import string\n+\n+# Define Morse code dictionary\n+MORSE_CODE_DICT = {\n+    \"A\": \".-\",\n+    \"B\": \"-...\",\n+    \"C\": \"-.-.\",\n+    \"D\": \"-..\",\n+    \"E\": \".\",\n+    \"F\": \"..-.\",\n+    \"G\": \"--.\",\n+    \"H\": \"....\",\n+    \"I\": \"..\",\n+    \"J\": \".---\",\n+    \"K\": \"-.-\",\n+    \"L\": \".-..\",\n+    \"M\": \"--\",\n+    \"N\": \"-.\",\n+    \"O\": \"---\",\n+    \"P\": \".--.\",\n+    \"Q\": \"--.-\",\n+    \"R\": \".-.\",\n+    \"S\": \"...\",\n+    \"T\": \"-\",\n+    \"U\": \"..-\",\n+    \"V\": \"...-\",\n+    \"W\": \".--\",\n+    \"X\": \"-..-\",\n+    \"Y\": \"-.--\",\n+    \"Z\": \"--..\",\n+    \" \": \"\",\n+}\n+\n+# Define possible trigrams of Morse code\n+MORSE_COMBINATIONS = [\n+    \"...\",\n+    \"..-\",\n+    \"..x\",\n+    \".-.\",\n+    \".--\",\n+    \".-x\",\n+    \".x.\",\n+    \".x-\",\n+    \".xx\",\n+    \"-..\",\n+    \"-.-\",\n+    \"-.x\",\n+    \"--.\",\n+    \"---\",\n+    \"--x\",\n+    \"-x.\",\n+    \"-x-\",\n+    \"-xx\",\n+    \"x..\",\n+    \"x.-\",\n+    \"x.x\",\n+    \"x-.\",\n+    \"x--\",\n+    \"x-x\",\n+    \"xx.\",\n+    \"xx-\",\n+    \"xxx\",\n+]\n+\n+# Create a reverse dictionary for Morse code\n+REVERSE_DICT = {value: key for key, value in MORSE_CODE_DICT.items()}\n+\n+\n+def encode_to_morse(plaintext: str) -> str:\n+    \"\"\"Encode a plaintext message into Morse code.\n+\n+    Args:\n+        plaintext (str): The plaintext message to encode.\n+\n+    Returns:\n+        str: The Morse code representation of the plaintext message.\n+\n+    Example:\n+        >>> encode_to_morse(\"defend the east\")\n+        '-..x.x..-.x.x-.x-..xx-x....x.xx.x.-x...x-'\n+    \"\"\"\n+    return \"x\".join([MORSE_CODE_DICT.get(letter.upper(), \"\") for letter in plaintext])\n+\n+\n+def encrypt_fractionated_morse(plaintext: str, key: str) -> str:\n+    \"\"\"Encrypt a plaintext message using Fractionated Morse Cipher.\n+\n+    Args:\n+        plaintext (str): The plaintext message to encrypt.\n+        key (str): The encryption key.\n+\n+    Returns:\n+        str: The encrypted ciphertext.\n+\n+    Example:\n+        >>> encrypt_fractionated_morse(\"defend the east\",\"Roundtable\")\n+        'ESOAVVLJRSSTRX'\n+\n+    \"\"\"\n+    morse_code = encode_to_morse(plaintext)\n+    key = key.upper() + string.ascii_uppercase\n+    key = \"\".join(sorted(set(key), key=key.find))\n+\n+    # Ensure morse_code length is a multiple of 3\n+    padding_length = 3 - (len(morse_code) % 3)\n+    morse_code += \"x\" * padding_length\n+\n+    fractionated_morse_dict = {\n+        value: key for key, value in zip(key, MORSE_COMBINATIONS)\n+    }\n+    fractionated_morse_dict[\"xxx\"] = \"\"\n+    encrypted_text = \"\".join(\n+        [\n+            fractionated_morse_dict[morse_code[i : i + 3]]\n+            for i in range(0, len(morse_code), 3)\n+        ]\n+    )\n+    return encrypted_text\n+\n+\n+def decrypt_fractionated_morse(ciphertext: str, key: str) -> str:\n+    \"\"\"Decrypt a ciphertext message encrypted with Fractionated Morse Cipher.\n+\n+    Args:\n+        ciphertext (str): The ciphertext message to decrypt.\n+        key (str): The decryption key.\n+\n+    Returns:\n+        str: The decrypted plaintext message.\n+\n+    Example:\n+        >>> decrypt_fractionated_morse(\"ESOAVVLJRSSTRX\",\"Roundtable\")\n+        'DEFEND THE EAST'\n+    \"\"\"\n+    key = key.upper() + string.ascii_uppercase\n+    key = \"\".join(sorted(set(key), key=key.find))\n+\n+    inverse_fractionated_morse_dict = dict(zip(key, MORSE_COMBINATIONS))\n+    morse_code = \"\".join(\n+        [inverse_fractionated_morse_dict.get(letter, \"\") for letter in ciphertext]\n+    )\n+    decrypted_text = \"\".join(\n+        [REVERSE_DICT[code] for code in morse_code.split(\"x\")]\n+    ).strip()\n+    return decrypted_text\n+\n+\n+def main():\nComment: As there is no test file in this pull request nor any test function or class in the file `ciphers/fractionated_morse_cipher.py`, please provide doctest for the function `main`\n\nPlease provide return type hint for the function: `main`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "ciphers/fractionated_morse_cipher.py",
    "pr_number": 9442,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342521030,
    "comment_created_at": "2023-10-02T10:27:46Z"
  },
  {
    "code": "@@ -0,0 +1,44 @@\n+\"\"\"\n+This script demonstrates the implementation of the tangent hyperbolic",
    "comment": "one minor suggestion i have is to add a brief description of what the function does at the beginning of the docstring. for example:\r\n\r\n\"\"\"\r\nimplements the tangent hyperbolic or tanh function.\r\n\r\nthe function takes a vector of k real numbers as input and then applies\r\nthe tanh function to each element of the vector. the output values are\r\nmostly in the range (-1, 1).\r\n...",
    "line_number": 2,
    "enriched": "File: maths/tanh.py\nCode: @@ -0,0 +1,44 @@\n+\"\"\"\n+This script demonstrates the implementation of the tangent hyperbolic\nComment: One minor suggestion I have is to add a brief description of what the function does at the beginning of the docstring. For example:\r\n\r\n\"\"\"\r\nImplements the tangent hyperbolic or tanh function.\r\n\r\nThe function takes a vector of K real numbers as input and then applies\r\nthe tanh function to each element of the vector. The output values are\r\nmostly in the range (-1, 1).\r\n...\r\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "maths/tanh.py",
    "pr_number": 8689,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1175673209,
    "comment_created_at": "2023-04-24T18:57:54Z"
  },
  {
    "code": "@@ -0,0 +1,134 @@\n+import random\n+def pick_pivot(l):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file arithmetic_analysis/mediana_best_algorithm.py, please provide doctest for the function pick_pivot\n\nplease provide return type hint for the function: pick_pivot. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: l\n\nplease provide descriptive name for the parameter: l",
    "line_number": 2,
    "enriched": "File: arithmetic_analysis/mediana_best_algorithm.py\nCode: @@ -0,0 +1,134 @@\n+import random\n+def pick_pivot(l):\nComment: As there is no test file in this pull request nor any test function or class in the file `arithmetic_analysis/mediana_best_algorithm.py`, please provide doctest for the function `pick_pivot`\n\nPlease provide return type hint for the function: `pick_pivot`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `l`\n\nPlease provide descriptive name for the parameter: `l`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "arithmetic_analysis/mediana_best_algorithm.py",
    "pr_number": 6969,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 991742757,
    "comment_created_at": "2022-10-11T01:38:19Z"
  },
  {
    "code": "@@ -0,0 +1,67 @@\n+\"\"\"\n+Title: Bernoulli's Principle Implementation\n+\n+Description:\n+This Python script implements Bernoulli's Principle, which describes the behavior of \n+a fluid under varying conditions of pressure, velocity, and height. Bernoulli's equation\n+is applied to an incompressible, frictionless fluid to calculate the unknown variable \n+(pressure, velocity, or height) when the others are known.\n+\n+Bernoulli's Equation:\n+P1 + 0.5 * \u03c1 * v1^2 + \u03c1 * g * h1 = P2 + 0.5 * \u03c1 * v2^2 + \u03c1 * g * h2\n+\n+Where:\n+- P1, P2 are pressures at points 1 and 2 (in Pascals),\n+- v1, v2 are velocities at points 1 and 2 (in m/s),\n+- h1, h2 are heights at points 1 and 2 (in meters),\n+- \u03c1 is the fluid density (in kg/m\u00b3, default is 1000 for water),\n+- g is the acceleration due to gravity (default is 9.81 m/s\u00b2).\n+\n+The function `bernoullis_principle` calculates one unknown variable based on inputs and returns the result.\n+\"\"\"\n+\n+def bernoullis_principle(P1, v1, h1, P2=None, v2=None, h2=None, density=1000, g=9.81):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file physics/bernoullis_principle.py, please provide doctest for the function bernoullis_principle\n\nplease provide return type hint for the function: bernoullis_principle. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: p1\n\nplease provide type hint for the parameter: v1\n\nplease provide type hint for the parameter: h1\n\nplease provide type hint for the parameter: p2\n\nplease provide type hint for the parameter: v2\n\nplease provide type hint for the parameter: h2\n\nplease provide type hint for the parameter: density\n\nplease provide descriptive name for the parameter: g\n\nplease provide type hint for the parameter: g",
    "line_number": 23,
    "enriched": "File: physics/bernoullis_principle.py\nCode: @@ -0,0 +1,67 @@\n+\"\"\"\n+Title: Bernoulli's Principle Implementation\n+\n+Description:\n+This Python script implements Bernoulli's Principle, which describes the behavior of \n+a fluid under varying conditions of pressure, velocity, and height. Bernoulli's equation\n+is applied to an incompressible, frictionless fluid to calculate the unknown variable \n+(pressure, velocity, or height) when the others are known.\n+\n+Bernoulli's Equation:\n+P1 + 0.5 * \u03c1 * v1^2 + \u03c1 * g * h1 = P2 + 0.5 * \u03c1 * v2^2 + \u03c1 * g * h2\n+\n+Where:\n+- P1, P2 are pressures at points 1 and 2 (in Pascals),\n+- v1, v2 are velocities at points 1 and 2 (in m/s),\n+- h1, h2 are heights at points 1 and 2 (in meters),\n+- \u03c1 is the fluid density (in kg/m\u00b3, default is 1000 for water),\n+- g is the acceleration due to gravity (default is 9.81 m/s\u00b2).\n+\n+The function `bernoullis_principle` calculates one unknown variable based on inputs and returns the result.\n+\"\"\"\n+\n+def bernoullis_principle(P1, v1, h1, P2=None, v2=None, h2=None, density=1000, g=9.81):\nComment: As there is no test file in this pull request nor any test function or class in the file `physics/bernoullis_principle.py`, please provide doctest for the function `bernoullis_principle`\n\nPlease provide return type hint for the function: `bernoullis_principle`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `P1`\n\nPlease provide type hint for the parameter: `v1`\n\nPlease provide type hint for the parameter: `h1`\n\nPlease provide type hint for the parameter: `P2`\n\nPlease provide type hint for the parameter: `v2`\n\nPlease provide type hint for the parameter: `h2`\n\nPlease provide type hint for the parameter: `density`\n\nPlease provide descriptive name for the parameter: `g`\n\nPlease provide type hint for the parameter: `g`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "physics/bernoullis_principle.py",
    "pr_number": 11915,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1792877833,
    "comment_created_at": "2024-10-09T05:44:37Z"
  },
  {
    "code": "@@ -0,0 +1,18 @@\n+def maximumSubarraySum(arr):",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: maximumsubarraysum\n\nas there is no test file in this pull request nor any test function or class in the file data_structures/arrays/kadanealgo.py, please provide doctest for the function maximumsubarraysum\n\nplease provide return type hint for the function: maximumsubarraysum. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: arr",
    "line_number": 1,
    "enriched": "File: data_structures/arrays/KadaneAlgo.py\nCode: @@ -0,0 +1,18 @@\n+def maximumSubarraySum(arr):\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `maximumSubarraySum`\n\nAs there is no test file in this pull request nor any test function or class in the file `data_structures/arrays/KadaneAlgo.py`, please provide doctest for the function `maximumSubarraySum`\n\nPlease provide return type hint for the function: `maximumSubarraySum`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `arr`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/arrays/KadaneAlgo.py",
    "pr_number": 11839,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1789363893,
    "comment_created_at": "2024-10-07T01:52:32Z"
  },
  {
    "code": "@@ -0,0 +1,50 @@\n+# Recursive Python3 code to sort\r\n+# an array using selection sort\r\n+\r\n+# Return minimum index\r\n+def minIndex( a , i , j ):\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file sorts/recursive selection sort.py, please provide doctest for the function minindex\n\nplease provide return type hint for the function: minindex. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: minindex\n\nplease provide type hint for the parameter: a\n\nplease provide descriptive name for the parameter: a\n\nplease provide type hint for the parameter: i\n\nplease provide descriptive name for the parameter: i\n\nplease provide type hint for the parameter: j\n\nplease provide descriptive name for the parameter: j",
    "line_number": 5,
    "enriched": "File: sorts/Recursive Selection Sort.py\nCode: @@ -0,0 +1,50 @@\n+# Recursive Python3 code to sort\r\n+# an array using selection sort\r\n+\r\n+# Return minimum index\r\n+def minIndex( a , i , j ):\r\nComment: As there is no test file in this pull request nor any test function or class in the file `sorts/Recursive Selection Sort.py`, please provide doctest for the function `minIndex`\n\nPlease provide return type hint for the function: `minIndex`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `minIndex`\n\nPlease provide type hint for the parameter: `a`\n\nPlease provide descriptive name for the parameter: `a`\n\nPlease provide type hint for the parameter: `i`\n\nPlease provide descriptive name for the parameter: `i`\n\nPlease provide type hint for the parameter: `j`\n\nPlease provide descriptive name for the parameter: `j`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "sorts/Recursive Selection Sort.py",
    "pr_number": 6886,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 990833316,
    "comment_created_at": "2022-10-09T20:06:28Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+def spiral_order(matrix):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/spiralmatrix.py, please provide doctest for the function spiral_order\n\nplease provide return type hint for the function: spiral_order. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: matrix",
    "line_number": 1,
    "enriched": "File: dynamic_programming/spiralmatrix.py\nCode: @@ -0,0 +1,39 @@\n+def spiral_order(matrix):\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/spiralmatrix.py`, please provide doctest for the function `spiral_order`\n\nPlease provide return type hint for the function: `spiral_order`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `matrix`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/spiralmatrix.py",
    "pr_number": 11057,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375397540,
    "comment_created_at": "2023-10-29T09:07:26Z"
  },
  {
    "code": "@@ -39,6 +39,59 @@ def binary_cross_entropy(\n     return np.mean(bce_loss)\n \n \n+def binary_focal_cross_entropy(\n+    y_true: np.ndarray,\n+    y_pred: np.ndarray,\n+    gamma: float = 2.0,\n+    alpha: float = 0.25,\n+    epsilon: float = 1e-15,\n+) -> float:\n+    \"\"\"\n+    Calculate the mean binary focal cross-entropy (BFCE) loss between true labels\n+    and predicted probabilities.\n+\n+    BFCE loss quantifies dissimilarity between true labels (0 or 1) and predicted\n+    probabilities. It's a variation of binary cross-entropy that addresses class\n+    imbalance by focusing on hard examples.\n+\n+    BCFE = -\u03a3(alpha * (1 - y_pred)**gamma * y_true * log(y_pred)\n+                + (1 - alpha) * y_pred**gamma * (1 - y_true) * log(1 - y_pred))\n+\n+    Reference: [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf)\n+\n+    Parameters:\n+    - y_true: True binary labels (0 or 1).\n+    - y_pred: Predicted probabilities for class 1.\n+    - gamma: Focusing parameter for modulating the loss (default: 2.0).\n+    - alpha: Weighting factor for class 1 (default: 0.25).\n+    - epsilon: Small constant to avoid numerical instability.\n+\n+    >>> true_labels = np.array([0, 1, 1, 0, 1])\n+    >>> predicted_probs = np.array([0.2, 0.7, 0.9, 0.3, 0.8])\n+    >>> binary_focal_cross_entropy(true_labels, predicted_probs)\n+    0.008257977659239775\n+    >>> true_labels = np.array([0, 1, 1, 0, 1])\n+    >>> predicted_probs = np.array([0.3, 0.8, 0.9, 0.2])\n+    >>> binary_focal_cross_entropy(true_labels, predicted_probs)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Input arrays must have the same length.\n+    \"\"\"\n+    if len(y_true) != len(y_pred):\n+        raise ValueError(\"Input arrays must have the same length.\")\n+    # Clip predicted probabilities to avoid log(0) and log(1)\n+    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n+\n+    # Focal loss calculation\n+    bcfe_loss = -(\n+        alpha * (1 - y_pred) ** gamma * y_true * np.log(y_pred)\n+        + (1 - alpha) * y_pred**gamma * (1 - y_true) * np.log(1 - y_pred)\n+    )\n+\n+    # Take the mean over all samples\n+    return np.mean(bcfe_loss)",
    "comment": "for the first comment, there's nothing wrong with computing log(1), since that just evaluates to 0 with no issue. the issue is that the formula includes log(1 - x), which still may evaluate to log(0).\r\n\r\nfor the second and third comments, i feel that the code and variable names were already self-explanatory and didn't need the comments.",
    "line_number": 92,
    "enriched": "File: machine_learning/loss_functions.py\nCode: @@ -39,6 +39,59 @@ def binary_cross_entropy(\n     return np.mean(bce_loss)\n \n \n+def binary_focal_cross_entropy(\n+    y_true: np.ndarray,\n+    y_pred: np.ndarray,\n+    gamma: float = 2.0,\n+    alpha: float = 0.25,\n+    epsilon: float = 1e-15,\n+) -> float:\n+    \"\"\"\n+    Calculate the mean binary focal cross-entropy (BFCE) loss between true labels\n+    and predicted probabilities.\n+\n+    BFCE loss quantifies dissimilarity between true labels (0 or 1) and predicted\n+    probabilities. It's a variation of binary cross-entropy that addresses class\n+    imbalance by focusing on hard examples.\n+\n+    BCFE = -\u03a3(alpha * (1 - y_pred)**gamma * y_true * log(y_pred)\n+                + (1 - alpha) * y_pred**gamma * (1 - y_true) * log(1 - y_pred))\n+\n+    Reference: [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf)\n+\n+    Parameters:\n+    - y_true: True binary labels (0 or 1).\n+    - y_pred: Predicted probabilities for class 1.\n+    - gamma: Focusing parameter for modulating the loss (default: 2.0).\n+    - alpha: Weighting factor for class 1 (default: 0.25).\n+    - epsilon: Small constant to avoid numerical instability.\n+\n+    >>> true_labels = np.array([0, 1, 1, 0, 1])\n+    >>> predicted_probs = np.array([0.2, 0.7, 0.9, 0.3, 0.8])\n+    >>> binary_focal_cross_entropy(true_labels, predicted_probs)\n+    0.008257977659239775\n+    >>> true_labels = np.array([0, 1, 1, 0, 1])\n+    >>> predicted_probs = np.array([0.3, 0.8, 0.9, 0.2])\n+    >>> binary_focal_cross_entropy(true_labels, predicted_probs)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Input arrays must have the same length.\n+    \"\"\"\n+    if len(y_true) != len(y_pred):\n+        raise ValueError(\"Input arrays must have the same length.\")\n+    # Clip predicted probabilities to avoid log(0) and log(1)\n+    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n+\n+    # Focal loss calculation\n+    bcfe_loss = -(\n+        alpha * (1 - y_pred) ** gamma * y_true * np.log(y_pred)\n+        + (1 - alpha) * y_pred**gamma * (1 - y_true) * np.log(1 - y_pred)\n+    )\n+\n+    # Take the mean over all samples\n+    return np.mean(bcfe_loss)\nComment: ```suggestion\r\n    # Clip predicted probabilities to avoid log(0)\r\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\r\n\r\n    bcfe_loss = -(\r\n        alpha * (1 - y_pred) ** gamma * y_true * np.log(y_pred)\r\n        + (1 - alpha) * y_pred**gamma * (1 - y_true) * np.log(1 - y_pred)\r\n    )\r\n\r\n    return np.mean(bcfe_loss)\r\n```\r\nFor the first comment, there's nothing wrong with computing log(1), since that just evaluates to 0 with no issue. The issue is that the formula includes log(1 - x), which still may evaluate to log(0).\r\n\r\nFor the second and third comments, I feel that the code and variable names were already self-explanatory and didn't need the comments.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "machine_learning/loss_functions.py",
    "pr_number": 10674,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368142391,
    "comment_created_at": "2023-10-23T05:20:39Z"
  },
  {
    "code": "@@ -0,0 +1,28 @@\n+import numpy as np\n+def best_response_dynamics(payoff_matrix_a, payoff_matrix_b, iterations=10):",
    "comment": "please provide return type hint for the function: best_response_dynamics. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file game_theory/best_response_dynamics.py, please provide doctest for the function best_response_dynamics\n\nplease provide type hint for the parameter: payoff_matrix_a\n\nplease provide type hint for the parameter: payoff_matrix_b\n\nplease provide type hint for the parameter: iterations",
    "line_number": 2,
    "enriched": "File: game_theory/best_response_dynamics.py\nCode: @@ -0,0 +1,28 @@\n+import numpy as np\n+def best_response_dynamics(payoff_matrix_a, payoff_matrix_b, iterations=10):\nComment: Please provide return type hint for the function: `best_response_dynamics`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `game_theory/best_response_dynamics.py`, please provide doctest for the function `best_response_dynamics`\n\nPlease provide type hint for the parameter: `payoff_matrix_a`\n\nPlease provide type hint for the parameter: `payoff_matrix_b`\n\nPlease provide type hint for the parameter: `iterations`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "game_theory/best_response_dynamics.py",
    "pr_number": 11864,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1790559473,
    "comment_created_at": "2024-10-07T16:43:01Z"
  },
  {
    "code": "@@ -54,17 +54,20 @@ def hubble_parameter(\n     result : Hubble parameter in and the unit km/s/Mpc (the unit can be\n     changed if you want, just need to change the unit of the Hubble constant)\n \n-    >>> hubble_parameter(hubble_constant=68.3, radiation_density=1e-4, matter_density=-0.3, dark_energy=0.7, redshift=1)\n+    >>> hubble_parameter(hubble_constant=68.3, radiation_density=1e-4, \\",
    "comment": "pep8 says that backslashes are a bad idea in python code.",
    "line_number": 57,
    "enriched": "File: physics/hubble_parameter.py\nCode: @@ -54,17 +54,20 @@ def hubble_parameter(\n     result : Hubble parameter in and the unit km/s/Mpc (the unit can be\n     changed if you want, just need to change the unit of the Hubble constant)\n \n-    >>> hubble_parameter(hubble_constant=68.3, radiation_density=1e-4, matter_density=-0.3, dark_energy=0.7, redshift=1)\n+    >>> hubble_parameter(hubble_constant=68.3, radiation_density=1e-4, \\\nComment: PEP8 says that backslashes are a bad idea in Python code.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "physics/hubble_parameter.py",
    "pr_number": 7807,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008921694,
    "comment_created_at": "2022-10-30T20:26:26Z"
  },
  {
    "code": "@@ -0,0 +1,65 @@\n+\"\"\"\n+Principal Component Analysis (PCA) is a dimensionality reduction technique\n+commonly used in machine learning. It transforms high-dimensional data into\n+lower dimensions while retaining most of the information.\n+\n+Here,we use a dataset (Iris dataset) and apply PCA to reduce the\n+dimensionality. We compute the principal components and transform the dataset\n+into a lower-dimensional space.\n+\n+We reduce the number of columns form 4 to 2\n+\n+\"\"\"\n+\n+import numpy as np\n+from sklearn.decomposition import PCA\n+from sklearn.preprocessing import StandardScaler\n+from sklearn.datasets import load_iris\n+\n+\n+def collect_dataset():",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/principle_component_analysis.py, please provide doctest for the function collect_dataset\n\nplease provide return type hint for the function: collect_dataset. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 20,
    "enriched": "File: machine_learning/principle_component_analysis.py\nCode: @@ -0,0 +1,65 @@\n+\"\"\"\n+Principal Component Analysis (PCA) is a dimensionality reduction technique\n+commonly used in machine learning. It transforms high-dimensional data into\n+lower dimensions while retaining most of the information.\n+\n+Here,we use a dataset (Iris dataset) and apply PCA to reduce the\n+dimensionality. We compute the principal components and transform the dataset\n+into a lower-dimensional space.\n+\n+We reduce the number of columns form 4 to 2\n+\n+\"\"\"\n+\n+import numpy as np\n+from sklearn.decomposition import PCA\n+from sklearn.preprocessing import StandardScaler\n+from sklearn.datasets import load_iris\n+\n+\n+def collect_dataset():\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/principle_component_analysis.py`, please provide doctest for the function `collect_dataset`\n\nPlease provide return type hint for the function: `collect_dataset`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/principle_component_analysis.py",
    "pr_number": 12595,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1976290122,
    "comment_created_at": "2025-03-01T04:33:50Z"
  },
  {
    "code": "@@ -0,0 +1,27 @@\n+def best_response_dynamics(payoff_matrix_A, payoff_matrix_B, iterations=10):",
    "comment": "please provide return type hint for the function: best_response_dynamics. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file game_theory/best_response_dynamics.py, please provide doctest for the function best_response_dynamics\n\nplease provide type hint for the parameter: payoff_matrix_a\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: payoff_matrix_a\n\nplease provide type hint for the parameter: payoff_matrix_b\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: payoff_matrix_b\n\nplease provide type hint for the parameter: iterations",
    "line_number": 1,
    "enriched": "File: game_theory/best_response_dynamics.py\nCode: @@ -0,0 +1,27 @@\n+def best_response_dynamics(payoff_matrix_A, payoff_matrix_B, iterations=10):\nComment: Please provide return type hint for the function: `best_response_dynamics`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `game_theory/best_response_dynamics.py`, please provide doctest for the function `best_response_dynamics`\n\nPlease provide type hint for the parameter: `payoff_matrix_A`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `payoff_matrix_A`\n\nPlease provide type hint for the parameter: `payoff_matrix_B`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `payoff_matrix_B`\n\nPlease provide type hint for the parameter: `iterations`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "game_theory/best_response_dynamics.py",
    "pr_number": 11859,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1790458348,
    "comment_created_at": "2024-10-07T15:36:19Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+import numpy as np\n+\n+def travelling_salesman(city):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/travelling_salesman.py, please provide doctest for the function travelling_salesman\n\nplease provide return type hint for the function: travelling_salesman. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: city",
    "line_number": 3,
    "enriched": "File: dynamic_programming/travelling_salesman.py\nCode: @@ -0,0 +1,53 @@\n+import numpy as np\n+\n+def travelling_salesman(city):\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/travelling_salesman.py`, please provide doctest for the function `travelling_salesman`\n\nPlease provide return type hint for the function: `travelling_salesman`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `city`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/travelling_salesman.py",
    "pr_number": 10544,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359901657,
    "comment_created_at": "2023-10-15T15:49:20Z"
  },
  {
    "code": "@@ -0,0 +1,31 @@\n+\"\"\"\r\n+There is a robot on an m x n grid. \r\n+The robot is initially located at the top-left corner of grid the robot tries to move to the bottom-right corner. \r\n+The robot can only move either down or right at any point in time.\r\n+Return number of all  possible unique paths robot can take.\r\n+\r\n+\"\"\"\r\n+def uniquepaths(self, m, n):\r",
    "comment": "please provide return type hint for the function: uniquepaths. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file dynamic_programming/unique_paths.py, please provide doctest for the function uniquepaths\n\nplease provide descriptive name for the parameter: m\n\nplease provide type hint for the parameter: m\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: n",
    "line_number": 8,
    "enriched": "File: dynamic_programming/unique_paths.py\nCode: @@ -0,0 +1,31 @@\n+\"\"\"\r\n+There is a robot on an m x n grid. \r\n+The robot is initially located at the top-left corner of grid the robot tries to move to the bottom-right corner. \r\n+The robot can only move either down or right at any point in time.\r\n+Return number of all  possible unique paths robot can take.\r\n+\r\n+\"\"\"\r\n+def uniquepaths(self, m, n):\r\nComment: Please provide return type hint for the function: `uniquepaths`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `dynamic_programming/unique_paths.py`, please provide doctest for the function `uniquepaths`\n\nPlease provide descriptive name for the parameter: `m`\n\nPlease provide type hint for the parameter: `m`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide type hint for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/unique_paths.py",
    "pr_number": 10198,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1350729477,
    "comment_created_at": "2023-10-09T19:38:12Z"
  },
  {
    "code": "@@ -0,0 +1,59 @@\n+\"\"\"\n+You are given a rows x cols matrix grid representing a field of cherries where grid[i][j] represents the number of cherries that you can collect from the (i, j) cell.\n+\n+You have two robots that can collect cherries for you:\n+\n+Robot #1 is located at the top-left corner (0, 0), and\n+Robot #2 is located at the top-right corner (0, cols - 1).\n+Return the maximum number of cherries collection using both robots by following the rules below:\n+\n+1. From a cell (i, j), robots can move to cell (i + 1, j - 1), (i + 1, j), or (i + 1, j + 1).\n+2. When any robot passes through a cell, It picks up all cherries, and the cell becomes an empty cell.\n+3. When both robots stay in the same cell, only one takes the cherries.\n+4. Both robots cannot move outside of the grid at any moment.\n+5. Both robots should reach the bottom row in grid.\n+\n+Problem Statement:- https://leetcode.com/problems/cherry-pickup-ii\n+\n+\"\"\"\n+\n+\n+from typing import List\n+from collections import defaultdict\n+\n+\n+class Solution:\n+    def cherryPickup(self, grid: List[List[int]]) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/cherry_pickup_ii.py, please provide doctest for the function cherrypickup\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: cherrypickup",
    "line_number": 26,
    "enriched": "File: dynamic_programming/cherry_pickup_ii.py\nCode: @@ -0,0 +1,59 @@\n+\"\"\"\n+You are given a rows x cols matrix grid representing a field of cherries where grid[i][j] represents the number of cherries that you can collect from the (i, j) cell.\n+\n+You have two robots that can collect cherries for you:\n+\n+Robot #1 is located at the top-left corner (0, 0), and\n+Robot #2 is located at the top-right corner (0, cols - 1).\n+Return the maximum number of cherries collection using both robots by following the rules below:\n+\n+1. From a cell (i, j), robots can move to cell (i + 1, j - 1), (i + 1, j), or (i + 1, j + 1).\n+2. When any robot passes through a cell, It picks up all cherries, and the cell becomes an empty cell.\n+3. When both robots stay in the same cell, only one takes the cherries.\n+4. Both robots cannot move outside of the grid at any moment.\n+5. Both robots should reach the bottom row in grid.\n+\n+Problem Statement:- https://leetcode.com/problems/cherry-pickup-ii\n+\n+\"\"\"\n+\n+\n+from typing import List\n+from collections import defaultdict\n+\n+\n+class Solution:\n+    def cherryPickup(self, grid: List[List[int]]) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/cherry_pickup_ii.py`, please provide doctest for the function `cherryPickup`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `cherryPickup`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/cherry_pickup_ii.py",
    "pr_number": 9932,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349013914,
    "comment_created_at": "2023-10-06T17:12:17Z"
  },
  {
    "code": "@@ -0,0 +1,124 @@\n+\"\"\"\n+Animating Conway's Game of Life on Python\n+\n+For more information, see:\n+https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life\n+https://conwaylife.com/wiki/\n+\n+Usage:\n+  - $python3 gol.py <url:str>\n+  - $python3 gol.py <grid_size:int>\n+\"\"\"\n+\n+\n+import re\n+import sys\n+\n+import matplotlib as mlb\n+import matplotlib.pyplot as plt\n+import numpy as np\n+import requests\n+from matplotlib.animation import FuncAnimation\n+\n+\n+def update(_: int, img: mlb.image.AxesImage, grid: np.ndarray) -> mlb.image.AxesImage:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file cellular_automata/game_of_life_2.py, please provide doctest for the function update\n\nplease provide descriptive name for the parameter: _",
    "line_number": 24,
    "enriched": "File: cellular_automata/game_of_life_2.py\nCode: @@ -0,0 +1,124 @@\n+\"\"\"\n+Animating Conway's Game of Life on Python\n+\n+For more information, see:\n+https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life\n+https://conwaylife.com/wiki/\n+\n+Usage:\n+  - $python3 gol.py <url:str>\n+  - $python3 gol.py <grid_size:int>\n+\"\"\"\n+\n+\n+import re\n+import sys\n+\n+import matplotlib as mlb\n+import matplotlib.pyplot as plt\n+import numpy as np\n+import requests\n+from matplotlib.animation import FuncAnimation\n+\n+\n+def update(_: int, img: mlb.image.AxesImage, grid: np.ndarray) -> mlb.image.AxesImage:\nComment: As there is no test file in this pull request nor any test function or class in the file `cellular_automata/game_of_life_2.py`, please provide doctest for the function `update`\n\nPlease provide descriptive name for the parameter: `_`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "cellular_automata/game_of_life_2.py",
    "pr_number": 8722,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1192556695,
    "comment_created_at": "2023-05-12T16:07:40Z"
  },
  {
    "code": "@@ -0,0 +1,79 @@\n+import cirq\n+import numpy as np\n+import matplotlib.pyplot as plt\n+from sklearn.datasets import make_blobs\n+from sklearn.preprocessing import MinMaxScaler\n+\n+def generate_data(n_samples=100, n_features=2, n_clusters=2):",
    "comment": "please provide return type hint for the function: generate_data. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file quantum/quantum_kmeans_clustering.py, please provide doctest for the function generate_data\n\nplease provide type hint for the parameter: n_samples\n\nplease provide type hint for the parameter: n_features\n\nplease provide type hint for the parameter: n_clusters",
    "line_number": 7,
    "enriched": "File: quantum/quantum_kmeans_clustering.py\nCode: @@ -0,0 +1,79 @@\n+import cirq\n+import numpy as np\n+import matplotlib.pyplot as plt\n+from sklearn.datasets import make_blobs\n+from sklearn.preprocessing import MinMaxScaler\n+\n+def generate_data(n_samples=100, n_features=2, n_clusters=2):\nComment: Please provide return type hint for the function: `generate_data`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `quantum/quantum_kmeans_clustering.py`, please provide doctest for the function `generate_data`\n\nPlease provide type hint for the parameter: `n_samples`\n\nPlease provide type hint for the parameter: `n_features`\n\nPlease provide type hint for the parameter: `n_clusters`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "quantum/quantum_kmeans_clustering.py",
    "pr_number": 11664,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1783903563,
    "comment_created_at": "2024-10-02T06:40:21Z"
  },
  {
    "code": "@@ -0,0 +1,78 @@\n+\"\"\"\n+A Python implementation of the Median of Medians algorithm to select pivots for QuickSelect,\n+which is efficient for calculating the value that would appear in the index of a list if it\n+would be sorted, even if it is not already sorted. Search in time complexity O(n) at any rank\n+deterministically\n+https://en.wikipedia.org/wiki/Median_of_medians\n+\"\"\"\n+\n+def MedianofFive(arr:list) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/median_of_medians.py, please provide doctest for the function medianoffive\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: medianoffive",
    "line_number": 9,
    "enriched": "File: searches/median_of_medians.py\nCode: @@ -0,0 +1,78 @@\n+\"\"\"\n+A Python implementation of the Median of Medians algorithm to select pivots for QuickSelect,\n+which is efficient for calculating the value that would appear in the index of a list if it\n+would be sorted, even if it is not already sorted. Search in time complexity O(n) at any rank\n+deterministically\n+https://en.wikipedia.org/wiki/Median_of_medians\n+\"\"\"\n+\n+def MedianofFive(arr:list) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/median_of_medians.py`, please provide doctest for the function `MedianofFive`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `MedianofFive`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "searches/median_of_medians.py",
    "pr_number": 9859,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347812780,
    "comment_created_at": "2023-10-05T18:18:10Z"
  },
  {
    "code": "@@ -0,0 +1,45 @@\n+# Python program to check if a number is Kaprekar number or not\n+\n+import math\n+\n+# Returns true if n is a Kaprekar number, else false\n+def iskaprekar(n):",
    "comment": "please provide return type hint for the function: iskaprekar. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file maths/kaprekar_number.py, please provide doctest for the function iskaprekar\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n",
    "line_number": 6,
    "enriched": "File: maths/kaprekar_number.py\nCode: @@ -0,0 +1,45 @@\n+# Python program to check if a number is Kaprekar number or not\n+\n+import math\n+\n+# Returns true if n is a Kaprekar number, else false\n+def iskaprekar(n):\nComment: Please provide return type hint for the function: `iskaprekar`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `maths/kaprekar_number.py`, please provide doctest for the function `iskaprekar`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/kaprekar_number.py",
    "pr_number": 9675,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1345266390,
    "comment_created_at": "2023-10-04T06:35:25Z"
  },
  {
    "code": "@@ -0,0 +1,75 @@\n+import numpy as np\n+import pandas as pd\n+\n+class RidgeRegression:\n+    def __init__(self, alpha=0.001, regularization_param=0.1, num_iterations=1000):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: alpha\n\nplease provide type hint for the parameter: regularization_param\n\nplease provide type hint for the parameter: num_iterations",
    "line_number": 5,
    "enriched": "File: machine_learning/ridge_regression/model.py\nCode: @@ -0,0 +1,75 @@\n+import numpy as np\n+import pandas as pd\n+\n+class RidgeRegression:\n+    def __init__(self, alpha=0.001, regularization_param=0.1, num_iterations=1000):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `alpha`\n\nPlease provide type hint for the parameter: `regularization_param`\n\nPlease provide type hint for the parameter: `num_iterations`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/ridge_regression/model.py",
    "pr_number": 12250,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1812864508,
    "comment_created_at": "2024-10-23T14:10:23Z"
  },
  {
    "code": "@@ -0,0 +1,46 @@\n+# import the necessary packages\n+from skimage.feature import peak_local_max\n+from skimage.segmentation import watershed\n+from scipy import ndimage\n+import numpy as np\n+import imutils\n+import cv2\n+\n+def watershed_image(path_image):",
    "comment": "please provide return type hint for the function: watershed_image. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file digital_image_processing/edge_detection/watershed.py, please provide doctest for the function watershed_image\n\nplease provide type hint for the parameter: path_image",
    "line_number": 9,
    "enriched": "File: digital_image_processing/edge_detection/watershed.py\nCode: @@ -0,0 +1,46 @@\n+# import the necessary packages\n+from skimage.feature import peak_local_max\n+from skimage.segmentation import watershed\n+from scipy import ndimage\n+import numpy as np\n+import imutils\n+import cv2\n+\n+def watershed_image(path_image):\nComment: Please provide return type hint for the function: `watershed_image`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `digital_image_processing/edge_detection/watershed.py`, please provide doctest for the function `watershed_image`\n\nPlease provide type hint for the parameter: `path_image`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "digital_image_processing/edge_detection/watershed.py",
    "pr_number": 11042,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375110991,
    "comment_created_at": "2023-10-27T23:06:06Z"
  },
  {
    "code": "@@ -0,0 +1,69 @@\n+\"\"\"\n+This is pure Python implementation of exponential search.\n+\n+Resources used:\n+https://en.wikipedia.org/wiki/Exponential_search\n+\n+For doctests run following command:\n+python3 -m doctest -v expontial_search.py\n+\n+For manual testing run:\n+python3 expontial_search.py\n+\"\"\"\n+\n+\n+def binary_search(arr, left, right, target) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/exponential_search.py, please provide doctest for the function binary_search\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: left\n\nplease provide type hint for the parameter: right\n\nplease provide type hint for the parameter: target",
    "line_number": 15,
    "enriched": "File: searches/exponential_search.py\nCode: @@ -0,0 +1,69 @@\n+\"\"\"\n+This is pure Python implementation of exponential search.\n+\n+Resources used:\n+https://en.wikipedia.org/wiki/Exponential_search\n+\n+For doctests run following command:\n+python3 -m doctest -v expontial_search.py\n+\n+For manual testing run:\n+python3 expontial_search.py\n+\"\"\"\n+\n+\n+def binary_search(arr, left, right, target) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/exponential_search.py`, please provide doctest for the function `binary_search`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `left`\n\nPlease provide type hint for the parameter: `right`\n\nPlease provide type hint for the parameter: `target`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "searches/exponential_search.py",
    "pr_number": 10764,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367729658,
    "comment_created_at": "2023-10-21T13:22:35Z"
  },
  {
    "code": "@@ -0,0 +1,258 @@\n+from collections import OrderedDict\n+\n+def isterminal(char):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file compiler design/ll1_parser.py, please provide doctest for the function isterminal\n\nplease provide return type hint for the function: isterminal. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: char",
    "line_number": 3,
    "enriched": "File: Compiler Design/LL1_Parser.py\nCode: @@ -0,0 +1,258 @@\n+from collections import OrderedDict\n+\n+def isterminal(char):\nComment: As there is no test file in this pull request nor any test function or class in the file `Compiler Design/LL1_Parser.py`, please provide doctest for the function `isterminal`\n\nPlease provide return type hint for the function: `isterminal`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `char`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "Compiler Design/LL1_Parser.py",
    "pr_number": 7540,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002642297,
    "comment_created_at": "2022-10-23T05:53:46Z"
  },
  {
    "code": "@@ -52,7 +52,7 @@ def gaussian(x, mu: float = 0.0, sigma: float = 1.0) -> int:\n     >>> gaussian(2523, mu=234234, sigma=3425)\n     0.0\n     \"\"\"\n-    return 1 / sqrt(2 * pi * sigma**2) * exp(-((x - mu) ** 2) / (2 * sigma**2))\n+    return (1 / sigma * sqrt(2 * pi)) * exp(-((x - mu) ** 2) / (2 * sigma**2))\n ",
    "comment": "edited the notation",
    "line_number": 56,
    "enriched": "File: maths/gaussian.py\nCode: @@ -52,7 +52,7 @@ def gaussian(x, mu: float = 0.0, sigma: float = 1.0) -> int:\n     >>> gaussian(2523, mu=234234, sigma=3425)\n     0.0\n     \"\"\"\n-    return 1 / sqrt(2 * pi * sigma**2) * exp(-((x - mu) ** 2) / (2 * sigma**2))\n+    return (1 / sigma * sqrt(2 * pi)) * exp(-((x - mu) ** 2) / (2 * sigma**2))\n \nComment: edited the notation ",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "maths/gaussian.py",
    "pr_number": 7645,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004629333,
    "comment_created_at": "2022-10-25T15:18:15Z"
  },
  {
    "code": "@@ -0,0 +1,50 @@\n+# Kruskal's algorithm in Python\n+",
    "comment": "add some more description and if possible, a credible source.",
    "line_number": 2,
    "enriched": "File: graphs/Kruskals.py\nCode: @@ -0,0 +1,50 @@\n+# Kruskal's algorithm in Python\n+\nComment: Add some more description and if possible, a credible source.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "graphs/Kruskals.py",
    "pr_number": 7415,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 999248322,
    "comment_created_at": "2022-10-19T10:23:27Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+# Function to perform partition of the array\n+def partition(arr, low, high):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file divide_and_conquer/quicksort.py, please provide doctest for the function partition\n\nplease provide return type hint for the function: partition. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: low\n\nplease provide type hint for the parameter: high",
    "line_number": 2,
    "enriched": "File: divide_and_conquer/quicksort.py\nCode: @@ -0,0 +1,48 @@\n+# Function to perform partition of the array\n+def partition(arr, low, high):\nComment: As there is no test file in this pull request nor any test function or class in the file `divide_and_conquer/quicksort.py`, please provide doctest for the function `partition`\n\nPlease provide return type hint for the function: `partition`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `low`\n\nPlease provide type hint for the parameter: `high`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "divide_and_conquer/quicksort.py",
    "pr_number": 12057,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1799121266,
    "comment_created_at": "2024-10-14T10:01:16Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+def infix_to_postfix(expression):\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file infix_evaluation/infix_to_postfix.py, please provide doctest for the function infix_to_postfix\n\nplease provide return type hint for the function: infix_to_postfix. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: expression",
    "line_number": 1,
    "enriched": "File: infix_evaluation/infix_to_postfix.py\nCode: @@ -0,0 +1,42 @@\n+def infix_to_postfix(expression):\r\nComment: As there is no test file in this pull request nor any test function or class in the file `infix_evaluation/infix_to_postfix.py`, please provide doctest for the function `infix_to_postfix`\n\nPlease provide return type hint for the function: `infix_to_postfix`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `expression`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "infix_evaluation/infix_to_postfix.py",
    "pr_number": 9614,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1344358907,
    "comment_created_at": "2023-10-03T16:04:00Z"
  },
  {
    "code": "@@ -0,0 +1,150 @@\n+\"\"\"\n+QUESTION\n+\n+Remember those images where they'd show some boxes and ask you to count the squares?\n+\n+Given a 2-D matrix represented by a list of lists and an optional parameter thresh which represents the highest order to be considered and has a default value of None, write a function named my_exes that\n+returnss the number of Xs in the 2-D matrix as an integer.\n+\n+If thresh is None the total number of Xs from all possibilities should be returned else it represents the maximum dimension to be considered. \n+The 2-D matrix will consist of 0s and 1s only. All Xs must have every along them as 1.\n+\n+Below is a first grade X.(thresh==1)\n+[[1, 1]\n+ [1, 1]]\n+\n+Below is a second grade X (thresh==2)\n+[[1, 0, 1]\n+ [0, 1, 0]\n+ [1, 0, 1]]\n+\n+Note that even if the 0s here were 1s, it will still be considered as an X because it obeys the rule.\n+\n+Example:\n+matrix = [[1, 0, 1],\n+          [0, 1, 1],\n+          [1, 1, 1]]\n+          \n+my_exes(matrix) returns 2\n+IF matrix[0][1] was set to 1:\n+my_exes(matrix) returns 3\n+my_exes(matrix,thresh=1) returns 2\n+\"\"\"\n+\n+def check_right_diagonals(image,start):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file matrix/number_of_exes(x).py, please provide doctest for the function check_right_diagonals\n\nplease provide return type hint for the function: check_right_diagonals. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: image\n\nplease provide type hint for the parameter: start",
    "line_number": 34,
    "enriched": "File: matrix/number_of_exes(X).py\nCode: @@ -0,0 +1,150 @@\n+\"\"\"\n+QUESTION\n+\n+Remember those images where they'd show some boxes and ask you to count the squares?\n+\n+Given a 2-D matrix represented by a list of lists and an optional parameter thresh which represents the highest order to be considered and has a default value of None, write a function named my_exes that\n+returnss the number of Xs in the 2-D matrix as an integer.\n+\n+If thresh is None the total number of Xs from all possibilities should be returned else it represents the maximum dimension to be considered. \n+The 2-D matrix will consist of 0s and 1s only. All Xs must have every along them as 1.\n+\n+Below is a first grade X.(thresh==1)\n+[[1, 1]\n+ [1, 1]]\n+\n+Below is a second grade X (thresh==2)\n+[[1, 0, 1]\n+ [0, 1, 0]\n+ [1, 0, 1]]\n+\n+Note that even if the 0s here were 1s, it will still be considered as an X because it obeys the rule.\n+\n+Example:\n+matrix = [[1, 0, 1],\n+          [0, 1, 1],\n+          [1, 1, 1]]\n+          \n+my_exes(matrix) returns 2\n+IF matrix[0][1] was set to 1:\n+my_exes(matrix) returns 3\n+my_exes(matrix,thresh=1) returns 2\n+\"\"\"\n+\n+def check_right_diagonals(image,start):\nComment: As there is no test file in this pull request nor any test function or class in the file `matrix/number_of_exes(X).py`, please provide doctest for the function `check_right_diagonals`\n\nPlease provide return type hint for the function: `check_right_diagonals`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `image`\n\nPlease provide type hint for the parameter: `start`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "matrix/number_of_exes(X).py",
    "pr_number": 7923,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1009816438,
    "comment_created_at": "2022-10-31T20:02:20Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+from collections.abc import Callable\n+\n+from numpy.random import rand, randint\n+\n+\"\"\"\n+This program performs genetic evolution on a list of bits.\n+Its goal is to transform every bit to a 1, using mutations and crossovers.\n+\"\"\"\n+\n+\n+# Goal of the algorithm, all 1s\n+def goal(x: list[int]) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file genetic_algorithm/one_max.py, please provide doctest for the function goal\n\nplease provide descriptive name for the parameter: x",
    "line_number": 12,
    "enriched": "File: genetic_algorithm/one_max.py\nCode: @@ -0,0 +1,94 @@\n+from collections.abc import Callable\n+\n+from numpy.random import rand, randint\n+\n+\"\"\"\n+This program performs genetic evolution on a list of bits.\n+Its goal is to transform every bit to a 1, using mutations and crossovers.\n+\"\"\"\n+\n+\n+# Goal of the algorithm, all 1s\n+def goal(x: list[int]) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `genetic_algorithm/one_max.py`, please provide doctest for the function `goal`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "genetic_algorithm/one_max.py",
    "pr_number": 11162,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1395119391,
    "comment_created_at": "2023-11-16T03:52:02Z"
  },
  {
    "code": "@@ -0,0 +1,479 @@\n+# This python program is made by me (Sahil Hooda) to simulate a simple interactive game(Subway Surfers) in real life.\n+# Using the power of openCV (Computer Vision) it can be used to play the game in real life by capturing body movements.                               \n+\n+import cv2\n+import pyautogui\n+from time import time\n+from math import hypot\n+import mediapipe as mp\n+import matplotlib.pyplot as plt\n+\n+# Initialize mediapipe pose class.\n+mp_pose = mp.solutions.pose\n+\n+# Setup the Pose function for images.\n+pose_image = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=1)\n+\n+# Setup the Pose function for videos.\n+pose_video = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.7,\n+                          min_tracking_confidence=0.7)\n+\n+# Initialize mediapipe drawing class.\n+mp_drawing = mp.solutions.drawing_utils\n+\n+\n+def detectPose(image, pose, draw=False, display=False):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file computer_vision/pose_detection.py, please provide doctest for the function detectpose\n\nplease provide return type hint for the function: detectpose. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: detectpose\n\nplease provide type hint for the parameter: image\n\nplease provide type hint for the parameter: pose\n\nplease provide type hint for the parameter: draw\n\nplease provide type hint for the parameter: display",
    "line_number": 25,
    "enriched": "File: computer_vision/Pose_detection.py\nCode: @@ -0,0 +1,479 @@\n+# This python program is made by me (Sahil Hooda) to simulate a simple interactive game(Subway Surfers) in real life.\n+# Using the power of openCV (Computer Vision) it can be used to play the game in real life by capturing body movements.                               \n+\n+import cv2\n+import pyautogui\n+from time import time\n+from math import hypot\n+import mediapipe as mp\n+import matplotlib.pyplot as plt\n+\n+# Initialize mediapipe pose class.\n+mp_pose = mp.solutions.pose\n+\n+# Setup the Pose function for images.\n+pose_image = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=1)\n+\n+# Setup the Pose function for videos.\n+pose_video = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.7,\n+                          min_tracking_confidence=0.7)\n+\n+# Initialize mediapipe drawing class.\n+mp_drawing = mp.solutions.drawing_utils\n+\n+\n+def detectPose(image, pose, draw=False, display=False):\nComment: As there is no test file in this pull request nor any test function or class in the file `computer_vision/Pose_detection.py`, please provide doctest for the function `detectPose`\n\nPlease provide return type hint for the function: `detectPose`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `detectPose`\n\nPlease provide type hint for the parameter: `image`\n\nPlease provide type hint for the parameter: `pose`\n\nPlease provide type hint for the parameter: `draw`\n\nPlease provide type hint for the parameter: `display`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "computer_vision/Pose_detection.py",
    "pr_number": 10333,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1357150058,
    "comment_created_at": "2023-10-12T17:17:45Z"
  },
  {
    "code": "@@ -0,0 +1,28 @@\n+def vernam_encrypt(plaintext, key):",
    "comment": "please provide return type hint for the function: vernam_encrypt. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file ciphers/vernam_cipher.py, please provide doctest for the function vernam_encrypt\n\nplease provide type hint for the parameter: plaintext\n\nplease provide type hint for the parameter: key",
    "line_number": 1,
    "enriched": "File: ciphers/vernam_cipher.py\nCode: @@ -0,0 +1,28 @@\n+def vernam_encrypt(plaintext, key):\nComment: Please provide return type hint for the function: `vernam_encrypt`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `ciphers/vernam_cipher.py`, please provide doctest for the function `vernam_encrypt`\n\nPlease provide type hint for the parameter: `plaintext`\n\nPlease provide type hint for the parameter: `key`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "ciphers/vernam_cipher.py",
    "pr_number": 10702,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1365878182,
    "comment_created_at": "2023-10-19T17:15:36Z"
  },
  {
    "code": "@@ -0,0 +1,22 @@\n+\"\"\"\n+Features from accelerated segment test (FAST) Corner Detector\n+https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test\n+\"\"\"\n+\n+import cv2 as cv\n+\n+def fast_detector(img):",
    "comment": "please provide return type hint for the function: fast_detector. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file computer_vision/fast_corner.py, please provide doctest for the function fast_detector\n\nplease provide type hint for the parameter: img",
    "line_number": 8,
    "enriched": "File: computer_vision/fast_corner.py\nCode: @@ -0,0 +1,22 @@\n+\"\"\"\n+Features from accelerated segment test (FAST) Corner Detector\n+https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test\n+\"\"\"\n+\n+import cv2 as cv\n+\n+def fast_detector(img):\nComment: Please provide return type hint for the function: `fast_detector`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `computer_vision/fast_corner.py`, please provide doctest for the function `fast_detector`\n\nPlease provide type hint for the parameter: `img`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "computer_vision/fast_corner.py",
    "pr_number": 8159,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1118120351,
    "comment_created_at": "2023-02-26T17:07:50Z"
  },
  {
    "code": "@@ -0,0 +1,26 @@\n+\"\"\"\n+https://en.wikipedia.org/wiki/Fast_inverse_square_root\n+Used in Quake III Arena\n+Source: https://ajcr.net/fast-inverse-square-root-python/\n+\"\"\"\n+from ctypes import POINTER, byref, c_float, c_int32, cast\n+\n+\n+def Q_rsqrt(x):",
    "comment": "please provide return type hint for the function: q_rsqrt. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: q_rsqrt\n\nas there is no test file in this pull request nor any test function or class in the file maths/fast_inverse_square_root.py, please provide doctest for the function q_rsqrt\n\nplease provide type hint for the parameter: x\n\nplease provide descriptive name for the parameter: x",
    "line_number": 9,
    "enriched": "File: maths/fast_inverse_square_root.py\nCode: @@ -0,0 +1,26 @@\n+\"\"\"\n+https://en.wikipedia.org/wiki/Fast_inverse_square_root\n+Used in Quake III Arena\n+Source: https://ajcr.net/fast-inverse-square-root-python/\n+\"\"\"\n+from ctypes import POINTER, byref, c_float, c_int32, cast\n+\n+\n+def Q_rsqrt(x):\nComment: Please provide return type hint for the function: `Q_rsqrt`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `Q_rsqrt`\n\nAs there is no test file in this pull request nor any test function or class in the file `maths/fast_inverse_square_root.py`, please provide doctest for the function `Q_rsqrt`\n\nPlease provide type hint for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/fast_inverse_square_root.py",
    "pr_number": 7824,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008571762,
    "comment_created_at": "2022-10-28T23:14:01Z"
  },
  {
    "code": "@@ -0,0 +1,141 @@\n+# Classification Problem\n+# Here we will find the best model for breast cancer dataset by trying different models on the dataset and then will improve our best model.\n+# https://scikit-learn.org/stable/datasets/toy_dataset.html\n+\n+import matplotlib.pyplot as plt\n+import numpy as np\n+import pandas as pd\n+import seaborn as sns\n+import sklearn.metrics\n+from sklearn.datasets import load_breast_cancer\n+from sklearn.ensemble import RandomForestClassifier\n+from sklearn.linear_model import LogisticRegression\n+from sklearn.model_selection import (\n+    GridSearchCV,\n+    cross_val_score,\n+    cross_validate,\n+    train_test_split,\n+)\n+from sklearn.neighbors import KNeighborsClassifier\n+from sklearn.svm import LinearSVC\n+\n+# Breast cancer wisconsin (diagnostic) Data Set Characteristics:\n+\n+# Attribute Information:\n+\n+#  radius (mean of distances from center to points on the perimeter)\n+#  texture (standard deviation of gray-scale values)\n+#  perimeter\n+#  area\n+#  smoothness (local variation in radius lengths)\n+#  compactness (perimeter^2 / area - 1.0)\n+#  concavity (severity of concave portions of the contour)\n+#  concave points (number of concave portions of the contour)\n+#  symmetry\n+#  fractal dimension (\u201ccoastline approximation\u201d - 1)\n+\n+\n+\n+cancer = load_breast_cancer()\n+cancer\n+cancer_df = pd.DataFrame(cancer[\"data\"], columns=cancer[\"feature_names\"])\n+cancer_df[\"target\"] = pd.DataFrame(cancer[\"target\"])\n+cancer_df\n+cancer_df.describe()\n+\n+cancer_df.isna().sum()\n+# this shows that there is no missing data\n+\n+cancer_df.dtypes\n+# this shows that each and every column is int or float type so we need not to convert them to integers\n+\n+cancer_df.target.value_counts()\n+# so this is a good dataset as there is a good ratio of both the values of target column\n+\n+cancer_df.corr()\n+# more the positive correlation more positive dependency is there\n+# and more the negative correlation more is the negative dependency\n+# and more closer to 0 means they are not related to each other\n+\n+fig, ax = plt.subplots(figsize=(32, 26))\n+ax = sns.heatmap(\n+    cancer_df.corr(), cmap=\"YlGnBu\", annot=True, fmt=\".2f\", linewidths=0.5, cbar=False\n+)\n+# this heatmap shows how different parameters are correlated to each other and helps in visualising it\n+# now we will divide our data into x and y i.e. input and output data\n+\n+np.random.seed(43)\n+x = cancer_df.drop(\"target\", axis=1)\n+y = cancer_df[\"target\"]\n+# we will not split our data into test and training set instead we will use cross validation (cv) which will automatically divide our data and give better results\n+\n+# lets list the models which we will use for this classification problem\n+models = {\n+    \"Random\": RandomForestClassifier(),\n+    \"Logistic\": LogisticRegression(max_iter=5000),\n+    \"KNN\": KNeighborsClassifier(),\n+    \"SVC\": LinearSVC(),\n+}\n+\n+# score is a list which will contain accuracy score for all the estimators\n+score = {}\n+for model_name, model in models.items():\n+    cv_score = cross_val_score(model, x, y, cv=5).mean()\n+    score[model_name] = cv_score * 100\n+score\n+\n+# mean function will give average of all the scores for various estimating metrices\n+def mean(score):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/multiple_models_4_one.py.py, please provide doctest for the function mean\n\nplease provide return type hint for the function: mean. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: score",
    "line_number": 88,
    "enriched": "File: machine_learning/multiple_models_4_one.py.py\nCode: @@ -0,0 +1,141 @@\n+# Classification Problem\n+# Here we will find the best model for breast cancer dataset by trying different models on the dataset and then will improve our best model.\n+# https://scikit-learn.org/stable/datasets/toy_dataset.html\n+\n+import matplotlib.pyplot as plt\n+import numpy as np\n+import pandas as pd\n+import seaborn as sns\n+import sklearn.metrics\n+from sklearn.datasets import load_breast_cancer\n+from sklearn.ensemble import RandomForestClassifier\n+from sklearn.linear_model import LogisticRegression\n+from sklearn.model_selection import (\n+    GridSearchCV,\n+    cross_val_score,\n+    cross_validate,\n+    train_test_split,\n+)\n+from sklearn.neighbors import KNeighborsClassifier\n+from sklearn.svm import LinearSVC\n+\n+# Breast cancer wisconsin (diagnostic) Data Set Characteristics:\n+\n+# Attribute Information:\n+\n+#  radius (mean of distances from center to points on the perimeter)\n+#  texture (standard deviation of gray-scale values)\n+#  perimeter\n+#  area\n+#  smoothness (local variation in radius lengths)\n+#  compactness (perimeter^2 / area - 1.0)\n+#  concavity (severity of concave portions of the contour)\n+#  concave points (number of concave portions of the contour)\n+#  symmetry\n+#  fractal dimension (\u201ccoastline approximation\u201d - 1)\n+\n+\n+\n+cancer = load_breast_cancer()\n+cancer\n+cancer_df = pd.DataFrame(cancer[\"data\"], columns=cancer[\"feature_names\"])\n+cancer_df[\"target\"] = pd.DataFrame(cancer[\"target\"])\n+cancer_df\n+cancer_df.describe()\n+\n+cancer_df.isna().sum()\n+# this shows that there is no missing data\n+\n+cancer_df.dtypes\n+# this shows that each and every column is int or float type so we need not to convert them to integers\n+\n+cancer_df.target.value_counts()\n+# so this is a good dataset as there is a good ratio of both the values of target column\n+\n+cancer_df.corr()\n+# more the positive correlation more positive dependency is there\n+# and more the negative correlation more is the negative dependency\n+# and more closer to 0 means they are not related to each other\n+\n+fig, ax = plt.subplots(figsize=(32, 26))\n+ax = sns.heatmap(\n+    cancer_df.corr(), cmap=\"YlGnBu\", annot=True, fmt=\".2f\", linewidths=0.5, cbar=False\n+)\n+# this heatmap shows how different parameters are correlated to each other and helps in visualising it\n+# now we will divide our data into x and y i.e. input and output data\n+\n+np.random.seed(43)\n+x = cancer_df.drop(\"target\", axis=1)\n+y = cancer_df[\"target\"]\n+# we will not split our data into test and training set instead we will use cross validation (cv) which will automatically divide our data and give better results\n+\n+# lets list the models which we will use for this classification problem\n+models = {\n+    \"Random\": RandomForestClassifier(),\n+    \"Logistic\": LogisticRegression(max_iter=5000),\n+    \"KNN\": KNeighborsClassifier(),\n+    \"SVC\": LinearSVC(),\n+}\n+\n+# score is a list which will contain accuracy score for all the estimators\n+score = {}\n+for model_name, model in models.items():\n+    cv_score = cross_val_score(model, x, y, cv=5).mean()\n+    score[model_name] = cv_score * 100\n+score\n+\n+# mean function will give average of all the scores for various estimating metrices\n+def mean(score):\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/multiple_models_4_one.py.py`, please provide doctest for the function `mean`\n\nPlease provide return type hint for the function: `mean`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `score`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/multiple_models_4_one.py.py",
    "pr_number": 6944,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 991615336,
    "comment_created_at": "2022-10-10T20:21:36Z"
  },
  {
    "code": "@@ -0,0 +1,154 @@\n+\n+\"\"\"\n+Gravitational Search Algorithm\n+\n+Author - Vivek Kumar Sahu\n+Email- vivekkumarsahu979@gmail.com\n+\n+Requirements:\n+  - numpy\n+  - matplotlib (for visualization, optional)\n+Python:\n+  - 3.6 or higher\n+Inputs:\n+  - n_agents, number of agents (int)\n+  - n_dimensions, number of problem dimensions (int)\n+  - n_iterations, number of iterations for the algorithm (int)\n+  - search_space, a list of tuples defining the search space for each dimension\n+  - objective_function, a function that takes an agent's position and returns its fitness value\n+  - G0, initial gravitational constant (float)\n+  - alpha, gravitational scaling factor (float)\n+  - beta, attraction scaling factor (float)\n+Usage:\n+  1. Define the problem by specifying the search space and objective function.\n+  2. Initialize the algorithm with parameters: n_agents, n_dimensions, n_iterations, search_space, objective_function, G0, alpha, beta.\n+  3. Run the algorithm using the `run_gsa` function.\n+  4. Access the best solution and fitness value after the algorithm converges.\n+\"\"\"\n+from typing import List, Tuple, Callable\n+import numpy as np\n+\n+def objective_function(x: np.ndarray) -> float:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/gravitational_search_algorithm.py, please provide doctest for the function objective_function\n\nplease provide descriptive name for the parameter: x",
    "line_number": 31,
    "enriched": "File: machine_learning/gravitational_search_algorithm.py\nCode: @@ -0,0 +1,154 @@\n+\n+\"\"\"\n+Gravitational Search Algorithm\n+\n+Author - Vivek Kumar Sahu\n+Email- vivekkumarsahu979@gmail.com\n+\n+Requirements:\n+  - numpy\n+  - matplotlib (for visualization, optional)\n+Python:\n+  - 3.6 or higher\n+Inputs:\n+  - n_agents, number of agents (int)\n+  - n_dimensions, number of problem dimensions (int)\n+  - n_iterations, number of iterations for the algorithm (int)\n+  - search_space, a list of tuples defining the search space for each dimension\n+  - objective_function, a function that takes an agent's position and returns its fitness value\n+  - G0, initial gravitational constant (float)\n+  - alpha, gravitational scaling factor (float)\n+  - beta, attraction scaling factor (float)\n+Usage:\n+  1. Define the problem by specifying the search space and objective function.\n+  2. Initialize the algorithm with parameters: n_agents, n_dimensions, n_iterations, search_space, objective_function, G0, alpha, beta.\n+  3. Run the algorithm using the `run_gsa` function.\n+  4. Access the best solution and fitness value after the algorithm converges.\n+\"\"\"\n+from typing import List, Tuple, Callable\n+import numpy as np\n+\n+def objective_function(x: np.ndarray) -> float:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/gravitational_search_algorithm.py`, please provide doctest for the function `objective_function`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/gravitational_search_algorithm.py",
    "pr_number": 9836,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347639988,
    "comment_created_at": "2023-10-05T15:47:43Z"
  },
  {
    "code": "@@ -21,6 +21,56 @@ def create_state_space_tree(\n     Creates a state space tree to iterate through each branch using DFS.\n     We know that each state has exactly two children.\n     It terminates when it reaches the end of the given sequence.\n+\n+        :param sequence: The input sequence for which subsequences are generated.",
    "comment": "fix indentation",
    "line_number": 25,
    "enriched": "File: backtracking/all_subsequences.py\nCode: @@ -21,6 +21,56 @@ def create_state_space_tree(\n     Creates a state space tree to iterate through each branch using DFS.\n     We know that each state has exactly two children.\n     It terminates when it reaches the end of the given sequence.\n+\n+        :param sequence: The input sequence for which subsequences are generated.\nComment: ```suggestion\r\n    :param sequence: The input sequence for which subsequences are generated.\r\n```\r\nFix indentation",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "backtracking/all_subsequences.py",
    "pr_number": 10252,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1623732622,
    "comment_created_at": "2024-06-03T02:54:24Z"
  },
  {
    "code": "@@ -0,0 +1,57 @@\n+import numpy as np\n+from matplotlib import pyplot as plt\n+from sklearn.datasets import load_iris\n+from sklearn.metrics import ConfusionMatrixDisplay\n+from sklearn.model_selection import train_test_split\n+from sklearn.ensemble import ExtraTreesClassifier\n+from sklearn.metrics import recall_score, f1_score, precision_score, accuracy_score\n+\n+\n+def data_split(data):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/extra_trees_classifier.py, please provide doctest for the function data_split\n\nplease provide return type hint for the function: data_split. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: data",
    "line_number": 10,
    "enriched": "File: machine_learning/extra_trees_classifier.py\nCode: @@ -0,0 +1,57 @@\n+import numpy as np\n+from matplotlib import pyplot as plt\n+from sklearn.datasets import load_iris\n+from sklearn.metrics import ConfusionMatrixDisplay\n+from sklearn.model_selection import train_test_split\n+from sklearn.ensemble import ExtraTreesClassifier\n+from sklearn.metrics import recall_score, f1_score, precision_score, accuracy_score\n+\n+\n+def data_split(data):\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/extra_trees_classifier.py`, please provide doctest for the function `data_split`\n\nPlease provide return type hint for the function: `data_split`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `data`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/extra_trees_classifier.py",
    "pr_number": 7860,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008777200,
    "comment_created_at": "2022-10-30T01:16:58Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+# Python program for implementation of Bubble Sort\r\n+\r\n+def bubbleSort(arr):\r",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: bubblesort\n\nplease provide return type hint for the function: bubblesort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file dsa/all_sorting/bubble_sort.py, please provide doctest for the function bubblesort\n\nplease provide type hint for the parameter: arr",
    "line_number": 3,
    "enriched": "File: dsa/all_sorting/bubble_sort.py\nCode: @@ -0,0 +1,35 @@\n+# Python program for implementation of Bubble Sort\r\n+\r\n+def bubbleSort(arr):\r\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `bubbleSort`\n\nPlease provide return type hint for the function: `bubbleSort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `dsa/all_sorting/bubble_sort.py`, please provide doctest for the function `bubbleSort`\n\nPlease provide type hint for the parameter: `arr`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dsa/all_sorting/bubble_sort.py",
    "pr_number": 10848,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368774540,
    "comment_created_at": "2023-10-23T14:29:32Z"
  },
  {
    "code": "@@ -0,0 +1,89 @@\n+# Python3 program for the above approach\n+\n+# Number of vertices in the graph\n+# define 4 4\n+\n+# check if the colored\n+# graph is safe or not\n+\n+\n+def isSafe(graph, color):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file backtracking/m_coloringproblem.py, please provide doctest for the function issafe\n\nplease provide return type hint for the function: issafe. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: issafe\n\nplease provide type hint for the parameter: graph\n\nplease provide type hint for the parameter: color",
    "line_number": 10,
    "enriched": "File: backtracking/m_coloringproblem.py\nCode: @@ -0,0 +1,89 @@\n+# Python3 program for the above approach\n+\n+# Number of vertices in the graph\n+# define 4 4\n+\n+# check if the colored\n+# graph is safe or not\n+\n+\n+def isSafe(graph, color):\nComment: As there is no test file in this pull request nor any test function or class in the file `backtracking/m_coloringproblem.py`, please provide doctest for the function `isSafe`\n\nPlease provide return type hint for the function: `isSafe`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `isSafe`\n\nPlease provide type hint for the parameter: `graph`\n\nPlease provide type hint for the parameter: `color`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "backtracking/m_coloringproblem.py",
    "pr_number": 7618,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004126120,
    "comment_created_at": "2022-10-25T07:52:00Z"
  },
  {
    "code": "@@ -0,0 +1,451 @@\n+\"\"\"\n+Simple Interactive 3D Renderer in Tkinter.\n+\n+- Demonstrates 3D meshes (Cube), perspective projection, camera movement/rotation,\n+  lighting, and triangle sorting for correct display order.\n+- Key controls: WASD/Arrow keys to move/rotate camera, Shift/Space to move up/down.\n+\n+References:\n+https://en.wikipedia.org/wiki/3D_projection\n+\"\"\"\n+\n+import tkinter as tk\n+import math\n+\n+\n+class Vector3D:\n+    \"\"\"\n+    3D vector class with basic arithmetic, geometric, and transformation operations.\n+\n+    Examples:\n+        >>> v1 = Vector3D(1, 2, 3)\n+        >>> v2 = Vector3D(4, 5, 6)\n+        >>> v1 + v2\n+        Vector3D(5, 7, 9)\n+        >>> v1 - v2\n+        Vector3D(-3, -3, -3)\n+        >>> v1 * 2\n+        Vector3D(2, 4, 6)\n+        >>> v1.dot(v2)\n+        32\n+        >>> v1.cross(v2)\n+        Vector3D(-3, 6, -3)\n+        >>> round(v1.magnitude(), 9)\n+        3.741657387\n+        >>> v1.normalize()\n+        Vector3D(0.2672612419124244, 0.5345224838248488, 0.8017837257372732)\n+        >>> v1.rotate(0, 90, 0)\n+        Vector3D(3.0, 2.0, -1.0)\n+    \"\"\"\n+\n+    def __init__(self, x: float = 0.0, y: float = 0.0, z: float = 0.0):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide descriptive name for the parameter: x\n\nplease provide descriptive name for the parameter: y\n\nplease provide descriptive name for the parameter: z",
    "line_number": 41,
    "enriched": "File: graphics/interactive_3d_renderer.py\nCode: @@ -0,0 +1,451 @@\n+\"\"\"\n+Simple Interactive 3D Renderer in Tkinter.\n+\n+- Demonstrates 3D meshes (Cube), perspective projection, camera movement/rotation,\n+  lighting, and triangle sorting for correct display order.\n+- Key controls: WASD/Arrow keys to move/rotate camera, Shift/Space to move up/down.\n+\n+References:\n+https://en.wikipedia.org/wiki/3D_projection\n+\"\"\"\n+\n+import tkinter as tk\n+import math\n+\n+\n+class Vector3D:\n+    \"\"\"\n+    3D vector class with basic arithmetic, geometric, and transformation operations.\n+\n+    Examples:\n+        >>> v1 = Vector3D(1, 2, 3)\n+        >>> v2 = Vector3D(4, 5, 6)\n+        >>> v1 + v2\n+        Vector3D(5, 7, 9)\n+        >>> v1 - v2\n+        Vector3D(-3, -3, -3)\n+        >>> v1 * 2\n+        Vector3D(2, 4, 6)\n+        >>> v1.dot(v2)\n+        32\n+        >>> v1.cross(v2)\n+        Vector3D(-3, 6, -3)\n+        >>> round(v1.magnitude(), 9)\n+        3.741657387\n+        >>> v1.normalize()\n+        Vector3D(0.2672612419124244, 0.5345224838248488, 0.8017837257372732)\n+        >>> v1.rotate(0, 90, 0)\n+        Vector3D(3.0, 2.0, -1.0)\n+    \"\"\"\n+\n+    def __init__(self, x: float = 0.0, y: float = 0.0, z: float = 0.0):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide descriptive name for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `y`\n\nPlease provide descriptive name for the parameter: `z`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "graphics/interactive_3d_renderer.py",
    "pr_number": 13064,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2395551289,
    "comment_created_at": "2025-10-01T18:53:55Z"
  },
  {
    "code": "@@ -0,0 +1,395 @@\n+\"\"\"\n+Creates a random wordsearch with eight different directions\n+that are best described as compass locations.\n+\n+@ https://en.wikipedia.org/wiki/Word_search\n+\"\"\"\n+\n+\n+from random import choice, randint, shuffle\n+\n+# The words to display on the word search -\n+# can be made dynamic by randonly selecting a certain number of\n+# words from a predefined word file, while ensuring the character\n+# count fits within the matrix size (n x m)\n+WORDS = [\"cat\", \"dog\", \"snake\", \"fish\"]\n+\n+WIDTH = 10\n+HEIGHT = 10\n+\n+\n+class WordSearch:\n+    \"\"\"\n+    >>> ws = WordSearch(WORDS, WIDTH, HEIGHT)\n+    >>> ws.board  # doctest: +ELLIPSIS\n+    [[None, ..., None], ..., [None, ..., None]]\n+    >>> ws.generate_board()\n+    \"\"\"\n+\n+    def __init__(self, words: list[str], width: int, height: int) -> None:\n+        self.words = words\n+        self.width = width\n+        self.height = height\n+\n+        # Board matrix holding each letter\n+        self.board: list[list[str | None]] = [[None] * width for _ in range(height)]\n+\n+    def insert_north(self, word: str, rows: list[int], cols: list[int]) -> None:",
    "comment": "it'd help to have the block comment explain what rows and cols are for",
    "line_number": 37,
    "enriched": "File: other/word_search.py\nCode: @@ -0,0 +1,395 @@\n+\"\"\"\n+Creates a random wordsearch with eight different directions\n+that are best described as compass locations.\n+\n+@ https://en.wikipedia.org/wiki/Word_search\n+\"\"\"\n+\n+\n+from random import choice, randint, shuffle\n+\n+# The words to display on the word search -\n+# can be made dynamic by randonly selecting a certain number of\n+# words from a predefined word file, while ensuring the character\n+# count fits within the matrix size (n x m)\n+WORDS = [\"cat\", \"dog\", \"snake\", \"fish\"]\n+\n+WIDTH = 10\n+HEIGHT = 10\n+\n+\n+class WordSearch:\n+    \"\"\"\n+    >>> ws = WordSearch(WORDS, WIDTH, HEIGHT)\n+    >>> ws.board  # doctest: +ELLIPSIS\n+    [[None, ..., None], ..., [None, ..., None]]\n+    >>> ws.generate_board()\n+    \"\"\"\n+\n+    def __init__(self, words: list[str], width: int, height: int) -> None:\n+        self.words = words\n+        self.width = width\n+        self.height = height\n+\n+        # Board matrix holding each letter\n+        self.board: list[list[str | None]] = [[None] * width for _ in range(height)]\n+\n+    def insert_north(self, word: str, rows: list[int], cols: list[int]) -> None:\nComment: It'd help to have the block comment explain what `rows` and `cols` are for",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "other/word_search.py",
    "pr_number": 8906,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1315368431,
    "comment_created_at": "2023-09-05T05:05:30Z"
  },
  {
    "code": "@@ -0,0 +1,10 @@\n+def sublist (l):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/sublist.py, please provide doctest for the function sublist\n\nplease provide return type hint for the function: sublist. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: l\n\nplease provide descriptive name for the parameter: l",
    "line_number": 1,
    "enriched": "File: data_structures/Sublist.py\nCode: @@ -0,0 +1,10 @@\n+def sublist (l):\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/Sublist.py`, please provide doctest for the function `sublist`\n\nPlease provide return type hint for the function: `sublist`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `l`\n\nPlease provide descriptive name for the parameter: `l`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/Sublist.py",
    "pr_number": 7325,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996466537,
    "comment_created_at": "2022-10-16T16:56:48Z"
  },
  {
    "code": "@@ -0,0 +1,55 @@\n+\"\"\"\n+References for Binary, Octal, and Hexadecimal Numbers\n+\n+https://en.wikipedia.org/wiki/Binary_number\n+https://en.wikipedia.org/wiki/Octal\n+https://en.wikipedia.org/wiki/Hexadecimal\n+\n+\"\"\"\n+\n+def DecimalConversions(dec):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/decimalconversions.py, please provide doctest for the function decimalconversions\n\nplease provide return type hint for the function: decimalconversions. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: decimalconversions\n\nplease provide type hint for the parameter: dec",
    "line_number": 10,
    "enriched": "File: maths/DecimalConversions.py\nCode: @@ -0,0 +1,55 @@\n+\"\"\"\n+References for Binary, Octal, and Hexadecimal Numbers\n+\n+https://en.wikipedia.org/wiki/Binary_number\n+https://en.wikipedia.org/wiki/Octal\n+https://en.wikipedia.org/wiki/Hexadecimal\n+\n+\"\"\"\n+\n+def DecimalConversions(dec):\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/DecimalConversions.py`, please provide doctest for the function `DecimalConversions`\n\nPlease provide return type hint for the function: `DecimalConversions`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `DecimalConversions`\n\nPlease provide type hint for the parameter: `dec`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/DecimalConversions.py",
    "pr_number": 7192,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996101096,
    "comment_created_at": "2022-10-14T20:08:55Z"
  },
  {
    "code": "@@ -0,0 +1,62 @@\n+#!/usr/bin/env python3\r\n+\r\n+def build_suffix_array(s: str) -> list[int]:\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file divide_and_conquer/suffix_array_lcp.py, please provide doctest for the function build_suffix_array\n\nplease provide descriptive name for the parameter: s",
    "line_number": 3,
    "enriched": "File: divide_and_conquer/suffix_array_lcp.py\nCode: @@ -0,0 +1,62 @@\n+#!/usr/bin/env python3\r\n+\r\n+def build_suffix_array(s: str) -> list[int]:\r\nComment: As there is no test file in this pull request nor any test function or class in the file `divide_and_conquer/suffix_array_lcp.py`, please provide doctest for the function `build_suffix_array`\n\nPlease provide descriptive name for the parameter: `s`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "divide_and_conquer/suffix_array_lcp.py",
    "pr_number": 12161,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1807232520,
    "comment_created_at": "2024-10-19T07:53:30Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+#url: https://en.wikipedia.org/wiki/Law_of_cosines\n+\n+\n+import math\n+\n+def law_of_cosines(a, b, angle_c):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/law_of_cosines.py, please provide doctest for the function law_of_cosines\n\nplease provide return type hint for the function: law_of_cosines. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: a\n\nplease provide descriptive name for the parameter: a\n\nplease provide type hint for the parameter: b\n\nplease provide descriptive name for the parameter: b\n\nplease provide type hint for the parameter: angle_c",
    "line_number": 6,
    "enriched": "File: maths/law_of_cosines.py\nCode: @@ -0,0 +1,35 @@\n+#url: https://en.wikipedia.org/wiki/Law_of_cosines\n+\n+\n+import math\n+\n+def law_of_cosines(a, b, angle_c):\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/law_of_cosines.py`, please provide doctest for the function `law_of_cosines`\n\nPlease provide return type hint for the function: `law_of_cosines`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `a`\n\nPlease provide descriptive name for the parameter: `a`\n\nPlease provide type hint for the parameter: `b`\n\nPlease provide descriptive name for the parameter: `b`\n\nPlease provide type hint for the parameter: `angle_c`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/law_of_cosines.py",
    "pr_number": 9594,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1344114632,
    "comment_created_at": "2023-10-03T13:32:13Z"
  },
  {
    "code": "@@ -73,6 +73,23 @@ That's it! The plugin will run every time you commit any changes. If there are a\n pre-commit run --all-files --show-diff-on-failure\n ```\n \n+\n+ <details>\n+  <summary>Hint for windows users</summary>\n+  <br/>\n+  On Windows, the python3 command is not recognized. This can lead to the error below.",
    "comment": "windows users should use py\r\n\r\nor in a [venv](https://docs.python.org/3/library/venv.html), everyone should use python",
    "line_number": 80,
    "enriched": "File: CONTRIBUTING.md\nCode: @@ -73,6 +73,23 @@ That's it! The plugin will run every time you commit any changes. If there are a\n pre-commit run --all-files --show-diff-on-failure\n ```\n \n+\n+ <details>\n+  <summary>Hint for windows users</summary>\n+  <br/>\n+  On Windows, the python3 command is not recognized. This can lead to the error below.\nComment: Windows users should use `py`\r\n\r\nOr in a [venv](https://docs.python.org/3/library/venv.html), everyone should use `python`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "CONTRIBUTING.md",
    "pr_number": 12369,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1837141235,
    "comment_created_at": "2024-11-11T20:07:36Z"
  },
  {
    "code": "@@ -0,0 +1,34 @@\n+def job_sequence_with_deadlines(jobs):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file greedy_methods/job_sequencing.py, please provide doctest for the function job_sequence_with_deadlines\n\nplease provide return type hint for the function: job_sequence_with_deadlines. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: jobs",
    "line_number": 1,
    "enriched": "File: greedy_methods/job_sequencing.py\nCode: @@ -0,0 +1,34 @@\n+def job_sequence_with_deadlines(jobs):\nComment: As there is no test file in this pull request nor any test function or class in the file `greedy_methods/job_sequencing.py`, please provide doctest for the function `job_sequence_with_deadlines`\n\nPlease provide return type hint for the function: `job_sequence_with_deadlines`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `jobs`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "greedy_methods/job_sequencing.py",
    "pr_number": 10668,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1363586208,
    "comment_created_at": "2023-10-18T09:51:50Z"
  },
  {
    "code": "@@ -0,0 +1,170 @@\n+\"\"\"\n+Problem Statement: Given a binary perform an postorder traversal using Morris Postorder\n+traversal algorithm. (Iterative version of Postorder traversal of tree)\n+\n+https://www.geeksforgeeks.org/morris-traversal-for-postorder/\n+\"\"\"\n+\n+\n+class TreeNode:\n+    \"\"\"\n+    Class representing a node in a binary tree.\n+\n+    Attributes:\n+    -----------\n+    value : int\n+        The value stored at the node.\n+    left : TreeNode\n+        Pointer to the left child node (default is None).\n+    right : TreeNode\n+        Pointer to the right child node (default is None).\n+    \"\"\"\n+\n+    def __init__(self, value: int) -> None:\n+        self.value = value\n+        self.left: TreeNode | None = None\n+        self.right: TreeNode | None = None\n+\n+\n+class BinaryTree:\n+    \"\"\"\n+    Class representing a binary tree.\n+\n+    Methods:\n+    --------\n+    insert(value: int) -> None:\n+        Insert a value into the binary tree following binary search tree (BST) rules.\n+\n+    morris_postorder_traversal() -> List[int]:\n+        Perform postorder traversal and return list of node values.\n+\n+\n+    >>> bt = BinaryTree()\n+    >>> bt.insert(9)\n+    >>> bt.insert(6)\n+    >>> bt.insert(10)\n+    >>> bt.insert(3)\n+    >>> bt.insert(7)\n+    >>> bt.insert(12)\n+    >>> bt.insert(2)\n+    >>> bt.insert(5)\n+    >>> bt.insert(4)\n+    >>> bt.morris_postorder_traversal()\n+    [2, 4, 5, 3, 7, 6, 12, 10, 9]\n+\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        self.root: TreeNode | None = None\n+\n+    def insert(self, value: int) -> None:\n+        \"\"\"\n+        Insert a value into the binary tree.\n+\n+        Parameters:\n+        -----------\n+        value : int\n+            The value to be inserted into the binary tree.\n+        \"\"\"\n+        if self.root is None:\n+            self.root = TreeNode(value)\n+        else:\n+            self._insert_recursive(self.root, value)\n+\n+    def _insert_recursive(self, node: TreeNode, value: int) -> None:\n+        \"\"\"\n+        Helper function to insert a value recursively into the tree.\n+\n+        Parameters:\n+        -----------\n+        node : TreeNode\n+            The current node in the binary tree.\n+        value : int\n+            The value to be inserted.\n+        \"\"\"\n+        if value < node.value:\n+            if node.left is None:\n+                node.left = TreeNode(value)\n+            else:\n+                self._insert_recursive(node.left, value)\n+        elif node.right is None:\n+            node.right = TreeNode(value)\n+        else:\n+            self._insert_recursive(node.right, value)\n+\n+    def _successor(self, node: TreeNode) -> TreeNode:\n+        \"\"\"\n+        Helper Function to return successor of the given node in a binary tree\n+\n+        Parameters:\n+        -----------\n+        node : TreeNode\n+            A node in the binary tree.\n+\n+        Returns:\n+        --------\n+        TreeNode:\n+            The successor of the node passed in the parameter\n+        \"\"\"",
    "comment": "see my comment on your previous pr: https://github.com/thealgorithms/python/pull/11774#discussion_r1788653204",
    "line_number": 108,
    "enriched": "File: data_structures/binary_tree/morris_postorder_traversal.py\nCode: @@ -0,0 +1,170 @@\n+\"\"\"\n+Problem Statement: Given a binary perform an postorder traversal using Morris Postorder\n+traversal algorithm. (Iterative version of Postorder traversal of tree)\n+\n+https://www.geeksforgeeks.org/morris-traversal-for-postorder/\n+\"\"\"\n+\n+\n+class TreeNode:\n+    \"\"\"\n+    Class representing a node in a binary tree.\n+\n+    Attributes:\n+    -----------\n+    value : int\n+        The value stored at the node.\n+    left : TreeNode\n+        Pointer to the left child node (default is None).\n+    right : TreeNode\n+        Pointer to the right child node (default is None).\n+    \"\"\"\n+\n+    def __init__(self, value: int) -> None:\n+        self.value = value\n+        self.left: TreeNode | None = None\n+        self.right: TreeNode | None = None\n+\n+\n+class BinaryTree:\n+    \"\"\"\n+    Class representing a binary tree.\n+\n+    Methods:\n+    --------\n+    insert(value: int) -> None:\n+        Insert a value into the binary tree following binary search tree (BST) rules.\n+\n+    morris_postorder_traversal() -> List[int]:\n+        Perform postorder traversal and return list of node values.\n+\n+\n+    >>> bt = BinaryTree()\n+    >>> bt.insert(9)\n+    >>> bt.insert(6)\n+    >>> bt.insert(10)\n+    >>> bt.insert(3)\n+    >>> bt.insert(7)\n+    >>> bt.insert(12)\n+    >>> bt.insert(2)\n+    >>> bt.insert(5)\n+    >>> bt.insert(4)\n+    >>> bt.morris_postorder_traversal()\n+    [2, 4, 5, 3, 7, 6, 12, 10, 9]\n+\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        self.root: TreeNode | None = None\n+\n+    def insert(self, value: int) -> None:\n+        \"\"\"\n+        Insert a value into the binary tree.\n+\n+        Parameters:\n+        -----------\n+        value : int\n+            The value to be inserted into the binary tree.\n+        \"\"\"\n+        if self.root is None:\n+            self.root = TreeNode(value)\n+        else:\n+            self._insert_recursive(self.root, value)\n+\n+    def _insert_recursive(self, node: TreeNode, value: int) -> None:\n+        \"\"\"\n+        Helper function to insert a value recursively into the tree.\n+\n+        Parameters:\n+        -----------\n+        node : TreeNode\n+            The current node in the binary tree.\n+        value : int\n+            The value to be inserted.\n+        \"\"\"\n+        if value < node.value:\n+            if node.left is None:\n+                node.left = TreeNode(value)\n+            else:\n+                self._insert_recursive(node.left, value)\n+        elif node.right is None:\n+            node.right = TreeNode(value)\n+        else:\n+            self._insert_recursive(node.right, value)\n+\n+    def _successor(self, node: TreeNode) -> TreeNode:\n+        \"\"\"\n+        Helper Function to return successor of the given node in a binary tree\n+\n+        Parameters:\n+        -----------\n+        node : TreeNode\n+            A node in the binary tree.\n+\n+        Returns:\n+        --------\n+        TreeNode:\n+            The successor of the node passed in the parameter\n+        \"\"\"\nComment: ```suggestion\r\n        Parameters:\r\n        -----------\r\n        node: A node in the binary tree.\r\n\r\n        Returns:\r\n        --------\r\n        The successor of the node passed in the parameter\r\n        \"\"\"\r\n```\r\nSee my comment on your previous PR: https://github.com/TheAlgorithms/Python/pull/11774#discussion_r1788653204",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "data_structures/binary_tree/morris_postorder_traversal.py",
    "pr_number": 11775,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1788653708,
    "comment_created_at": "2024-10-05T17:54:36Z"
  },
  {
    "code": "@@ -0,0 +1,44 @@\n+class Node:\r\n+    def __init__(self,data:int)->None:\r\n+        self.data=data\r\n+        self.ref= None \r\n+class LinkedList:\r\n+    def __init__(self)->None:\r\n+        self.head=None\r\n+    def print_ll(self):\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/linked_list/add_node.py, please provide doctest for the function print_ll\n\nplease provide return type hint for the function: print_ll. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 8,
    "enriched": "File: data_structures/linked_list/add_node.py\nCode: @@ -0,0 +1,44 @@\n+class Node:\r\n+    def __init__(self,data:int)->None:\r\n+        self.data=data\r\n+        self.ref= None \r\n+class LinkedList:\r\n+    def __init__(self)->None:\r\n+        self.head=None\r\n+    def print_ll(self):\r\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/linked_list/add_node.py`, please provide doctest for the function `print_ll`\n\nPlease provide return type hint for the function: `print_ll`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/linked_list/add_node.py",
    "pr_number": 9308,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342158359,
    "comment_created_at": "2023-10-01T16:24:24Z"
  },
  {
    "code": "@@ -0,0 +1,106 @@\n+import cv2\n+import numpy as np\n+\n+\n+def backward_warping(M, img):",
    "comment": "please provide return type hint for the function: backward_warping. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file digital_image_processing/backward_warping/backward_warping.py, please provide doctest for the function backward_warping\n\nplease provide type hint for the parameter: m\n\nplease provide descriptive name for the parameter: m\n\nplease provide type hint for the parameter: img",
    "line_number": 5,
    "enriched": "File: digital_image_processing/backward_warping/backward_warping.py\nCode: @@ -0,0 +1,106 @@\n+import cv2\n+import numpy as np\n+\n+\n+def backward_warping(M, img):\nComment: Please provide return type hint for the function: `backward_warping`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `digital_image_processing/backward_warping/backward_warping.py`, please provide doctest for the function `backward_warping`\n\nPlease provide type hint for the parameter: `M`\n\nPlease provide descriptive name for the parameter: `M`\n\nPlease provide type hint for the parameter: `img`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "digital_image_processing/backward_warping/backward_warping.py",
    "pr_number": 8014,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1038970023,
    "comment_created_at": "2022-12-04T13:23:05Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+def lexical_order(n: int) -> str:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/stacks/lexicographical_numbers.py, please provide doctest for the function lexical_order\n\nplease provide descriptive name for the parameter: n",
    "line_number": 1,
    "enriched": "File: data_structures/stacks/lexicographical_numbers.py\nCode: @@ -0,0 +1,21 @@\n+def lexical_order(n: int) -> str:\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/stacks/lexicographical_numbers.py`, please provide doctest for the function `lexical_order`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/stacks/lexicographical_numbers.py",
    "pr_number": 11674,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1784294355,
    "comment_created_at": "2024-10-02T10:48:41Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+# Given an array of N non-negative integers arr[] representing an elevation map where the width of each bar is 1,\n+# compute how much water it is able to trap after raining.\n+# Input: arr[] = {2, 0, 2}\n+# Output: 2\n+# Input: arr[]   = {3, 0, 2, 0, 4}\n+# Output: 7\n+\n+# Function to return the maximum\n+# water that can be stored\n+def maxWater(height):",
    "comment": "please provide return type hint for the function: maxwater. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: maxwater\n\nas there is no test file in this pull request nor any test function or class in the file data_structures/stacks/trappingrainwater.py, please provide doctest for the function maxwater\n\nplease provide type hint for the parameter: height",
    "line_number": 10,
    "enriched": "File: data_structures/stacks/TrappingRainWater.py\nCode: @@ -0,0 +1,56 @@\n+# Given an array of N non-negative integers arr[] representing an elevation map where the width of each bar is 1,\n+# compute how much water it is able to trap after raining.\n+# Input: arr[] = {2, 0, 2}\n+# Output: 2\n+# Input: arr[]   = {3, 0, 2, 0, 4}\n+# Output: 7\n+\n+# Function to return the maximum\n+# water that can be stored\n+def maxWater(height):\nComment: Please provide return type hint for the function: `maxWater`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `maxWater`\n\nAs there is no test file in this pull request nor any test function or class in the file `data_structures/stacks/TrappingRainWater.py`, please provide doctest for the function `maxWater`\n\nPlease provide type hint for the parameter: `height`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/stacks/TrappingRainWater.py",
    "pr_number": 7147,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 995383487,
    "comment_created_at": "2022-10-14T06:21:10Z"
  },
  {
    "code": "@@ -0,0 +1,37 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Instalation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interval_start: The start of your internal scale.\n+            - interval_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Exemples:\n+    interval_start:0\n+    interval_end:100\n+    number:50\n+    output: 50.0\n+    interval_start:6\n+    interval_end:12\n+    number:9\n+    output:50.0\n+\"\"\"\n+\n+\n+def absolute_conversion(interval_start: float, interval_end: float, number: float):",
    "comment": "please provide return type hint for the function: absolute_conversion. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file conversions/absolute_conversion.py, please provide doctest for the function absolute_conversion",
    "line_number": 24,
    "enriched": "File: conversions/absolute_conversion.py\nCode: @@ -0,0 +1,37 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Instalation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interval_start: The start of your internal scale.\n+            - interval_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Exemples:\n+    interval_start:0\n+    interval_end:100\n+    number:50\n+    output: 50.0\n+    interval_start:6\n+    interval_end:12\n+    number:9\n+    output:50.0\n+\"\"\"\n+\n+\n+def absolute_conversion(interval_start: float, interval_end: float, number: float):\nComment: Please provide return type hint for the function: `absolute_conversion`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `conversions/absolute_conversion.py`, please provide doctest for the function `absolute_conversion`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "conversions/absolute_conversion.py",
    "pr_number": 7249,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996338247,
    "comment_created_at": "2022-10-15T18:14:09Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+def longestValidParentheses(self, s: str) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file strings/longestvalidparanthesis.py, please provide doctest for the function longestvalidparentheses\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: longestvalidparentheses\n\nplease provide descriptive name for the parameter: s",
    "line_number": 1,
    "enriched": "File: strings/longestvalidparanthesis.py\nCode: @@ -0,0 +1,21 @@\n+def longestValidParentheses(self, s: str) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `strings/longestvalidparanthesis.py`, please provide doctest for the function `longestValidParentheses`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `longestValidParentheses`\n\nPlease provide descriptive name for the parameter: `s`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "strings/longestvalidparanthesis.py",
    "pr_number": 6921,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 991453632,
    "comment_created_at": "2022-10-10T16:12:15Z"
  },
  {
    "code": "@@ -0,0 +1,33 @@\n+# The idea of Kadane\u2019s algorithm is to maintain a variable max_ending_here\n+# hat stores the maximum sum contiguous subarray ending at current index and a\n+# variable max_so_far stores the maximum sum of contiguous subarray found so far,\n+# Everytime there is a positive-sum value in max_ending_here\n+# compare it with max_so_far and update max_so_far if it is greater than max_so_far.\n+# https://www.geeksforgeeks.org/largest-sum-contiguous-subarray/\n+\n+\n+def get_max_sum(arr, length):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file sliding_window_and_kadane_algorithm/kadane_algorithm.py, please provide doctest for the function get_max_sum\n\nplease provide return type hint for the function: get_max_sum. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: length",
    "line_number": 9,
    "enriched": "File: sliding_window_and_kadane_algorithm/kadane_algorithm.py\nCode: @@ -0,0 +1,33 @@\n+# The idea of Kadane\u2019s algorithm is to maintain a variable max_ending_here\n+# hat stores the maximum sum contiguous subarray ending at current index and a\n+# variable max_so_far stores the maximum sum of contiguous subarray found so far,\n+# Everytime there is a positive-sum value in max_ending_here\n+# compare it with max_so_far and update max_so_far if it is greater than max_so_far.\n+# https://www.geeksforgeeks.org/largest-sum-contiguous-subarray/\n+\n+\n+def get_max_sum(arr, length):\nComment: As there is no test file in this pull request nor any test function or class in the file `sliding_window_and_kadane_algorithm/kadane_algorithm.py`, please provide doctest for the function `get_max_sum`\n\nPlease provide return type hint for the function: `get_max_sum`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `length`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "sliding_window_and_kadane_algorithm/kadane_algorithm.py",
    "pr_number": 8598,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1155084688,
    "comment_created_at": "2023-04-01T09:32:43Z"
  },
  {
    "code": "@@ -0,0 +1,12 @@\n+print(\n+    \"INSTRUCTIONS: USE 'and' / 'or' / 'not' when entering equation. For booleans use True/False or 1/0\"\n+)\n+\n+print()\n+while True:\n+    try:\n+        eq = input(\"Enter equation: \")\n+\n+        print(eval(eq))",
    "comment": "please read the file contributing.md to understand our definition of an algorithm and our requirements for tests and type hints.\r\n\r\nalso, https://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html",
    "line_number": 10,
    "enriched": "File: boolean_algebra/boolean algebra calculator.py\nCode: @@ -0,0 +1,12 @@\n+print(\n+    \"INSTRUCTIONS: USE 'and' / 'or' / 'not' when entering equation. For booleans use True/False or 1/0\"\n+)\n+\n+print()\n+while True:\n+    try:\n+        eq = input(\"Enter equation: \")\n+\n+        print(eval(eq))\nComment: Please read the file `CONTRIBUTING.md` to understand our definition of an algorithm and our requirements for tests and type hints.\r\n\r\nAlso, https://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "boolean_algebra/boolean algebra calculator.py",
    "pr_number": 7077,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994289201,
    "comment_created_at": "2022-10-13T07:57:44Z"
  },
  {
    "code": "@@ -0,0 +1,55 @@\n+\"\"\"\n+Barnsley Fern\n+\n+The Barnsley fern is a fractal that uses an iterated function system (IFS)\n+to generate a realistic-looking fern shape.\n+\n+Reference:\n+https://en.wikipedia.org/wiki/Barnsley_fern\n+\n+Requirements:\n+    - matplotlib\n+    - numpy\n+\"\"\"\n+\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\n+def barnsley_farn(n_points=100000):",
    "comment": "please provide return type hint for the function: barnsley_farn. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file fractals/barnsley_farn.py, please provide doctest for the function barnsley_farn\n\nplease provide type hint for the parameter: n_points",
    "line_number": 19,
    "enriched": "File: fractals/barnsley_farn.py\nCode: @@ -0,0 +1,55 @@\n+\"\"\"\n+Barnsley Fern\n+\n+The Barnsley fern is a fractal that uses an iterated function system (IFS)\n+to generate a realistic-looking fern shape.\n+\n+Reference:\n+https://en.wikipedia.org/wiki/Barnsley_fern\n+\n+Requirements:\n+    - matplotlib\n+    - numpy\n+\"\"\"\n+\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\n+def barnsley_farn(n_points=100000):\nComment: Please provide return type hint for the function: `barnsley_farn`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `fractals/barnsley_farn.py`, please provide doctest for the function `barnsley_farn`\n\nPlease provide type hint for the parameter: `n_points`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "fractals/barnsley_farn.py",
    "pr_number": 12274,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1815615283,
    "comment_created_at": "2024-10-24T19:47:01Z"
  },
  {
    "code": "@@ -0,0 +1,717 @@\n+\"\"\"\n+Enhanced Decision Tree with Pruning functionality.\n+\n+This implementation extends the basic decision tree with advanced pruning techniques\n+to reduce overfitting and improve generalization. It includes both pre-pruning\n+(constraints during tree building) and post-pruning (reduced error pruning and\n+cost-complexity pruning).\n+\n+Key features:\n+- Pre-pruning: Maximum depth, minimum samples per leaf, minimum impurity decrease\n+- Post-pruning: Reduced error pruning and cost-complexity pruning\n+- Support for both regression and classification\n+- Comprehensive validation and testing\n+\n+Reference: https://en.wikipedia.org/wiki/Decision_tree_pruning\n+\"\"\"\n+\n+import doctest\n+from typing import Literal\n+\n+import numpy as np\n+\n+\n+class DecisionTreePruning:\n+    \"\"\"\n+    Enhanced Decision Tree with pruning capabilities.\n+\n+    This implementation provides both regression and classification decision trees\n+    with various pruning techniques to prevent overfitting.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        max_depth: int | None = None,\n+        min_samples_split: int = 2,\n+        min_samples_leaf: int = 1,\n+        min_impurity_decrease: float = 0.0,\n+        pruning_method: Literal[\"none\", \"reduced_error\", \"cost_complexity\"] = \"none\",\n+        ccp_alpha: float = 0.0,\n+        random_state: int | None = None,\n+    ) -> None:\n+        \"\"\"\n+        Initialize Decision Tree with pruning parameters.\n+\n+        Args:\n+            max_depth: Maximum depth of the tree\n+            min_samples_split: Minimum samples required to split a node\n+            min_samples_leaf: Minimum samples required at a leaf node\n+            min_impurity_decrease: Minimum impurity decrease for a split\n+            pruning_method: Pruning method to use\n+            ccp_alpha: Cost complexity pruning parameter\n+            random_state: Random seed for reproducibility\n+\n+        >>> tree = DecisionTreePruning(max_depth=5, min_samples_leaf=2)\n+        >>> tree.max_depth\n+        5\n+        >>> tree.min_samples_leaf\n+        2\n+        \"\"\"\n+        self.max_depth = max_depth\n+        self.min_samples_split = min_samples_split\n+        self.min_samples_leaf = min_samples_leaf\n+        self.min_impurity_decrease = min_impurity_decrease\n+        self.pruning_method = pruning_method\n+        self.ccp_alpha = ccp_alpha\n+        self.random_state = random_state\n+\n+        # Tree structure\n+        self.root_: TreeNode | None = None\n+        self.n_features_: int | None = None\n+        self.feature_names_: list[str] | None = None\n+\n+        if random_state is not None:\n+            self.rng_ = np.random.default_rng(random_state)\n+        else:\n+            self.rng_ = np.random.default_rng()\n+\n+    def _mse(self, y: np.ndarray) -> float:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/decision_tree_pruning.py, please provide doctest for the function _mse\n\nplease provide descriptive name for the parameter: y",
    "line_number": 78,
    "enriched": "File: machine_learning/decision_tree_pruning.py\nCode: @@ -0,0 +1,717 @@\n+\"\"\"\n+Enhanced Decision Tree with Pruning functionality.\n+\n+This implementation extends the basic decision tree with advanced pruning techniques\n+to reduce overfitting and improve generalization. It includes both pre-pruning\n+(constraints during tree building) and post-pruning (reduced error pruning and\n+cost-complexity pruning).\n+\n+Key features:\n+- Pre-pruning: Maximum depth, minimum samples per leaf, minimum impurity decrease\n+- Post-pruning: Reduced error pruning and cost-complexity pruning\n+- Support for both regression and classification\n+- Comprehensive validation and testing\n+\n+Reference: https://en.wikipedia.org/wiki/Decision_tree_pruning\n+\"\"\"\n+\n+import doctest\n+from typing import Literal\n+\n+import numpy as np\n+\n+\n+class DecisionTreePruning:\n+    \"\"\"\n+    Enhanced Decision Tree with pruning capabilities.\n+\n+    This implementation provides both regression and classification decision trees\n+    with various pruning techniques to prevent overfitting.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        max_depth: int | None = None,\n+        min_samples_split: int = 2,\n+        min_samples_leaf: int = 1,\n+        min_impurity_decrease: float = 0.0,\n+        pruning_method: Literal[\"none\", \"reduced_error\", \"cost_complexity\"] = \"none\",\n+        ccp_alpha: float = 0.0,\n+        random_state: int | None = None,\n+    ) -> None:\n+        \"\"\"\n+        Initialize Decision Tree with pruning parameters.\n+\n+        Args:\n+            max_depth: Maximum depth of the tree\n+            min_samples_split: Minimum samples required to split a node\n+            min_samples_leaf: Minimum samples required at a leaf node\n+            min_impurity_decrease: Minimum impurity decrease for a split\n+            pruning_method: Pruning method to use\n+            ccp_alpha: Cost complexity pruning parameter\n+            random_state: Random seed for reproducibility\n+\n+        >>> tree = DecisionTreePruning(max_depth=5, min_samples_leaf=2)\n+        >>> tree.max_depth\n+        5\n+        >>> tree.min_samples_leaf\n+        2\n+        \"\"\"\n+        self.max_depth = max_depth\n+        self.min_samples_split = min_samples_split\n+        self.min_samples_leaf = min_samples_leaf\n+        self.min_impurity_decrease = min_impurity_decrease\n+        self.pruning_method = pruning_method\n+        self.ccp_alpha = ccp_alpha\n+        self.random_state = random_state\n+\n+        # Tree structure\n+        self.root_: TreeNode | None = None\n+        self.n_features_: int | None = None\n+        self.feature_names_: list[str] | None = None\n+\n+        if random_state is not None:\n+            self.rng_ = np.random.default_rng(random_state)\n+        else:\n+            self.rng_ = np.random.default_rng()\n+\n+    def _mse(self, y: np.ndarray) -> float:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/decision_tree_pruning.py`, please provide doctest for the function `_mse`\n\nPlease provide descriptive name for the parameter: `y`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/decision_tree_pruning.py",
    "pr_number": 13352,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2414851614,
    "comment_created_at": "2025-10-08T19:41:20Z"
  },
  {
    "code": "@@ -0,0 +1,27 @@\n+import smtplib\r\n+import requests\r\n+\r\n+\r\n+EMAIL_HOST_USER = \"your email id\"\r\n+EMAIL_HOST_PASSWORD = \"your app password (generate it from gmail app passwords)\"\r\n+\r\n+connection = smtplib.SMTP(\"smtp.gmail.com\")\r\n+connection.starttls()\r\n+connection.login(EMAIL_HOST_USER, EMAIL_HOST_PASSWORD)\r\n+\r\n+\r\n+def get_random_quote():\r",
    "comment": "please provide return type hint for the function: get_random_quote. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file motivational quotes email sender using python/index.py, please provide doctest for the function get_random_quote",
    "line_number": 13,
    "enriched": "File: Motivational Quotes Email Sender using Python/index.py\nCode: @@ -0,0 +1,27 @@\n+import smtplib\r\n+import requests\r\n+\r\n+\r\n+EMAIL_HOST_USER = \"your email id\"\r\n+EMAIL_HOST_PASSWORD = \"your app password (generate it from gmail app passwords)\"\r\n+\r\n+connection = smtplib.SMTP(\"smtp.gmail.com\")\r\n+connection.starttls()\r\n+connection.login(EMAIL_HOST_USER, EMAIL_HOST_PASSWORD)\r\n+\r\n+\r\n+def get_random_quote():\r\nComment: Please provide return type hint for the function: `get_random_quote`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `Motivational Quotes Email Sender using Python/index.py`, please provide doctest for the function `get_random_quote`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "Motivational Quotes Email Sender using Python/index.py",
    "pr_number": 9130,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1341966919,
    "comment_created_at": "2023-09-30T14:14:01Z"
  },
  {
    "code": "@@ -1,40 +1,49 @@\n name: sphinx\n \n on:\n-  # Triggers the workflow on push or pull request events but only for the \"master\" branch\n   push:\n     branches: [\"master\"]\n   pull_request:\n     branches: [\"master\"]\n-  # Or manually from the Actions tab\n   workflow_dispatch:\n \n-# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages\n permissions:\n   contents: read\n   pages: write\n   id-token: write\n \n-# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.\n-# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.\n concurrency:\n   group: \"pages\"\n   cancel-in-progress: false\n \n jobs:\n   build_docs:\n-    runs-on: ubuntu-24.04-arm\n+    runs-on: ubuntu-latest\n     steps:\n-      - uses: actions/checkout@v5\n-      - uses: astral-sh/setup-uv@v6\n-      - uses: actions/setup-python@v5\n+      - name: Checkout repository\n+        uses: actions/checkout@v5",
    "comment": "actions/checkout@v5 tells the reader more than checkout repository does.",
    "line_number": 24,
    "enriched": "File: .github/workflows/sphinx.yml\nCode: @@ -1,40 +1,49 @@\n name: sphinx\n \n on:\n-  # Triggers the workflow on push or pull request events but only for the \"master\" branch\n   push:\n     branches: [\"master\"]\n   pull_request:\n     branches: [\"master\"]\n-  # Or manually from the Actions tab\n   workflow_dispatch:\n \n-# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages\n permissions:\n   contents: read\n   pages: write\n   id-token: write\n \n-# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.\n-# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.\n concurrency:\n   group: \"pages\"\n   cancel-in-progress: false\n \n jobs:\n   build_docs:\n-    runs-on: ubuntu-24.04-arm\n+    runs-on: ubuntu-latest\n     steps:\n-      - uses: actions/checkout@v5\n-      - uses: astral-sh/setup-uv@v6\n-      - uses: actions/setup-python@v5\n+      - name: Checkout repository\n+        uses: actions/checkout@v5\nComment: `actions/checkout@v5` tells the reader more than `Checkout repository` does.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": ".github/workflows/sphinx.yml",
    "pr_number": 12945,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2314586625,
    "comment_created_at": "2025-09-01T21:32:24Z"
  },
  {
    "code": "@@ -0,0 +1,27 @@\n+\n+#Code Contributed by Atharv Patil, 2nd year, IIIT Pune\n+# kadanes algorithm calculates maximum subarray sum in O(n) time complexity and\n+# O(1) space complexity\n+\n+\n+\n+def kadane(array):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/kadanes_algorithm.py, please provide doctest for the function kadane\n\nplease provide return type hint for the function: kadane. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: array",
    "line_number": 8,
    "enriched": "File: dynamic_programming/kadanes_algorithm.py\nCode: @@ -0,0 +1,27 @@\n+\n+#Code Contributed by Atharv Patil, 2nd year, IIIT Pune\n+# kadanes algorithm calculates maximum subarray sum in O(n) time complexity and\n+# O(1) space complexity\n+\n+\n+\n+def kadane(array):\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/kadanes_algorithm.py`, please provide doctest for the function `kadane`\n\nPlease provide return type hint for the function: `kadane`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `array`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/kadanes_algorithm.py",
    "pr_number": 7374,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 997266322,
    "comment_created_at": "2022-10-17T16:18:30Z"
  },
  {
    "code": "@@ -0,0 +1,385 @@\n+# # Dataset imports\n+\n+# Get the file name from the uploaded files\n+train_file = './data/brown.train.txt'\n+test_file = './data/brown.test.txt'\n+dev_file = './data/brown.dev.txt'\n+\n+# Read text from the uploaded file\n+with open(train_file, 'r') as file:\n+    train_text = file.read()\n+\n+with open(test_file, 'r') as file:\n+    test_text = file.read()\n+\n+with open(dev_file, 'r') as file:\n+    dev_text = file.read()\n+\n+print(\"Text from the file(s):\")\n+\n+print(train_text[:500], \" ...\\n\\n\")\n+print(test_text[:500], \" ...\\n\\n\")\n+print(dev_text[:500], \" ...\\n\\n\")\n+\n+# # Text Cleaning\n+# \n+# - In this section, the input corpus is cleaned and **all non-alphanumeric** symbols are dropped\n+# \n+# - `<start>` and `<stop>` tags are added at the *start* and *end* of every sentence\n+\n+import math\n+import re\n+from collections import Counter\n+\n+def clean_text(corpus):",
    "comment": "please provide return type hint for the function: clean_text. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file natural-language-processing/interpolation.py, please provide doctest for the function clean_text\n\nplease provide type hint for the parameter: corpus",
    "line_number": 34,
    "enriched": "File: natural-language-processing/interpolation.py\nCode: @@ -0,0 +1,385 @@\n+# # Dataset imports\n+\n+# Get the file name from the uploaded files\n+train_file = './data/brown.train.txt'\n+test_file = './data/brown.test.txt'\n+dev_file = './data/brown.dev.txt'\n+\n+# Read text from the uploaded file\n+with open(train_file, 'r') as file:\n+    train_text = file.read()\n+\n+with open(test_file, 'r') as file:\n+    test_text = file.read()\n+\n+with open(dev_file, 'r') as file:\n+    dev_text = file.read()\n+\n+print(\"Text from the file(s):\")\n+\n+print(train_text[:500], \" ...\\n\\n\")\n+print(test_text[:500], \" ...\\n\\n\")\n+print(dev_text[:500], \" ...\\n\\n\")\n+\n+# # Text Cleaning\n+# \n+# - In this section, the input corpus is cleaned and **all non-alphanumeric** symbols are dropped\n+# \n+# - `<start>` and `<stop>` tags are added at the *start* and *end* of every sentence\n+\n+import math\n+import re\n+from collections import Counter\n+\n+def clean_text(corpus):\nComment: Please provide return type hint for the function: `clean_text`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `natural-language-processing/interpolation.py`, please provide doctest for the function `clean_text`\n\nPlease provide type hint for the parameter: `corpus`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "natural-language-processing/interpolation.py",
    "pr_number": 10603,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1360805868,
    "comment_created_at": "2023-10-16T14:59:20Z"
  },
  {
    "code": "@@ -12,8 +12,55 @@ addopts = [\n omit = [\".env/*\"]\n sort = \"Cover\"\n \n-#[report]\n-#sort = Cover\n-#omit =\n-#    .env/*\n-#    backtracking/*\n+[tool.codespell]\n+ignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,mater,secant,som,sur,tim,zar\"\n+skip = \"./.*,*.json,ciphers/prehistoric_men.txt,project_euler/problem_022/p022_names.txt,pyproject.toml,strings/dictionary.txt,strings/words.txt\"\n+\n+[tool.ruff]\n+ignore = [",
    "comment": "it would be useful for posterity to specify why these rules are being ignored.",
    "line_number": 20,
    "enriched": "File: pyproject.toml\nCode: @@ -12,8 +12,55 @@ addopts = [\n omit = [\".env/*\"]\n sort = \"Cover\"\n \n-#[report]\n-#sort = Cover\n-#omit =\n-#    .env/*\n-#    backtracking/*\n+[tool.codespell]\n+ignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,mater,secant,som,sur,tim,zar\"\n+skip = \"./.*,*.json,ciphers/prehistoric_men.txt,project_euler/problem_022/p022_names.txt,pyproject.toml,strings/dictionary.txt,strings/words.txt\"\n+\n+[tool.ruff]\n+ignore = [\nComment: It would be useful for posterity to specify why these rules are being ignored.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "pyproject.toml",
    "pr_number": 8178,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1134994748,
    "comment_created_at": "2023-03-14T05:36:36Z"
  },
  {
    "code": "@@ -0,0 +1,57 @@\n+def counting_sort(arr):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file sorts/adaptive_range_sort.py, please provide doctest for the function counting_sort\n\nplease provide return type hint for the function: counting_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: arr",
    "line_number": 1,
    "enriched": "File: sorts/adaptive_range_sort.py\nCode: @@ -0,0 +1,57 @@\n+def counting_sort(arr):\nComment: As there is no test file in this pull request nor any test function or class in the file `sorts/adaptive_range_sort.py`, please provide doctest for the function `counting_sort`\n\nPlease provide return type hint for the function: `counting_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `arr`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "sorts/adaptive_range_sort.py",
    "pr_number": 9299,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342144019,
    "comment_created_at": "2023-10-01T14:26:48Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+\"\"\"\n+Adaptive Merge Sort Algorithm\n+@see https://www.tutorialspoint.com/adaptive-merging-and-sorting-in-data-structure\n+\"\"\"\n+\n+def adaptive_merge_sort(sequence: list) -> list:\n+    \"\"\"\n+    Sorts a list using the Adaptive Merge Sort algorithm.\n+\n+    :param sequence: list of elements to be sorted\n+    :return: sorted list\n+\n+    >>> adaptive_merge_sort([12, 11, 13, 5, 6, 7])\n+    [5, 6, 7, 11, 12, 13]\n+\n+    >>> adaptive_merge_sort([5, 4, 3, 2, 1])\n+    [1, 2, 3, 4, 5]\n+\n+    >>> adaptive_merge_sort(['zebra', 'apple', 'mango', 'banana'])\n+    ['apple', 'banana', 'mango', 'zebra']\n+    \"\"\"\n+    if len(sequence) < 2:\n+        return sequence\n+\n+    aux = sequence[:]\n+    adaptive_merge_sort_helper(sequence, aux, 0, len(sequence) - 1)\n+    return sequence\n+\n+\n+def adaptive_merge_sort_helper(array: list, aux: list, low: int, high: int):",
    "comment": "please provide return type hint for the function: adaptive_merge_sort_helper. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file sorts/adaptive_merge_sort.py, please provide doctest for the function adaptive_merge_sort_helper",
    "line_number": 30,
    "enriched": "File: sorts/adaptive_merge_sort.py\nCode: @@ -0,0 +1,61 @@\n+\"\"\"\n+Adaptive Merge Sort Algorithm\n+@see https://www.tutorialspoint.com/adaptive-merging-and-sorting-in-data-structure\n+\"\"\"\n+\n+def adaptive_merge_sort(sequence: list) -> list:\n+    \"\"\"\n+    Sorts a list using the Adaptive Merge Sort algorithm.\n+\n+    :param sequence: list of elements to be sorted\n+    :return: sorted list\n+\n+    >>> adaptive_merge_sort([12, 11, 13, 5, 6, 7])\n+    [5, 6, 7, 11, 12, 13]\n+\n+    >>> adaptive_merge_sort([5, 4, 3, 2, 1])\n+    [1, 2, 3, 4, 5]\n+\n+    >>> adaptive_merge_sort(['zebra', 'apple', 'mango', 'banana'])\n+    ['apple', 'banana', 'mango', 'zebra']\n+    \"\"\"\n+    if len(sequence) < 2:\n+        return sequence\n+\n+    aux = sequence[:]\n+    adaptive_merge_sort_helper(sequence, aux, 0, len(sequence) - 1)\n+    return sequence\n+\n+\n+def adaptive_merge_sort_helper(array: list, aux: list, low: int, high: int):\nComment: Please provide return type hint for the function: `adaptive_merge_sort_helper`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `sorts/adaptive_merge_sort.py`, please provide doctest for the function `adaptive_merge_sort_helper`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "sorts/adaptive_merge_sort.py",
    "pr_number": 11992,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1797301325,
    "comment_created_at": "2024-10-11T18:30:37Z"
  },
  {
    "code": "@@ -0,0 +1,49 @@\n+\"\"\"\n+Problem 95\n+Url: https://projecteuler.net/problem=95\n+Statement:\n+The proper divisors of a number are all the divisors excluding the number itself. For example, the proper divisors of  28 is 1,2,4,7 and 14 and . As the sum of these divisors is equal to 28, we call it a perfect number.\n+\n+Interestingly the sum of the proper divisors of 220 is 284 and the sum of the proper divisors of 284 is 220 forming a chain of two numbers. For this reason, 220 and 284 are called an amicable pair.\n+\n+Perhaps less well known are longer chains. For example, starting with 12496, we form a chain of five numbers:\n+12496 -> 14288 -> 15472 -> 14536 -> 14264(->12496 -> ....)\n+Since this chain returns to its starting point, it is called an amicable chain.\n+\n+Find the smallest member of the longest amicable chain with no element exceeding one million.\n+\"\"\"\n+\n+def find_smallest_member(n : int) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file project_euler/sol1.py, please provide doctest for the function find_smallest_member\n\nplease provide descriptive name for the parameter: n",
    "line_number": 16,
    "enriched": "File: project_euler/sol1.py\nCode: @@ -0,0 +1,49 @@\n+\"\"\"\n+Problem 95\n+Url: https://projecteuler.net/problem=95\n+Statement:\n+The proper divisors of a number are all the divisors excluding the number itself. For example, the proper divisors of  28 is 1,2,4,7 and 14 and . As the sum of these divisors is equal to 28, we call it a perfect number.\n+\n+Interestingly the sum of the proper divisors of 220 is 284 and the sum of the proper divisors of 284 is 220 forming a chain of two numbers. For this reason, 220 and 284 are called an amicable pair.\n+\n+Perhaps less well known are longer chains. For example, starting with 12496, we form a chain of five numbers:\n+12496 -> 14288 -> 15472 -> 14536 -> 14264(->12496 -> ....)\n+Since this chain returns to its starting point, it is called an amicable chain.\n+\n+Find the smallest member of the longest amicable chain with no element exceeding one million.\n+\"\"\"\n+\n+def find_smallest_member(n : int) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `project_euler/sol1.py`, please provide doctest for the function `find_smallest_member`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "project_euler/sol1.py",
    "pr_number": 9169,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342069129,
    "comment_created_at": "2023-10-01T03:46:38Z"
  },
  {
    "code": "@@ -0,0 +1,34 @@\n+#python prog to capitilize the string\n+def custom_upper(string):",
    "comment": "please provide return type hint for the function: custom_upper. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file strings/capitilize_without_builtin_methods.py, please provide doctest for the function custom_upper\n\nplease provide type hint for the parameter: string",
    "line_number": 2,
    "enriched": "File: strings/capitilize_without_builtin_Methods.py\nCode: @@ -0,0 +1,34 @@\n+#python prog to capitilize the string\n+def custom_upper(string):\nComment: Please provide return type hint for the function: `custom_upper`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `strings/capitilize_without_builtin_Methods.py`, please provide doctest for the function `custom_upper`\n\nPlease provide type hint for the parameter: `string`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "strings/capitilize_without_builtin_Methods.py",
    "pr_number": 11026,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1374349280,
    "comment_created_at": "2023-10-27T10:01:49Z"
  },
  {
    "code": "@@ -0,0 +1,63 @@\n+# Python program for implementation of Quicksort Sort\r\n+\r\n+# This implementation utilizes pivot as the last element in the nums list\r\n+# It has a pointer to keep track of the elements smaller than the pivot\r\n+# At the very end of partition() function, the pointer is swapped with the pivot\r\n+# to come up with a \"sorted\" nums relative to the pivot\r\n+\r\n+\r\n+# Function to find the partition position\r\n+def partition(array, low, high):\r",
    "comment": "please provide return type hint for the function: partition. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file divide_and_conquer/quicksort.py, please provide doctest for the function partition\n\nplease provide type hint for the parameter: array\n\nplease provide type hint for the parameter: low\n\nplease provide type hint for the parameter: high",
    "line_number": 10,
    "enriched": "File: divide_and_conquer/QuickSort.py\nCode: @@ -0,0 +1,63 @@\n+# Python program for implementation of Quicksort Sort\r\n+\r\n+# This implementation utilizes pivot as the last element in the nums list\r\n+# It has a pointer to keep track of the elements smaller than the pivot\r\n+# At the very end of partition() function, the pointer is swapped with the pivot\r\n+# to come up with a \"sorted\" nums relative to the pivot\r\n+\r\n+\r\n+# Function to find the partition position\r\n+def partition(array, low, high):\r\nComment: Please provide return type hint for the function: `partition`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `divide_and_conquer/QuickSort.py`, please provide doctest for the function `partition`\n\nPlease provide type hint for the parameter: `array`\n\nPlease provide type hint for the parameter: `low`\n\nPlease provide type hint for the parameter: `high`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "divide_and_conquer/QuickSort.py",
    "pr_number": 10381,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1358500257,
    "comment_created_at": "2023-10-13T16:41:48Z"
  },
  {
    "code": "@@ -0,0 +1,12 @@\n+def bubblesort(arr):",
    "comment": "please provide return type hint for the function: bubblesort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file sorting/bubble_sort.py, please provide doctest for the function bubblesort\n\nplease provide type hint for the parameter: arr",
    "line_number": 1,
    "enriched": "File: Sorting/bubble_sort.py\nCode: @@ -0,0 +1,12 @@\n+def bubblesort(arr):\nComment: Please provide return type hint for the function: `bubblesort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `Sorting/bubble_sort.py`, please provide doctest for the function `bubblesort`\n\nPlease provide type hint for the parameter: `arr`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "Sorting/bubble_sort.py",
    "pr_number": 9891,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1348211579,
    "comment_created_at": "2023-10-06T03:46:30Z"
  },
  {
    "code": "@@ -0,0 +1,22 @@\n+#A palindromic number (also known as a numeral palindrome or a numeric palindrome) is a number (such as 16461) that remains the same when its digits are reversed\n+\n+def is_palindrome_number(number):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/palindrome_check.py, please provide doctest for the function is_palindrome_number\n\nplease provide return type hint for the function: is_palindrome_number. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: number",
    "line_number": 3,
    "enriched": "File: maths/palindrome_check.py\nCode: @@ -0,0 +1,22 @@\n+#A palindromic number (also known as a numeral palindrome or a numeric palindrome) is a number (such as 16461) that remains the same when its digits are reversed\n+\n+def is_palindrome_number(number):\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/palindrome_check.py`, please provide doctest for the function `is_palindrome_number`\n\nPlease provide return type hint for the function: `is_palindrome_number`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `number`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/palindrome_check.py",
    "pr_number": 9957,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349442060,
    "comment_created_at": "2023-10-07T02:46:03Z"
  },
  {
    "code": "@@ -0,0 +1,43 @@\n+import cv2\n+import numpy as np\n+\"\"\"\n+  Implemented Image dehazer using OpenCV and Dark Channel\n+\"\"\"\n+def dehaze_image(image, omega=0.78, t0=0.01):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file digital_image_processing/image_dehazer.py, please provide doctest for the function dehaze_image\n\nplease provide return type hint for the function: dehaze_image. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: image\n\nplease provide type hint for the parameter: omega\n\nplease provide type hint for the parameter: t0",
    "line_number": 6,
    "enriched": "File: digital_image_processing/image_dehazer.py\nCode: @@ -0,0 +1,43 @@\n+import cv2\n+import numpy as np\n+\"\"\"\n+  Implemented Image dehazer using OpenCV and Dark Channel\n+\"\"\"\n+def dehaze_image(image, omega=0.78, t0=0.01):\nComment: As there is no test file in this pull request nor any test function or class in the file `digital_image_processing/image_dehazer.py`, please provide doctest for the function `dehaze_image`\n\nPlease provide return type hint for the function: `dehaze_image`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `image`\n\nPlease provide type hint for the parameter: `omega`\n\nPlease provide type hint for the parameter: `t0`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "digital_image_processing/image_dehazer.py",
    "pr_number": 10835,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368406028,
    "comment_created_at": "2023-10-23T09:49:06Z"
  },
  {
    "code": "@@ -0,0 +1,106 @@\n+import numpy as np\n+from sklearn.metrics import accuracy_score\n+from tqdm import  tqdm\n+import random\n+\n+def randomized_search_cv_custom(x_train_total, y_train_total, classifier, param_range, num_of_total_fold):",
    "comment": "please provide return type hint for the function: randomized_search_cv_custom. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file machine_learning/randomsearchcv.py, please provide doctest for the function randomized_search_cv_custom\n\nplease provide type hint for the parameter: x_train_total\n\nplease provide type hint for the parameter: y_train_total\n\nplease provide type hint for the parameter: classifier\n\nplease provide type hint for the parameter: param_range\n\nplease provide type hint for the parameter: num_of_total_fold",
    "line_number": 6,
    "enriched": "File: machine_learning/randomsearchCV.py\nCode: @@ -0,0 +1,106 @@\n+import numpy as np\n+from sklearn.metrics import accuracy_score\n+from tqdm import  tqdm\n+import random\n+\n+def randomized_search_cv_custom(x_train_total, y_train_total, classifier, param_range, num_of_total_fold):\nComment: Please provide return type hint for the function: `randomized_search_cv_custom`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `machine_learning/randomsearchCV.py`, please provide doctest for the function `randomized_search_cv_custom`\n\nPlease provide type hint for the parameter: `x_train_total`\n\nPlease provide type hint for the parameter: `y_train_total`\n\nPlease provide type hint for the parameter: `classifier`\n\nPlease provide type hint for the parameter: `param_range`\n\nPlease provide type hint for the parameter: `num_of_total_fold`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/randomsearchCV.py",
    "pr_number": 9235,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342104470,
    "comment_created_at": "2023-10-01T09:11:29Z"
  },
  {
    "code": "@@ -0,0 +1,10 @@\n+\n+def find_char():",
    "comment": "please provide return type hint for the function: find_char. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file strings/findnextindexvalue.py, please provide doctest for the function find_char",
    "line_number": 2,
    "enriched": "File: strings/findNextIndexValue.py\nCode: @@ -0,0 +1,10 @@\n+\n+def find_char():\nComment: Please provide return type hint for the function: `find_char`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `strings/findNextIndexValue.py`, please provide doctest for the function `find_char`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "strings/findNextIndexValue.py",
    "pr_number": 7690,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1005985235,
    "comment_created_at": "2022-10-26T17:31:57Z"
  },
  {
    "code": "@@ -0,0 +1,27 @@\n+def addBinary(a: str, b: str) -> str:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file bit_manipulation/add_binary.py, please provide doctest for the function addbinary\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: addbinary\n\nplease provide descriptive name for the parameter: a\n\nplease provide descriptive name for the parameter: b",
    "line_number": 1,
    "enriched": "File: bit_manipulation/add_binary.py\nCode: @@ -0,0 +1,27 @@\n+def addBinary(a: str, b: str) -> str:\nComment: As there is no test file in this pull request nor any test function or class in the file `bit_manipulation/add_binary.py`, please provide doctest for the function `addBinary`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `addBinary`\n\nPlease provide descriptive name for the parameter: `a`\n\nPlease provide descriptive name for the parameter: `b`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "bit_manipulation/add_binary.py",
    "pr_number": 13268,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2407066848,
    "comment_created_at": "2025-10-06T15:36:28Z"
  },
  {
    "code": "@@ -0,0 +1,67 @@\n+# Author:- Arnab Nath\n+# Python program to find an element x\n+# in a sorted array using Exponential Search\n+\n+# A recursive binary search function returns\n+# location of x in given array arr[l..r] is\n+# present, otherwise -1\n+def binarySearch(arr, l, r, x):",
    "comment": "please provide return type hint for the function: binarysearch. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: binarysearch\n\nas there is no test file in this pull request nor any test function or class in the file searches/exponentialsearch.py, please provide doctest for the function binarysearch\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: l\n\nplease provide descriptive name for the parameter: l\n\nplease provide type hint for the parameter: r\n\nplease provide descriptive name for the parameter: r\n\nplease provide type hint for the parameter: x\n\nplease provide descriptive name for the parameter: x",
    "line_number": 8,
    "enriched": "File: searches/exponentialsearch.py\nCode: @@ -0,0 +1,67 @@\n+# Author:- Arnab Nath\n+# Python program to find an element x\n+# in a sorted array using Exponential Search\n+\n+# A recursive binary search function returns\n+# location of x in given array arr[l..r] is\n+# present, otherwise -1\n+def binarySearch(arr, l, r, x):\nComment: Please provide return type hint for the function: `binarySearch`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `binarySearch`\n\nAs there is no test file in this pull request nor any test function or class in the file `searches/exponentialsearch.py`, please provide doctest for the function `binarySearch`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `l`\n\nPlease provide descriptive name for the parameter: `l`\n\nPlease provide type hint for the parameter: `r`\n\nPlease provide descriptive name for the parameter: `r`\n\nPlease provide type hint for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "searches/exponentialsearch.py",
    "pr_number": 7293,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996414394,
    "comment_created_at": "2022-10-16T09:15:53Z"
  },
  {
    "code": "@@ -0,0 +1,16 @@\n+def pig_latin(sentence):\r",
    "comment": "please provide return type hint for the function: pig_latin. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file strings/pig_latin.py, please provide doctest for the function pig_latin\n\nplease provide type hint for the parameter: sentence",
    "line_number": 1,
    "enriched": "File: strings/pig_latin.py\nCode: @@ -0,0 +1,16 @@\n+def pig_latin(sentence):\r\nComment: Please provide return type hint for the function: `pig_latin`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `strings/pig_latin.py`, please provide doctest for the function `pig_latin`\n\nPlease provide type hint for the parameter: `sentence`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "strings/pig_latin.py",
    "pr_number": 9712,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346113595,
    "comment_created_at": "2023-10-04T15:56:25Z"
  },
  {
    "code": "@@ -0,0 +1,38 @@\n+def minimum_platforms(arrival, departure):",
    "comment": "please provide return type hint for the function: minimum_platforms. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file dynamic_programming/minimum_platforms.py, please provide doctest for the function minimum_platforms\n\nplease provide type hint for the parameter: arrival\n\nplease provide type hint for the parameter: departure",
    "line_number": 1,
    "enriched": "File: dynamic_programming/minimum_platforms.py\nCode: @@ -0,0 +1,38 @@\n+def minimum_platforms(arrival, departure):\nComment: Please provide return type hint for the function: `minimum_platforms`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `dynamic_programming/minimum_platforms.py`, please provide doctest for the function `minimum_platforms`\n\nPlease provide type hint for the parameter: `arrival`\n\nPlease provide type hint for the parameter: `departure`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/minimum_platforms.py",
    "pr_number": 9222,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342095600,
    "comment_created_at": "2023-10-01T08:03:57Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+# Random Forest Classifier Example\n+from matplotlib import pyplot as plt\n+from sklearn.datasets import load_iris\n+from sklearn.ensemble import RandomForestClassifier\n+from sklearn.metrics import plot_confusion_matrix\n+from sklearn.model_selection import train_test_split\n+import numpy as np\n+from sklearn.model_selection import GridSearchCV\n+\n+\n+def main():",
    "comment": "please provide return type hint for the function: main. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file machine_learning/random_forest_classifier_2022.py, please provide doctest for the function main",
    "line_number": 11,
    "enriched": "File: machine_learning/random_forest_classifier_2022.py\nCode: @@ -0,0 +1,56 @@\n+# Random Forest Classifier Example\n+from matplotlib import pyplot as plt\n+from sklearn.datasets import load_iris\n+from sklearn.ensemble import RandomForestClassifier\n+from sklearn.metrics import plot_confusion_matrix\n+from sklearn.model_selection import train_test_split\n+import numpy as np\n+from sklearn.model_selection import GridSearchCV\n+\n+\n+def main():\nComment: Please provide return type hint for the function: `main`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `machine_learning/random_forest_classifier_2022.py`, please provide doctest for the function `main`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/random_forest_classifier_2022.py",
    "pr_number": 6989,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 992141224,
    "comment_created_at": "2022-10-11T10:27:23Z"
  },
  {
    "code": "@@ -0,0 +1,117 @@\n+\"\"\"\n+wikipedia - https://en.wikipedia.org/wiki/Pong\n+\n+\"\"\"\n+import turtle\n+\n+trtle = turtle.Screen()\n+trtle.title(\"My First game using turtle\")\n+trtle.bgcolor(\"white\")\n+trtle.setup(width=800, height=600)\n+trtle.tracer(0)                         #stops the window from updating\n+\n+score_a = 0\n+score_b = 0\n+\n+\n+# required for game: Paddle A and B, Ball\n+#Paddle A\n+\n+paddle_a = turtle.Turtle()\n+paddle_a.speed(1)\n+paddle_a.shape(\"square\")\n+paddle_a.color(\"black\")\n+paddle_a.shapesize(stretch_wid=5, stretch_len=1)\n+paddle_a.penup()\n+paddle_a.goto(-370, 0)\n+\n+#Paddle B\n+\n+paddle_b = turtle.Turtle()\n+paddle_b.speed(1)\n+paddle_b.shape(\"square\")\n+paddle_b.color(\"black\")\n+paddle_b.shapesize(stretch_wid=5, stretch_len=1)\n+paddle_b.penup()\n+paddle_b.goto(+370, 0)\n+\n+#Ball\n+\n+b = turtle.Turtle()\n+b.speed(1)\n+b.shape(\"circle\")\n+b.color(\"black\")\n+b.penup()\n+b.goto(0, 0)\n+b.dx = 0.2\n+b.dy = 0.2\n+\n+#pen\n+pen = turtle.Turtle()\n+pen.speed(0)\n+pen.color(\"black\")\n+pen.penup()\n+pen.hideturtle()\n+pen.goto(0,260)\n+pen.write(fr\"Player A : {score_a}  Player B : {score_b}\", align='center', font=(\"Courier\", 24, \"normal\"))\n+\n+\n+def paddle_a_up():",
    "comment": "please provide return type hint for the function: paddle_a_up. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file games/pong_game/pong.py, please provide doctest for the function paddle_a_up",
    "line_number": 59,
    "enriched": "File: games/pong_game/pong.py\nCode: @@ -0,0 +1,117 @@\n+\"\"\"\n+wikipedia - https://en.wikipedia.org/wiki/Pong\n+\n+\"\"\"\n+import turtle\n+\n+trtle = turtle.Screen()\n+trtle.title(\"My First game using turtle\")\n+trtle.bgcolor(\"white\")\n+trtle.setup(width=800, height=600)\n+trtle.tracer(0)                         #stops the window from updating\n+\n+score_a = 0\n+score_b = 0\n+\n+\n+# required for game: Paddle A and B, Ball\n+#Paddle A\n+\n+paddle_a = turtle.Turtle()\n+paddle_a.speed(1)\n+paddle_a.shape(\"square\")\n+paddle_a.color(\"black\")\n+paddle_a.shapesize(stretch_wid=5, stretch_len=1)\n+paddle_a.penup()\n+paddle_a.goto(-370, 0)\n+\n+#Paddle B\n+\n+paddle_b = turtle.Turtle()\n+paddle_b.speed(1)\n+paddle_b.shape(\"square\")\n+paddle_b.color(\"black\")\n+paddle_b.shapesize(stretch_wid=5, stretch_len=1)\n+paddle_b.penup()\n+paddle_b.goto(+370, 0)\n+\n+#Ball\n+\n+b = turtle.Turtle()\n+b.speed(1)\n+b.shape(\"circle\")\n+b.color(\"black\")\n+b.penup()\n+b.goto(0, 0)\n+b.dx = 0.2\n+b.dy = 0.2\n+\n+#pen\n+pen = turtle.Turtle()\n+pen.speed(0)\n+pen.color(\"black\")\n+pen.penup()\n+pen.hideturtle()\n+pen.goto(0,260)\n+pen.write(fr\"Player A : {score_a}  Player B : {score_b}\", align='center', font=(\"Courier\", 24, \"normal\"))\n+\n+\n+def paddle_a_up():\nComment: Please provide return type hint for the function: `paddle_a_up`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `games/pong_game/pong.py`, please provide doctest for the function `paddle_a_up`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "games/pong_game/pong.py",
    "pr_number": 7090,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994244849,
    "comment_created_at": "2022-10-13T07:14:04Z"
  },
  {
    "code": "@@ -0,0 +1,10 @@\n+def pathSum(self, root: TreeNode, target_sum: int) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file bt_path_sum.py, please provide doctest for the function pathsum\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: pathsum",
    "line_number": 1,
    "enriched": "File: bt_path_sum.py\nCode: @@ -0,0 +1,10 @@\n+def pathSum(self, root: TreeNode, target_sum: int) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `bt_path_sum.py`, please provide doctest for the function `pathSum`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `pathSum`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "bt_path_sum.py",
    "pr_number": 7756,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1007307486,
    "comment_created_at": "2022-10-27T19:50:20Z"
  },
  {
    "code": "@@ -0,0 +1,26 @@\n+def butterfly_pattern(n: int) -> str:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file graphics/butterfly_pattern.py, please provide doctest for the function butterfly_pattern\n\nplease provide descriptive name for the parameter: n",
    "line_number": 1,
    "enriched": "File: graphics/butterfly_pattern.py\nCode: @@ -0,0 +1,26 @@\n+def butterfly_pattern(n: int) -> str:\nComment: As there is no test file in this pull request nor any test function or class in the file `graphics/butterfly_pattern.py`, please provide doctest for the function `butterfly_pattern`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "graphics/butterfly_pattern.py",
    "pr_number": 12493,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1915584954,
    "comment_created_at": "2025-01-14T20:52:32Z"
  },
  {
    "code": "@@ -0,0 +1,108 @@\n+import doctest\n+from typing import List\n+\n+import numpy as np\n+from sklearn.datasets import load_iris\n+from sklearn.tree import DecisionTreeClassifier\n+\n+\"\"\"\n+Implementation of basic AdaBoost classifier on iris dataset.\n+The following classifier uses a pre-built DecisionTreeClassifier\n+from scikit-learn as a weak learner.\n+\n+AdaBoost (Adaptive Boosting) is an ensemble learning technique\n+used for classification problems. It combines multiple weak learners\n+(in this case, decision trees with maximum depth 1) to create a\n+strong classifier. The key idea behind AdaBoost is to give\n+more weight to the training instances that are misclassified by\n+the previous weak learners, so that subsequent weak learners focus more on\n+these misclassified instances.\n+\"\"\"\n+\n+class AdaBoost:\n+    def __init__(self, n_estimators: int = 50) -> None:\n+        \"\"\"\n+        Initialize the AdaBoost classifier.\n+\n+        Parameters:\n+        - n_estimators (int): The number of weak learners to combine in the ensemble.\n+\n+        Attributes:\n+        - n_estimators (int): The number of weak learners.\n+        - alphas (List[float]): List to store alpha values.\n+        - models (List[DecisionTreeClassifier]): List to store individual decision tree models.\n+        \"\"\"\n+        self.n_estimators = n_estimators\n+        self.alphas: List[float] = []\n+        self.models: List[DecisionTreeClassifier] = []\n+\n+    def train(self, x: np.ndarray, y: np.ndarray) -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/adaboost_classifier.py, please provide doctest for the function train\n\nplease provide descriptive name for the parameter: x\n\nplease provide descriptive name for the parameter: y",
    "line_number": 39,
    "enriched": "File: machine_learning/adaboost_classifier.py\nCode: @@ -0,0 +1,108 @@\n+import doctest\n+from typing import List\n+\n+import numpy as np\n+from sklearn.datasets import load_iris\n+from sklearn.tree import DecisionTreeClassifier\n+\n+\"\"\"\n+Implementation of basic AdaBoost classifier on iris dataset.\n+The following classifier uses a pre-built DecisionTreeClassifier\n+from scikit-learn as a weak learner.\n+\n+AdaBoost (Adaptive Boosting) is an ensemble learning technique\n+used for classification problems. It combines multiple weak learners\n+(in this case, decision trees with maximum depth 1) to create a\n+strong classifier. The key idea behind AdaBoost is to give\n+more weight to the training instances that are misclassified by\n+the previous weak learners, so that subsequent weak learners focus more on\n+these misclassified instances.\n+\"\"\"\n+\n+class AdaBoost:\n+    def __init__(self, n_estimators: int = 50) -> None:\n+        \"\"\"\n+        Initialize the AdaBoost classifier.\n+\n+        Parameters:\n+        - n_estimators (int): The number of weak learners to combine in the ensemble.\n+\n+        Attributes:\n+        - n_estimators (int): The number of weak learners.\n+        - alphas (List[float]): List to store alpha values.\n+        - models (List[DecisionTreeClassifier]): List to store individual decision tree models.\n+        \"\"\"\n+        self.n_estimators = n_estimators\n+        self.alphas: List[float] = []\n+        self.models: List[DecisionTreeClassifier] = []\n+\n+    def train(self, x: np.ndarray, y: np.ndarray) -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/adaboost_classifier.py`, please provide doctest for the function `train`\n\nPlease provide descriptive name for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `y`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/adaboost_classifier.py",
    "pr_number": 10559,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359941995,
    "comment_created_at": "2023-10-15T20:05:53Z"
  },
  {
    "code": "@@ -0,0 +1,76 @@\n+\"\"\"\n+In this problem, we want to multiply the matrix using Strassen's algorithm.\n+\n+Article about it here:\n+https://www.interviewbit.com/blog/strassens-matrix-multiplication/\n+\"\"\"\n+\n+import numpy as np\n+\n+def input_matrix(m, n):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file matrix/strassen_matrix_multiplication.py, please provide doctest for the function input_matrix\n\nplease provide return type hint for the function: input_matrix. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: m\n\nplease provide descriptive name for the parameter: m\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n",
    "line_number": 10,
    "enriched": "File: matrix/strassen_matrix_multiplication.py\nCode: @@ -0,0 +1,76 @@\n+\"\"\"\n+In this problem, we want to multiply the matrix using Strassen's algorithm.\n+\n+Article about it here:\n+https://www.interviewbit.com/blog/strassens-matrix-multiplication/\n+\"\"\"\n+\n+import numpy as np\n+\n+def input_matrix(m, n):\nComment: As there is no test file in this pull request nor any test function or class in the file `matrix/strassen_matrix_multiplication.py`, please provide doctest for the function `input_matrix`\n\nPlease provide return type hint for the function: `input_matrix`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `m`\n\nPlease provide descriptive name for the parameter: `m`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "matrix/strassen_matrix_multiplication.py",
    "pr_number": 9820,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347445037,
    "comment_created_at": "2023-10-05T13:42:43Z"
  },
  {
    "code": "@@ -4,101 +4,84 @@\n def solve_maze(maze: list[list[int]]) -> bool:\n     \"\"\"\n     This method solves the \"rat in maze\" problem.\n-    In this problem we have some n by n matrix, a start point and an end point.\n-    We want to go from the start to the end. In this matrix zeroes represent walls\n-    and ones paths we can use.\n-    Parameters :\n-        maze(2D matrix) : maze\n+    In this problem, we have an n by n matrix, a start point, and an end point.\n+    We want to go from the start to the end. In this matrix, ones represent walls,\n+    and zeros represent paths we can use.\n+\n+    Parameters:\n+        maze (2D matrix): The maze where 1 represents walls, and 0 represents paths.\n+\n     Returns:\n-        Return: True if the maze has a solution or False if it does not.\n+        bool: True if a solution exists, False otherwise.\n+\n     >>> maze = [[0, 1, 0, 1, 1],\n     ...         [0, 0, 0, 0, 0],\n     ...         [1, 0, 1, 0, 1],\n     ...         [0, 0, 1, 0, 0],\n     ...         [1, 0, 0, 1, 0]]\n     >>> solve_maze(maze)\n+    True\n+    Path:",
    "comment": "changing this code is modifying what the doctest is looking for.  as discussed in contributing.md you can run these tests on your local computer by doing.\r\n% python3 -m doctest -v backtracking/rat_in_maze.py\r\n\r\nhttps://github.com/thealgorithms/python/actions/runs/6284643240/job/17066208866?pr=9082#step:6:709\r\n\r\n=================================== failures ===================================\r\n________________ [doctest] backtracking.rat_in_maze.solve_maze _________________\r\n013 \r\n014     returns:\r\n015         bool: true if a solution exists, false otherwise.\r\n016 \r\n017     >>> maze = [[0, 1, 0, 1, 1],\r\n018     ...         [0, 0, 0, 0, 0],\r\n019     ...         [1, 0, 1, 0, 1],\r\n020     ...         [0, 0, 1, 0, 0],\r\n021     ...         [1, 0, 0, 1, 0]]\r\n022     >>> solve_maze(maze)\r\ndifferences (unified diff with -expected +actual):\r\n    @@ -1,3 +1,2 @@\r\n    -true\r\n     path:\r\n     [1, 0, 0, 0, 0]\r\n    @@ -6,2 +5,3 @@\r\n     [0, 0, 0, 1, 1]\r\n     [0, 0, 0, 0, 1]\r\n    +true",
    "line_number": 24,
    "enriched": "File: backtracking/rat_in_maze.py\nCode: @@ -4,101 +4,84 @@\n def solve_maze(maze: list[list[int]]) -> bool:\n     \"\"\"\n     This method solves the \"rat in maze\" problem.\n-    In this problem we have some n by n matrix, a start point and an end point.\n-    We want to go from the start to the end. In this matrix zeroes represent walls\n-    and ones paths we can use.\n-    Parameters :\n-        maze(2D matrix) : maze\n+    In this problem, we have an n by n matrix, a start point, and an end point.\n+    We want to go from the start to the end. In this matrix, ones represent walls,\n+    and zeros represent paths we can use.\n+\n+    Parameters:\n+        maze (2D matrix): The maze where 1 represents walls, and 0 represents paths.\n+\n     Returns:\n-        Return: True if the maze has a solution or False if it does not.\n+        bool: True if a solution exists, False otherwise.\n+\n     >>> maze = [[0, 1, 0, 1, 1],\n     ...         [0, 0, 0, 0, 0],\n     ...         [1, 0, 1, 0, 1],\n     ...         [0, 0, 1, 0, 0],\n     ...         [1, 0, 0, 1, 0]]\n     >>> solve_maze(maze)\n+    True\n+    Path:\nComment: Changing this code is modifying what the doctest is looking for.  As discussed in `CONTRIBUTING.md` you can run these tests on your local computer by doing.\r\n% `python3 -m doctest -v backtracking/rat_in_maze.py`\r\n\r\nhttps://github.com/TheAlgorithms/Python/actions/runs/6284643240/job/17066208866?pr=9082#step:6:709\r\n```\r\n=================================== FAILURES ===================================\r\n________________ [doctest] backtracking.rat_in_maze.solve_maze _________________\r\n013 \r\n014     Returns:\r\n015         bool: True if a solution exists, False otherwise.\r\n016 \r\n017     >>> maze = [[0, 1, 0, 1, 1],\r\n018     ...         [0, 0, 0, 0, 0],\r\n019     ...         [1, 0, 1, 0, 1],\r\n020     ...         [0, 0, 1, 0, 0],\r\n021     ...         [1, 0, 0, 1, 0]]\r\n022     >>> solve_maze(maze)\r\nDifferences (unified diff with -expected +actual):\r\n    @@ -1,3 +1,2 @@\r\n    -True\r\n     Path:\r\n     [1, 0, 0, 0, 0]\r\n    @@ -6,2 +5,3 @@\r\n     [0, 0, 0, 1, 1]\r\n     [0, 0, 0, 0, 1]\r\n    +True\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "backtracking/rat_in_maze.py",
    "pr_number": 9082,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1335133316,
    "comment_created_at": "2023-09-24T06:46:27Z"
  },
  {
    "code": "@@ -0,0 +1,65 @@\n+\"\"\"\n+Project Euler Problem: https://projecteuler.net/problem=95\n+\n+An amicable chain is a sequence of numbers where each number is the sum of the proper divisors of the previous one, and the chain eventually returns to the starting number.\n+\n+The problem is to find the smallest member of the longest amicable chain under a given limit.\n+\n+In this implementation, we aim to identify all amicable chains and find the one with the maximum length, while also returning the smallest member of that chain.\n+\"\"\"\n+\n+def sum_of_proper_divisors(n):",
    "comment": "please provide return type hint for the function: sum_of_proper_divisors. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file project_euler/problem_095/sol1.py, please provide doctest for the function sum_of_proper_divisors\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n",
    "line_number": 11,
    "enriched": "File: project_euler/problem_095/sol1.py\nCode: @@ -0,0 +1,65 @@\n+\"\"\"\n+Project Euler Problem: https://projecteuler.net/problem=95\n+\n+An amicable chain is a sequence of numbers where each number is the sum of the proper divisors of the previous one, and the chain eventually returns to the starting number.\n+\n+The problem is to find the smallest member of the longest amicable chain under a given limit.\n+\n+In this implementation, we aim to identify all amicable chains and find the one with the maximum length, while also returning the smallest member of that chain.\n+\"\"\"\n+\n+def sum_of_proper_divisors(n):\nComment: Please provide return type hint for the function: `sum_of_proper_divisors`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `project_euler/problem_095/sol1.py`, please provide doctest for the function `sum_of_proper_divisors`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "project_euler/problem_095/sol1.py",
    "pr_number": 12113,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1802426737,
    "comment_created_at": "2024-10-16T06:17:39Z"
  },
  {
    "code": "@@ -0,0 +1,41 @@\n+# Python3 program for implementation of Shell Sort \n+# Python3 program for implementation of Shell Sort \n+\n+def shellSort(arr, n): ",
    "comment": "please provide return type hint for the function: shellsort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file shell-sort.py, please provide doctest for the function shellsort\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: shellsort\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n",
    "line_number": 4,
    "enriched": "File: shell-sort.py\nCode: @@ -0,0 +1,41 @@\n+# Python3 program for implementation of Shell Sort \n+# Python3 program for implementation of Shell Sort \n+\n+def shellSort(arr, n): \nComment: Please provide return type hint for the function: `shellSort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `shell-sort.py`, please provide doctest for the function `shellSort`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `shellSort`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "shell-sort.py",
    "pr_number": 10775,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367746370,
    "comment_created_at": "2023-10-21T15:22:13Z"
  },
  {
    "code": "@@ -0,0 +1,23 @@\n+'''\n+Maximum Product Subarray\n+This algorithm finds the contiguous subarray within an array that has the largest product. \n+'''\n+def maxProoductSubarray(nums, n):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file divide_and_conquer/maximumproductsubarray.py, please provide doctest for the function maxprooductsubarray\n\nplease provide return type hint for the function: maxprooductsubarray. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: maxprooductsubarray\n\nplease provide type hint for the parameter: nums\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n",
    "line_number": 5,
    "enriched": "File: divide_and_conquer/MaximumProductSubarray.py\nCode: @@ -0,0 +1,23 @@\n+'''\n+Maximum Product Subarray\n+This algorithm finds the contiguous subarray within an array that has the largest product. \n+'''\n+def maxProoductSubarray(nums, n):\nComment: As there is no test file in this pull request nor any test function or class in the file `divide_and_conquer/MaximumProductSubarray.py`, please provide doctest for the function `maxProoductSubarray`\n\nPlease provide return type hint for the function: `maxProoductSubarray`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `maxProoductSubarray`\n\nPlease provide type hint for the parameter: `nums`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "divide_and_conquer/MaximumProductSubarray.py",
    "pr_number": 10231,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1352710346,
    "comment_created_at": "2023-10-10T14:50:19Z"
  },
  {
    "code": "@@ -266,7 +266,7 @@ def get(self, key: T) -> U | None:\n         self.miss += 1\n         return None\n \n-    def set(self, key: T, value: U) -> None:\n+    def set_key(self, key: T, value: U) -> None:",
    "comment": "set() is still called in the doctests.",
    "line_number": 269,
    "enriched": "File: other/lru_cache.py\nCode: @@ -266,7 +266,7 @@ def get(self, key: T) -> U | None:\n         self.miss += 1\n         return None\n \n-    def set(self, key: T, value: U) -> None:\n+    def set_key(self, key: T, value: U) -> None:\nComment: `set()` is still called in the doctests.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "other/lru_cache.py",
    "pr_number": 7105,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994538317,
    "comment_created_at": "2022-10-13T11:45:48Z"
  },
  {
    "code": "@@ -0,0 +1,17 @@\n+import sys\n+def palindrome(string):",
    "comment": "please provide return type hint for the function: palindrome. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file strings/checkifpalindrome.py, please provide doctest for the function palindrome\n\nplease provide type hint for the parameter: string",
    "line_number": 2,
    "enriched": "File: strings/CheckIfPalindrome.py\nCode: @@ -0,0 +1,17 @@\n+import sys\n+def palindrome(string):\nComment: Please provide return type hint for the function: `palindrome`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `strings/CheckIfPalindrome.py`, please provide doctest for the function `palindrome`\n\nPlease provide type hint for the parameter: `string`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "strings/CheckIfPalindrome.py",
    "pr_number": 7204,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996238178,
    "comment_created_at": "2022-10-15T02:09:56Z"
  },
  {
    "code": "@@ -0,0 +1,10 @@\n+import re\n+\n+def camel_with_numbers_to_snake(camel_case):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file strings/camel_to_snake_case.py, please provide doctest for the function camel_with_numbers_to_snake\n\nplease provide return type hint for the function: camel_with_numbers_to_snake. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: camel_case",
    "line_number": 3,
    "enriched": "File: strings/camel_to_snake_case.py\nCode: @@ -0,0 +1,10 @@\n+import re\n+\n+def camel_with_numbers_to_snake(camel_case):\nComment: As there is no test file in this pull request nor any test function or class in the file `strings/camel_to_snake_case.py`, please provide doctest for the function `camel_with_numbers_to_snake`\n\nPlease provide return type hint for the function: `camel_with_numbers_to_snake`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `camel_case`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "strings/camel_to_snake_case.py",
    "pr_number": 9763,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346658559,
    "comment_created_at": "2023-10-05T01:20:52Z"
  },
  {
    "code": "@@ -0,0 +1,57 @@\n+'''\n+Meta Binary Search, also known as One-Sided Binary Search, is a variation of the binary search algorithm used to search ordered lists. \n+It reduces the number of comparisons needed by performing controlled jumps across the list. It starts with a large interval, compares \n+the middle element to the target, and adjusts the interval based on the comparison. While it may perform fewer comparisons for targets \n+near the beginning, it can require more for targets near the end. It works best when the list order aligns with target distribution.\n+\n+Refer this link for more details : https://www.geeksforgeeks.org/meta-binary-search-one-sided-binary-search/\n+\n+Time Complexity : O(log N)\n+Space Complexity : O(1)\n+'''\n+\n+import math\n+def metaBinarySearch(_Array, key_to_search):",
    "comment": "please provide return type hint for the function: metabinarysearch. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file searches/meta_binary_search.py, please provide doctest for the function metabinarysearch\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: metabinarysearch\n\nplease provide type hint for the parameter: _array\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: _array\n\nplease provide type hint for the parameter: key_to_search",
    "line_number": 14,
    "enriched": "File: searches/meta_binary_search.py\nCode: @@ -0,0 +1,57 @@\n+'''\n+Meta Binary Search, also known as One-Sided Binary Search, is a variation of the binary search algorithm used to search ordered lists. \n+It reduces the number of comparisons needed by performing controlled jumps across the list. It starts with a large interval, compares \n+the middle element to the target, and adjusts the interval based on the comparison. While it may perform fewer comparisons for targets \n+near the beginning, it can require more for targets near the end. It works best when the list order aligns with target distribution.\n+\n+Refer this link for more details : https://www.geeksforgeeks.org/meta-binary-search-one-sided-binary-search/\n+\n+Time Complexity : O(log N)\n+Space Complexity : O(1)\n+'''\n+\n+import math\n+def metaBinarySearch(_Array, key_to_search):\nComment: Please provide return type hint for the function: `metaBinarySearch`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `searches/meta_binary_search.py`, please provide doctest for the function `metaBinarySearch`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `metaBinarySearch`\n\nPlease provide type hint for the parameter: `_Array`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `_Array`\n\nPlease provide type hint for the parameter: `key_to_search`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "searches/meta_binary_search.py",
    "pr_number": 9227,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342098607,
    "comment_created_at": "2023-10-01T08:27:04Z"
  },
  {
    "code": "@@ -3,14 +3,16 @@\n predictive analysis. The idea is pretty simple: we have a dataset and we have\n features associated with it. Features should be chosen very cautiously\n as they determine how much our model will be able to make future predictions.\n-We try to set the weight of these features, over many iterations, so that they best\n-fit our dataset. In this particular code, I had used a CSGO dataset (ADR vs\n-Rating). We try to best fit a line through dataset and estimate the parameters.\n+We try to set the weight of these features, using \"sum of rectangular area\n+over sum of square area\" method which is a direct method.\n+In this particular code, I had used a CSGO dataset (ADR vs Rating).\n+We try to best fit a line through dataset and estimate the parameters.\n \"\"\"\n import numpy as np\n import requests\n \n \n+# Function to collect the CSGO dataset",
    "comment": "this comment is unnecessary because we already have a docstring for the function.",
    "line_number": 15,
    "enriched": "File: machine_learning/linear_regression.py\nCode: @@ -3,14 +3,16 @@\n predictive analysis. The idea is pretty simple: we have a dataset and we have\n features associated with it. Features should be chosen very cautiously\n as they determine how much our model will be able to make future predictions.\n-We try to set the weight of these features, over many iterations, so that they best\n-fit our dataset. In this particular code, I had used a CSGO dataset (ADR vs\n-Rating). We try to best fit a line through dataset and estimate the parameters.\n+We try to set the weight of these features, using \"sum of rectangular area\n+over sum of square area\" method which is a direct method.\n+In this particular code, I had used a CSGO dataset (ADR vs Rating).\n+We try to best fit a line through dataset and estimate the parameters.\n \"\"\"\n import numpy as np\n import requests\n \n \n+# Function to collect the CSGO dataset\nComment: ```suggestion\r\n```\r\nThis comment is unnecessary because we already have a docstring for the function.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "machine_learning/linear_regression.py",
    "pr_number": 9141,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1341999071,
    "comment_created_at": "2023-09-30T19:07:44Z"
  },
  {
    "code": "@@ -0,0 +1,64 @@\n+\"\"\"\n+    Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses.\n+    Constraints: 1 <= n <= 8\n+    Problem source: https://leetcode.com/problems/generate-parentheses/\n+\"\"\"\n+\n+class Solution:\n+    def generateParenthesis(self, n):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file backtracking/generate_parenthesis.py, please provide doctest for the function generateparenthesis\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: generateparenthesis\n\nplease provide return type hint for the function: generateparenthesis. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: n",
    "line_number": 8,
    "enriched": "File: backtracking/generate_parenthesis.py\nCode: @@ -0,0 +1,64 @@\n+\"\"\"\n+    Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses.\n+    Constraints: 1 <= n <= 8\n+    Problem source: https://leetcode.com/problems/generate-parentheses/\n+\"\"\"\n+\n+class Solution:\n+    def generateParenthesis(self, n):\nComment: As there is no test file in this pull request nor any test function or class in the file `backtracking/generate_parenthesis.py`, please provide doctest for the function `generateParenthesis`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `generateParenthesis`\n\nPlease provide return type hint for the function: `generateParenthesis`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide type hint for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "backtracking/generate_parenthesis.py",
    "pr_number": 10118,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349710160,
    "comment_created_at": "2023-10-08T14:58:56Z"
  },
  {
    "code": "@@ -0,0 +1,111 @@\n+def encrypt(plaintext, key) -> str:\n+    \"\"\"\n+    Function that encrypt a given plaintext (string)\n+    and key (string), returning the encrypted ciphertext\n+\n+    @params\n+    plaintext - a normal text to be encrypted (string)\n+    key - a small text or word to start the replacing (sFtring)\n+\n+    @return\n+    A string with the ciphertext\n+",
    "comment": "typically, we want the parameter names to be self-documenting so this isn't really necessary",
    "line_number": 12,
    "enriched": "File: ciphers/autoclave.py\nCode: @@ -0,0 +1,111 @@\n+def encrypt(plaintext, key) -> str:\n+    \"\"\"\n+    Function that encrypt a given plaintext (string)\n+    and key (string), returning the encrypted ciphertext\n+\n+    @params\n+    plaintext - a normal text to be encrypted (string)\n+    key - a small text or word to start the replacing (sFtring)\n+\n+    @return\n+    A string with the ciphertext\n+\nComment: ```suggestion\n```\nTypically, we want the parameter names to be self-documenting so this isn't really necessary ",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "ciphers/autoclave.py",
    "pr_number": 8029,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1048752085,
    "comment_created_at": "2022-12-14T17:19:28Z"
  },
  {
    "code": "@@ -0,0 +1,25 @@\n+\"\"\" An AND Gate is a logic gate in boolean algebra which results to false(0)\n+    if any of the input is 0, and True(1) if  both the inputs are 1.\n+    \"\"\"\n+def and_gate(input_1, input_2):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file boolean_algebra/andgate.py, please provide doctest for the function and_gate\n\nplease provide return type hint for the function: and_gate. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: input_1\n\nplease provide type hint for the parameter: input_2",
    "line_number": 4,
    "enriched": "File: boolean_algebra/andgate.py\nCode: @@ -0,0 +1,25 @@\n+\"\"\" An AND Gate is a logic gate in boolean algebra which results to false(0)\n+    if any of the input is 0, and True(1) if  both the inputs are 1.\n+    \"\"\"\n+def and_gate(input_1, input_2):\nComment: As there is no test file in this pull request nor any test function or class in the file `boolean_algebra/andgate.py`, please provide doctest for the function `and_gate`\n\nPlease provide return type hint for the function: `and_gate`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `input_1`\n\nPlease provide type hint for the parameter: `input_2`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "boolean_algebra/andgate.py",
    "pr_number": 7384,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 997373425,
    "comment_created_at": "2022-10-17T18:17:10Z"
  },
  {
    "code": "@@ -0,0 +1,80 @@\n+\"\"\"\n+A list that has the common distinct element from both arrays and\n+if there are repetitions of the element then only one occurrence\n+is considered, known as the union of both arrays.\n+\n+A list that has common distinct elements from both arrays,\n+is the intersection of both arrays.\n+\n+\n+Example:\n+Input: a[] = {1, 2, 2, 3, 4, 8, 10}, b[] = {4, 6, 3, 1}\n+Output: {1, 2, 3, 4, 6, 8, 10}\n+Explanation: 1, 2, 3, 4, 6, 8 and 10 is the union of\n+elements present in array a[] and array b[].\n+\n+Input: a[] = {1, 2, 2, 3, 4, 8, 10}, b[] = {4, 6, 3, 1}\n+Output: {1, 2, 3, 4}\n+Explanation: 1, 2, 3, and 4 are the intersection(common elements)\n+of elements present in array a[] and array b[].\n+\n+https://www.geeksforgeeks.org/union-and-intersection-of-two-sorted-arrays-2/\n+\n+\n+\"\"\"\n+\n+# Taking input lists from user\n+\n+a = list(map(int, input(\"Enter elements of first list:\").split()))\n+b = list(map(int, input(\"Enter elements of second list:\").split()))\n+# Example Input\n+# Enter elements of first list: 3 4 6 4  4 6 7 41\n+# Enter elements of second list: 78 3 5 7 -1 9 2 -5\n+\n+\n+\"\"\"bitwise or (|) between the sets of both arrays\n+ to find union and assign it into a variable A\n+  in the form of lists.\n+  bitwise and (&) between the sets of both arrays\n+ to find intersection and assign it into a variable A\n+  in the form of lists.\n+\"\"\"\n+\n+# Mocking input by predefined lists\n+def mock_input():",
    "comment": "please provide return type hint for the function: mock_input. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file data_structures/arrays/union_intersection_array.py, please provide doctest for the function mock_input",
    "line_number": 44,
    "enriched": "File: data_structures/arrays/union_intersection_array.py\nCode: @@ -0,0 +1,80 @@\n+\"\"\"\n+A list that has the common distinct element from both arrays and\n+if there are repetitions of the element then only one occurrence\n+is considered, known as the union of both arrays.\n+\n+A list that has common distinct elements from both arrays,\n+is the intersection of both arrays.\n+\n+\n+Example:\n+Input: a[] = {1, 2, 2, 3, 4, 8, 10}, b[] = {4, 6, 3, 1}\n+Output: {1, 2, 3, 4, 6, 8, 10}\n+Explanation: 1, 2, 3, 4, 6, 8 and 10 is the union of\n+elements present in array a[] and array b[].\n+\n+Input: a[] = {1, 2, 2, 3, 4, 8, 10}, b[] = {4, 6, 3, 1}\n+Output: {1, 2, 3, 4}\n+Explanation: 1, 2, 3, and 4 are the intersection(common elements)\n+of elements present in array a[] and array b[].\n+\n+https://www.geeksforgeeks.org/union-and-intersection-of-two-sorted-arrays-2/\n+\n+\n+\"\"\"\n+\n+# Taking input lists from user\n+\n+a = list(map(int, input(\"Enter elements of first list:\").split()))\n+b = list(map(int, input(\"Enter elements of second list:\").split()))\n+# Example Input\n+# Enter elements of first list: 3 4 6 4  4 6 7 41\n+# Enter elements of second list: 78 3 5 7 -1 9 2 -5\n+\n+\n+\"\"\"bitwise or (|) between the sets of both arrays\n+ to find union and assign it into a variable A\n+  in the form of lists.\n+  bitwise and (&) between the sets of both arrays\n+ to find intersection and assign it into a variable A\n+  in the form of lists.\n+\"\"\"\n+\n+# Mocking input by predefined lists\n+def mock_input():\nComment: Please provide return type hint for the function: `mock_input`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `data_structures/arrays/union_intersection_array.py`, please provide doctest for the function `mock_input`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/arrays/union_intersection_array.py",
    "pr_number": 11891,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1792189696,
    "comment_created_at": "2024-10-08T16:33:35Z"
  },
  {
    "code": "@@ -0,0 +1,30 @@\n+\"\"\"\n+\tImplemented an algorithm using opencv to convert a colored image into its grayscale\n+\"\"\"\n+from cv2 import destroyAllWindows, imread, imshow, waitKey\n+\n+\n+def convert_to_grayscale(img):",
    "comment": "please provide return type hint for the function: convert_to_grayscale. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file digital_image_processing/convert_to_grayscale.py, please provide doctest for the function convert_to_grayscale\n\nplease provide type hint for the parameter: img",
    "line_number": 7,
    "enriched": "File: digital_image_processing/convert_to_grayscale.py\nCode: @@ -0,0 +1,30 @@\n+\"\"\"\n+\tImplemented an algorithm using opencv to convert a colored image into its grayscale\n+\"\"\"\n+from cv2 import destroyAllWindows, imread, imshow, waitKey\n+\n+\n+def convert_to_grayscale(img):\nComment: Please provide return type hint for the function: `convert_to_grayscale`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `digital_image_processing/convert_to_grayscale.py`, please provide doctest for the function `convert_to_grayscale`\n\nPlease provide type hint for the parameter: `img`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "digital_image_processing/convert_to_grayscale.py",
    "pr_number": 8051,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1057516735,
    "comment_created_at": "2022-12-27T08:17:39Z"
  },
  {
    "code": "@@ -0,0 +1,85 @@\n+from flask import Flask, request, render_template_string\n+\n+app = Flask(__name__)\n+\n+def calculate(num1: float, num2: float, operation: str) -> float:\n+    \"\"\"\n+    Perform basic arithmetic operations: add, subtract, multiply, divide.\n+\n+    >>> calculate(2, 3, 'add')\n+    5\n+    >>> calculate(5, 3, 'subtract')\n+    2\n+    >>> calculate(4, 2, 'multiply')\n+    8\n+    >>> calculate(10, 2, 'divide')\n+    5.0\n+    >>> calculate(5, 0, 'divide')\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Division by zero is not allowed.\n+    \"\"\"\n+    if operation == \"add\":\n+        return num1 + num2\n+    elif operation == \"subtract\":\n+        return num1 - num2\n+    elif operation == \"multiply\":\n+        return num1 * num2\n+    elif operation == \"divide\":\n+        if num2 == 0:\n+            raise ValueError(\"Division by zero is not allowed.\")\n+        return num1 / num2\n+    else:\n+        raise ValueError(f\"Unknown operation: {operation}\")\n+\n+# HTML template for the web interface\n+template = \"\"\"\n+<!DOCTYPE html>\n+<html>\n+<head>\n+    <title>Flask Calculator</title>\n+    <style>\n+        body { font-family: Arial, sans-serif; text-align: center; padding: 50px; background: #f7f7f7; }\n+        input, select { padding: 10px; margin: 5px; width: 150px; }\n+        input[type=submit] { width: auto; cursor: pointer; }\n+        .result { margin-top: 20px; font-size: 1.5em; color: #27ae60; }\n+        h1 { color: #2c3e50; }\n+    </style>\n+</head>\n+<body>\n+    <h1>Flask Calculator</h1>\n+    <form method=\"POST\">\n+        <input type=\"number\" name=\"num1\" step=\"any\" placeholder=\"First Number\" required>\n+        <input type=\"number\" name=\"num2\" step=\"any\" placeholder=\"Second Number\" required>\n+        <br>\n+        <select name=\"operation\">\n+            <option value=\"add\">Add (+)</option>\n+            <option value=\"subtract\">Subtract (-)</option>\n+            <option value=\"multiply\">Multiply (\u00d7)</option>\n+            <option value=\"divide\">Divide (\u00f7)</option>\n+        </select>\n+        <br>\n+        <input type=\"submit\" value=\"Calculate\">\n+    </form>\n+    {% if result is not none %}\n+        <div class=\"result\">Result: {{ result }}</div>\n+    {% endif %}\n+</body>\n+</html>\n+\"\"\"\n+\n+@app.route(\"/\", methods=[\"GET\", \"POST\"])\n+def home():",
    "comment": "please provide return type hint for the function: home. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file other/calc.py, please provide doctest for the function home",
    "line_number": 72,
    "enriched": "File: other/calc.py\nCode: @@ -0,0 +1,85 @@\n+from flask import Flask, request, render_template_string\n+\n+app = Flask(__name__)\n+\n+def calculate(num1: float, num2: float, operation: str) -> float:\n+    \"\"\"\n+    Perform basic arithmetic operations: add, subtract, multiply, divide.\n+\n+    >>> calculate(2, 3, 'add')\n+    5\n+    >>> calculate(5, 3, 'subtract')\n+    2\n+    >>> calculate(4, 2, 'multiply')\n+    8\n+    >>> calculate(10, 2, 'divide')\n+    5.0\n+    >>> calculate(5, 0, 'divide')\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Division by zero is not allowed.\n+    \"\"\"\n+    if operation == \"add\":\n+        return num1 + num2\n+    elif operation == \"subtract\":\n+        return num1 - num2\n+    elif operation == \"multiply\":\n+        return num1 * num2\n+    elif operation == \"divide\":\n+        if num2 == 0:\n+            raise ValueError(\"Division by zero is not allowed.\")\n+        return num1 / num2\n+    else:\n+        raise ValueError(f\"Unknown operation: {operation}\")\n+\n+# HTML template for the web interface\n+template = \"\"\"\n+<!DOCTYPE html>\n+<html>\n+<head>\n+    <title>Flask Calculator</title>\n+    <style>\n+        body { font-family: Arial, sans-serif; text-align: center; padding: 50px; background: #f7f7f7; }\n+        input, select { padding: 10px; margin: 5px; width: 150px; }\n+        input[type=submit] { width: auto; cursor: pointer; }\n+        .result { margin-top: 20px; font-size: 1.5em; color: #27ae60; }\n+        h1 { color: #2c3e50; }\n+    </style>\n+</head>\n+<body>\n+    <h1>Flask Calculator</h1>\n+    <form method=\"POST\">\n+        <input type=\"number\" name=\"num1\" step=\"any\" placeholder=\"First Number\" required>\n+        <input type=\"number\" name=\"num2\" step=\"any\" placeholder=\"Second Number\" required>\n+        <br>\n+        <select name=\"operation\">\n+            <option value=\"add\">Add (+)</option>\n+            <option value=\"subtract\">Subtract (-)</option>\n+            <option value=\"multiply\">Multiply (\u00d7)</option>\n+            <option value=\"divide\">Divide (\u00f7)</option>\n+        </select>\n+        <br>\n+        <input type=\"submit\" value=\"Calculate\">\n+    </form>\n+    {% if result is not none %}\n+        <div class=\"result\">Result: {{ result }}</div>\n+    {% endif %}\n+</body>\n+</html>\n+\"\"\"\n+\n+@app.route(\"/\", methods=[\"GET\", \"POST\"])\n+def home():\nComment: Please provide return type hint for the function: `home`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `other/calc.py`, please provide doctest for the function `home`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "other/calc.py",
    "pr_number": 13265,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2406650279,
    "comment_created_at": "2025-10-06T14:26:11Z"
  },
  {
    "code": "@@ -0,0 +1,46 @@\n+from cv2 import COLOR_BGR2GRAY, cvtColor, imread, imshow, waitKey\n+from numpy import array, dot, pad, ravel, uint8, zeros\n+\n+\n+def im2col(image, block_size):",
    "comment": "please provide return type hint for the function: im2col. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file digital_image_processing/simple_blur.py, please provide doctest for the function im2col\n\nplease provide type hint for the parameter: image\n\nplease provide type hint for the parameter: block_size",
    "line_number": 5,
    "enriched": "File: digital_image_processing/simple_blur.py\nCode: @@ -0,0 +1,46 @@\n+from cv2 import COLOR_BGR2GRAY, cvtColor, imread, imshow, waitKey\n+from numpy import array, dot, pad, ravel, uint8, zeros\n+\n+\n+def im2col(image, block_size):\nComment: Please provide return type hint for the function: `im2col`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `digital_image_processing/simple_blur.py`, please provide doctest for the function `im2col`\n\nPlease provide type hint for the parameter: `image`\n\nPlease provide type hint for the parameter: `block_size`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "digital_image_processing/simple_blur.py",
    "pr_number": 8052,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1057517037,
    "comment_created_at": "2022-12-27T08:18:19Z"
  },
  {
    "code": "@@ -0,0 +1,55 @@\n+import random\n+\n+# Define the fitness function (objective function)\n+def fitness_function(x):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file genetic_algorithm/generic_optimisation.py, please provide doctest for the function fitness_function\n\nplease provide return type hint for the function: fitness_function. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: x\n\nplease provide descriptive name for the parameter: x",
    "line_number": 4,
    "enriched": "File: genetic_algorithm/generic_optimisation.py\nCode: @@ -0,0 +1,55 @@\n+import random\n+\n+# Define the fitness function (objective function)\n+def fitness_function(x):\nComment: As there is no test file in this pull request nor any test function or class in the file `genetic_algorithm/generic_optimisation.py`, please provide doctest for the function `fitness_function`\n\nPlease provide return type hint for the function: `fitness_function`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "genetic_algorithm/generic_optimisation.py",
    "pr_number": 9557,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1343508835,
    "comment_created_at": "2023-10-03T05:21:35Z"
  },
  {
    "code": "@@ -0,0 +1,36 @@\n+\n+def seidel(a, x ,b): ",
    "comment": "as there is no test file in this pull request nor any test function or class in the file arithmetic_analysis/gauss_seidal.py, please provide doctest for the function seidel\n\nplease provide return type hint for the function: seidel. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide descriptive name for the parameter: a\n\nplease provide type hint for the parameter: a\n\nplease provide descriptive name for the parameter: x\n\nplease provide type hint for the parameter: x\n\nplease provide descriptive name for the parameter: b\n\nplease provide type hint for the parameter: b",
    "line_number": 2,
    "enriched": "File: arithmetic_analysis/gauss_seidal.py\nCode: @@ -0,0 +1,36 @@\n+\n+def seidel(a, x ,b): \nComment: As there is no test file in this pull request nor any test function or class in the file `arithmetic_analysis/gauss_seidal.py`, please provide doctest for the function `seidel`\n\nPlease provide return type hint for the function: `seidel`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide descriptive name for the parameter: `a`\n\nPlease provide type hint for the parameter: `a`\n\nPlease provide descriptive name for the parameter: `x`\n\nPlease provide type hint for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `b`\n\nPlease provide type hint for the parameter: `b`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "arithmetic_analysis/gauss_seidal.py",
    "pr_number": 9984,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349469529,
    "comment_created_at": "2023-10-07T06:09:11Z"
  },
  {
    "code": "@@ -0,0 +1,13 @@\n+\n+def checkPowerOf4(n):",
    "comment": "please provide return type hint for the function: checkpowerof4. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file bit_manipulation/number_is_a_power_of_4.py, please provide doctest for the function checkpowerof4\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: checkpowerof4\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: n",
    "line_number": 2,
    "enriched": "File: bit_manipulation/number_is_a_power_of_4.py\nCode: @@ -0,0 +1,13 @@\n+\n+def checkPowerOf4(n):\nComment: Please provide return type hint for the function: `checkPowerOf4`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `bit_manipulation/number_is_a_power_of_4.py`, please provide doctest for the function `checkPowerOf4`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `checkPowerOf4`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide type hint for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "bit_manipulation/number_is_a_power_of_4.py",
    "pr_number": 7927,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1009917282,
    "comment_created_at": "2022-10-31T22:13:04Z"
  },
  {
    "code": "@@ -0,0 +1,32 @@\n+\"\"\"\n+interval scheduling is a class of problems. The programs take a number of tasks into account. Every task is represented by an interval that indicates the amount of time it should take a machine to complete it. If there is no overlap between any two intervals on the system or resource, a subset of intervals is compatible.\n+\n+The goal of the interval scheduling maximization problem is to identify the largest compatible set or a collection of intervals with the least possible overlap. The idea is to optimize throughput by completing as many tasks as you can.\n+\n+\"\"\"\n+\n+def interval_scheduling(stimes, ftimes):",
    "comment": "please provide return type hint for the function: interval_scheduling. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file scheduling/interval_scheduling_algorithm.py, please provide doctest for the function interval_scheduling\n\nplease provide type hint for the parameter: stimes\n\nplease provide type hint for the parameter: ftimes",
    "line_number": 8,
    "enriched": "File: scheduling/interval_scheduling_algorithm.py\nCode: @@ -0,0 +1,32 @@\n+\"\"\"\n+interval scheduling is a class of problems. The programs take a number of tasks into account. Every task is represented by an interval that indicates the amount of time it should take a machine to complete it. If there is no overlap between any two intervals on the system or resource, a subset of intervals is compatible.\n+\n+The goal of the interval scheduling maximization problem is to identify the largest compatible set or a collection of intervals with the least possible overlap. The idea is to optimize throughput by completing as many tasks as you can.\n+\n+\"\"\"\n+\n+def interval_scheduling(stimes, ftimes):\nComment: Please provide return type hint for the function: `interval_scheduling`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `scheduling/interval_scheduling_algorithm.py`, please provide doctest for the function `interval_scheduling`\n\nPlease provide type hint for the parameter: `stimes`\n\nPlease provide type hint for the parameter: `ftimes`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "scheduling/interval_scheduling_algorithm.py",
    "pr_number": 11853,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1790100875,
    "comment_created_at": "2024-10-07T12:08:01Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+def calculate_parity_bit(message):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file bit_manipulation/error_detection.py, please provide doctest for the function calculate_parity_bit\n\nplease provide return type hint for the function: calculate_parity_bit. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: message",
    "line_number": 1,
    "enriched": "File: bit_manipulation/error_detection.py\nCode: @@ -0,0 +1,35 @@\n+def calculate_parity_bit(message):\nComment: As there is no test file in this pull request nor any test function or class in the file `bit_manipulation/error_detection.py`, please provide doctest for the function `calculate_parity_bit`\n\nPlease provide return type hint for the function: `calculate_parity_bit`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `message`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "bit_manipulation/error_detection.py",
    "pr_number": 10681,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1364321989,
    "comment_created_at": "2023-10-18T18:12:11Z"
  },
  {
    "code": "@@ -11,11 +11,16 @@ def find_missing_number(nums: list[int]) -> int:\n     Example:\n         >>> find_missing_number([0, 1, 3, 4])\n         2\n+        >>> find_missing_number([1, 3, 4, 5, 6])\n+        2\n+        >>> find_missing_number([6, 5, 4, 2, 1])\n+        3\n     \"\"\"\n-    n = len(nums)\n-    missing_number = n\n+    low = min(nums[0], nums[-1])\n+    high = max(nums[0], nums[-1])",
    "comment": "this wouldn't work if the numbers are out of order. try adding this doctest and you'll see what i mean:\r\npy\r\n>>> find_missing_number([6, 1, 5, 3, 4])\r\n2",
    "line_number": 20,
    "enriched": "File: bit_manipulation/missing_number.py\nCode: @@ -11,11 +11,16 @@ def find_missing_number(nums: list[int]) -> int:\n     Example:\n         >>> find_missing_number([0, 1, 3, 4])\n         2\n+        >>> find_missing_number([1, 3, 4, 5, 6])\n+        2\n+        >>> find_missing_number([6, 5, 4, 2, 1])\n+        3\n     \"\"\"\n-    n = len(nums)\n-    missing_number = n\n+    low = min(nums[0], nums[-1])\n+    high = max(nums[0], nums[-1])\nComment: This wouldn't work if the numbers are out of order. Try adding this doctest and you'll see what I mean:\r\n```py\r\n>>> find_missing_number([6, 1, 5, 3, 4])\r\n2\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "bit_manipulation/missing_number.py",
    "pr_number": 10361,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359234336,
    "comment_created_at": "2023-10-14T08:11:50Z"
  },
  {
    "code": "@@ -0,0 +1,45 @@\n+# Python Program implementation\n+# of binary insertion sort\n+\n+\n+def binary_search(arr, val, start, end):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file binaryinsertionsort.py, please provide doctest for the function binary_search\n\nplease provide return type hint for the function: binary_search. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: val\n\nplease provide type hint for the parameter: start\n\nplease provide type hint for the parameter: end",
    "line_number": 5,
    "enriched": "File: Binaryinsertionsort.py\nCode: @@ -0,0 +1,45 @@\n+# Python Program implementation\n+# of binary insertion sort\n+\n+\n+def binary_search(arr, val, start, end):\nComment: As there is no test file in this pull request nor any test function or class in the file `Binaryinsertionsort.py`, please provide doctest for the function `binary_search`\n\nPlease provide return type hint for the function: `binary_search`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `val`\n\nPlease provide type hint for the parameter: `start`\n\nPlease provide type hint for the parameter: `end`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "Binaryinsertionsort.py",
    "pr_number": 7628,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004250517,
    "comment_created_at": "2022-10-25T09:44:53Z"
  },
  {
    "code": "@@ -0,0 +1,146 @@\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\"\"\"\"\n+ FuzzySet class for triangular fuzzy sets\n+ Author: Shreya123714\n+ Source: https://en.wikipedia.org/wiki/Fuzzy_set\n+\"\"\"\n+\n+\n+class FuzzySet:",
    "comment": "please make this a https://docs.python.org/3/library/dataclasses.html",
    "line_number": 11,
    "enriched": "File: fuzzy_logic/fuzzy_operations.py\nCode: @@ -0,0 +1,146 @@\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\"\"\"\"\n+ FuzzySet class for triangular fuzzy sets\n+ Author: Shreya123714\n+ Source: https://en.wikipedia.org/wiki/Fuzzy_set\n+\"\"\"\n+\n+\n+class FuzzySet:\nComment: Please make this a https://docs.python.org/3/library/dataclasses.html",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "fuzzy_logic/fuzzy_operations.py",
    "pr_number": 11036,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375008262,
    "comment_created_at": "2023-10-27T20:16:37Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+\"\"\"\n+References for Binary, Octal, and Hexadecimal Numbers\n+\n+https://en.wikipedia.org/wiki/Binary_number\n+https://en.wikipedia.org/wiki/Octal\n+https://en.wikipedia.org/wiki/Hexadecimal\n+\n+\"\"\"\n+from __future__ import annotations\n+\n+def DecimalConversions(dec: int) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/decimalconversions.py, please provide doctest for the function decimalconversions\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: decimalconversions",
    "line_number": 11,
    "enriched": "File: maths/DecimalConversions.py\nCode: @@ -0,0 +1,56 @@\n+\"\"\"\n+References for Binary, Octal, and Hexadecimal Numbers\n+\n+https://en.wikipedia.org/wiki/Binary_number\n+https://en.wikipedia.org/wiki/Octal\n+https://en.wikipedia.org/wiki/Hexadecimal\n+\n+\"\"\"\n+from __future__ import annotations\n+\n+def DecimalConversions(dec: int) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/DecimalConversions.py`, please provide doctest for the function `DecimalConversions`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `DecimalConversions`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/DecimalConversions.py",
    "pr_number": 7206,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996248946,
    "comment_created_at": "2022-10-15T04:10:48Z"
  },
  {
    "code": "@@ -0,0 +1,141 @@\n+\"\"\"\n+Given a n by n board where n is of form 2k where k >= 1 (Basically n is a power of 2 with minimum value as 2). \n+The board has one missing cell (of size 1 x 1). Fill the board using L shaped tiles. \n+A L shaped tile is a 2 x 2 square with one cell of size 1\u00d71 missing.\n+\n+This problem can be solved using Divide and Conquer. Below is the recursive algorithm.\n+\n+// n is size of given square, p is location of missing cell\n+Tile(int n, Point p)\n+\n+1) Base case: n = 2, A 2 x 2 square with one cell missing is nothing \n+   but a tile and can be filled with a single tile.\n+\n+2) Place a L shaped tile at the center such that it does not cover\n+   the n/2 * n/2 subsquare that has a missing square. Now all four \n+   subsquares of size n/2 x n/2 have a missing cell (a cell that doesn't\n+   need to be filled).  See figure 2 below.\n+\n+3) Solve the problem recursively for following four. Let p1, p2, p3 and\n+   p4 be positions of the 4 missing cells in 4 squares.\n+   a) Tile(n/2, p1)\n+   b) Tile(n/2, p2)\n+   c) Tile(n/2, p3)\n+   d) Tile(n/2, p3)\n+\n+\"\"\"\n+import numpy as np\n+\n+tile_count = 0\n+\n+def place(grid, x1, y1, x2, y2, x3, y3):",
    "comment": "please provide return type hint for the function: place. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file divide_and_conquer/tiling.py, please provide doctest for the function place\n\nplease provide type hint for the parameter: grid\n\nplease provide type hint for the parameter: x1\n\nplease provide type hint for the parameter: y1\n\nplease provide type hint for the parameter: x2\n\nplease provide type hint for the parameter: y2\n\nplease provide type hint for the parameter: x3\n\nplease provide type hint for the parameter: y3",
    "line_number": 31,
    "enriched": "File: divide_and_conquer/tiling.py\nCode: @@ -0,0 +1,141 @@\n+\"\"\"\n+Given a n by n board where n is of form 2k where k >= 1 (Basically n is a power of 2 with minimum value as 2). \n+The board has one missing cell (of size 1 x 1). Fill the board using L shaped tiles. \n+A L shaped tile is a 2 x 2 square with one cell of size 1\u00d71 missing.\n+\n+This problem can be solved using Divide and Conquer. Below is the recursive algorithm.\n+\n+// n is size of given square, p is location of missing cell\n+Tile(int n, Point p)\n+\n+1) Base case: n = 2, A 2 x 2 square with one cell missing is nothing \n+   but a tile and can be filled with a single tile.\n+\n+2) Place a L shaped tile at the center such that it does not cover\n+   the n/2 * n/2 subsquare that has a missing square. Now all four \n+   subsquares of size n/2 x n/2 have a missing cell (a cell that doesn't\n+   need to be filled).  See figure 2 below.\n+\n+3) Solve the problem recursively for following four. Let p1, p2, p3 and\n+   p4 be positions of the 4 missing cells in 4 squares.\n+   a) Tile(n/2, p1)\n+   b) Tile(n/2, p2)\n+   c) Tile(n/2, p3)\n+   d) Tile(n/2, p3)\n+\n+\"\"\"\n+import numpy as np\n+\n+tile_count = 0\n+\n+def place(grid, x1, y1, x2, y2, x3, y3):\nComment: Please provide return type hint for the function: `place`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `divide_and_conquer/tiling.py`, please provide doctest for the function `place`\n\nPlease provide type hint for the parameter: `grid`\n\nPlease provide type hint for the parameter: `x1`\n\nPlease provide type hint for the parameter: `y1`\n\nPlease provide type hint for the parameter: `x2`\n\nPlease provide type hint for the parameter: `y2`\n\nPlease provide type hint for the parameter: `x3`\n\nPlease provide type hint for the parameter: `y3`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "divide_and_conquer/tiling.py",
    "pr_number": 7763,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1007397111,
    "comment_created_at": "2022-10-27T21:29:26Z"
  },
  {
    "code": "@@ -0,0 +1,36 @@\n+# The function returns maximum circular contiguous sum in a[]\n+def maxCircularSum(arr: list[int], n: int) -> int:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: maxcircularsum\n\nas there is no test file in this pull request nor any test function or class in the file dynamic_programming/maximum_circular_subarray.py, please provide doctest for the function maxcircularsum\n\nplease provide descriptive name for the parameter: n",
    "line_number": 2,
    "enriched": "File: dynamic_programming/maximum_circular_subarray.py\nCode: @@ -0,0 +1,36 @@\n+# The function returns maximum circular contiguous sum in a[]\n+def maxCircularSum(arr: list[int], n: int) -> int:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `maxCircularSum`\n\nAs there is no test file in this pull request nor any test function or class in the file `dynamic_programming/maximum_circular_subarray.py`, please provide doctest for the function `maxCircularSum`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "dynamic_programming/maximum_circular_subarray.py",
    "pr_number": 7098,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994366283,
    "comment_created_at": "2022-10-13T09:04:57Z"
  },
  {
    "code": "@@ -0,0 +1,62 @@\n+\"\"\"Proof of Work (PoW) is a consensus algorithm used in blockchain systems\n+like Bitcoin. In PoW, network participants (miners) solve complex mathematical\n+puzzles to validate transactions and add new blocks to the blockchain.\n+This resource-intensive process ensures the security and integrity of\n+the network by requiring computational work and discouraging malicious actors.\"\"\"\n+\n+\n+import hashlib\n+import random\n+import string\n+\n+# Define the difficulty target (number of leading zeros)\n+difficulty = 4\n+\n+\n+def generate_random_string(length):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file blockchain/proof_of_work.py, please provide doctest for the function generate_random_string\n\nplease provide return type hint for the function: generate_random_string. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: length",
    "line_number": 16,
    "enriched": "File: blockchain/proof_of_work.py\nCode: @@ -0,0 +1,62 @@\n+\"\"\"Proof of Work (PoW) is a consensus algorithm used in blockchain systems\n+like Bitcoin. In PoW, network participants (miners) solve complex mathematical\n+puzzles to validate transactions and add new blocks to the blockchain.\n+This resource-intensive process ensures the security and integrity of\n+the network by requiring computational work and discouraging malicious actors.\"\"\"\n+\n+\n+import hashlib\n+import random\n+import string\n+\n+# Define the difficulty target (number of leading zeros)\n+difficulty = 4\n+\n+\n+def generate_random_string(length):\nComment: As there is no test file in this pull request nor any test function or class in the file `blockchain/proof_of_work.py`, please provide doctest for the function `generate_random_string`\n\nPlease provide return type hint for the function: `generate_random_string`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `length`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "blockchain/proof_of_work.py",
    "pr_number": 9061,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1327317809,
    "comment_created_at": "2023-09-15T13:50:07Z"
  },
  {
    "code": "@@ -0,0 +1,43 @@\n+def max_sum_subarray(arr, k):\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/arrays/sliding_window.py, please provide doctest for the function max_sum_subarray\n\nplease provide return type hint for the function: max_sum_subarray. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: arr\n\nplease provide descriptive name for the parameter: k\n\nplease provide type hint for the parameter: k",
    "line_number": 1,
    "enriched": "File: data_structures/arrays/sliding_window.py\nCode: @@ -0,0 +1,43 @@\n+def max_sum_subarray(arr, k):\r\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/arrays/sliding_window.py`, please provide doctest for the function `max_sum_subarray`\n\nPlease provide return type hint for the function: `max_sum_subarray`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide descriptive name for the parameter: `k`\n\nPlease provide type hint for the parameter: `k`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/arrays/sliding_window.py",
    "pr_number": 12022,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1797775119,
    "comment_created_at": "2024-10-12T20:50:52Z"
  },
  {
    "code": "@@ -0,0 +1,115 @@\n+\"\"\"\n+Linear regression is the most basic type of regression commonly used for\n+predictive analysis. The idea is pretty simple: we have a dataset and we have\n+features associated with it. Features should be chosen very cautiously\n+as they determine how much our model will be able to make future predictions.\n+We try to set the weight of these features, over many iterations, so that they best\n+fit our dataset. In this particular code, I had used a CSGO dataset (ADR vs\n+Rating). We try to best fit a line through dataset and estimate the parameters.\n+\"\"\"\n+import numpy as np\n+import requests\n+\n+\n+def collect_dataset():",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/reg_linear.py, please provide doctest for the function collect_dataset\n\nplease provide return type hint for the function: collect_dataset. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 14,
    "enriched": "File: machine_learning/reg_linear.py\nCode: @@ -0,0 +1,115 @@\n+\"\"\"\n+Linear regression is the most basic type of regression commonly used for\n+predictive analysis. The idea is pretty simple: we have a dataset and we have\n+features associated with it. Features should be chosen very cautiously\n+as they determine how much our model will be able to make future predictions.\n+We try to set the weight of these features, over many iterations, so that they best\n+fit our dataset. In this particular code, I had used a CSGO dataset (ADR vs\n+Rating). We try to best fit a line through dataset and estimate the parameters.\n+\"\"\"\n+import numpy as np\n+import requests\n+\n+\n+def collect_dataset():\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/reg_linear.py`, please provide doctest for the function `collect_dataset`\n\nPlease provide return type hint for the function: `collect_dataset`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/reg_linear.py",
    "pr_number": 10099,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349679899,
    "comment_created_at": "2023-10-08T11:17:02Z"
  },
  {
    "code": "@@ -0,0 +1,22 @@\n+def binary_search(arr, target, low, high):",
    "comment": "please provide return type hint for the function: binary_search. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file searches/recursive_binary_search.py, please provide doctest for the function binary_search\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: target\n\nplease provide type hint for the parameter: low\n\nplease provide type hint for the parameter: high",
    "line_number": 1,
    "enriched": "File: searches/recursive_binary_search.py\nCode: @@ -0,0 +1,22 @@\n+def binary_search(arr, target, low, high):\nComment: Please provide return type hint for the function: `binary_search`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `searches/recursive_binary_search.py`, please provide doctest for the function `binary_search`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `target`\n\nPlease provide type hint for the parameter: `low`\n\nPlease provide type hint for the parameter: `high`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "searches/recursive_binary_search.py",
    "pr_number": 10922,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1370811737,
    "comment_created_at": "2023-10-24T21:05:07Z"
  },
  {
    "code": "@@ -0,0 +1,14 @@\n+import random\n+\n+\n+def random_sort(arr):",
    "comment": "please provide return type hint for the function: random_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file sorts/random_sort.py, please provide doctest for the function random_sort\n\nplease provide type hint for the parameter: arr",
    "line_number": 4,
    "enriched": "File: sorts/random_sort.py\nCode: @@ -0,0 +1,14 @@\n+import random\n+\n+\n+def random_sort(arr):\nComment: Please provide return type hint for the function: `random_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `sorts/random_sort.py`, please provide doctest for the function `random_sort`\n\nPlease provide type hint for the parameter: `arr`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "sorts/random_sort.py",
    "pr_number": 9125,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1341450984,
    "comment_created_at": "2023-09-29T14:40:36Z"
  },
  {
    "code": "@@ -0,0 +1,326 @@\n+\"\"\"\n+Paillier cryptosystem was introduced in 1999 by Pascal Paillier. It is \n+an asymmetric algorithm for public key cryptography.\n+    ----Public-Key Cryptosystems Based on Composite Degree Residuosity Classes\n+    ----https://link.springer.com/content/pdf/10.1007/3-540-48910-X_16.pdf\n+\n+It is famous for its Homomorphic Properties present between cypher and\n+message space. i.e.\n+    E(m1 + m2) = E(m1) * E(m2)\n+    where:\n+        E is encryption scheme\n+        m1, m2 are messages\n+        + is addition operation in message space\n+        * is multiplication operation in cypher space\n+\n+Due to its Homomorphic properties it is highly studied in fields like:\n+    Electronic voting\n+    Electronic cash\n+    Electronic auction\n+    Zero Knowledge Proofs\n+\n+Read more : https://en.wikipedia.org/wiki/Paillier_cryptosystem\n+\"\"\"\n+\n+\n+# Implementation details:\n+#     Utils:\n+#         primeGenerator()  ++\n+#         GCD()             ++\n+#         LCM()             ++\n+#         getInverse()      ++\n+#         L()               ++\n+#         Zn:\n+#            __init__()     ++\n+#            sample()       ++\n+#            __contains__() ++\n+#         Zn_star:\n+#            __init__()     ++\n+#            sample()       ++\n+#            __contains__() ++\n+#         Zn2_star:\n+#            __init__()     ++\n+#            sample()       ++\n+#            __contains__() ++\n+#     PaillierCryptosystem:\n+#         __init__()        ++\n+#         keyGen()          ++\n+#         getPublicKey()    ++\n+#         getPrivateKey()   ++\n+#         encrypt()         ++\n+#         decrypt()         ++\n+#         homomorphicADD()  ++\n+#         homomorphicMUL()  ++\n+\n+\n+\n+import sympy\n+import random\n+\n+class Utils:\n+    def primeGenerator(bit_size):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file ciphers/paillier_cryptosystem.py, please provide doctest for the function primegenerator\n\nplease provide return type hint for the function: primegenerator. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: primegenerator\n\nplease provide type hint for the parameter: bit_size",
    "line_number": 61,
    "enriched": "File: ciphers/paillier_cryptosystem.py\nCode: @@ -0,0 +1,326 @@\n+\"\"\"\n+Paillier cryptosystem was introduced in 1999 by Pascal Paillier. It is \n+an asymmetric algorithm for public key cryptography.\n+    ----Public-Key Cryptosystems Based on Composite Degree Residuosity Classes\n+    ----https://link.springer.com/content/pdf/10.1007/3-540-48910-X_16.pdf\n+\n+It is famous for its Homomorphic Properties present between cypher and\n+message space. i.e.\n+    E(m1 + m2) = E(m1) * E(m2)\n+    where:\n+        E is encryption scheme\n+        m1, m2 are messages\n+        + is addition operation in message space\n+        * is multiplication operation in cypher space\n+\n+Due to its Homomorphic properties it is highly studied in fields like:\n+    Electronic voting\n+    Electronic cash\n+    Electronic auction\n+    Zero Knowledge Proofs\n+\n+Read more : https://en.wikipedia.org/wiki/Paillier_cryptosystem\n+\"\"\"\n+\n+\n+# Implementation details:\n+#     Utils:\n+#         primeGenerator()  ++\n+#         GCD()             ++\n+#         LCM()             ++\n+#         getInverse()      ++\n+#         L()               ++\n+#         Zn:\n+#            __init__()     ++\n+#            sample()       ++\n+#            __contains__() ++\n+#         Zn_star:\n+#            __init__()     ++\n+#            sample()       ++\n+#            __contains__() ++\n+#         Zn2_star:\n+#            __init__()     ++\n+#            sample()       ++\n+#            __contains__() ++\n+#     PaillierCryptosystem:\n+#         __init__()        ++\n+#         keyGen()          ++\n+#         getPublicKey()    ++\n+#         getPrivateKey()   ++\n+#         encrypt()         ++\n+#         decrypt()         ++\n+#         homomorphicADD()  ++\n+#         homomorphicMUL()  ++\n+\n+\n+\n+import sympy\n+import random\n+\n+class Utils:\n+    def primeGenerator(bit_size):\nComment: As there is no test file in this pull request nor any test function or class in the file `ciphers/paillier_cryptosystem.py`, please provide doctest for the function `primeGenerator`\n\nPlease provide return type hint for the function: `primeGenerator`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `primeGenerator`\n\nPlease provide type hint for the parameter: `bit_size`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "ciphers/paillier_cryptosystem.py",
    "pr_number": 12038,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1798435264,
    "comment_created_at": "2024-10-13T16:13:30Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+def truth_table_and():",
    "comment": "as there is no test file in this pull request nor any test function or class in the file boolean_algebra/truth_table_and_or_not.py, please provide doctest for the function truth_table_and\n\nplease provide return type hint for the function: truth_table_and. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 1,
    "enriched": "File: boolean_algebra/truth_table_and_or_not.py\nCode: @@ -0,0 +1,61 @@\n+def truth_table_and():\nComment: As there is no test file in this pull request nor any test function or class in the file `boolean_algebra/truth_table_and_or_not.py`, please provide doctest for the function `truth_table_and`\n\nPlease provide return type hint for the function: `truth_table_and`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "boolean_algebra/truth_table_and_or_not.py",
    "pr_number": 10155,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349940498,
    "comment_created_at": "2023-10-09T07:27:34Z"
  },
  {
    "code": "@@ -0,0 +1,75 @@\n+from Crypto.Cipher import AES\n+from Crypto.Random import get_random_bytes\n+from Crypto.Protocol.KDF import PBKDF2\n+import hashlib\n+import hmac\n+\n+def pad(data):",
    "comment": "please provide return type hint for the function: pad. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file ciphers/aes_cbc.py, please provide doctest for the function pad\n\nplease provide type hint for the parameter: data",
    "line_number": 7,
    "enriched": "File: ciphers/AES_CBC.py\nCode: @@ -0,0 +1,75 @@\n+from Crypto.Cipher import AES\n+from Crypto.Random import get_random_bytes\n+from Crypto.Protocol.KDF import PBKDF2\n+import hashlib\n+import hmac\n+\n+def pad(data):\nComment: Please provide return type hint for the function: `pad`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `ciphers/AES_CBC.py`, please provide doctest for the function `pad`\n\nPlease provide type hint for the parameter: `data`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "ciphers/AES_CBC.py",
    "pr_number": 9595,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1345609221,
    "comment_created_at": "2023-10-04T11:00:44Z"
  },
  {
    "code": "@@ -0,0 +1,149 @@\n+\n+# Starter values for computing\n+# Network address; Mask; Num of subnets; Max value of devices that need ip\n+def start_v():",
    "comment": "as there is no test file in this pull request nor any test function or class in the file subnetting flsm/subnetting_flsm.py, please provide doctest for the function start_v\n\nplease provide return type hint for the function: start_v. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 4,
    "enriched": "File: Subnetting Flsm/subnetting_flsm.py\nCode: @@ -0,0 +1,149 @@\n+\n+# Starter values for computing\n+# Network address; Mask; Num of subnets; Max value of devices that need ip\n+def start_v():\nComment: As there is no test file in this pull request nor any test function or class in the file `Subnetting Flsm/subnetting_flsm.py`, please provide doctest for the function `start_v`\n\nPlease provide return type hint for the function: `start_v`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "Subnetting Flsm/subnetting_flsm.py",
    "pr_number": 7033,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 993201127,
    "comment_created_at": "2022-10-12T09:06:55Z"
  },
  {
    "code": "@@ -0,0 +1,31 @@\n+def fractional_knapsack(items, capacity):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/fractional_knapsack.py, please provide doctest for the function fractional_knapsack\n\nplease provide return type hint for the function: fractional_knapsack. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: items\n\nplease provide type hint for the parameter: capacity",
    "line_number": 1,
    "enriched": "File: dynamic_programming/Fractional_Knapsack.py\nCode: @@ -0,0 +1,31 @@\n+def fractional_knapsack(items, capacity):\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/Fractional_Knapsack.py`, please provide doctest for the function `fractional_knapsack`\n\nPlease provide return type hint for the function: `fractional_knapsack`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `items`\n\nPlease provide type hint for the parameter: `capacity`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/Fractional_Knapsack.py",
    "pr_number": 9444,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342531408,
    "comment_created_at": "2023-10-02T10:40:32Z"
  },
  {
    "code": "@@ -0,0 +1,19 @@\n+class Solution:\r\n+    def subsetSums(self, arr, N):\r",
    "comment": "please provide return type hint for the function: subsetsums. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: subsetsums\n\nas there is no test file in this pull request nor any test function or class in the file recursion/subsetsum ii.py, please provide doctest for the function subsetsums\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n",
    "line_number": 2,
    "enriched": "File: recursion/SubsetSum II.py\nCode: @@ -0,0 +1,19 @@\n+class Solution:\r\n+    def subsetSums(self, arr, N):\r\nComment: Please provide return type hint for the function: `subsetSums`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `subsetSums`\n\nAs there is no test file in this pull request nor any test function or class in the file `recursion/SubsetSum II.py`, please provide doctest for the function `subsetSums`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `N`\n\nPlease provide descriptive name for the parameter: `N`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "recursion/SubsetSum II.py",
    "pr_number": 7858,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008755919,
    "comment_created_at": "2022-10-29T20:45:54Z"
  },
  {
    "code": "@@ -0,0 +1,14 @@\n+''' This code finds the greatest prime number that is smaller than or equal to the input by user '''\n+def nextSmallPrime(n):",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: nextsmallprime\n\nplease provide return type hint for the function: nextsmallprime. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file maths/next_small_prime.py, please provide doctest for the function nextsmallprime\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: n",
    "line_number": 2,
    "enriched": "File: maths/Next_Small_Prime.py\nCode: @@ -0,0 +1,14 @@\n+''' This code finds the greatest prime number that is smaller than or equal to the input by user '''\n+def nextSmallPrime(n):\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `nextSmallPrime`\n\nPlease provide return type hint for the function: `nextSmallPrime`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `maths/Next_Small_Prime.py`, please provide doctest for the function `nextSmallPrime`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide type hint for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/Next_Small_Prime.py",
    "pr_number": 9693,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1345709003,
    "comment_created_at": "2023-10-04T12:23:02Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+class Solution(object):\n+    def setZeroes(self, matrix):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/arrays/set_matrix_zeroes.py, please provide doctest for the function setzeroes\n\nplease provide return type hint for the function: setzeroes. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: setzeroes\n\nplease provide type hint for the parameter: matrix",
    "line_number": 2,
    "enriched": "File: data_structures/arrays/set_matrix_zeroes.py\nCode: @@ -0,0 +1,35 @@\n+class Solution(object):\n+    def setZeroes(self, matrix):\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/arrays/set_matrix_zeroes.py`, please provide doctest for the function `setZeroes`\n\nPlease provide return type hint for the function: `setZeroes`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `setZeroes`\n\nPlease provide type hint for the parameter: `matrix`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/arrays/set_matrix_zeroes.py",
    "pr_number": 13368,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2416360274,
    "comment_created_at": "2025-10-09T10:45:09Z"
  },
  {
    "code": "@@ -0,0 +1,24 @@\n+def cashier():\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/cashier.py, please provide doctest for the function cashier\n\nplease provide return type hint for the function: cashier. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 1,
    "enriched": "File: maths/cashier.py\nCode: @@ -0,0 +1,24 @@\n+def cashier():\r\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/cashier.py`, please provide doctest for the function `cashier`\n\nPlease provide return type hint for the function: `cashier`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/cashier.py",
    "pr_number": 9798,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347302885,
    "comment_created_at": "2023-10-05T11:54:16Z"
  },
  {
    "code": "@@ -0,0 +1,25 @@\n+from typing import List\n+\n+\n+class Solution:\n+    def generate(self, numRows: int) -> List[List[int]]:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/arrays/pascals_triangle.py, please provide doctest for the function generate\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: numrows",
    "line_number": 5,
    "enriched": "File: data_structures/arrays/pascals_triangle.py\nCode: @@ -0,0 +1,25 @@\n+from typing import List\n+\n+\n+class Solution:\n+    def generate(self, numRows: int) -> List[List[int]]:\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/arrays/pascals_triangle.py`, please provide doctest for the function `generate`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `numRows`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/arrays/pascals_triangle.py",
    "pr_number": 10642,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1362236441,
    "comment_created_at": "2023-10-17T14:34:04Z"
  },
  {
    "code": "@@ -1,14 +1,41 @@\n def print_dist(dist, v):\n+    \"\"\"\n+    Print vertex distances.\n+\n+    Parameters:\n+    dist (list): A list of distances.\n+    v (int): The number of vertices.",
    "comment": "putting the type hints into the function signature allows [mypy](https://mypy.readthedocs.io) to check them in our tests.  please do not repeat them in the docstring below because readers will get confused if one is changed but the other is not.",
    "line_number": 7,
    "enriched": "File: graphs/dijkstra_2.py\nCode: @@ -1,14 +1,41 @@\n def print_dist(dist, v):\n+    \"\"\"\n+    Print vertex distances.\n+\n+    Parameters:\n+    dist (list): A list of distances.\n+    v (int): The number of vertices.\nComment: Putting the type hints into the function signature allows [mypy](https://mypy.readthedocs.io) to check them in our tests.  Please do not repeat them in the docstring below because readers will get confused if one is changed but the other is not.\r\n```suggestion\r\ndef print_dist(dist: list[float], v: int):\r\n    \"\"\"\r\n    Print vertex distances.\r\n\r\n    Parameters:\r\n    dist: A list of distances.\r\n    v: The number of vertices.\r\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "graphs/dijkstra_2.py",
    "pr_number": 9967,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1351582320,
    "comment_created_at": "2023-10-10T06:16:40Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+from datetime import datetime\n+from functools import wraps\n+import logging\n+\n+import tzlocal\n+\n+\n+def log_entry_and_exit(func):",
    "comment": "please provide return type hint for the function: log_entry_and_exit. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file utils/decorators/log_entry_and_exit.py, please provide doctest for the function log_entry_and_exit\n\nplease provide type hint for the parameter: func",
    "line_number": 8,
    "enriched": "File: utils/decorators/log_entry_and_exit.py\nCode: @@ -0,0 +1,42 @@\n+from datetime import datetime\n+from functools import wraps\n+import logging\n+\n+import tzlocal\n+\n+\n+def log_entry_and_exit(func):\nComment: Please provide return type hint for the function: `log_entry_and_exit`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `utils/decorators/log_entry_and_exit.py`, please provide doctest for the function `log_entry_and_exit`\n\nPlease provide type hint for the parameter: `func`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "utils/decorators/log_entry_and_exit.py",
    "pr_number": 11616,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1782649560,
    "comment_created_at": "2024-10-01T12:06:33Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+# A class to represent a disjoint set\n+class DisjointSet:\n+\tparent = {}\n+\n+\t# Find the root of the set in which element `k` belongs\n+\tdef Find(self, k):",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: find\n\nas there is no test file in this pull request nor any test function or class in the file linear_algebra/src/operations.py, please provide doctest for the function find\n\nplease provide return type hint for the function: find. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide descriptive name for the parameter: k\n\nplease provide type hint for the parameter: k",
    "line_number": 6,
    "enriched": "File: linear_algebra/src/operations.py\nCode: @@ -0,0 +1,53 @@\n+# A class to represent a disjoint set\n+class DisjointSet:\n+\tparent = {}\n+\n+\t# Find the root of the set in which element `k` belongs\n+\tdef Find(self, k):\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `Find`\n\nAs there is no test file in this pull request nor any test function or class in the file `linear_algebra/src/operations.py`, please provide doctest for the function `Find`\n\nPlease provide return type hint for the function: `Find`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide descriptive name for the parameter: `k`\n\nPlease provide type hint for the parameter: `k`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "linear_algebra/src/operations.py",
    "pr_number": 7641,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004562840,
    "comment_created_at": "2022-10-25T14:26:36Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : October 14, 2022\n+This is implementation Dynamic Programming up bottom approach to find edit distance.\n+The aim is to demonstate up bottom approach for solving the task.\n+The implementation was tested on the leetcode: https://leetcode.com/problems/edit-distance/\n+\"\"\"\n+\n+\"\"\"\n+Levinstein distance\n+Dynamic Programming: up -> down.\n+\"\"\"\n+def min_distance_up_bottom(word1: str, word2: str) -> int:\n+    \"\"\"\n+    >>> min_distance_up_bottom(\"intention\", \"execution\")\n+    5\n+    >>> min_distance_up_bottom(\"intention\", \"\")\n+    9\n+    >>> min_distance_up_bottom(\"\", \"\")\n+    0\n+    >>> min_distance_up_bottom(\"zooicoarchaeologist\", \"zoologist\")\n+    10\n+    \"\"\"\n+\n+    from functools import lru_cache\n+\n+    len_word1 = len(word1)\n+    len_word2 = len(word2)\n+\n+    @lru_cache(maxsize=None)\n+    def min_distance(index1, index2):",
    "comment": "please provide return type hint for the function: min_distance. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file dynamic_programming/min_distance_up_bottom.py, please provide doctest for the function min_distance\n\nplease provide type hint for the parameter: index1\n\nplease provide type hint for the parameter: index2",
    "line_number": 31,
    "enriched": "File: dynamic_programming/min_distance_up_bottom.py\nCode: @@ -0,0 +1,56 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : October 14, 2022\n+This is implementation Dynamic Programming up bottom approach to find edit distance.\n+The aim is to demonstate up bottom approach for solving the task.\n+The implementation was tested on the leetcode: https://leetcode.com/problems/edit-distance/\n+\"\"\"\n+\n+\"\"\"\n+Levinstein distance\n+Dynamic Programming: up -> down.\n+\"\"\"\n+def min_distance_up_bottom(word1: str, word2: str) -> int:\n+    \"\"\"\n+    >>> min_distance_up_bottom(\"intention\", \"execution\")\n+    5\n+    >>> min_distance_up_bottom(\"intention\", \"\")\n+    9\n+    >>> min_distance_up_bottom(\"\", \"\")\n+    0\n+    >>> min_distance_up_bottom(\"zooicoarchaeologist\", \"zoologist\")\n+    10\n+    \"\"\"\n+\n+    from functools import lru_cache\n+\n+    len_word1 = len(word1)\n+    len_word2 = len(word2)\n+\n+    @lru_cache(maxsize=None)\n+    def min_distance(index1, index2):\nComment: Please provide return type hint for the function: `min_distance`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `dynamic_programming/min_distance_up_bottom.py`, please provide doctest for the function `min_distance`\n\nPlease provide type hint for the parameter: `index1`\n\nPlease provide type hint for the parameter: `index2`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/min_distance_up_bottom.py",
    "pr_number": 7171,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 995772172,
    "comment_created_at": "2022-10-14T13:37:44Z"
  },
  {
    "code": "@@ -0,0 +1,37 @@\n+from functools import lru_cache\n+\n+@lru_cache\n+def bell_number(n):",
    "comment": "please provide return type hint for the function: bell_number. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n",
    "line_number": 4,
    "enriched": "File: dynamic_programming/bell_number.py\nCode: @@ -0,0 +1,37 @@\n+from functools import lru_cache\n+\n+@lru_cache\n+def bell_number(n):\nComment: Please provide return type hint for the function: `bell_number`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/bell_number.py",
    "pr_number": 9817,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347420791,
    "comment_created_at": "2023-10-05T13:26:59Z"
  },
  {
    "code": "@@ -0,0 +1,19 @@\n+# Function to convert hexadecimal to octal\n+def hex_to_octal(hex_number):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file conversions/hexadecimal_to_octal.py, please provide doctest for the function hex_to_octal\n\nplease provide return type hint for the function: hex_to_octal. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: hex_number",
    "line_number": 2,
    "enriched": "File: conversions/hexadecimal_to_octal.py\nCode: @@ -0,0 +1,19 @@\n+# Function to convert hexadecimal to octal\n+def hex_to_octal(hex_number):\nComment: As there is no test file in this pull request nor any test function or class in the file `conversions/hexadecimal_to_octal.py`, please provide doctest for the function `hex_to_octal`\n\nPlease provide return type hint for the function: `hex_to_octal`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `hex_number`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "conversions/hexadecimal_to_octal.py",
    "pr_number": 11032,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1374714209,
    "comment_created_at": "2023-10-27T15:09:16Z"
  },
  {
    "code": "@@ -0,0 +1,308 @@\n+import yfinance as yf\n+import datetime\n+import pandas as pd\n+import numpy as np\n+from finta import TA\n+import matplotlib.pyplot as plt\n+\n+from sklearn import svm\n+from sklearn.ensemble import RandomForestClassifier\n+from sklearn.neighbors import KNeighborsClassifier\n+from sklearn.ensemble import AdaBoostClassifier\n+from sklearn.ensemble import GradientBoostingClassifier\n+from sklearn.ensemble import VotingClassifier\n+from sklearn.model_selection import train_test_split, GridSearchCV\n+from sklearn.metrics import confusion_matrix, classification_report\n+from sklearn import metrics\n+\n+\"\"\"\n+Defining some constants for data mining\n+\"\"\"\n+\n+NUM_DAYS = 10000     # The number of days of historical data to retrieve\n+INTERVAL = '1d'     # Sample rate of historical data\n+symbol = 'SPY'      # Symbol of the desired stock\n+\n+# List of symbols for technical indicators\n+INDICATORS = ['RSI', 'MACD', 'STOCH','ADL', 'ATR', 'MOM', 'MFI', 'ROC', 'OBV', 'CCI', 'EMV', 'VORTEX']\n+\n+\n+\"\"\"\n+Next we pull the historical data using yfinance\n+Rename the column names because finta uses the lowercase names\n+\"\"\"\n+\n+start = (datetime.date.today() - datetime.timedelta( NUM_DAYS ) )\n+end = datetime.datetime.today()\n+\n+data = yf.download(symbol, start=start, end=end, interval=INTERVAL)\n+data.rename(columns={\"Close\": 'close', \"High\": 'high', \"Low\": 'low', 'Volume': 'volume', 'Open': 'open'}, inplace=True)\n+print(data.head())\n+\n+tmp = data.iloc[-60:]\n+tmp['close'].plot()\n+\n+\"\"\"\n+Next we clean our data and perform feature engineering to create new technical indicator features that our\n+model can learn from\n+\"\"\"\n+\n+def _exponential_smooth(data, alpha):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file stockpricepredictionalgo.py, please provide doctest for the function _exponential_smooth\n\nplease provide return type hint for the function: _exponential_smooth. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: data\n\nplease provide type hint for the parameter: alpha",
    "line_number": 50,
    "enriched": "File: stockpricepredictionalgo.py\nCode: @@ -0,0 +1,308 @@\n+import yfinance as yf\n+import datetime\n+import pandas as pd\n+import numpy as np\n+from finta import TA\n+import matplotlib.pyplot as plt\n+\n+from sklearn import svm\n+from sklearn.ensemble import RandomForestClassifier\n+from sklearn.neighbors import KNeighborsClassifier\n+from sklearn.ensemble import AdaBoostClassifier\n+from sklearn.ensemble import GradientBoostingClassifier\n+from sklearn.ensemble import VotingClassifier\n+from sklearn.model_selection import train_test_split, GridSearchCV\n+from sklearn.metrics import confusion_matrix, classification_report\n+from sklearn import metrics\n+\n+\"\"\"\n+Defining some constants for data mining\n+\"\"\"\n+\n+NUM_DAYS = 10000     # The number of days of historical data to retrieve\n+INTERVAL = '1d'     # Sample rate of historical data\n+symbol = 'SPY'      # Symbol of the desired stock\n+\n+# List of symbols for technical indicators\n+INDICATORS = ['RSI', 'MACD', 'STOCH','ADL', 'ATR', 'MOM', 'MFI', 'ROC', 'OBV', 'CCI', 'EMV', 'VORTEX']\n+\n+\n+\"\"\"\n+Next we pull the historical data using yfinance\n+Rename the column names because finta uses the lowercase names\n+\"\"\"\n+\n+start = (datetime.date.today() - datetime.timedelta( NUM_DAYS ) )\n+end = datetime.datetime.today()\n+\n+data = yf.download(symbol, start=start, end=end, interval=INTERVAL)\n+data.rename(columns={\"Close\": 'close', \"High\": 'high', \"Low\": 'low', 'Volume': 'volume', 'Open': 'open'}, inplace=True)\n+print(data.head())\n+\n+tmp = data.iloc[-60:]\n+tmp['close'].plot()\n+\n+\"\"\"\n+Next we clean our data and perform feature engineering to create new technical indicator features that our\n+model can learn from\n+\"\"\"\n+\n+def _exponential_smooth(data, alpha):\nComment: As there is no test file in this pull request nor any test function or class in the file `stockpricepredictionalgo.py`, please provide doctest for the function `_exponential_smooth`\n\nPlease provide return type hint for the function: `_exponential_smooth`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `data`\n\nPlease provide type hint for the parameter: `alpha`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "stockpricepredictionalgo.py",
    "pr_number": 7627,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004239938,
    "comment_created_at": "2022-10-25T09:34:53Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+\"\"\"\n+Calculate one of the parameters (current, voltage, or resistance) in an electrical circuit based on Ohm's law.\n+Ohm's law(\u03a9) states that the current through a conductor between two points is directly proportional to the voltage across the two points.\n+V is proportional to I.\n+V = I*R (where R is a proportionality constant)\n+\n+    Input Parameters:\n+    - current: Current in Amperes (A)\n+    - voltage: Voltage in Volts (V)\n+    - resistance: Resistance in Ohms (\u03a9)\n+\n+    Returns:\n+    A dictionary with the calculated parameter and its value.\n+\n+\"\"\"\n+def ohms_law(current=None, voltage=None, resistance=None):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file physics/ohms_law.py, please provide doctest for the function ohms_law\n\nplease provide return type hint for the function: ohms_law. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: current\n\nplease provide type hint for the parameter: voltage\n\nplease provide type hint for the parameter: resistance",
    "line_number": 16,
    "enriched": "File: physics/ohms_law.py\nCode: @@ -0,0 +1,58 @@\n+\"\"\"\n+Calculate one of the parameters (current, voltage, or resistance) in an electrical circuit based on Ohm's law.\n+Ohm's law(\u03a9) states that the current through a conductor between two points is directly proportional to the voltage across the two points.\n+V is proportional to I.\n+V = I*R (where R is a proportionality constant)\n+\n+    Input Parameters:\n+    - current: Current in Amperes (A)\n+    - voltage: Voltage in Volts (V)\n+    - resistance: Resistance in Ohms (\u03a9)\n+\n+    Returns:\n+    A dictionary with the calculated parameter and its value.\n+\n+\"\"\"\n+def ohms_law(current=None, voltage=None, resistance=None):\nComment: As there is no test file in this pull request nor any test function or class in the file `physics/ohms_law.py`, please provide doctest for the function `ohms_law`\n\nPlease provide return type hint for the function: `ohms_law`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `current`\n\nPlease provide type hint for the parameter: `voltage`\n\nPlease provide type hint for the parameter: `resistance`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "physics/ohms_law.py",
    "pr_number": 10582,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1360306483,
    "comment_created_at": "2023-10-16T08:28:31Z"
  },
  {
    "code": "@@ -0,0 +1,12 @@\n+import re\n+\n+\n+def camel_with_numbers_to_snake(camel_case) -> str:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file strings/camel_to_snake_case.py, please provide doctest for the function camel_with_numbers_to_snake\n\nplease provide type hint for the parameter: camel_case",
    "line_number": 4,
    "enriched": "File: strings/camel_to_snake_case.py\nCode: @@ -0,0 +1,12 @@\n+import re\n+\n+\n+def camel_with_numbers_to_snake(camel_case) -> str:\nComment: As there is no test file in this pull request nor any test function or class in the file `strings/camel_to_snake_case.py`, please provide doctest for the function `camel_with_numbers_to_snake`\n\nPlease provide type hint for the parameter: `camel_case`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "strings/camel_to_snake_case.py",
    "pr_number": 10299,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1355921281,
    "comment_created_at": "2023-10-12T00:49:58Z"
  },
  {
    "code": "@@ -0,0 +1,168 @@\n+\"\"\"\n+Problem Statement: Given a binary perform an preorder traversal using Morris Preorder\n+traversal algorithm. (Iterative version of Preorder traversal of tree)\n+\n+https://www.geeksforgeeks.org/morris-traversal-for-preorder/\n+\"\"\"\n+\n+\n+class TreeNode:\n+    \"\"\"\n+    Class representing a node in a binary tree.\n+\n+    Attributes:\n+    -----------\n+    value : int\n+        The value stored at the node.\n+    left : TreeNode\n+        Pointer to the left child node (default is None).\n+    right : TreeNode\n+        Pointer to the right child node (default is None).\n+    \"\"\"\n+\n+    def __init__(self, value: int) -> None:\n+        self.value = value\n+        self.left: TreeNode | None = None\n+        self.right: TreeNode | None = None\n+\n+\n+class BinaryTree:\n+    \"\"\"\n+    Class representing a binary tree.\n+\n+    Methods:\n+    --------\n+    insert(value: int) -> None:\n+        Insert a value into the binary tree following binary search tree (BST) rules.\n+\n+    morris_preorder_traversal() -> List[int]:\n+        Perform preorder traversal and return list of node values.\n+\n+",
    "comment": "i don't think it's necessary to specify the methods in the docstring because the methods themselves should already be documented anyway.",
    "line_number": 41,
    "enriched": "File: data_structures/binary_tree/morris_preorder_traversal.py\nCode: @@ -0,0 +1,168 @@\n+\"\"\"\n+Problem Statement: Given a binary perform an preorder traversal using Morris Preorder\n+traversal algorithm. (Iterative version of Preorder traversal of tree)\n+\n+https://www.geeksforgeeks.org/morris-traversal-for-preorder/\n+\"\"\"\n+\n+\n+class TreeNode:\n+    \"\"\"\n+    Class representing a node in a binary tree.\n+\n+    Attributes:\n+    -----------\n+    value : int\n+        The value stored at the node.\n+    left : TreeNode\n+        Pointer to the left child node (default is None).\n+    right : TreeNode\n+        Pointer to the right child node (default is None).\n+    \"\"\"\n+\n+    def __init__(self, value: int) -> None:\n+        self.value = value\n+        self.left: TreeNode | None = None\n+        self.right: TreeNode | None = None\n+\n+\n+class BinaryTree:\n+    \"\"\"\n+    Class representing a binary tree.\n+\n+    Methods:\n+    --------\n+    insert(value: int) -> None:\n+        Insert a value into the binary tree following binary search tree (BST) rules.\n+\n+    morris_preorder_traversal() -> List[int]:\n+        Perform preorder traversal and return list of node values.\n+\n+\nComment: ```suggestion\r\n```\r\nI don't think it's necessary to specify the methods in the docstring because the methods themselves should already be documented anyway.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "data_structures/binary_tree/morris_preorder_traversal.py",
    "pr_number": 11776,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1788654072,
    "comment_created_at": "2024-10-05T17:56:47Z"
  },
  {
    "code": "@@ -0,0 +1,37 @@\n+#secret code generator :\r\n+str1=\"axz\"\r\n+str2=\"byz\"\r\n+def code():\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file strings/encryption_decryption.py, please provide doctest for the function code\n\nplease provide return type hint for the function: code. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 4,
    "enriched": "File: strings/encryption_decryption.py\nCode: @@ -0,0 +1,37 @@\n+#secret code generator :\r\n+str1=\"axz\"\r\n+str2=\"byz\"\r\n+def code():\r\nComment: As there is no test file in this pull request nor any test function or class in the file `strings/encryption_decryption.py`, please provide doctest for the function `code`\n\nPlease provide return type hint for the function: `code`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "strings/encryption_decryption.py",
    "pr_number": 10233,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1352762746,
    "comment_created_at": "2023-10-10T15:02:53Z"
  },
  {
    "code": "@@ -0,0 +1,25 @@\n+\"\"\"\n+Title : Calculate Einstein's Energy-Mass Equivalence\n+\n+Description :\n+    The below mentioned code gives Einstein's mass-energy equivalence, expressed by the famous equation E=mc^2\n+    states that energy (E) and mass (m) are interchangeable. \n+    It means that a certain amount of mass can be converted into an equivalent amount of energy, and vice versa, under the right conditions.\n+    This concept revolutionized our understanding of the fundamental relationship between matter and energy in the universe.\n+\n+\"\"\"\n+def energy_equivalence(mass):",
    "comment": "please provide return type hint for the function: energy_equivalence. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file physics/einstein's energy-mass equivalence.py, please provide doctest for the function energy_equivalence\n\nplease provide type hint for the parameter: mass",
    "line_number": 11,
    "enriched": "File: physics/Einstein's Energy-Mass Equivalence.py\nCode: @@ -0,0 +1,25 @@\n+\"\"\"\n+Title : Calculate Einstein's Energy-Mass Equivalence\n+\n+Description :\n+    The below mentioned code gives Einstein's mass-energy equivalence, expressed by the famous equation E=mc^2\n+    states that energy (E) and mass (m) are interchangeable. \n+    It means that a certain amount of mass can be converted into an equivalent amount of energy, and vice versa, under the right conditions.\n+    This concept revolutionized our understanding of the fundamental relationship between matter and energy in the universe.\n+\n+\"\"\"\n+def energy_equivalence(mass):\nComment: Please provide return type hint for the function: `energy_equivalence`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `physics/Einstein's Energy-Mass Equivalence.py`, please provide doctest for the function `energy_equivalence`\n\nPlease provide type hint for the parameter: `mass`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "physics/Einstein's Energy-Mass Equivalence.py",
    "pr_number": 9220,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342095234,
    "comment_created_at": "2023-10-01T08:00:56Z"
  },
  {
    "code": "@@ -0,0 +1,57 @@\n+#!/bin/python3\n+\n+import random\n+\n+import pygame\n+\n+\n+class Gene:\n+    def __init__(self, i, o):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: i\n\nplease provide descriptive name for the parameter: i\n\nplease provide type hint for the parameter: o\n\nplease provide descriptive name for the parameter: o",
    "line_number": 9,
    "enriched": "File: neat_algorithm/gene.py\nCode: @@ -0,0 +1,57 @@\n+#!/bin/python3\n+\n+import random\n+\n+import pygame\n+\n+\n+class Gene:\n+    def __init__(self, i, o):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `i`\n\nPlease provide descriptive name for the parameter: `i`\n\nPlease provide type hint for the parameter: `o`\n\nPlease provide descriptive name for the parameter: `o`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "neat_algorithm/gene.py",
    "pr_number": 10661,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1363057629,
    "comment_created_at": "2023-10-18T02:55:09Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+def minsubarraysum(target, nums):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file sliding_window/minimum_size_subarray_sum.py, please provide doctest for the function minsubarraysum\n\nplease provide return type hint for the function: minsubarraysum. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: target\n\nplease provide type hint for the parameter: nums",
    "line_number": 1,
    "enriched": "File: sliding_window/minimum_size_subarray_sum.py\nCode: @@ -0,0 +1,35 @@\n+def minsubarraysum(target, nums):\nComment: As there is no test file in this pull request nor any test function or class in the file `sliding_window/minimum_size_subarray_sum.py`, please provide doctest for the function `minsubarraysum`\n\nPlease provide return type hint for the function: `minsubarraysum`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `target`\n\nPlease provide type hint for the parameter: `nums`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "sliding_window/minimum_size_subarray_sum.py",
    "pr_number": 8566,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1150982306,
    "comment_created_at": "2023-03-28T18:01:07Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+# Python3 Program for Floyd Warshall Algorithm\n+\n+# Number of vertices in the graph\n+V = 4\n+\n+# Define infinity as the large\n+# enough value. This value will be\n+# used for vertices not connected to each other\n+INF = 99999\n+\n+# Solves all pair shortest path\n+# via Floyd Warshall Algorithm\n+\n+\n+def floydWarshall(graph):",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: floydwarshall\n\nas there is no test file in this pull request nor any test function or class in the file graphs/floyd_warshall.py, please provide doctest for the function floydwarshall\n\nplease provide return type hint for the function: floydwarshall. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: graph",
    "line_number": 15,
    "enriched": "File: graphs/floyd_warshall.py\nCode: @@ -0,0 +1,94 @@\n+# Python3 Program for Floyd Warshall Algorithm\n+\n+# Number of vertices in the graph\n+V = 4\n+\n+# Define infinity as the large\n+# enough value. This value will be\n+# used for vertices not connected to each other\n+INF = 99999\n+\n+# Solves all pair shortest path\n+# via Floyd Warshall Algorithm\n+\n+\n+def floydWarshall(graph):\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `floydWarshall`\n\nAs there is no test file in this pull request nor any test function or class in the file `graphs/floyd_warshall.py`, please provide doctest for the function `floydWarshall`\n\nPlease provide return type hint for the function: `floydWarshall`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `graph`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "graphs/floyd_warshall.py",
    "pr_number": 10658,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1362792372,
    "comment_created_at": "2023-10-17T21:17:17Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+def find_missing_number(nums):\n+    \"\"\"\n+    Finds the missing number in a list of consecutive integers.\n+\n+    Args:\n+        nums (List[int]): A list of integers.\n+\n+    Returns:\n+        int: The missing number.",
    "comment": "[mypy](mypy.readthedocs.io) can test data types in the function signature, but not in the comments.\r\n\r\ndo not repeat the datatypes in both places because readers will be confused if one is changed and the other is not.",
    "line_number": 9,
    "enriched": "File: bit_manipulation/missing_number.py\nCode: @@ -0,0 +1,21 @@\n+def find_missing_number(nums):\n+    \"\"\"\n+    Finds the missing number in a list of consecutive integers.\n+\n+    Args:\n+        nums (List[int]): A list of integers.\n+\n+    Returns:\n+        int: The missing number.\nComment: [Mypy](mypy.readthedocs.io) can test data types in the function signature, but not in the comments.\r\n\r\nDo not repeat the datatypes in both places because readers will be confused if one is changed and the other is not.\r\n\r\n```suggestion\r\ndef find_missing_number(nums: list[int]) -> int:\r\n    \"\"\"\r\n    Finds the missing number in a list of consecutive integers.\r\n\r\n    Args:\r\n        nums: A list of integers.\r\n\r\n    Returns:\r\n        The missing number.\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "bit_manipulation/missing_number.py",
    "pr_number": 9203,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342120483,
    "comment_created_at": "2023-10-01T11:23:39Z"
  },
  {
    "code": "@@ -0,0 +1,44 @@\n+\"\"\"\n+Project Euler Problem 762: https://projecteuler.net/problem=762\n+\n+Consider a two-dimensional grid of squares. The grid has 4 rows but infinitely many columns.\n+An amoeba in square (x, y) can divide itself into two amoebas to occupy the squares (x+1, y) and (x+1, (y+1) mod 4), provided these squares are empty.\n+\n+The goal is to find the number of different possible arrangements of amoebas after N divisions, where each arrangement is counted only once. This function is denoted as C(N).\n+\n+For example, when N=2, there are 2 different possible arrangements. When N=10, there are 1301 different possible arrangements. When N=20, there are 5895236 different possible arrangements. And the problem asks for the last nine digits of C(100,000), which should be found and provided as the final answer.\n+\n+\"\"\"\n+\n+def solution(N:int=100):",
    "comment": "please provide return type hint for the function: solution. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide descriptive name for the parameter: n",
    "line_number": 13,
    "enriched": "File: project_euler/problem_762/sol1.py\nCode: @@ -0,0 +1,44 @@\n+\"\"\"\n+Project Euler Problem 762: https://projecteuler.net/problem=762\n+\n+Consider a two-dimensional grid of squares. The grid has 4 rows but infinitely many columns.\n+An amoeba in square (x, y) can divide itself into two amoebas to occupy the squares (x+1, y) and (x+1, (y+1) mod 4), provided these squares are empty.\n+\n+The goal is to find the number of different possible arrangements of amoebas after N divisions, where each arrangement is counted only once. This function is denoted as C(N).\n+\n+For example, when N=2, there are 2 different possible arrangements. When N=10, there are 1301 different possible arrangements. When N=20, there are 5895236 different possible arrangements. And the problem asks for the last nine digits of C(100,000), which should be found and provided as the final answer.\n+\n+\"\"\"\n+\n+def solution(N:int=100):\nComment: Please provide return type hint for the function: `solution`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide descriptive name for the parameter: `N`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "project_euler/problem_762/sol1.py",
    "pr_number": 10041,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349547924,
    "comment_created_at": "2023-10-07T17:18:32Z"
  },
  {
    "code": "@@ -0,0 +1,66 @@\n+\"\"\"\n+This is pure Python implementation of exponential search.\n+\n+Resources used:\n+https://en.wikipedia.org/wiki/Exponential_search\n+\n+For doctests run following command:\n+python3 -m doctest -v expontial_search.py\n+\n+For manual testing run:\n+python3 expontial_search.py\n+\"\"\"\n+\n+def binary_search(arr, left, right, target) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/exponential.py, please provide doctest for the function binary_search\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: left\n\nplease provide type hint for the parameter: right\n\nplease provide type hint for the parameter: target",
    "line_number": 14,
    "enriched": "File: searches/exponential.py\nCode: @@ -0,0 +1,66 @@\n+\"\"\"\n+This is pure Python implementation of exponential search.\n+\n+Resources used:\n+https://en.wikipedia.org/wiki/Exponential_search\n+\n+For doctests run following command:\n+python3 -m doctest -v expontial_search.py\n+\n+For manual testing run:\n+python3 expontial_search.py\n+\"\"\"\n+\n+def binary_search(arr, left, right, target) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/exponential.py`, please provide doctest for the function `binary_search`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `left`\n\nPlease provide type hint for the parameter: `right`\n\nPlease provide type hint for the parameter: `target`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "searches/exponential.py",
    "pr_number": 10947,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1371847757,
    "comment_created_at": "2023-10-25T14:14:53Z"
  },
  {
    "code": "@@ -0,0 +1,130 @@\n+\"\"\"\n+The Permutation Cipher, implemented above, is a simple encryption\n+technique that rearranges the characters in a message based on a secret key.\n+It divides the message into blocks and applies a permutation to the characters\n+within each block according to the key. The key is a sequence of unique integers\n+that determine the order of character rearrangement. For more info read:-\n+https://www.nku.edu/~christensen/1402%20permutation%20ciphers.pdf\n+\n+\"\"\"\n+import random\n+\n+\n+def generate_valid_block_size(message_length) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file ciphers/permutation_cipher.py, please provide doctest for the function generate_valid_block_size\n\nplease provide type hint for the parameter: message_length",
    "line_number": 13,
    "enriched": "File: ciphers/permutation_cipher.py\nCode: @@ -0,0 +1,130 @@\n+\"\"\"\n+The Permutation Cipher, implemented above, is a simple encryption\n+technique that rearranges the characters in a message based on a secret key.\n+It divides the message into blocks and applies a permutation to the characters\n+within each block according to the key. The key is a sequence of unique integers\n+that determine the order of character rearrangement. For more info read:-\n+https://www.nku.edu/~christensen/1402%20permutation%20ciphers.pdf\n+\n+\"\"\"\n+import random\n+\n+\n+def generate_valid_block_size(message_length) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `ciphers/permutation_cipher.py`, please provide doctest for the function `generate_valid_block_size`\n\nPlease provide type hint for the parameter: `message_length`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "ciphers/permutation_cipher.py",
    "pr_number": 9163,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342055112,
    "comment_created_at": "2023-10-01T03:21:35Z"
  },
  {
    "code": "@@ -0,0 +1,22 @@\n+def romanToInt(s: str) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/roman_numeral_to_integer.py, please provide doctest for the function roman_to_int\n\nplease provide descriptive name for the parameter: s",
    "line_number": 1,
    "enriched": "File: maths/roman_numeral_to_integer.py\nCode: @@ -0,0 +1,22 @@\n+def romanToInt(s: str) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/roman_numeral_to_integer.py`, please provide doctest for the function `roman_to_int`\n\nPlease provide descriptive name for the parameter: `s`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/roman_numeral_to_integer.py",
    "pr_number": 8019,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1041480183,
    "comment_created_at": "2022-12-06T21:31:24Z"
  },
  {
    "code": "@@ -0,0 +1,22 @@\n+# A Dynamic Programming based Python Program for 0-1 Knapsack problem\n+\n+\n+def knapSack(W, wt, val, n):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file knapsack/assignment_problem.py, please provide doctest for the function knapsack\n\nplease provide return type hint for the function: knapsack. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: knapsack\n\nplease provide descriptive name for the parameter: w\n\nplease provide type hint for the parameter: w\n\nplease provide type hint for the parameter: wt\n\nplease provide type hint for the parameter: val\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: n",
    "line_number": 4,
    "enriched": "File: knapsack/assignment_problem.py\nCode: @@ -0,0 +1,22 @@\n+# A Dynamic Programming based Python Program for 0-1 Knapsack problem\n+\n+\n+def knapSack(W, wt, val, n):\nComment: As there is no test file in this pull request nor any test function or class in the file `knapsack/assignment_problem.py`, please provide doctest for the function `knapSack`\n\nPlease provide return type hint for the function: `knapSack`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `knapSack`\n\nPlease provide descriptive name for the parameter: `W`\n\nPlease provide type hint for the parameter: `W`\n\nPlease provide type hint for the parameter: `wt`\n\nPlease provide type hint for the parameter: `val`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide type hint for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "knapsack/assignment_problem.py",
    "pr_number": 7137,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996253187,
    "comment_created_at": "2022-10-15T04:59:02Z"
  },
  {
    "code": "@@ -0,0 +1,90 @@\n+Gravitational Search Algorithm\n+Reference: https://www.sciencedirect.com/science/article/abs/pii/S0020025509001200",
    "comment": "an error occurred while parsing the file: machine_learning/gravitational_search_algorithm.py\npython\ntraceback (most recent call last):\n  file \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 146, in parse\n    reports = lint_file(\n              ^^^^^^^^^^\nlibcst._exceptions.parsersyntaxerror: syntax error @ 2:22.\nparser error: error at 1:21: expected one of !=, %, &, (, *, **, +, ,, -, ., /, //, :, ;, <, <<, <=, =, ==, >, >=, >>, @, newline, [, ^, and, if, in, is, not, or, |\n\nreference: https://www.sciencedirect.com/science/article/abs/pii/s0020025509001200\n                     ^",
    "line_number": 2,
    "enriched": "File: machine_learning/gravitational_search_algorithm.py\nCode: @@ -0,0 +1,90 @@\n+Gravitational Search Algorithm\n+Reference: https://www.sciencedirect.com/science/article/abs/pii/S0020025509001200\nComment: An error occurred while parsing the file: `machine_learning/gravitational_search_algorithm.py`\n```python\nTraceback (most recent call last):\n  File \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 146, in parse\n    reports = lint_file(\n              ^^^^^^^^^^\nlibcst._exceptions.ParserSyntaxError: Syntax Error @ 2:22.\nparser error: error at 1:21: expected one of !=, %, &, (, *, **, +, ,, -, ., /, //, :, ;, <, <<, <=, =, ==, >, >=, >>, @, NEWLINE, [, ^, and, if, in, is, not, or, |\n\nReference: https://www.sciencedirect.com/science/article/abs/pii/S0020025509001200\n                     ^\n\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "machine_learning/gravitational_search_algorithm.py",
    "pr_number": 9676,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1345282864,
    "comment_created_at": "2023-10-04T06:51:12Z"
  },
  {
    "code": "@@ -0,0 +1,10 @@\n+def no_of_graphs(n):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file graphs/no_of_graphs.py, please provide doctest for the function no_of_graphs\n\nplease provide return type hint for the function: no_of_graphs. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: n",
    "line_number": 1,
    "enriched": "File: graphs/no_of_graphs.py\nCode: @@ -0,0 +1,10 @@\n+def no_of_graphs(n):\nComment: As there is no test file in this pull request nor any test function or class in the file `graphs/no_of_graphs.py`, please provide doctest for the function `no_of_graphs`\n\nPlease provide return type hint for the function: `no_of_graphs`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide type hint for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "graphs/no_of_graphs.py",
    "pr_number": 10914,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1370646551,
    "comment_created_at": "2023-10-24T18:18:56Z"
  },
  {
    "code": "@@ -0,0 +1,29 @@\n+class Node:\n+    def _init_(self,val):",
    "comment": "please provide return type hint for the function: _init_. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file data_structures/binary_tree/left_view_of_binary_tree.py, please provide doctest for the function _init_\n\nplease provide type hint for the parameter: val",
    "line_number": 2,
    "enriched": "File: data_structures/binary_tree/left_view_of_binary_tree.py\nCode: @@ -0,0 +1,29 @@\n+class Node:\n+    def _init_(self,val):\nComment: Please provide return type hint for the function: `_init_`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `data_structures/binary_tree/left_view_of_binary_tree.py`, please provide doctest for the function `_init_`\n\nPlease provide type hint for the parameter: `val`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/binary_tree/left_view_of_binary_tree.py",
    "pr_number": 10766,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367739637,
    "comment_created_at": "2023-10-21T14:39:10Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+def to_title_case(input_str: str) -> str:\r\n+    \"\"\"\r\n+    Converts a string to title case, preserving the input as is\r\n+\r\n+    >>> to_title_case(\"Aakash Giri\")\r\n+    'Aakash Giri'\r\n+\r\n+    >>> to_title_case(\"aakash giri\")\r\n+    'Aakash Giri'\r\n+\r\n+    >>> to_title_case(\"AAKASH GIRI\")\r\n+    'Aakash Giri'\r\n+\r\n+    >>> to_title_case(\"aAkAsH gIrI\")\r\n+    'Aakash Giri'\r\n+    \"\"\"\r\n+\r\n+    def convert_word(word):\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file strings/title.py, please provide doctest for the function convert_word\n\nplease provide return type hint for the function: convert_word. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: word",
    "line_number": 18,
    "enriched": "File: strings/title.py\nCode: @@ -0,0 +1,42 @@\n+def to_title_case(input_str: str) -> str:\r\n+    \"\"\"\r\n+    Converts a string to title case, preserving the input as is\r\n+\r\n+    >>> to_title_case(\"Aakash Giri\")\r\n+    'Aakash Giri'\r\n+\r\n+    >>> to_title_case(\"aakash giri\")\r\n+    'Aakash Giri'\r\n+\r\n+    >>> to_title_case(\"AAKASH GIRI\")\r\n+    'Aakash Giri'\r\n+\r\n+    >>> to_title_case(\"aAkAsH gIrI\")\r\n+    'Aakash Giri'\r\n+    \"\"\"\r\n+\r\n+    def convert_word(word):\r\nComment: As there is no test file in this pull request nor any test function or class in the file `strings/title.py`, please provide doctest for the function `convert_word`\n\nPlease provide return type hint for the function: `convert_word`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `word`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "strings/title.py",
    "pr_number": 10439,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359433862,
    "comment_created_at": "2023-10-14T15:07:22Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+from genericpath import isfile\n+import os\n+import subprocess\n+import sys\n+\n+def install(package):",
    "comment": "please provide return type hint for the function: install. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file ransomewarevirus/antivirus.py, please provide doctest for the function install\n\nplease provide type hint for the parameter: package",
    "line_number": 6,
    "enriched": "File: RansomewareVirus/AntiVirus.py\nCode: @@ -0,0 +1,61 @@\n+from genericpath import isfile\n+import os\n+import subprocess\n+import sys\n+\n+def install(package):\nComment: Please provide return type hint for the function: `install`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `RansomewareVirus/AntiVirus.py`, please provide doctest for the function `install`\n\nPlease provide type hint for the parameter: `package`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "RansomewareVirus/AntiVirus.py",
    "pr_number": 7125,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994701440,
    "comment_created_at": "2022-10-13T14:13:23Z"
  },
  {
    "code": "@@ -0,0 +1,38 @@\n+# Python 3 program for recursive binary search.\n+# Modifications needed for the older Python 2 are found in comments.\n+\n+# Returns index of x in arr if present, else -1\n+def binary_search(arr, low, high, x):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/binarysearchusingrecursion.py, please provide doctest for the function binary_search\n\nplease provide return type hint for the function: binary_search. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: low\n\nplease provide type hint for the parameter: high\n\nplease provide descriptive name for the parameter: x\n\nplease provide type hint for the parameter: x",
    "line_number": 5,
    "enriched": "File: searches/binarysearchusingrecursion.py\nCode: @@ -0,0 +1,38 @@\n+# Python 3 program for recursive binary search.\n+# Modifications needed for the older Python 2 are found in comments.\n+\n+# Returns index of x in arr if present, else -1\n+def binary_search(arr, low, high, x):\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/binarysearchusingrecursion.py`, please provide doctest for the function `binary_search`\n\nPlease provide return type hint for the function: `binary_search`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `low`\n\nPlease provide type hint for the parameter: `high`\n\nPlease provide descriptive name for the parameter: `x`\n\nPlease provide type hint for the parameter: `x`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "searches/binarysearchusingrecursion.py",
    "pr_number": 7518,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002460473,
    "comment_created_at": "2022-10-22T12:45:47Z"
  },
  {
    "code": "@@ -32,17 +45,18 @@ def frac_knapsack(vl, wt, w, n):\n     TypeError: unsupported operand type(s) for /: 'str' and 'int'\n     \"\"\"\n \n+    # sort in descending order of value/weight ratio\n     r = sorted(zip(vl, wt), key=lambda x: x[0] / x[1], reverse=True)",
    "comment": "please add this and fix the tests to match.\r\n\r\nhttps://docs.astral.sh/ruff/rules/zip-without-explicit-strict/",
    "line_number": 49,
    "enriched": "File: greedy_methods/fractional_knapsack.py\nCode: @@ -32,17 +45,18 @@ def frac_knapsack(vl, wt, w, n):\n     TypeError: unsupported operand type(s) for /: 'str' and 'int'\n     \"\"\"\n \n+    # sort in descending order of value/weight ratio\n     r = sorted(zip(vl, wt), key=lambda x: x[0] / x[1], reverse=True)\nComment: Please add this and fix the tests to match.\r\n```suggestion\r\n    r = sorted(zip(vl, wt), key=lambda x: x[0] / x[1], reverse=True, strict=True)\r\n```\r\nhttps://docs.astral.sh/ruff/rules/zip-without-explicit-strict/",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "greedy_methods/fractional_knapsack.py",
    "pr_number": 10888,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1370441006,
    "comment_created_at": "2023-10-24T15:51:00Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+\"\"\"\n+References for Binary, Octal, and Hexadecimal Numbers\n+\n+https://en.wikipedia.org/wiki/Binary_number\n+https://en.wikipedia.org/wiki/Octal\n+https://en.wikipedia.org/wiki/Hexadecimal\n+\n+\"\"\"\n+from __future__ import annotations\n+\n+def DecimalConversions(dec):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/decimal_conversions.py, please provide doctest for the function decimalconversions\n\nplease provide return type hint for the function: decimalconversions. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: decimalconversions\n\nplease provide type hint for the parameter: dec",
    "line_number": 11,
    "enriched": "File: maths/decimal_conversions.py\nCode: @@ -0,0 +1,56 @@\n+\"\"\"\n+References for Binary, Octal, and Hexadecimal Numbers\n+\n+https://en.wikipedia.org/wiki/Binary_number\n+https://en.wikipedia.org/wiki/Octal\n+https://en.wikipedia.org/wiki/Hexadecimal\n+\n+\"\"\"\n+from __future__ import annotations\n+\n+def DecimalConversions(dec):\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/decimal_conversions.py`, please provide doctest for the function `DecimalConversions`\n\nPlease provide return type hint for the function: `DecimalConversions`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `DecimalConversions`\n\nPlease provide type hint for the parameter: `dec`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/decimal_conversions.py",
    "pr_number": 7199,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996220125,
    "comment_created_at": "2022-10-14T23:56:55Z"
  },
  {
    "code": "@@ -0,0 +1,37 @@\n+\"\"\"\n+Dynamic Programming to solve Brust Balloons.py\n+Tabulation Approach\n+https://leetcode.com/problems/burst-balloons/description/\n+\"\"\"\n+def maxCoins(nums):",
    "comment": "please provide return type hint for the function: maxcoins. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: maxcoins\n\nplease provide type hint for the parameter: nums",
    "line_number": 6,
    "enriched": "File: dynamic_programming/brust_balloons.py\nCode: @@ -0,0 +1,37 @@\n+\"\"\"\n+Dynamic Programming to solve Brust Balloons.py\n+Tabulation Approach\n+https://leetcode.com/problems/burst-balloons/description/\n+\"\"\"\n+def maxCoins(nums):\nComment: Please provide return type hint for the function: `maxCoins`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `maxCoins`\n\nPlease provide type hint for the parameter: `nums`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/brust_balloons.py",
    "pr_number": 6905,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 991119009,
    "comment_created_at": "2022-10-10T10:05:09Z"
  },
  {
    "code": "@@ -0,0 +1,38 @@\n+def sum_of_hp(first_term, common_difference, no_of_terms):",
    "comment": "please provide return type hint for the function: sum_of_hp. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file maths/sum_of_harmonic_series.py, please provide doctest for the function sum_of_hp\n\nplease provide type hint for the parameter: first_term\n\nplease provide type hint for the parameter: common_difference\n\nplease provide type hint for the parameter: no_of_terms",
    "line_number": 1,
    "enriched": "File: maths/sum_of_harmonic_series.py\nCode: @@ -0,0 +1,38 @@\n+def sum_of_hp(first_term, common_difference, no_of_terms):\nComment: Please provide return type hint for the function: `sum_of_hp`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `maths/sum_of_harmonic_series.py`, please provide doctest for the function `sum_of_hp`\n\nPlease provide type hint for the parameter: `first_term`\n\nPlease provide type hint for the parameter: `common_difference`\n\nPlease provide type hint for the parameter: `no_of_terms`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/sum_of_harmonic_series.py",
    "pr_number": 7504,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002375426,
    "comment_created_at": "2022-10-22T06:40:03Z"
  },
  {
    "code": "@@ -11,10 +11,8 @@\n * [ ] This pull request is all my own work -- I have not plagiarized.\n * [ ] I know that pull requests will not be merged if they fail the automated tests.\n * [ ] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.\n-* [ ] All new Python files are placed inside an existing directory.\n-* [ ] All filenames are in all lowercase characters with no spaces or dashes.\n * [ ] All functions and variable names follow Python naming conventions.\n * [ ] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).\n * [ ] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.\n-* [ ] All new algorithms have a URL in its comments that points to Wikipedia or other similar explanation.\n+* [ ] All new algorithms have a URL in their comments that point to Wikipedia or another similar explanation.",
    "comment": "perhaps this\r\n\r\nor \r\n\r\nthese make a bit more sense in my opinion",
    "line_number": 17,
    "enriched": "File: .github/pull_request_template.md\nCode: @@ -11,10 +11,8 @@\n * [ ] This pull request is all my own work -- I have not plagiarized.\n * [ ] I know that pull requests will not be merged if they fail the automated tests.\n * [ ] This PR only changes one algorithm file.  To ease review, please open separate PRs for separate algorithms.\n-* [ ] All new Python files are placed inside an existing directory.\n-* [ ] All filenames are in all lowercase characters with no spaces or dashes.\n * [ ] All functions and variable names follow Python naming conventions.\n * [ ] All function parameters and return values are annotated with Python [type hints](https://docs.python.org/3/library/typing.html).\n * [ ] All functions have [doctests](https://docs.python.org/3/library/doctest.html) that pass the automated testing.\n-* [ ] All new algorithms have a URL in its comments that points to Wikipedia or other similar explanation.\n+* [ ] All new algorithms have a URL in their comments that point to Wikipedia or another similar explanation.\nComment: \r\nPerhaps this\r\n```suggestion\r\n* [ ] All new algorithms have a docstring containing a URL that point to Wikipedia or another similar explanation.\r\n```\r\nor \r\n```suggestion\r\n* [ ] All new algorithms include at least one URL that points to Wikipedia or another similar explanation.\r\n```\r\nThese make a bit more sense in my opinion",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": ".github/pull_request_template.md",
    "pr_number": 7794,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008297769,
    "comment_created_at": "2022-10-28T17:32:24Z"
  },
  {
    "code": "@@ -0,0 +1,36 @@\n+def isSafe(node, color, graph, n, col):",
    "comment": "please provide return type hint for the function: issafe. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file backtracking/m-coloring-problem.py, please provide doctest for the function issafe\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: issafe\n\nplease provide type hint for the parameter: node\n\nplease provide type hint for the parameter: color\n\nplease provide type hint for the parameter: graph\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: n\n\nplease provide type hint for the parameter: col",
    "line_number": 1,
    "enriched": "File: backtracking/m-coloring-problem.py\nCode: @@ -0,0 +1,36 @@\n+def isSafe(node, color, graph, n, col):\nComment: Please provide return type hint for the function: `isSafe`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `backtracking/m-coloring-problem.py`, please provide doctest for the function `isSafe`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `isSafe`\n\nPlease provide type hint for the parameter: `node`\n\nPlease provide type hint for the parameter: `color`\n\nPlease provide type hint for the parameter: `graph`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide type hint for the parameter: `col`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "backtracking/m-coloring-problem.py",
    "pr_number": 13272,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2407649093,
    "comment_created_at": "2025-10-06T17:41:29Z"
  },
  {
    "code": "@@ -0,0 +1,70 @@\n+\"\"\"\n+Description :\n+Calculate one of the parameters (current, voltage, or resistance) \n+in an electrical circuit based on Ohm's law.\n+Ohm's law(\u03a9) states that the current through a \n+conductor between two points is directly \n+proportional to the voltage across the two points.\n+V is proportional to I.\n+V = I*R (where R is a proportionality constant)\n+\n+    Input Parameters:\n+    - current: Current in Amperes (A)\n+    - voltage: Voltage in Volts (V)\n+    - resistance: Resistance in Ohms (\u03a9)\n+\n+    Returns:\n+    A dictionary with the calculated parameter and its value.\n+Source :\n+- https://byjus.com/physics/ohms-law/\n+\"\"\"\n+\n+\n+def ohms_law(voltage=None, current=None, resistance=None) -> float:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file physics/ohms_law.py, please provide doctest for the function ohms_law\n\nplease provide type hint for the parameter: voltage\n\nplease provide type hint for the parameter: current\n\nplease provide type hint for the parameter: resistance",
    "line_number": 23,
    "enriched": "File: physics/ohms_law.py\nCode: @@ -0,0 +1,70 @@\n+\"\"\"\n+Description :\n+Calculate one of the parameters (current, voltage, or resistance) \n+in an electrical circuit based on Ohm's law.\n+Ohm's law(\u03a9) states that the current through a \n+conductor between two points is directly \n+proportional to the voltage across the two points.\n+V is proportional to I.\n+V = I*R (where R is a proportionality constant)\n+\n+    Input Parameters:\n+    - current: Current in Amperes (A)\n+    - voltage: Voltage in Volts (V)\n+    - resistance: Resistance in Ohms (\u03a9)\n+\n+    Returns:\n+    A dictionary with the calculated parameter and its value.\n+Source :\n+- https://byjus.com/physics/ohms-law/\n+\"\"\"\n+\n+\n+def ohms_law(voltage=None, current=None, resistance=None) -> float:\nComment: As there is no test file in this pull request nor any test function or class in the file `physics/ohms_law.py`, please provide doctest for the function `ohms_law`\n\nPlease provide type hint for the parameter: `voltage`\n\nPlease provide type hint for the parameter: `current`\n\nPlease provide type hint for the parameter: `resistance`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "physics/ohms_law.py",
    "pr_number": 10586,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1360465754,
    "comment_created_at": "2023-10-16T10:39:37Z"
  },
  {
    "code": "@@ -1,106 +1,83 @@\n import math\n \n \n-def convert(number: int) -> str:\n-    \"\"\"\n-    Given a number return the number in words.\n+def int_to_en(num):\n+    d = {\n+        0: \"zero\",\n+        1: \"one\",\n+        2: \"two\",\n+        3: \"three\",\n+        4: \"four\",\n+        5: \"five\",\n+        6: \"six\",\n+        7: \"seven\",\n+        8: \"eight\",\n+        9: \"nine\",\n+        10: \"ten\",\n+        11: \"eleven\",\n+        12: \"twelve\",\n+        13: \"thirteen\",\n+        14: \"fourteen\",\n+        15: \"fifteen\",\n+        16: \"sixteen\",\n+        17: \"seventeen\",\n+        18: \"eighteen\",\n+        19: \"nineteen\",\n+        20: \"twenty\",\n+        30: \"thirty\",\n+        40: \"forty\",\n+        50: \"fifty\",\n+        60: \"sixty\",\n+        70: \"seventy\",\n+        80: \"eighty\",\n+        90: \"ninety\",\n+    }\n+    k = 1000\n+    m = k * 1000\n+    b = m * 1000\n+    t = b * 1000\n \n-    >>> convert(123)\n-    'OneHundred,TwentyThree'",
    "comment": "please do not remove doctests.  please add a doctest for -8.",
    "line_number": 9,
    "enriched": "File: web_programming/convert_number_to_words.py\nCode: @@ -1,106 +1,83 @@\n import math\n \n \n-def convert(number: int) -> str:\n-    \"\"\"\n-    Given a number return the number in words.\n+def int_to_en(num):\n+    d = {\n+        0: \"zero\",\n+        1: \"one\",\n+        2: \"two\",\n+        3: \"three\",\n+        4: \"four\",\n+        5: \"five\",\n+        6: \"six\",\n+        7: \"seven\",\n+        8: \"eight\",\n+        9: \"nine\",\n+        10: \"ten\",\n+        11: \"eleven\",\n+        12: \"twelve\",\n+        13: \"thirteen\",\n+        14: \"fourteen\",\n+        15: \"fifteen\",\n+        16: \"sixteen\",\n+        17: \"seventeen\",\n+        18: \"eighteen\",\n+        19: \"nineteen\",\n+        20: \"twenty\",\n+        30: \"thirty\",\n+        40: \"forty\",\n+        50: \"fifty\",\n+        60: \"sixty\",\n+        70: \"seventy\",\n+        80: \"eighty\",\n+        90: \"ninety\",\n+    }\n+    k = 1000\n+    m = k * 1000\n+    b = m * 1000\n+    t = b * 1000\n \n-    >>> convert(123)\n-    'OneHundred,TwentyThree'\nComment: Please do not remove doctests.  Please add a doctest for `-8`.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "web_programming/convert_number_to_words.py",
    "pr_number": 8890,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1273149439,
    "comment_created_at": "2023-07-25T07:57:19Z"
  },
  {
    "code": "@@ -0,0 +1,80 @@\n+'''\n+Let G be a tree. For every query of the form (u, v) we want to find the lowest common ancestor of the nodes u and v, i.e. we want to find a node w that lies on the path from u to the root node, that lies on the path from v to the root node, and if there are multiple nodes we pick the one that is farthest away from the root node. In other words the desired node w is the lowest ancestor of u and v. In particular if u is an ancestor of v, then u is their lowest common ancestor.\n+\n+This algorithm will need O(Nlog N) for preprocessing the tree, and then O(log N) for each LCA query.\n+'''\n+\n+import math\n+\n+n = 0\n+l = 0\n+adj = []\n+\n+timer = 0\n+tin = []\n+tout = []\n+up = []\n+\n+def dfs(v, p):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/binary_tree/lca_binary_lifting.py, please provide doctest for the function dfs\n\nplease provide return type hint for the function: dfs. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: v\n\nplease provide descriptive name for the parameter: v\n\nplease provide type hint for the parameter: p\n\nplease provide descriptive name for the parameter: p",
    "line_number": 18,
    "enriched": "File: data_structures/binary_tree/lca_binary_lifting.py\nCode: @@ -0,0 +1,80 @@\n+'''\n+Let G be a tree. For every query of the form (u, v) we want to find the lowest common ancestor of the nodes u and v, i.e. we want to find a node w that lies on the path from u to the root node, that lies on the path from v to the root node, and if there are multiple nodes we pick the one that is farthest away from the root node. In other words the desired node w is the lowest ancestor of u and v. In particular if u is an ancestor of v, then u is their lowest common ancestor.\n+\n+This algorithm will need O(Nlog N) for preprocessing the tree, and then O(log N) for each LCA query.\n+'''\n+\n+import math\n+\n+n = 0\n+l = 0\n+adj = []\n+\n+timer = 0\n+tin = []\n+tout = []\n+up = []\n+\n+def dfs(v, p):\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/binary_tree/lca_binary_lifting.py`, please provide doctest for the function `dfs`\n\nPlease provide return type hint for the function: `dfs`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `v`\n\nPlease provide descriptive name for the parameter: `v`\n\nPlease provide type hint for the parameter: `p`\n\nPlease provide descriptive name for the parameter: `p`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/binary_tree/lca_binary_lifting.py",
    "pr_number": 9360,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342285166,
    "comment_created_at": "2023-10-02T05:05:37Z"
  },
  {
    "code": "@@ -0,0 +1,40 @@\n+'''\n+This program will search every even number less than the inputted number and then show which numbers passed the test and which didn't.\n+'''\n+\n+def isprime(num):",
    "comment": "please provide return type hint for the function: isprime. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file maths/goldbach's conjecture.py, please provide doctest for the function isprime\n\nplease provide type hint for the parameter: num",
    "line_number": 5,
    "enriched": "File: maths/Goldbach's conjecture.py\nCode: @@ -0,0 +1,40 @@\n+'''\n+This program will search every even number less than the inputted number and then show which numbers passed the test and which didn't.\n+'''\n+\n+def isprime(num):\nComment: Please provide return type hint for the function: `isprime`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `maths/Goldbach's conjecture.py`, please provide doctest for the function `isprime`\n\nPlease provide type hint for the parameter: `num`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/Goldbach's conjecture.py",
    "pr_number": 11131,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1377994225,
    "comment_created_at": "2023-10-31T18:07:22Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+\"\"\"\r\n+There is a robot on an m x n grid.\r\n+The robot is initially located at the top-left corner of grid \r\n+the robot tries to move to the bottom-right corner.\r\n+The robot can only move either down or right at any point in time.\r\n+Return number of all  possible unique paths robot can take.\r\n+\"\"\"\r\n+\r\n+\r\n+def uniquepaths(self, m, n):\r",
    "comment": "please provide return type hint for the function: uniquepaths. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide descriptive name for the parameter: m\n\nplease provide type hint for the parameter: m\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: n",
    "line_number": 10,
    "enriched": "File: dynamic_programming/unique_paths.py\nCode: @@ -0,0 +1,35 @@\n+\"\"\"\r\n+There is a robot on an m x n grid.\r\n+The robot is initially located at the top-left corner of grid \r\n+the robot tries to move to the bottom-right corner.\r\n+The robot can only move either down or right at any point in time.\r\n+Return number of all  possible unique paths robot can take.\r\n+\"\"\"\r\n+\r\n+\r\n+def uniquepaths(self, m, n):\r\nComment: Please provide return type hint for the function: `uniquepaths`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide descriptive name for the parameter: `m`\n\nPlease provide type hint for the parameter: `m`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide type hint for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/unique_paths.py",
    "pr_number": 10199,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1350750269,
    "comment_created_at": "2023-10-09T20:10:22Z"
  },
  {
    "code": "@@ -0,0 +1,13 @@\n+from time import sleep\n+\n+import thread\n+\n+items = []\n+n = int(input())\n+for i in range(0,n):\n+  a = int(input())\n+  items.append(a)\n+def sleep_sort(i):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file sorts/sleepsort.py, please provide doctest for the function sleep_sort\n\nplease provide return type hint for the function: sleep_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: i\n\nplease provide descriptive name for the parameter: i",
    "line_number": 10,
    "enriched": "File: sorts/sleepsort.py\nCode: @@ -0,0 +1,13 @@\n+from time import sleep\n+\n+import thread\n+\n+items = []\n+n = int(input())\n+for i in range(0,n):\n+  a = int(input())\n+  items.append(a)\n+def sleep_sort(i):\nComment: As there is no test file in this pull request nor any test function or class in the file `sorts/sleepsort.py`, please provide doctest for the function `sleep_sort`\n\nPlease provide return type hint for the function: `sleep_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `i`\n\nPlease provide descriptive name for the parameter: `i`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "sorts/sleepsort.py",
    "pr_number": 6959,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 991649650,
    "comment_created_at": "2022-10-10T21:25:29Z"
  },
  {
    "code": "@@ -6,9 +6,20 @@\n \n def next_greatest_element_slow(arr: list[float]) -> list[float]:\n     \"\"\"\n-    Get the Next Greatest Element (NGE) for all elements in a list.\n-    Maximum element present after the current one which is also greater than the\n-    current one.\n+    Get the Next Greatest Element (NGE) for each element in the array\n+    by checking all subsequent elements to find the next greater one.\n+\n+    This is a brute-force implementation, and it has a time complexity\n+    of O(n^2), where n is the size of the array.\n+\n+    Args:\n+        arr (list[float]): List of numbers for which the NGE is calculated.\n+\n+    Returns:\n+        list[float]: List containing the next greatest elements. If no\n+        greater element is found, -1 is placed in the result.",
    "comment": "if the type of the variable ever changes for any reason, we'd need to remember to update the type hint in two different places. as such, in this repo we prefer to avoid specifying the variable types in the docstrings.",
    "line_number": 20,
    "enriched": "File: data_structures/stacks/next_greater_element.py\nCode: @@ -6,9 +6,20 @@\n \n def next_greatest_element_slow(arr: list[float]) -> list[float]:\n     \"\"\"\n-    Get the Next Greatest Element (NGE) for all elements in a list.\n-    Maximum element present after the current one which is also greater than the\n-    current one.\n+    Get the Next Greatest Element (NGE) for each element in the array\n+    by checking all subsequent elements to find the next greater one.\n+\n+    This is a brute-force implementation, and it has a time complexity\n+    of O(n^2), where n is the size of the array.\n+\n+    Args:\n+        arr (list[float]): List of numbers for which the NGE is calculated.\n+\n+    Returns:\n+        list[float]: List containing the next greatest elements. If no\n+        greater element is found, -1 is placed in the result.\nComment: ```suggestion\r\n    Args:\r\n        arr: List of numbers for which the NGE is calculated.\r\n\r\n    Returns:\r\n        List containing the next greatest elements. If no\r\n        greater element is found, -1 is placed in the result.\r\n```\r\nIf the type of the variable ever changes for any reason, we'd need to remember to update the type hint in two different places. As such, in this repo we prefer to avoid specifying the variable types in the docstrings.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/stacks/next_greater_element.py",
    "pr_number": 11685,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1785442421,
    "comment_created_at": "2024-10-03T00:07:49Z"
  },
  {
    "code": "@@ -0,0 +1,111 @@\n+import pandas as pd\n+import numpy as np\n+from sklearn.datasets import load_digits\n+from sklearn.model_selection import train_test_split\n+from sklearn.ensemble import GradientBoostingClassifier\n+from sklearn.metrics import accuracy_score, classification_report\n+from sklearn.preprocessing import LabelEncoder\n+from sklearn.feature_selection import SelectKBest, chi2\n+import matplotlib.pyplot as plt\n+\n+def load_data():",
    "comment": "please provide return type hint for the function: load_data. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file machine_learning/gradient_boosting_classifier.py, please provide doctest for the function load_data",
    "line_number": 11,
    "enriched": "File: machine_learning/gradient_boosting_classifier.py\nCode: @@ -0,0 +1,111 @@\n+import pandas as pd\n+import numpy as np\n+from sklearn.datasets import load_digits\n+from sklearn.model_selection import train_test_split\n+from sklearn.ensemble import GradientBoostingClassifier\n+from sklearn.metrics import accuracy_score, classification_report\n+from sklearn.preprocessing import LabelEncoder\n+from sklearn.feature_selection import SelectKBest, chi2\n+import matplotlib.pyplot as plt\n+\n+def load_data():\nComment: Please provide return type hint for the function: `load_data`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `machine_learning/gradient_boosting_classifier.py`, please provide doctest for the function `load_data`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/gradient_boosting_classifier.py",
    "pr_number": 8824,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1233015815,
    "comment_created_at": "2023-06-17T10:09:03Z"
  },
  {
    "code": "@@ -0,0 +1,28 @@\n+\"\"\"\n+wiki: https://en.wikipedia.org/wiki/Heterogram_(literature)#Isograms\n+\"\"\"\n+\n+\n+def check_isogram(string: str) -> bool:",
    "comment": "this is a slightly more self-documenting name for a function that returns a bool.\r\n\r\n\r\nalso, please add a test for true123.  should that return true or raise a valueerror?",
    "line_number": 6,
    "enriched": "File: strings/check_isogram.py\nCode: @@ -0,0 +1,28 @@\n+\"\"\"\n+wiki: https://en.wikipedia.org/wiki/Heterogram_(literature)#Isograms\n+\"\"\"\n+\n+\n+def check_isogram(string: str) -> bool:\nComment: This is a slightly more self-documenting name for a function that returns a bool.\r\n```suggestion\r\ndef is_isogram(string: str) -> bool:\r\n```\r\n\r\nAlso, please add a test for `true123`.  Should that return True or raise a ValueError?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "strings/check_isogram.py",
    "pr_number": 7608,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004945882,
    "comment_created_at": "2022-10-25T20:42:01Z"
  },
  {
    "code": "@@ -0,0 +1,52 @@\n+#############################\n+# Author: yash kesharwani\n+# File: best_time_to_buy_sell_stock.py\n+# comments: This program output the\n+# best time to buy and sell stock with fees\n+#############################\n+import doctest\n+\n+\n+class Solution:\n+    def solve(self, prices, index, fee, buy, dp):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/best_time_to_buy_sell_stock.py, please provide doctest for the function solve\n\nplease provide return type hint for the function: solve. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: prices\n\nplease provide type hint for the parameter: index\n\nplease provide type hint for the parameter: fee\n\nplease provide type hint for the parameter: buy\n\nplease provide type hint for the parameter: dp",
    "line_number": 11,
    "enriched": "File: dynamic_programming/best_time_to_buy_sell_stock.py\nCode: @@ -0,0 +1,52 @@\n+#############################\n+# Author: yash kesharwani\n+# File: best_time_to_buy_sell_stock.py\n+# comments: This program output the\n+# best time to buy and sell stock with fees\n+#############################\n+import doctest\n+\n+\n+class Solution:\n+    def solve(self, prices, index, fee, buy, dp):\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/best_time_to_buy_sell_stock.py`, please provide doctest for the function `solve`\n\nPlease provide return type hint for the function: `solve`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `prices`\n\nPlease provide type hint for the parameter: `index`\n\nPlease provide type hint for the parameter: `fee`\n\nPlease provide type hint for the parameter: `buy`\n\nPlease provide type hint for the parameter: `dp`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/best_time_to_buy_sell_stock.py",
    "pr_number": 10578,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1360201547,
    "comment_created_at": "2023-10-16T07:03:58Z"
  },
  {
    "code": "@@ -0,0 +1,85 @@\n+import math\r\n+import time\r\n+import pyautogui\r\n+import mediapipe as mp\r\n+import cv2\r\n+\r\n+CAMERA_ID = 0\r\n+FLIP_IMAGE = True\r\n+VOLUME_CHANGE_COOLDOWN = 0.5\r\n+VOLUME_CHANGE_THRESHOLD = 50\r\n+\r\n+def main():\r",
    "comment": "please provide return type hint for the function: main. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file computer_vision/wave_tune.py, please provide doctest for the function main",
    "line_number": 12,
    "enriched": "File: computer_vision/wave_tune.py\nCode: @@ -0,0 +1,85 @@\n+import math\r\n+import time\r\n+import pyautogui\r\n+import mediapipe as mp\r\n+import cv2\r\n+\r\n+CAMERA_ID = 0\r\n+FLIP_IMAGE = True\r\n+VOLUME_CHANGE_COOLDOWN = 0.5\r\n+VOLUME_CHANGE_THRESHOLD = 50\r\n+\r\n+def main():\r\nComment: Please provide return type hint for the function: `main`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `computer_vision/wave_tune.py`, please provide doctest for the function `main`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "computer_vision/wave_tune.py",
    "pr_number": 11661,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1783901391,
    "comment_created_at": "2024-10-02T06:37:44Z"
  },
  {
    "code": "@@ -0,0 +1,24 @@\n+def meta_binary_search(arr, target):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/meta_binary_search.py, please provide doctest for the function meta_binary_search\n\nplease provide return type hint for the function: meta_binary_search. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: target",
    "line_number": 1,
    "enriched": "File: searches/meta_binary_search.py\nCode: @@ -0,0 +1,24 @@\n+def meta_binary_search(arr, target):\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/meta_binary_search.py`, please provide doctest for the function `meta_binary_search`\n\nPlease provide return type hint for the function: `meta_binary_search`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `target`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "searches/meta_binary_search.py",
    "pr_number": 12197,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1807635275,
    "comment_created_at": "2024-10-20T04:35:26Z"
  },
  {
    "code": "@@ -0,0 +1,129 @@\n+from __future__ import annotations\n+class Node:\n+    \"\"\"\n+prints the inorder Traversal of transformed tree\n+>>> sum = 0\n+>>> root = Node(11)\n+>>> root.left = Node(2)\n+>>> root.right = Node(29)\n+>>> root.left.left = Node(1)\n+>>> root.left.right = Node(7)\n+>>> root.right.left = Node(15)\n+>>> root.right.right = Node(40)\n+>>> root.right.right.left = Node(35)\n+>>> printInorder(root)\n+1 2 7 11 15 29 35 40 \n+\n+>>> transformTree(root)\n+\n+>>> printInorder(root)\n+139 137 130 119 104 75 40 0\n+\n+\"\"\"\n+\n+    def __init__(self, x) -> None:",
    "comment": "please provide descriptive name for the parameter: x\n\nplease provide type hint for the parameter: x",
    "line_number": 24,
    "enriched": "File: data_structures/binary_tree/transform_bst_sum_tree.py\nCode: @@ -0,0 +1,129 @@\n+from __future__ import annotations\n+class Node:\n+    \"\"\"\n+prints the inorder Traversal of transformed tree\n+>>> sum = 0\n+>>> root = Node(11)\n+>>> root.left = Node(2)\n+>>> root.right = Node(29)\n+>>> root.left.left = Node(1)\n+>>> root.left.right = Node(7)\n+>>> root.right.left = Node(15)\n+>>> root.right.right = Node(40)\n+>>> root.right.right.left = Node(35)\n+>>> printInorder(root)\n+1 2 7 11 15 29 35 40 \n+\n+>>> transformTree(root)\n+\n+>>> printInorder(root)\n+139 137 130 119 104 75 40 0\n+\n+\"\"\"\n+\n+    def __init__(self, x) -> None:\nComment: Please provide descriptive name for the parameter: `x`\n\nPlease provide type hint for the parameter: `x`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "data_structures/binary_tree/transform_bst_sum_tree.py",
    "pr_number": 9772,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346786764,
    "comment_created_at": "2023-10-05T05:03:54Z"
  },
  {
    "code": "@@ -9,7 +9,7 @@ def permute(nums: list[int]) -> list[list[int]]:\n     \"\"\"\n     result = []\n     if len(nums) == 1:\n-        return [nums.copy()]\n+        return [nums.copy()] #returns a copy of list nums",
    "comment": "this is self explanatory",
    "line_number": 12,
    "enriched": "File: data_structures/arrays/permutations.py\nCode: @@ -9,7 +9,7 @@ def permute(nums: list[int]) -> list[list[int]]:\n     \"\"\"\n     result = []\n     if len(nums) == 1:\n-        return [nums.copy()]\n+        return [nums.copy()] #returns a copy of list nums\nComment: This is self explanatory",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "data_structures/arrays/permutations.py",
    "pr_number": 8129,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1102645891,
    "comment_created_at": "2023-02-10T11:30:44Z"
  },
  {
    "code": "@@ -0,0 +1,131 @@\n+\n+ip = (2, 6, 3, 1, 4, 8, 5, 7)\n+ip_inverse = (4, 1, 3, 5, 7, 2, 8, 6)\n+ep = (4, 1, 2, 3, 2, 3, 4, 1)\n+p10 = (3, 5, 2, 7, 4, 10, 1, 9, 8, 6)\n+p8 = (6, 3, 7, 4, 8, 5, 10, 9)\n+p4 = (2, 4, 3, 1)\n+\n+#predefined sboxes\n+sb0 = ((1, 0, 3, 2),\n+       (3, 2, 1, 0),\n+       (0, 2, 1, 3),\n+       (3, 1, 3, 2))\n+\n+sb1 = ((0, 1, 2, 3),\n+       (2, 0, 1, 3),\n+       (3, 0, 1, 0),\n+       (2, 1, 0, 3))\n+\n+#permute takes a bitstring and a predefined permutation as input and outputs a new permuted bitstring\n+def permute(bitstring, permutation):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file ciphers/sdes_cipher.py, please provide doctest for the function permute\n\nplease provide return type hint for the function: permute. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: bitstring\n\nplease provide type hint for the parameter: permutation",
    "line_number": 21,
    "enriched": "File: ciphers/sdes_cipher.py\nCode: @@ -0,0 +1,131 @@\n+\n+ip = (2, 6, 3, 1, 4, 8, 5, 7)\n+ip_inverse = (4, 1, 3, 5, 7, 2, 8, 6)\n+ep = (4, 1, 2, 3, 2, 3, 4, 1)\n+p10 = (3, 5, 2, 7, 4, 10, 1, 9, 8, 6)\n+p8 = (6, 3, 7, 4, 8, 5, 10, 9)\n+p4 = (2, 4, 3, 1)\n+\n+#predefined sboxes\n+sb0 = ((1, 0, 3, 2),\n+       (3, 2, 1, 0),\n+       (0, 2, 1, 3),\n+       (3, 1, 3, 2))\n+\n+sb1 = ((0, 1, 2, 3),\n+       (2, 0, 1, 3),\n+       (3, 0, 1, 0),\n+       (2, 1, 0, 3))\n+\n+#permute takes a bitstring and a predefined permutation as input and outputs a new permuted bitstring\n+def permute(bitstring, permutation):\nComment: As there is no test file in this pull request nor any test function or class in the file `ciphers/sdes_cipher.py`, please provide doctest for the function `permute`\n\nPlease provide return type hint for the function: `permute`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `bitstring`\n\nPlease provide type hint for the parameter: `permutation`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "ciphers/sdes_cipher.py",
    "pr_number": 10892,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1370063018,
    "comment_created_at": "2023-10-24T12:12:30Z"
  },
  {
    "code": "@@ -0,0 +1,126 @@\n+from random import randrange, choice\n+#curr_player = 'X'\n+rows = 3\n+cols = 3\n+board = [[\" \" for x in range(cols)] for y in range(rows)]  \n+player_moves = {'X':[], 'O':[]}\n+#board = [[0]*3]*3\n+#print(board)\n+\"\"\"\n+count = 1\n+for i in range(3): \n+    for j in range(3):\n+        board[i][j] = \n+        count += 1\n+#        print(count)\n+    #count = count + 1\n+\"\"\"\n+def game_name():",
    "comment": "please provide return type hint for the function: game_name. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file other/tic_tac_toe.py, please provide doctest for the function game_name",
    "line_number": 18,
    "enriched": "File: other/tic_tac_toe.py\nCode: @@ -0,0 +1,126 @@\n+from random import randrange, choice\n+#curr_player = 'X'\n+rows = 3\n+cols = 3\n+board = [[\" \" for x in range(cols)] for y in range(rows)]  \n+player_moves = {'X':[], 'O':[]}\n+#board = [[0]*3]*3\n+#print(board)\n+\"\"\"\n+count = 1\n+for i in range(3): \n+    for j in range(3):\n+        board[i][j] = \n+        count += 1\n+#        print(count)\n+    #count = count + 1\n+\"\"\"\n+def game_name():\nComment: Please provide return type hint for the function: `game_name`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `other/tic_tac_toe.py`, please provide doctest for the function `game_name`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "other/tic_tac_toe.py",
    "pr_number": 7287,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996399442,
    "comment_created_at": "2022-10-16T06:59:03Z"
  },
  {
    "code": "@@ -0,0 +1,70 @@\n+\n+def TOWUtil(arr, n, curr_elements, no_of_selected_elements,",
    "comment": "please provide return type hint for the function: towutil. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: towutil\n\nas there is no test file in this pull request nor any test function or class in the file backtracking/tug_of_war.py, please provide doctest for the function towutil\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: curr_elements\n\nplease provide type hint for the parameter: no_of_selected_elements",
    "line_number": 2,
    "enriched": "File: backtracking/Tug_of_war.py\nCode: @@ -0,0 +1,70 @@\n+\n+def TOWUtil(arr, n, curr_elements, no_of_selected_elements,\nComment: Please provide return type hint for the function: `TOWUtil`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `TOWUtil`\n\nAs there is no test file in this pull request nor any test function or class in the file `backtracking/Tug_of_war.py`, please provide doctest for the function `TOWUtil`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide type hint for the parameter: `curr_elements`\n\nPlease provide type hint for the parameter: `no_of_selected_elements`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "backtracking/Tug_of_war.py",
    "pr_number": 7775,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1007732848,
    "comment_created_at": "2022-10-28T07:17:39Z"
  },
  {
    "code": "@@ -0,0 +1,507 @@\n+import numpy as np\n+from tqdm import tqdm\n+from numpy.random import default_rng\n+\n+class Dataloader:\n+    \"\"\"\n+    DataLoader class for handling dataset operations. Supports:\n+      - data shuffling\n+      - one-hot encoding\n+      - train/test splitting\n+\n+    Example usage:\n+    >>> X = [[0.0, 0.0], [1.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n+    >>> y = [0, 1, 0, 0]\n+    >>> loader = Dataloader(X, y)\n+    >>> train_X, train_y, test_X, test_y = loader.get_train_test_data()\n+    >>> train_X.shape\n+    (3, 2)\n+    >>> len(train_y)\n+    3\n+    >>> test_X.shape\n+    (1, 2)\n+    >>> len(test_y)\n+    1\n+    >>> loader.one_hot_encode([0, 1, 0], 2)  # Returns one-hot encoded labels\n+    array([[0.99, 0.  ],\n+           [0.  , 0.99],\n+           [0.99, 0.  ]])\n+    >>> loader.get_inout_dim()\n+    (2, 3)\n+    >>> loader.one_hot_encode([0, 2], 3)\n+    array([[0.99, 0.  , 0.  ],\n+           [0.  , 0.  , 0.99]])\n+    \"\"\"\n+\n+    def __init__(self, features: list[list[float]], labels: list[int]) -> None:\n+        \"\"\"\n+        Initializes the Dataloader instance with a feature matrix (`features`)\n+        and corresponding labels (`labels`).\n+\n+        Args:\n+            features: Feature matrix of shape (n_samples, n_features).\n+            labels: List of labels of shape (n_samples,).\n+        \"\"\"\n+        # random seed\n+        self.rng = default_rng(42)  # Create a random number generator with a seed\n+        self.X = np.array(features)\n+        self.y = np.array(labels)\n+        self.class_weights = {0: 1.0, 1: 1.0}  # Example class weights, adjust as needed\n+\n+    def get_train_test_data(self) -> tuple[list[np.ndarray], list[np.ndarray], list[np.ndarray], list[np.ndarray]]:\n+        \"\"\"\n+        Splits the data into training and testing sets.\n+        Here, we manually split the data.\n+\n+        Returns:\n+            A tuple containing:\n+            - Train data\n+            - Train labels\n+            - Test data\n+            - Test labels\n+        \"\"\"\n+        train_data = np.array([self.X[0], self.X[1], self.X[2]])  # First 3 samples for training\n+        train_labels = [np.array([self.y[0]]), np.array([self.y[1]]), np.array([self.y[2]])]  # Labels as np.ndarray\n+        test_data = np.array([self.X[3]])  # Last sample for testing\n+        test_labels = [np.array([self.y[3]])]  # Labels as np.ndarray\n+        return train_data, train_labels, test_data, test_labels\n+\n+    def shuffle_data(self, paired_data: list[tuple[np.ndarray, int]]) -> list[tuple[np.ndarray, int]]:\n+        \"\"\"\n+        Shuffles the data randomly.\n+\n+        Args:\n+            paired_data: List of tuples containing data and corresponding labels.\n+\n+        Returns:\n+            A shuffled list of data-label pairs.\n+        \"\"\"\n+        default_rng.shuffle(paired_data)  # Using the new random number generator\n+        return paired_data\n+\n+    def get_inout_dim(self) -> tuple[int, int]:\n+        train_data, train_labels, test_data, test_labels = self.get_train_test_data()\n+        in_dim = train_data[0].shape[0]\n+        out_dim = len(train_labels)\n+        return in_dim, out_dim\n+\n+    @staticmethod\n+    def one_hot_encode(labels: list[int], num_classes: int) -> np.ndarray:\n+        \"\"\"\n+        Perform one-hot encoding for the given labels.\n+\n+        Args:\n+            labels: List of integer labels.\n+            num_classes: Total number of classes for encoding.\n+\n+        Returns:\n+            A numpy array representing one-hot encoded labels.\n+        \"\"\"\n+        one_hot = np.zeros((len(labels), num_classes))\n+        for idx, label in enumerate(labels):\n+            one_hot[idx, label] = 0.99\n+        return one_hot\n+\n+\n+class MLP():\n+    \"\"\"\n+        A custom MLP class for implementing a simple multi-layer perceptron with\n+        forward propagation, backpropagation.\n+\n+        Attributes:\n+            learning_rate (float): Learning rate for gradient descent.\n+            gamma (float): Parameter to control learning rate adjustment.\n+            epoch (int): Number of epochs for training.\n+            hidden_dim (int): Dimension of the hidden layer.\n+            batch_size (int): Number of samples per mini-batch.\n+            train_loss (List[float]): List to store training loss for each fold.\n+            train_accuracy (List[float]): List to store training accuracy for each fold.\n+            test_loss (List[float]): List to store test loss for each fold.\n+            test_accuracy (List[float]): List to store test accuracy for each fold.\n+            dataloader (Dataloader): DataLoader object for handling training data.\n+            inter_variable (dict):\n+                Dictionary to store intermediate variables for backpropagation.\n+            weights1_list (List[Tuple[np.ndarray, np.ndarray]]):\n+                List of weights for each fold.\n+\n+        Methods:\n+            get_inout_dim:obtain input dimension and output dimension.\n+            relu: Apply the ReLU activation function.\n+            relu_derivative: Compute the derivative of the ReLU function.\n+            forward: Perform a forward pass through the network.\n+            back_prop: Perform backpropagation to compute gradients.\n+            update_weights: Update the weights using gradients.\n+            update_learning_rate: Adjust the learning rate based on test accuracy.\n+            accuracy: Compute accuracy of the model.\n+            loss: Compute weighted MSE loss.\n+            train: Train the MLP over multiple folds with early stopping.\n+\n+\n+        \"\"\"\n+    def __init__(self, dataloader, epoch: int, learning_rate: float, gamma=1, hidden_dim=2):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: dataloader\n\nplease provide type hint for the parameter: gamma\n\nplease provide type hint for the parameter: hidden_dim",
    "line_number": 141,
    "enriched": "File: machine_learning/multilayer_perceptron_classifier_from_scratch.py\nCode: @@ -0,0 +1,507 @@\n+import numpy as np\n+from tqdm import tqdm\n+from numpy.random import default_rng\n+\n+class Dataloader:\n+    \"\"\"\n+    DataLoader class for handling dataset operations. Supports:\n+      - data shuffling\n+      - one-hot encoding\n+      - train/test splitting\n+\n+    Example usage:\n+    >>> X = [[0.0, 0.0], [1.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n+    >>> y = [0, 1, 0, 0]\n+    >>> loader = Dataloader(X, y)\n+    >>> train_X, train_y, test_X, test_y = loader.get_train_test_data()\n+    >>> train_X.shape\n+    (3, 2)\n+    >>> len(train_y)\n+    3\n+    >>> test_X.shape\n+    (1, 2)\n+    >>> len(test_y)\n+    1\n+    >>> loader.one_hot_encode([0, 1, 0], 2)  # Returns one-hot encoded labels\n+    array([[0.99, 0.  ],\n+           [0.  , 0.99],\n+           [0.99, 0.  ]])\n+    >>> loader.get_inout_dim()\n+    (2, 3)\n+    >>> loader.one_hot_encode([0, 2], 3)\n+    array([[0.99, 0.  , 0.  ],\n+           [0.  , 0.  , 0.99]])\n+    \"\"\"\n+\n+    def __init__(self, features: list[list[float]], labels: list[int]) -> None:\n+        \"\"\"\n+        Initializes the Dataloader instance with a feature matrix (`features`)\n+        and corresponding labels (`labels`).\n+\n+        Args:\n+            features: Feature matrix of shape (n_samples, n_features).\n+            labels: List of labels of shape (n_samples,).\n+        \"\"\"\n+        # random seed\n+        self.rng = default_rng(42)  # Create a random number generator with a seed\n+        self.X = np.array(features)\n+        self.y = np.array(labels)\n+        self.class_weights = {0: 1.0, 1: 1.0}  # Example class weights, adjust as needed\n+\n+    def get_train_test_data(self) -> tuple[list[np.ndarray], list[np.ndarray], list[np.ndarray], list[np.ndarray]]:\n+        \"\"\"\n+        Splits the data into training and testing sets.\n+        Here, we manually split the data.\n+\n+        Returns:\n+            A tuple containing:\n+            - Train data\n+            - Train labels\n+            - Test data\n+            - Test labels\n+        \"\"\"\n+        train_data = np.array([self.X[0], self.X[1], self.X[2]])  # First 3 samples for training\n+        train_labels = [np.array([self.y[0]]), np.array([self.y[1]]), np.array([self.y[2]])]  # Labels as np.ndarray\n+        test_data = np.array([self.X[3]])  # Last sample for testing\n+        test_labels = [np.array([self.y[3]])]  # Labels as np.ndarray\n+        return train_data, train_labels, test_data, test_labels\n+\n+    def shuffle_data(self, paired_data: list[tuple[np.ndarray, int]]) -> list[tuple[np.ndarray, int]]:\n+        \"\"\"\n+        Shuffles the data randomly.\n+\n+        Args:\n+            paired_data: List of tuples containing data and corresponding labels.\n+\n+        Returns:\n+            A shuffled list of data-label pairs.\n+        \"\"\"\n+        default_rng.shuffle(paired_data)  # Using the new random number generator\n+        return paired_data\n+\n+    def get_inout_dim(self) -> tuple[int, int]:\n+        train_data, train_labels, test_data, test_labels = self.get_train_test_data()\n+        in_dim = train_data[0].shape[0]\n+        out_dim = len(train_labels)\n+        return in_dim, out_dim\n+\n+    @staticmethod\n+    def one_hot_encode(labels: list[int], num_classes: int) -> np.ndarray:\n+        \"\"\"\n+        Perform one-hot encoding for the given labels.\n+\n+        Args:\n+            labels: List of integer labels.\n+            num_classes: Total number of classes for encoding.\n+\n+        Returns:\n+            A numpy array representing one-hot encoded labels.\n+        \"\"\"\n+        one_hot = np.zeros((len(labels), num_classes))\n+        for idx, label in enumerate(labels):\n+            one_hot[idx, label] = 0.99\n+        return one_hot\n+\n+\n+class MLP():\n+    \"\"\"\n+        A custom MLP class for implementing a simple multi-layer perceptron with\n+        forward propagation, backpropagation.\n+\n+        Attributes:\n+            learning_rate (float): Learning rate for gradient descent.\n+            gamma (float): Parameter to control learning rate adjustment.\n+            epoch (int): Number of epochs for training.\n+            hidden_dim (int): Dimension of the hidden layer.\n+            batch_size (int): Number of samples per mini-batch.\n+            train_loss (List[float]): List to store training loss for each fold.\n+            train_accuracy (List[float]): List to store training accuracy for each fold.\n+            test_loss (List[float]): List to store test loss for each fold.\n+            test_accuracy (List[float]): List to store test accuracy for each fold.\n+            dataloader (Dataloader): DataLoader object for handling training data.\n+            inter_variable (dict):\n+                Dictionary to store intermediate variables for backpropagation.\n+            weights1_list (List[Tuple[np.ndarray, np.ndarray]]):\n+                List of weights for each fold.\n+\n+        Methods:\n+            get_inout_dim:obtain input dimension and output dimension.\n+            relu: Apply the ReLU activation function.\n+            relu_derivative: Compute the derivative of the ReLU function.\n+            forward: Perform a forward pass through the network.\n+            back_prop: Perform backpropagation to compute gradients.\n+            update_weights: Update the weights using gradients.\n+            update_learning_rate: Adjust the learning rate based on test accuracy.\n+            accuracy: Compute accuracy of the model.\n+            loss: Compute weighted MSE loss.\n+            train: Train the MLP over multiple folds with early stopping.\n+\n+\n+        \"\"\"\n+    def __init__(self, dataloader, epoch: int, learning_rate: float, gamma=1, hidden_dim=2):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `dataloader`\n\nPlease provide type hint for the parameter: `gamma`\n\nPlease provide type hint for the parameter: `hidden_dim`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/multilayer_perceptron_classifier_from_scratch.py",
    "pr_number": 12749,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2088421166,
    "comment_created_at": "2025-05-14T08:50:09Z"
  },
  {
    "code": "@@ -0,0 +1,65 @@\n+# @Author  : ojas-wani\n+# @File    : laplacian_filter.py\n+# @Time    : 10/04/2023\n+\n+from cv2 import imread, cvtColor, COLOR_BGR2GRAY,  filter2D, imread, imshow, waitKey, CV_64F, BORDER_DEFAULT\n+import numpy as np\n+from gaussian_filter import gaussian_filter\n+\n+def my_laplacian(src, ddepth=-1, ksize=3, scale=1, delta=0, borderType='default') -> np.ndarray:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file digital_image_processing/filters/laplacian_filter.py, please provide doctest for the function my_laplacian\n\nplease provide type hint for the parameter: src\n\nplease provide type hint for the parameter: ddepth\n\nplease provide type hint for the parameter: ksize\n\nplease provide type hint for the parameter: scale\n\nplease provide type hint for the parameter: delta\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: bordertype\n\nplease provide type hint for the parameter: bordertype",
    "line_number": 9,
    "enriched": "File: digital_image_processing/filters/laplacian_filter.py\nCode: @@ -0,0 +1,65 @@\n+# @Author  : ojas-wani\n+# @File    : laplacian_filter.py\n+# @Time    : 10/04/2023\n+\n+from cv2 import imread, cvtColor, COLOR_BGR2GRAY,  filter2D, imread, imshow, waitKey, CV_64F, BORDER_DEFAULT\n+import numpy as np\n+from gaussian_filter import gaussian_filter\n+\n+def my_laplacian(src, ddepth=-1, ksize=3, scale=1, delta=0, borderType='default') -> np.ndarray:\nComment: As there is no test file in this pull request nor any test function or class in the file `digital_image_processing/filters/laplacian_filter.py`, please provide doctest for the function `my_laplacian`\n\nPlease provide type hint for the parameter: `src`\n\nPlease provide type hint for the parameter: `ddepth`\n\nPlease provide type hint for the parameter: `ksize`\n\nPlease provide type hint for the parameter: `scale`\n\nPlease provide type hint for the parameter: `delta`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `borderType`\n\nPlease provide type hint for the parameter: `borderType`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "digital_image_processing/filters/laplacian_filter.py",
    "pr_number": 9783,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346927501,
    "comment_created_at": "2023-10-05T07:27:27Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+import numpy as np\n+\n+def custom_pivoting(a, n, i):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file linear_algebra/src/gaussianeliminationpivoting.py, please provide doctest for the function custom_pivoting\n\nplease provide return type hint for the function: custom_pivoting. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: a\n\nplease provide descriptive name for the parameter: a\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: i\n\nplease provide descriptive name for the parameter: i",
    "line_number": 3,
    "enriched": "File: linear_algebra/src/gaussianeliminationpivoting.py\nCode: @@ -0,0 +1,39 @@\n+import numpy as np\n+\n+def custom_pivoting(a, n, i):\nComment: As there is no test file in this pull request nor any test function or class in the file `linear_algebra/src/gaussianeliminationpivoting.py`, please provide doctest for the function `custom_pivoting`\n\nPlease provide return type hint for the function: `custom_pivoting`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `a`\n\nPlease provide descriptive name for the parameter: `a`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide type hint for the parameter: `i`\n\nPlease provide descriptive name for the parameter: `i`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "linear_algebra/src/gaussianeliminationpivoting.py",
    "pr_number": 10457,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359494533,
    "comment_created_at": "2023-10-14T17:21:36Z"
  },
  {
    "code": "@@ -5,7 +5,7 @@\n import os\n import pathlib\n from types import ModuleType\n-\n+import datetime",
    "comment": "also you need to run pre-commit \r\n\r\n- install, before you can run hooks, you need to have the pre-commit package manager installed.\r\n-  >pip install pre-commit\r\n\r\nthen > pre-commit\r\n\r\n- how to expect?\r\n\r\nrun python pre-commit-#.#.#.pyz ... in place of pre-commit ...",
    "line_number": 8,
    "enriched": "File: scripts/validate_solutions.py\nCode: @@ -5,7 +5,7 @@\n import os\n import pathlib\n from types import ModuleType\n-\n+import datetime\nComment: Also you need to run` pre-commit `\r\n\r\n- Install, before you can run hooks, you need to have the pre-commit package manager installed.\r\n-  >`pip install pre-commit`\r\n\r\nthen > `pre-commit`\r\n\r\n- How to expect?\r\n\r\n`run python pre-commit-#.#.#.pyz ... in place of pre-commit ...`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "scripts/validate_solutions.py",
    "pr_number": 11351,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1550765825,
    "comment_created_at": "2024-04-04T02:00:15Z"
  },
  {
    "code": "@@ -0,0 +1,218 @@\n+from random import choice\r\n+from math import inf\r\n+\r\n+board = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\r\n+\r\n+\r\n+def Gameboard(board):\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/game theory/alphabetapruning/alphabetapruning.py, please provide doctest for the function gameboard\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: gameboard\n\nplease provide return type hint for the function: gameboard. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: board",
    "line_number": 7,
    "enriched": "File: maths/Game Theory/AlphaBetaPruning/alphabetapruning.py\nCode: @@ -0,0 +1,218 @@\n+from random import choice\r\n+from math import inf\r\n+\r\n+board = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\r\n+\r\n+\r\n+def Gameboard(board):\r\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/Game Theory/AlphaBetaPruning/alphabetapruning.py`, please provide doctest for the function `Gameboard`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `Gameboard`\n\nPlease provide return type hint for the function: `Gameboard`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `board`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/Game Theory/AlphaBetaPruning/alphabetapruning.py",
    "pr_number": 11678,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1784588355,
    "comment_created_at": "2024-10-02T14:01:29Z"
  },
  {
    "code": "@@ -0,0 +1,22 @@\n+def swap_case(s):",
    "comment": "please provide return type hint for the function: swap_case. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file strings/swap-case.py, please provide doctest for the function swap_case\n\nplease provide type hint for the parameter: s\n\nplease provide descriptive name for the parameter: s",
    "line_number": 1,
    "enriched": "File: strings/swap-case.py\nCode: @@ -0,0 +1,22 @@\n+def swap_case(s):\nComment: Please provide return type hint for the function: `swap_case`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `strings/swap-case.py`, please provide doctest for the function `swap_case`\n\nPlease provide type hint for the parameter: `s`\n\nPlease provide descriptive name for the parameter: `s`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "strings/swap-case.py",
    "pr_number": 11742,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1787630720,
    "comment_created_at": "2024-10-04T12:18:59Z"
  },
  {
    "code": "@@ -0,0 +1,91 @@\n+\n+\n+from itertools import product\n+\n+def findPassword(chars, function, show=50, format_=\"%s\"):",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: findpassword\n\nas there is no test file in this pull request nor any test function or class in the file password_check/password tester.py, please provide doctest for the function findpassword\n\nplease provide return type hint for the function: findpassword. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: chars\n\nplease provide type hint for the parameter: function\n\nplease provide type hint for the parameter: show\n\nplease provide type hint for the parameter: format_",
    "line_number": 5,
    "enriched": "File: Password_Check/Password tester.py\nCode: @@ -0,0 +1,91 @@\n+\n+\n+from itertools import product\n+\n+def findPassword(chars, function, show=50, format_=\"%s\"):\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `findPassword`\n\nAs there is no test file in this pull request nor any test function or class in the file `Password_Check/Password tester.py`, please provide doctest for the function `findPassword`\n\nPlease provide return type hint for the function: `findPassword`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `chars`\n\nPlease provide type hint for the parameter: `function`\n\nPlease provide type hint for the parameter: `show`\n\nPlease provide type hint for the parameter: `format_`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "Password_Check/Password tester.py",
    "pr_number": 7381,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 997335988,
    "comment_created_at": "2022-10-17T17:33:00Z"
  },
  {
    "code": "@@ -0,0 +1,46 @@\n+import numpy as np \n+import pandas as pd \t\n+\n+'''P(Y|X) = (P(X|Y)* P(Y))/P(X) #Bayes Theorem  \n+We are gonna ignore P(X), as it does not depend on Y and hence has no role in optimising P(Y|X), \n+since we are more concerned with relative values of probabilities of classes rather than actual probability of classes\n+\n+likelihood : P(X = x1,x2,x3 | Y)\n+prior : P(Y)\n+posterior : P(Y|X )'''\n+\n+def prior_prob(Y_train, target): ",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/naivebayes.py, please provide doctest for the function prior_prob\n\nplease provide return type hint for the function: prior_prob. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: y_train\n\nplease provide type hint for the parameter: y_train\n\nplease provide type hint for the parameter: target",
    "line_number": 12,
    "enriched": "File: machine_learning/naivebayes.py\nCode: @@ -0,0 +1,46 @@\n+import numpy as np \n+import pandas as pd \t\n+\n+'''P(Y|X) = (P(X|Y)* P(Y))/P(X) #Bayes Theorem  \n+We are gonna ignore P(X), as it does not depend on Y and hence has no role in optimising P(Y|X), \n+since we are more concerned with relative values of probabilities of classes rather than actual probability of classes\n+\n+likelihood : P(X = x1,x2,x3 | Y)\n+prior : P(Y)\n+posterior : P(Y|X )'''\n+\n+def prior_prob(Y_train, target): \nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/naivebayes.py`, please provide doctest for the function `prior_prob`\n\nPlease provide return type hint for the function: `prior_prob`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `Y_train`\n\nPlease provide type hint for the parameter: `Y_train`\n\nPlease provide type hint for the parameter: `target`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/naivebayes.py",
    "pr_number": 9373,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342301999,
    "comment_created_at": "2023-10-02T05:59:35Z"
  },
  {
    "code": "@@ -0,0 +1,28 @@\n+from threading import Timer\n+from time import sleep\n+\n+\"\"\" In sleep sort, the thread having\n+the least amount of sleeping time\n+wakes up first and the number gets\n+printed and hence list is sorted\"\"\"\n+\n+\n+def sleep_sort(values):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file sorts/sleep_sort.py, please provide doctest for the function sleep_sort\n\nplease provide return type hint for the function: sleep_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: values",
    "line_number": 10,
    "enriched": "File: sorts/sleep_sort.py\nCode: @@ -0,0 +1,28 @@\n+from threading import Timer\n+from time import sleep\n+\n+\"\"\" In sleep sort, the thread having\n+the least amount of sleeping time\n+wakes up first and the number gets\n+printed and hence list is sorted\"\"\"\n+\n+\n+def sleep_sort(values):\nComment: As there is no test file in this pull request nor any test function or class in the file `sorts/sleep_sort.py`, please provide doctest for the function `sleep_sort`\n\nPlease provide return type hint for the function: `sleep_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `values`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "sorts/sleep_sort.py",
    "pr_number": 7876,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008882887,
    "comment_created_at": "2022-10-30T15:25:29Z"
  },
  {
    "code": "@@ -0,0 +1,24 @@\n+# using hash set\n+\n+\n+def longestconsecutive(nums):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file hashes/longestconsecutivenumber.py, please provide doctest for the function longestconsecutive\n\nplease provide return type hint for the function: longestconsecutive. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: nums",
    "line_number": 4,
    "enriched": "File: hashes/longestconsecutivenumber.py\nCode: @@ -0,0 +1,24 @@\n+# using hash set\n+\n+\n+def longestconsecutive(nums):\nComment: As there is no test file in this pull request nor any test function or class in the file `hashes/longestconsecutivenumber.py`, please provide doctest for the function `longestconsecutive`\n\nPlease provide return type hint for the function: `longestconsecutive`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `nums`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "hashes/longestconsecutivenumber.py",
    "pr_number": 9455,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342613855,
    "comment_created_at": "2023-10-02T12:17:44Z"
  },
  {
    "code": "@@ -1,21 +1,36 @@\n+# Enables forward annotations for type hinting",
    "comment": "we do not want to slow readers down too much.",
    "line_number": 1,
    "enriched": "File: data_structures/linked_list/circular_linked_list.py\nCode: @@ -1,21 +1,36 @@\n+# Enables forward annotations for type hinting\nComment: We do not want to slow readers down too much.\r\n```suggestion\r\n```",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "data_structures/linked_list/circular_linked_list.py",
    "pr_number": 9668,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1345270029,
    "comment_created_at": "2023-10-04T06:38:28Z"
  },
  {
    "code": "@@ -0,0 +1,23 @@\n+def isprime(n):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file strings/lcm_of_array.py, please provide doctest for the function isprime\n\nplease provide return type hint for the function: isprime. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n",
    "line_number": 1,
    "enriched": "File: strings/lcm_of_array.py\nCode: @@ -0,0 +1,23 @@\n+def isprime(n):\nComment: As there is no test file in this pull request nor any test function or class in the file `strings/lcm_of_array.py`, please provide doctest for the function `isprime`\n\nPlease provide return type hint for the function: `isprime`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "strings/lcm_of_array.py",
    "pr_number": 7655,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1005211695,
    "comment_created_at": "2022-10-26T04:31:57Z"
  },
  {
    "code": "@@ -0,0 +1,98 @@\n+\"\"\"Batch convert videos to greyscale .\n+Install dependencies:\n+  pip install pillow docopt\n+Note: If you do not provide an output path, the generated files will be saved\n+in a folder named \"Converted\"\n+Usage:\n+  greyscale.py <in_path> [<out_path>]\n+  greyscale.py -h | --help\n+  greyscale.py --version\n+Arguments:\n+  <in_path>   Input directory\n+  <out_path>  Output directory [default: ./Converted]\n+Options:\n+  -h, --help  Show this help screen.\n+  --version     Show version.\n+\"\"\"\n+\n+import docopt\n+import cv2 as cv\n+import os\n+import time\n+import matplotlib.pyplot as plt\n+import glob\n+import sys\n+\n+\n+def process_video(video_inpath, video_outpath):",
    "comment": "please provide return type hint for the function: process_video. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file digital_image_processing/batchconvert_rgb_videos_togrey.py, please provide doctest for the function process_video\n\nplease provide type hint for the parameter: video_inpath\n\nplease provide type hint for the parameter: video_outpath",
    "line_number": 27,
    "enriched": "File: digital_image_processing/BatchConvert_rgb_videos_toGrey.py\nCode: @@ -0,0 +1,98 @@\n+\"\"\"Batch convert videos to greyscale .\n+Install dependencies:\n+  pip install pillow docopt\n+Note: If you do not provide an output path, the generated files will be saved\n+in a folder named \"Converted\"\n+Usage:\n+  greyscale.py <in_path> [<out_path>]\n+  greyscale.py -h | --help\n+  greyscale.py --version\n+Arguments:\n+  <in_path>   Input directory\n+  <out_path>  Output directory [default: ./Converted]\n+Options:\n+  -h, --help  Show this help screen.\n+  --version     Show version.\n+\"\"\"\n+\n+import docopt\n+import cv2 as cv\n+import os\n+import time\n+import matplotlib.pyplot as plt\n+import glob\n+import sys\n+\n+\n+def process_video(video_inpath, video_outpath):\nComment: Please provide return type hint for the function: `process_video`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `digital_image_processing/BatchConvert_rgb_videos_toGrey.py`, please provide doctest for the function `process_video`\n\nPlease provide type hint for the parameter: `video_inpath`\n\nPlease provide type hint for the parameter: `video_outpath`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "digital_image_processing/BatchConvert_rgb_videos_toGrey.py",
    "pr_number": 7275,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996387305,
    "comment_created_at": "2022-10-16T04:37:01Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+# https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm\n+\n+def knuth_morris_pratt(text, pattern):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/knuth-morris-prat-algorithm.py, please provide doctest for the function knuth_morris_pratt\n\nplease provide return type hint for the function: knuth_morris_pratt. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: text\n\nplease provide type hint for the parameter: pattern",
    "line_number": 3,
    "enriched": "File: searches/knuth-morris-prat-algorithm.py\nCode: @@ -0,0 +1,48 @@\n+# https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm\n+\n+def knuth_morris_pratt(text, pattern):\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/knuth-morris-prat-algorithm.py`, please provide doctest for the function `knuth_morris_pratt`\n\nPlease provide return type hint for the function: `knuth_morris_pratt`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `text`\n\nPlease provide type hint for the parameter: `pattern`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "searches/knuth-morris-prat-algorithm.py",
    "pr_number": 9349,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342262962,
    "comment_created_at": "2023-10-02T03:56:14Z"
  },
  {
    "code": "@@ -0,0 +1,46 @@\n+\"\"\"\n+You're a caring parent, and you want to give your children cookies. Each child has a specific cookie size they like (their \"greed factor\"). \n+You have different-sized cookies. Your goal is to give cookies to as many children as possible by matching the cookie size to \n+each child's greed factor. How many children can you make happy?\n+\n+Example: g = [1,2,3], s = [1,1], where g is the greed factor and s is the cookie size.\n+Answer: 1\n+Explanation: You have 3 children and 2 cookies. The child with greed factor 1 will be content with the cookie of size 1.\n+\n+\n+This problem can be solved using the concept of \"GREEDY ALGORITHM.\"\n+\n+As a caring parent, when distributing cookies to your children, prioritize efficiency by following this approach:\n+\n+1) Sort the children's greed factors (g) and cookie sizes (s).\n+\n+2) Begin with the smallest cookie and allocate it to the child with the lowest greed factor.\n+\n+3) Continue this process, moving to the next child as soon as their greed is satisfied with a cookie.\n+\n+4) Keep track of how many children you satisfy and return that count.\n+\n+This method ensures that you maximize the number of happy children efficiently.\"\n+\"\"\"\n+from typing import List\n+\n+def max_happy_children(g: List[int], s: List[int]) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file greedy_methods/assign_cookies.py, please provide doctest for the function max_happy_children\n\nplease provide descriptive name for the parameter: g\n\nplease provide descriptive name for the parameter: s",
    "line_number": 27,
    "enriched": "File: greedy_methods/assign_cookies.py\nCode: @@ -0,0 +1,46 @@\n+\"\"\"\n+You're a caring parent, and you want to give your children cookies. Each child has a specific cookie size they like (their \"greed factor\"). \n+You have different-sized cookies. Your goal is to give cookies to as many children as possible by matching the cookie size to \n+each child's greed factor. How many children can you make happy?\n+\n+Example: g = [1,2,3], s = [1,1], where g is the greed factor and s is the cookie size.\n+Answer: 1\n+Explanation: You have 3 children and 2 cookies. The child with greed factor 1 will be content with the cookie of size 1.\n+\n+\n+This problem can be solved using the concept of \"GREEDY ALGORITHM.\"\n+\n+As a caring parent, when distributing cookies to your children, prioritize efficiency by following this approach:\n+\n+1) Sort the children's greed factors (g) and cookie sizes (s).\n+\n+2) Begin with the smallest cookie and allocate it to the child with the lowest greed factor.\n+\n+3) Continue this process, moving to the next child as soon as their greed is satisfied with a cookie.\n+\n+4) Keep track of how many children you satisfy and return that count.\n+\n+This method ensures that you maximize the number of happy children efficiently.\"\n+\"\"\"\n+from typing import List\n+\n+def max_happy_children(g: List[int], s: List[int]) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `greedy_methods/assign_cookies.py`, please provide doctest for the function `max_happy_children`\n\nPlease provide descriptive name for the parameter: `g`\n\nPlease provide descriptive name for the parameter: `s`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "greedy_methods/assign_cookies.py",
    "pr_number": 10216,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1352258220,
    "comment_created_at": "2023-10-10T11:03:38Z"
  },
  {
    "code": "@@ -0,0 +1,3 @@\n+# This Algorithm calculates the cube root of a given no.\n+def cube_root(x):",
    "comment": "please provide return type hint for the function: cube_root. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file maths/cuberoot.py, please provide doctest for the function cube_root\n\nplease provide type hint for the parameter: x\n\nplease provide descriptive name for the parameter: x",
    "line_number": 2,
    "enriched": "File: maths/CubeRoot.py\nCode: @@ -0,0 +1,3 @@\n+# This Algorithm calculates the cube root of a given no.\n+def cube_root(x):\nComment: Please provide return type hint for the function: `cube_root`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `maths/CubeRoot.py`, please provide doctest for the function `cube_root`\n\nPlease provide type hint for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/CubeRoot.py",
    "pr_number": 7917,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1009565808,
    "comment_created_at": "2022-10-31T15:37:48Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+\"\"\"\n+Given 'n' Pairs of parenthesis,\n+this program generates all combinations of parenthesis \n+Example, n=3 :\n+[\n+    \"((()))\",\n+    \"(()())\",\n+    \"(())()\",\n+    \"()(())\",\n+    \"()()()\"\n+]\n+This problem is solved using Backtracking \n+\"\"\"\n+def solve(op,open,close, ans):",
    "comment": "please provide return type hint for the function: solve. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file backtracking/generate_parenthesis.py, please provide doctest for the function solve\n\nplease provide type hint for the parameter: op\n\nplease provide type hint for the parameter: open\n\nplease provide type hint for the parameter: close\n\nplease provide type hint for the parameter: ans",
    "line_number": 14,
    "enriched": "File: backtracking/generate_parenthesis.py\nCode: @@ -0,0 +1,48 @@\n+\"\"\"\n+Given 'n' Pairs of parenthesis,\n+this program generates all combinations of parenthesis \n+Example, n=3 :\n+[\n+    \"((()))\",\n+    \"(()())\",\n+    \"(())()\",\n+    \"()(())\",\n+    \"()()()\"\n+]\n+This problem is solved using Backtracking \n+\"\"\"\n+def solve(op,open,close, ans):\nComment: Please provide return type hint for the function: `solve`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `backtracking/generate_parenthesis.py`, please provide doctest for the function `solve`\n\nPlease provide type hint for the parameter: `op`\n\nPlease provide type hint for the parameter: `open`\n\nPlease provide type hint for the parameter: `close`\n\nPlease provide type hint for the parameter: `ans`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "backtracking/generate_parenthesis.py",
    "pr_number": 10068,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349620514,
    "comment_created_at": "2023-10-08T04:26:23Z"
  },
  {
    "code": "@@ -0,0 +1,31 @@\n+import socket\r\n+\r\n+def start_ftp_server(host, port):\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file socket_programming/file_transfer/receiver.py, please provide doctest for the function start_ftp_server\n\nplease provide return type hint for the function: start_ftp_server. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: host\n\nplease provide type hint for the parameter: port",
    "line_number": 3,
    "enriched": "File: socket_programming/file_transfer/receiver.py\nCode: @@ -0,0 +1,31 @@\n+import socket\r\n+\r\n+def start_ftp_server(host, port):\r\nComment: As there is no test file in this pull request nor any test function or class in the file `socket_programming/file_transfer/receiver.py`, please provide doctest for the function `start_ftp_server`\n\nPlease provide return type hint for the function: `start_ftp_server`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `host`\n\nPlease provide type hint for the parameter: `port`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "socket_programming/file_transfer/receiver.py",
    "pr_number": 11602,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1782209590,
    "comment_created_at": "2024-10-01T06:52:48Z"
  },
  {
    "code": "@@ -0,0 +1,50 @@\n+\"\"\"\n+Given 'n' pairs of parentheses,\n+this program generates all combinations of parentheses.\n+Example, n = 3 :\n+[\n+   \"((()))\",\n+   \"(()())\",\n+   \"(())()\",\n+   \"()(())\",\n+   \"()()()\"\n+]\n+This problem can be solved using the concept of \"BACKTRACKING\".\n+By adding an open parenthesis to the solution and\n+recursively add more till we get 'n' open parentheses.\n+Then we start adding close parentheses until the solution is valid\n+(open parenthesis is closed).\n+If we reach a point where we can not add more parentheses to the solution,\n+we backtrack to the previous step and try a different path.\n+\"\"\"\n+\n+def generate_parentheses(number : int = 0) -> list:\n+    \"\"\"\n+    >>> generate_parentheses(3)\n+    ['((()))', '(()())', '(())()', '()(())', '()()()']\n+\n+    >>> generate_parentheses(1)\n+    ['()']\n+\n+    >>> generate_parentheses(0)\n+    ['']\n+    \"\"\"\n+    def backtrack(x : str = \"\",left : int = 0,right : int = 0) -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file backtracking/generate_parentheses.py, please provide doctest for the function backtrack\n\nplease provide descriptive name for the parameter: x",
    "line_number": 32,
    "enriched": "File: backtracking/generate_parentheses.py\nCode: @@ -0,0 +1,50 @@\n+\"\"\"\n+Given 'n' pairs of parentheses,\n+this program generates all combinations of parentheses.\n+Example, n = 3 :\n+[\n+   \"((()))\",\n+   \"(()())\",\n+   \"(())()\",\n+   \"()(())\",\n+   \"()()()\"\n+]\n+This problem can be solved using the concept of \"BACKTRACKING\".\n+By adding an open parenthesis to the solution and\n+recursively add more till we get 'n' open parentheses.\n+Then we start adding close parentheses until the solution is valid\n+(open parenthesis is closed).\n+If we reach a point where we can not add more parentheses to the solution,\n+we backtrack to the previous step and try a different path.\n+\"\"\"\n+\n+def generate_parentheses(number : int = 0) -> list:\n+    \"\"\"\n+    >>> generate_parentheses(3)\n+    ['((()))', '(()())', '(())()', '()(())', '()()()']\n+\n+    >>> generate_parentheses(1)\n+    ['()']\n+\n+    >>> generate_parentheses(0)\n+    ['']\n+    \"\"\"\n+    def backtrack(x : str = \"\",left : int = 0,right : int = 0) -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `backtracking/generate_parentheses.py`, please provide doctest for the function `backtrack`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "backtracking/generate_parentheses.py",
    "pr_number": 9937,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349142290,
    "comment_created_at": "2023-10-06T17:41:48Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+from projectq.ops import H, Measure\n+from projectq import MainEngine\n+\n+def get_random_number(quantum_engine):",
    "comment": "please provide return type hint for the function: get_random_number. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file quantum/quantum_random.py, please provide doctest for the function get_random_number\n\nplease provide type hint for the parameter: quantum_engine",
    "line_number": 4,
    "enriched": "File: quantum/quantum_random.py\nCode: @@ -0,0 +1,21 @@\n+from projectq.ops import H, Measure\n+from projectq import MainEngine\n+\n+def get_random_number(quantum_engine):\nComment: Please provide return type hint for the function: `get_random_number`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `quantum/quantum_random.py`, please provide doctest for the function `get_random_number`\n\nPlease provide type hint for the parameter: `quantum_engine`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "quantum/quantum_random.py",
    "pr_number": 7446,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000173520,
    "comment_created_at": "2022-10-20T05:59:24Z"
  },
  {
    "code": "@@ -0,0 +1,34 @@\n+from sys import setrecursionlimit\n+\n+# Set the recursion limit to avoid reaching the maximum recursion depth\n+setrecursionlimit(10000)\n+\n+def hofstadter(n):",
    "comment": "please provide return type hint for the function: hofstadter. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file maths/hofstadter's q-sequence.py, please provide doctest for the function hofstadter\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n",
    "line_number": 6,
    "enriched": "File: maths/Hofstadter's Q-Sequence.py\nCode: @@ -0,0 +1,34 @@\n+from sys import setrecursionlimit\n+\n+# Set the recursion limit to avoid reaching the maximum recursion depth\n+setrecursionlimit(10000)\n+\n+def hofstadter(n):\nComment: Please provide return type hint for the function: `hofstadter`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `maths/Hofstadter's Q-Sequence.py`, please provide doctest for the function `hofstadter`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/Hofstadter's Q-Sequence.py",
    "pr_number": 9745,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346308838,
    "comment_created_at": "2023-10-04T18:41:12Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+def gcd(numerator: int, denominator: int) -> int:",
    "comment": "we already have tons of implementations of greatest_common_demonintor() so please just use https://docs.python.org/3/library/math.html#math.gcd instead.",
    "line_number": 1,
    "enriched": "File: maths/numerical_analysis/proper_fractions.py\nCode: @@ -0,0 +1,42 @@\n+def gcd(numerator: int, denominator: int) -> int:\nComment: We already have tons of implementations of `greatest_common_demonintor()` so please just use https://docs.python.org/3/library/math.html#math.gcd instead.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "maths/numerical_analysis/proper_fractions.py",
    "pr_number": 11224,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1526596481,
    "comment_created_at": "2024-03-15T17:12:31Z"
  },
  {
    "code": "@@ -0,0 +1,36 @@\n+from sys import setrecursionlimit\n+\n+# Set the recursion limit to avoid reaching the maximum recursion depth\n+setrecursionlimit(10000)\n+\n+\n+def hofstadter(n) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/hofstadter's_q_sequence.py, please provide doctest for the function hofstadter\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n",
    "line_number": 7,
    "enriched": "File: maths/hofstadter's_q_sequence.py\nCode: @@ -0,0 +1,36 @@\n+from sys import setrecursionlimit\n+\n+# Set the recursion limit to avoid reaching the maximum recursion depth\n+setrecursionlimit(10000)\n+\n+\n+def hofstadter(n) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/hofstadter's_q_sequence.py`, please provide doctest for the function `hofstadter`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/hofstadter's_q_sequence.py",
    "pr_number": 9746,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346315689,
    "comment_created_at": "2023-10-04T18:47:39Z"
  },
  {
    "code": "@@ -0,0 +1,23 @@\n+def merge_sorted_arrays(arr1: list[int], arr2: list[int]) -> list[int]:\n+    \"\"\"",
    "comment": "add description for this algo/code so that viewer can understand.",
    "line_number": 2,
    "enriched": "File: data_structures/arrays/mergesortedarrays.py\nCode: @@ -0,0 +1,23 @@\n+def merge_sorted_arrays(arr1: list[int], arr2: list[int]) -> list[int]:\n+    \"\"\"\nComment: add description for this algo/code so that viewer can understand.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "data_structures/arrays/mergesortedarrays.py",
    "pr_number": 8692,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1177998401,
    "comment_created_at": "2023-04-26T14:50:48Z"
  },
  {
    "code": "@@ -0,0 +1,62 @@\n+# https://www.geeksforgeeks.org/convert-ip-address-to-integer-and-vice-versa/\n+\n+\n+def ip_to_decimal(ip_address: str) -> int:\n+    \"\"\"\n+    Convert an IPv4 address to its decimal representation.\n+\n+    Args:\n+        ip_address (str): A string representing an IPv4 address (e.g., \"192.168.0.1\").\n+\n+    Returns:\n+        int: The decimal representation of the IP address.\n+\n+    >>> ip_to_decimal(\"192.168.0.1\")\n+    3232235521\n+    >>> ip_to_decimal(\"10.0.0.255\")\n+    167772415\n+    \"\"\"\n+\n+    ip_parts = ip_address.split(\".\")",
    "comment": "see: https://github.com/thealgorithms/python/blob/master/maths/is_ip_v4_address_valid.py",
    "line_number": 20,
    "enriched": "File: conversions/ipconversion.py\nCode: @@ -0,0 +1,62 @@\n+# https://www.geeksforgeeks.org/convert-ip-address-to-integer-and-vice-versa/\n+\n+\n+def ip_to_decimal(ip_address: str) -> int:\n+    \"\"\"\n+    Convert an IPv4 address to its decimal representation.\n+\n+    Args:\n+        ip_address (str): A string representing an IPv4 address (e.g., \"192.168.0.1\").\n+\n+    Returns:\n+        int: The decimal representation of the IP address.\n+\n+    >>> ip_to_decimal(\"192.168.0.1\")\n+    3232235521\n+    >>> ip_to_decimal(\"10.0.0.255\")\n+    167772415\n+    \"\"\"\n+\n+    ip_parts = ip_address.split(\".\")\nComment: See: https://github.com/TheAlgorithms/Python/blob/master/maths/is_ip_v4_address_valid.py\r\n```suggestion\r\n    octets = ip_address.split(\".\")\r\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "conversions/ipconversion.py",
    "pr_number": 11008,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1373828104,
    "comment_created_at": "2023-10-26T21:20:18Z"
  },
  {
    "code": "@@ -0,0 +1,45 @@\n+# Find Transition Point\r\n+# Given a sorted array containing only 0s and 1s, find the transition point. \r\n+# Example\r\n+# N = 5\r\n+# arr[] = {0,0,0,1,1}\r\n+# Output: 3\r\n+# Explanation: index 3 is the transition point where 1 begins.\r\n+\r\n+# If in case no trasistion point is found, then we return -1\r\n+\r\n+# Below are Three different Implementations.\r\n+\r\n+\r\n+def transitionPoint(arr, n):\r",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: transitionpoint\n\nplease provide return type hint for the function: transitionpoint. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file searches/find_transition_point.py, please provide doctest for the function transitionpoint\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n",
    "line_number": 14,
    "enriched": "File: searches/find_transition_point.py\nCode: @@ -0,0 +1,45 @@\n+# Find Transition Point\r\n+# Given a sorted array containing only 0s and 1s, find the transition point. \r\n+# Example\r\n+# N = 5\r\n+# arr[] = {0,0,0,1,1}\r\n+# Output: 3\r\n+# Explanation: index 3 is the transition point where 1 begins.\r\n+\r\n+# If in case no trasistion point is found, then we return -1\r\n+\r\n+# Below are Three different Implementations.\r\n+\r\n+\r\n+def transitionPoint(arr, n):\r\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `transitionPoint`\n\nPlease provide return type hint for the function: `transitionPoint`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `searches/find_transition_point.py`, please provide doctest for the function `transitionPoint`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "searches/find_transition_point.py",
    "pr_number": 7602,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1003420813,
    "comment_created_at": "2022-10-24T14:57:38Z"
  },
  {
    "code": "@@ -0,0 +1,17 @@\n+def search(arr, N, x):",
    "comment": "please provide return type hint for the function: search. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file searches/linearsearch.py, please provide doctest for the function search\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: n\n\nplease provide descriptive name for the parameter: n\n\nplease provide type hint for the parameter: x\n\nplease provide descriptive name for the parameter: x",
    "line_number": 1,
    "enriched": "File: searches/linearsearch.py\nCode: @@ -0,0 +1,17 @@\n+def search(arr, N, x):\nComment: Please provide return type hint for the function: `search`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `searches/linearsearch.py`, please provide doctest for the function `search`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `N`\n\nPlease provide descriptive name for the parameter: `N`\n\nPlease provide type hint for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "searches/linearsearch.py",
    "pr_number": 7640,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004472350,
    "comment_created_at": "2022-10-25T13:13:59Z"
  },
  {
    "code": "@@ -0,0 +1,22 @@\n+from time import sleep\n+from threading import Timer\n+\n+def sleep_sort(l):",
    "comment": "please provide return type hint for the function: sleep_sort. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file sleepsort.py, please provide doctest for the function sleep_sort\n\nplease provide type hint for the parameter: l\n\nplease provide descriptive name for the parameter: l",
    "line_number": 4,
    "enriched": "File: SleepSort.py\nCode: @@ -0,0 +1,22 @@\n+from time import sleep\n+from threading import Timer\n+\n+def sleep_sort(l):\nComment: Please provide return type hint for the function: `sleep_sort`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `SleepSort.py`, please provide doctest for the function `sleep_sort`\n\nPlease provide type hint for the parameter: `l`\n\nPlease provide descriptive name for the parameter: `l`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "SleepSort.py",
    "pr_number": 6988,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 992134379,
    "comment_created_at": "2022-10-11T10:20:14Z"
  },
  {
    "code": "@@ -0,0 +1,51 @@\n+def F(A: int, B: int) -> int:",
    "comment": "please provide descriptive name for the function: f\n\nplease provide descriptive name for the parameter: a\n\nplease provide descriptive name for the parameter: b",
    "line_number": 1,
    "enriched": "File: boolean_algebra/karnaugh_map_simplification.py\nCode: @@ -0,0 +1,51 @@\n+def F(A: int, B: int) -> int:\nComment: Please provide descriptive name for the function: `F`\n\nPlease provide descriptive name for the parameter: `A`\n\nPlease provide descriptive name for the parameter: `B`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "boolean_algebra/karnaugh_map_simplification.py",
    "pr_number": 11056,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375258950,
    "comment_created_at": "2023-10-28T14:02:06Z"
  },
  {
    "code": "@@ -0,0 +1,76 @@\n+import numpy as np\n+from matplotlib import pyplot as plt\n+from sklearn.datasets import load_iris\n+from sklearn.ensemble import AdaBoostClassifier\n+from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n+from sklearn.model_selection import train_test_split\n+import doctest\n+\n+\"\"\"\n+AdaBoost, short for Adaptive Boosting, is a powerful ensemble learning technique in machine learning.\n+It operates by combining multiple weak learners, often decision trees, in an iterative manner.\n+AdaBoost assigns varying weights to data points, prioritizing misclassified samples with each iteration.\n+\"\"\"\n+\n+def data_handling(data: dict) -> tuple:\n+    \"\"\"\n+    Split dataset into features and target.\n+\n+    >>> data_handling({'data':'[5.1, 3.5, 1.4, 0.2]','target':([0])})\n+    ('[5.1, 3.5, 1.4, 0.2]', [0])\n+    >>> data_handling({'data': '[4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2]', 'target': ([0, 0])})\n+    ('[4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2]', [0, 0])\n+    \"\"\"\n+    return (data[\"data\"], data[\"target\"])\n+\n+def adaboost(features: np.ndarray, target: np.ndarray) -> AdaBoostClassifier:\n+    \"\"\"\n+    Initialize and train an AdaBoost classifier.\n+\n+    >>> adaboost(np.array([[5.1, 3.6, 1.4, 0.2]]), np.array([0]))\n+    AdaBoostClassifier(...)\n+    \"\"\"\n+    classifier = AdaBoostClassifier()\n+    classifier.fit(features, target)\n+    return classifier\n+\n+def main():",
    "comment": "please provide return type hint for the function: main. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file machine_learning/adaboost_classifier.py, please provide doctest for the function main",
    "line_number": 37,
    "enriched": "File: machine_learning/adaboost_classifier.py\nCode: @@ -0,0 +1,76 @@\n+import numpy as np\n+from matplotlib import pyplot as plt\n+from sklearn.datasets import load_iris\n+from sklearn.ensemble import AdaBoostClassifier\n+from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n+from sklearn.model_selection import train_test_split\n+import doctest\n+\n+\"\"\"\n+AdaBoost, short for Adaptive Boosting, is a powerful ensemble learning technique in machine learning.\n+It operates by combining multiple weak learners, often decision trees, in an iterative manner.\n+AdaBoost assigns varying weights to data points, prioritizing misclassified samples with each iteration.\n+\"\"\"\n+\n+def data_handling(data: dict) -> tuple:\n+    \"\"\"\n+    Split dataset into features and target.\n+\n+    >>> data_handling({'data':'[5.1, 3.5, 1.4, 0.2]','target':([0])})\n+    ('[5.1, 3.5, 1.4, 0.2]', [0])\n+    >>> data_handling({'data': '[4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2]', 'target': ([0, 0])})\n+    ('[4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2]', [0, 0])\n+    \"\"\"\n+    return (data[\"data\"], data[\"target\"])\n+\n+def adaboost(features: np.ndarray, target: np.ndarray) -> AdaBoostClassifier:\n+    \"\"\"\n+    Initialize and train an AdaBoost classifier.\n+\n+    >>> adaboost(np.array([[5.1, 3.6, 1.4, 0.2]]), np.array([0]))\n+    AdaBoostClassifier(...)\n+    \"\"\"\n+    classifier = AdaBoostClassifier()\n+    classifier.fit(features, target)\n+    return classifier\n+\n+def main():\nComment: Please provide return type hint for the function: `main`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `machine_learning/adaboost_classifier.py`, please provide doctest for the function `main`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/adaboost_classifier.py",
    "pr_number": 10502,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359837721,
    "comment_created_at": "2023-10-15T09:03:49Z"
  },
  {
    "code": "@@ -122,6 +122,7 @@ def decrypt_caesar_with_chi_squared(\n \n     >>> decrypt_caesar_with_chi_squared(12)\n     Traceback (most recent call last):\n+    ...",
    "comment": "please indent four spaces like the doctest docs do.",
    "line_number": 125,
    "enriched": "File: ciphers/decrypt_caesar_with_chi_squared.py\nCode: @@ -122,6 +122,7 @@ def decrypt_caesar_with_chi_squared(\n \n     >>> decrypt_caesar_with_chi_squared(12)\n     Traceback (most recent call last):\n+    ...\nComment: Please indent four spaces like the doctest docs do.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "ciphers/decrypt_caesar_with_chi_squared.py",
    "pr_number": 7558,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002719462,
    "comment_created_at": "2022-10-23T14:27:30Z"
  },
  {
    "code": "@@ -0,0 +1,25 @@\n+'''Given an integer array nums, find the subarray with the largest sum, and return its sum.\r\n+Example 1:\r\n+Input: nums = [-2,1,-3,4,-1,2,1,-5,4]\r\n+Output: 6\r\n+Explanation: The subarray [4,-1,2,1] has the largest sum 6.\r\n+Example 2:\r\n+Input: nums = [1]\r\n+Output: 1\r\n+Explanation: The subarray [1] has the largest sum 1.\r\n+Example 3:\r\n+Input: nums = [5,4,-1,7,8]\r\n+Output: 23\r\n+Explanation: The subarray [5,4,-1,7,8] has the largest sum 23.\r\n+'''\r\n+def Max_SubArray(self, nums: List[int]) -> int:\r",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: max_subarray\n\nas there is no test file in this pull request nor any test function or class in the file data_structures/arrays/kadanes_algorithm.py, please provide doctest for the function max_subarray",
    "line_number": 15,
    "enriched": "File: data_structures/arrays/kadanes_algorithm.py\nCode: @@ -0,0 +1,25 @@\n+'''Given an integer array nums, find the subarray with the largest sum, and return its sum.\r\n+Example 1:\r\n+Input: nums = [-2,1,-3,4,-1,2,1,-5,4]\r\n+Output: 6\r\n+Explanation: The subarray [4,-1,2,1] has the largest sum 6.\r\n+Example 2:\r\n+Input: nums = [1]\r\n+Output: 1\r\n+Explanation: The subarray [1] has the largest sum 1.\r\n+Example 3:\r\n+Input: nums = [5,4,-1,7,8]\r\n+Output: 23\r\n+Explanation: The subarray [5,4,-1,7,8] has the largest sum 23.\r\n+'''\r\n+def Max_SubArray(self, nums: List[int]) -> int:\r\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `Max_SubArray`\n\nAs there is no test file in this pull request nor any test function or class in the file `data_structures/arrays/kadanes_algorithm.py`, please provide doctest for the function `Max_SubArray`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/arrays/kadanes_algorithm.py",
    "pr_number": 9354,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342281728,
    "comment_created_at": "2023-10-02T04:54:00Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+\"\"\"\n+Given a string s, partition s such that every substring of the\n+partition is a palindrome.\n+Find the minimum cuts needed for a palindrome partitioning of s.\n+\n+Time Complexity: O(n^2)\n+Space Complexity: O(n^2)\n+For other explanations refer to: https://www.youtube.com/watch?v=_H8V5hJUGd0\n+\"\"\"\n+\n+\n+def find_minimum_partitions(s):",
    "comment": "please provide return type hint for the function: find_minimum_partitions. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: s\n\nplease provide descriptive name for the parameter: s",
    "line_number": 12,
    "enriched": "File: dynamic_programming/palindrome_partitioning.py\nCode: @@ -0,0 +1,39 @@\n+\"\"\"\n+Given a string s, partition s such that every substring of the\n+partition is a palindrome.\n+Find the minimum cuts needed for a palindrome partitioning of s.\n+\n+Time Complexity: O(n^2)\n+Space Complexity: O(n^2)\n+For other explanations refer to: https://www.youtube.com/watch?v=_H8V5hJUGd0\n+\"\"\"\n+\n+\n+def find_minimum_partitions(s):\nComment: Please provide return type hint for the function: `find_minimum_partitions`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `s`\n\nPlease provide descriptive name for the parameter: `s`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/palindrome_partitioning.py",
    "pr_number": 7222,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996288412,
    "comment_created_at": "2022-10-15T10:43:54Z"
  },
  {
    "code": "@@ -0,0 +1,66 @@\n+\"\"\"\n+This is pure Python implementation of exponential search.\n+\n+Resources used:\n+https://en.wikipedia.org/wiki/Exponential_search\n+\n+For doctests run following command:\n+python3 -m doctest -v expontial_search.py\n+\n+For manual testing run:\n+python3 expontial_search.py\n+\"\"\"\n+\n+def binarySearch( arr, l, r, x):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/exponential_search.py, please provide doctest for the function binarysearch\n\nplease provide return type hint for the function: binarysearch. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: binarysearch\n\nplease provide type hint for the parameter: arr\n\nplease provide type hint for the parameter: l\n\nplease provide descriptive name for the parameter: l\n\nplease provide type hint for the parameter: r\n\nplease provide descriptive name for the parameter: r\n\nplease provide type hint for the parameter: x\n\nplease provide descriptive name for the parameter: x",
    "line_number": 14,
    "enriched": "File: searches/exponential_search.py\nCode: @@ -0,0 +1,66 @@\n+\"\"\"\n+This is pure Python implementation of exponential search.\n+\n+Resources used:\n+https://en.wikipedia.org/wiki/Exponential_search\n+\n+For doctests run following command:\n+python3 -m doctest -v expontial_search.py\n+\n+For manual testing run:\n+python3 expontial_search.py\n+\"\"\"\n+\n+def binarySearch( arr, l, r, x):\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/exponential_search.py`, please provide doctest for the function `binarySearch`\n\nPlease provide return type hint for the function: `binarySearch`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `binarySearch`\n\nPlease provide type hint for the parameter: `arr`\n\nPlease provide type hint for the parameter: `l`\n\nPlease provide descriptive name for the parameter: `l`\n\nPlease provide type hint for the parameter: `r`\n\nPlease provide descriptive name for the parameter: `r`\n\nPlease provide type hint for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "searches/exponential_search.py",
    "pr_number": 10759,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367723582,
    "comment_created_at": "2023-10-21T12:44:52Z"
  },
  {
    "code": "@@ -0,0 +1,67 @@\n+\"\"\"\n+    Silhouette Score Calculation : https://en.wikipedia.org/wiki/Silhouette_(clustering)\n+\n+    The Silhouette Score is a metric used to evaluate the quality of clusters in a clustering algorithm.\n+    It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n+    A higher silhouette score indicates better clustering quality.\n+\n+    In this algorithm:\n+    - Pairwise distances between data points are computed using Euclidean distance.\n+    - For each data point:\n+      1. 'a' is calculated as the average distance to other data points in the same cluster.\n+      2. 'b' is calculated as the minimum average distance to data points in different clusters.\n+      3. The silhouette score is determined as (b - a) / max(a, b) and ranges from -1 to +1.\n+         (0 indicates overlapping clusters, negative values suggest incorrect clustering, and positive values indicate good clustering)\n+\"\"\"\n+\n+\n+\n+import numpy as np\n+\n+def pairwise_distances(X):",
    "comment": "please provide return type hint for the function: pairwise_distances. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nas there is no test file in this pull request nor any test function or class in the file machine_learning/silhouette_score.py, please provide doctest for the function pairwise_distances\n\nplease provide type hint for the parameter: x\n\nplease provide descriptive name for the parameter: x",
    "line_number": 21,
    "enriched": "File: machine_learning/silhouette_score.py\nCode: @@ -0,0 +1,67 @@\n+\"\"\"\n+    Silhouette Score Calculation : https://en.wikipedia.org/wiki/Silhouette_(clustering)\n+\n+    The Silhouette Score is a metric used to evaluate the quality of clusters in a clustering algorithm.\n+    It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n+    A higher silhouette score indicates better clustering quality.\n+\n+    In this algorithm:\n+    - Pairwise distances between data points are computed using Euclidean distance.\n+    - For each data point:\n+      1. 'a' is calculated as the average distance to other data points in the same cluster.\n+      2. 'b' is calculated as the minimum average distance to data points in different clusters.\n+      3. The silhouette score is determined as (b - a) / max(a, b) and ranges from -1 to +1.\n+         (0 indicates overlapping clusters, negative values suggest incorrect clustering, and positive values indicate good clustering)\n+\"\"\"\n+\n+\n+\n+import numpy as np\n+\n+def pairwise_distances(X):\nComment: Please provide return type hint for the function: `pairwise_distances`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nAs there is no test file in this pull request nor any test function or class in the file `machine_learning/silhouette_score.py`, please provide doctest for the function `pairwise_distances`\n\nPlease provide type hint for the parameter: `X`\n\nPlease provide descriptive name for the parameter: `X`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/silhouette_score.py",
    "pr_number": 9194,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342084297,
    "comment_created_at": "2023-10-01T06:29:28Z"
  },
  {
    "code": "@@ -0,0 +1,79 @@\n+'''\n+This program finds the approximate integration value/area under the curve of\n+a specified function within specified limits using Monte Carlo integration method.\n+\n+Further, a graph of the individal areas under the curve considered for the calculation\n+is also plotted. (PLOT SECTION -> Optional implementation)\n+'''\n+\n+# importing the modules\n+from scipy import random\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+# limits of integration (specify limits)\n+# example limits\n+a = 0\n+b = np.pi # gets the value of pi\n+\n+N = 1000 # Number of individual ares to be considered\n+\n+# function to calculate the sin of a particular value of x\n+# define your function\n+def f(x):",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/monte_carlo_integration_univariate.py, please provide doctest for the function f\n\nplease provide return type hint for the function: f. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide descriptive name for the function: f\n\nplease provide type hint for the parameter: x\n\nplease provide descriptive name for the parameter: x",
    "line_number": 23,
    "enriched": "File: maths/monte_carlo_integration_univariate.py\nCode: @@ -0,0 +1,79 @@\n+'''\n+This program finds the approximate integration value/area under the curve of\n+a specified function within specified limits using Monte Carlo integration method.\n+\n+Further, a graph of the individal areas under the curve considered for the calculation\n+is also plotted. (PLOT SECTION -> Optional implementation)\n+'''\n+\n+# importing the modules\n+from scipy import random\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+# limits of integration (specify limits)\n+# example limits\n+a = 0\n+b = np.pi # gets the value of pi\n+\n+N = 1000 # Number of individual ares to be considered\n+\n+# function to calculate the sin of a particular value of x\n+# define your function\n+def f(x):\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/monte_carlo_integration_univariate.py`, please provide doctest for the function `f`\n\nPlease provide return type hint for the function: `f`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide descriptive name for the function: `f`\n\nPlease provide type hint for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/monte_carlo_integration_univariate.py",
    "pr_number": 7215,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996278845,
    "comment_created_at": "2022-10-15T09:14:51Z"
  },
  {
    "code": "@@ -0,0 +1,52 @@\n+\"\"\"\n+This is a pure Python implementation of the Cyclic Sort algorithm.\n+\n+For doctests run following command:\n+python -m doctest -v cyclic_sort.py\n+or\n+python3 -m doctest -v cyclic_sort.py\n+For manual testing run:\n+python cyclic_sort.py\n+\"\"\"\n+\n+\n+def cyclic_sort(nums: list) -> list:\n+    \"\"\"\n+    Sorts the input list in-place using the Cyclic Sort algorithm.",
    "comment": "i'd recommend adding the input constraints.\r\n\r\nthis algorithm only works correctly when the elements range from 1 to n with no duplicates. stating this constraint on the inputs to any potential users of the algorithm can help prevent confusion should they provide a list of integers that does not fit the criteria and end up in an infinite loop or some other unexpected behaviour.",
    "line_number": 15,
    "enriched": "File: sorts/cyclic_sort.py\nCode: @@ -0,0 +1,52 @@\n+\"\"\"\n+This is a pure Python implementation of the Cyclic Sort algorithm.\n+\n+For doctests run following command:\n+python -m doctest -v cyclic_sort.py\n+or\n+python3 -m doctest -v cyclic_sort.py\n+For manual testing run:\n+python cyclic_sort.py\n+\"\"\"\n+\n+\n+def cyclic_sort(nums: list) -> list:\n+    \"\"\"\n+    Sorts the input list in-place using the Cyclic Sort algorithm.\nComment: I'd recommend adding the input constraints.\r\n\r\nThis algorithm only works correctly when the elements range from `1` to `n` with no duplicates. Stating this constraint on the inputs to any potential users of the algorithm can help prevent confusion should they provide a list of integers that does not fit the criteria and end up in an infinite loop or some other unexpected behaviour.",
    "subcategory": "solution approach",
    "category": "refactoring",
    "file_path": "sorts/cyclic_sort.py",
    "pr_number": 9256,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1792504872,
    "comment_created_at": "2024-10-08T21:01:58Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+\"\"\"\n+Author  : Siddharth Warrier\n+Date    : October 3, 2023\n+\n+Task:\n+Count the no of pairs in a given array with given sum\n+\n+Implementation notes: Using hashing\n+The idea is that we hash the array in a dictionary\n+Then go through the elements of the array\n+We subtract this with the given sum\n+and check if that is there in the array\n+We also check the edge cases like if there are multiple same elements\n+Finally we divide the count by 2\n+to avoid the same pair getting counted twice\n+\"\"\"\n+\n+\n+def pairs_with_sum(arr: list, req_sum: int) -> int:\n+    \"\"\"\n+    Return the no. of pairs with sum \"sum\"\n+\n+    >>> pairs_with_sum([1,5,7,1],6)\n+    2\n+    >>> pairs_with_sum([1,1,1,1,1,1,1,1],2)\n+    28\n+    >>> pairs_with_sum([1,7,6,2,5,4,3,1,9,8],7)\n+    4\n+    \"\"\"\n+    d: dict = {}",
    "comment": "this type is inferred. also try to use more descriptive variable names",
    "line_number": 30,
    "enriched": "File: data_structures/arrays/pairs_with_given_sum.py\nCode: @@ -0,0 +1,48 @@\n+\"\"\"\n+Author  : Siddharth Warrier\n+Date    : October 3, 2023\n+\n+Task:\n+Count the no of pairs in a given array with given sum\n+\n+Implementation notes: Using hashing\n+The idea is that we hash the array in a dictionary\n+Then go through the elements of the array\n+We subtract this with the given sum\n+and check if that is there in the array\n+We also check the edge cases like if there are multiple same elements\n+Finally we divide the count by 2\n+to avoid the same pair getting counted twice\n+\"\"\"\n+\n+\n+def pairs_with_sum(arr: list, req_sum: int) -> int:\n+    \"\"\"\n+    Return the no. of pairs with sum \"sum\"\n+\n+    >>> pairs_with_sum([1,5,7,1],6)\n+    2\n+    >>> pairs_with_sum([1,1,1,1,1,1,1,1],2)\n+    28\n+    >>> pairs_with_sum([1,7,6,2,5,4,3,1,9,8],7)\n+    4\n+    \"\"\"\n+    d: dict = {}\nComment: ```suggestion\n    d = {}\n```\nThis type is inferred. Also try to use more descriptive variable names",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "data_structures/arrays/pairs_with_given_sum.py",
    "pr_number": 10282,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1355789685,
    "comment_created_at": "2023-10-11T21:15:07Z"
  },
  {
    "code": "@@ -0,0 +1,80 @@\n+import sys\n+\n+\n+\"\"\"\n+The famous Floyd Warshall Alogirthm to\n+find the shortest distance between all\n+pairs of given vertices.\n+Wikipedia link:  https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm\n+\"\"\"\n+\n+def floyd_warshall(adjacency_matrix: list[list[int]], n:int) -> list[list[int]]:",
    "comment": "please provide descriptive name for the parameter: n",
    "line_number": 11,
    "enriched": "File: graphs/floyd_warshall.py\nCode: @@ -0,0 +1,80 @@\n+import sys\n+\n+\n+\"\"\"\n+The famous Floyd Warshall Alogirthm to\n+find the shortest distance between all\n+pairs of given vertices.\n+Wikipedia link:  https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm\n+\"\"\"\n+\n+def floyd_warshall(adjacency_matrix: list[list[int]], n:int) -> list[list[int]]:\nComment: Please provide descriptive name for the parameter: `n`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "graphs/floyd_warshall.py",
    "pr_number": 9911,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1348633402,
    "comment_created_at": "2023-10-06T12:05:47Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+\"\"\"\n+This is the implementation of inter_quartile range (IQR).\n+\n+function takes the list of numeric values as input\n+and return the IQR as output.\n+\n+Script inspired from its corresponding Wikipedia article\n+https://en.wikipedia.org/wiki/Interquartile_range\n+\"\"\"\n+\n+from typing import List\n+\n+\n+def find_median(x: List[float]) -> float:",
    "comment": "please provide descriptive name for the parameter: x",
    "line_number": 14,
    "enriched": "File: maths/inter_quartile_range.py\nCode: @@ -0,0 +1,60 @@\n+\"\"\"\n+This is the implementation of inter_quartile range (IQR).\n+\n+function takes the list of numeric values as input\n+and return the IQR as output.\n+\n+Script inspired from its corresponding Wikipedia article\n+https://en.wikipedia.org/wiki/Interquartile_range\n+\"\"\"\n+\n+from typing import List\n+\n+\n+def find_median(x: List[float]) -> float:\nComment: Please provide descriptive name for the parameter: `x`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/inter_quartile_range.py",
    "pr_number": 8734,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1199636933,
    "comment_created_at": "2023-05-20T17:15:38Z"
  },
  {
    "code": "@@ -0,0 +1,34 @@\n+def longest_valid_parenthesis(s: str) -> int:",
    "comment": "please provide descriptive name for the parameter: s",
    "line_number": 1,
    "enriched": "File: strings/longest_valid_parenthesis.py\nCode: @@ -0,0 +1,34 @@\n+def longest_valid_parenthesis(s: str) -> int:\nComment: Please provide descriptive name for the parameter: `s`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "strings/longest_valid_parenthesis.py",
    "pr_number": 6954,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 991641818,
    "comment_created_at": "2022-10-10T21:10:32Z"
  },
  {
    "code": "@@ -0,0 +1,84 @@\n+#https://webstor.srmist.edu.in/web_assets/srm_mainsite/files/2018/transient-analysis.pdf\n+\n+\"\"\"Electric circuits will be subjected to sudden changes which may be in the form of opening and closing of\n+switches or sudden changes in sources etc. Whenever such a change occurs, the circuit which was in a particular steady state condition will go to another steady state condition. Transient analysis is the analysis of the circuits during the time it changes from one steady state condition to another steady state condition\n+Source : https://webstor.srmist.edu.in/web_assets/srm_mainsite/files/2018/transient-analysis.pdf\n+\"\"\"\n+\n+from math import e,pow\n+\n+def transient_resp_RL(resistance:float, inductance:float, voltage:float, current:float, time:float) -> tuple:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: transient_resp_rl",
    "line_number": 10,
    "enriched": "File: electronics/transient_analysis.py\nCode: @@ -0,0 +1,84 @@\n+#https://webstor.srmist.edu.in/web_assets/srm_mainsite/files/2018/transient-analysis.pdf\n+\n+\"\"\"Electric circuits will be subjected to sudden changes which may be in the form of opening and closing of\n+switches or sudden changes in sources etc. Whenever such a change occurs, the circuit which was in a particular steady state condition will go to another steady state condition. Transient analysis is the analysis of the circuits during the time it changes from one steady state condition to another steady state condition\n+Source : https://webstor.srmist.edu.in/web_assets/srm_mainsite/files/2018/transient-analysis.pdf\n+\"\"\"\n+\n+from math import e,pow\n+\n+def transient_resp_RL(resistance:float, inductance:float, voltage:float, current:float, time:float) -> tuple:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `transient_resp_RL`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "electronics/transient_analysis.py",
    "pr_number": 9606,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1344251912,
    "comment_created_at": "2023-10-03T14:52:08Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+# -*- coding: utf-8 -*-\r\n+\"\"\"Project-1\r\n+\r\n+Automatically generated by Colaboratory.\r\n+\r\n+Original file is located at\r\n+    https://colab.research.google.com/drive/1uJlXbRnd1mLtAv8fp2wDPVeWG1QHKbUw\r\n+\r\n+Importing libraries\r\n+\"\"\"\r\n+\r\n+import numpy as np\r\n+import pandas as pd\r\n+from sklearn.model_selection import train_test_split\r\n+from sklearn.linear_model import LogisticRegression\r\n+from sklearn.metrics import accuracy_score\r\n+\r\n+\"\"\"Data collection + processing\"\"\"\r\n+\r\n+#loading the dataset to a panda df\r\n+sonar_data = pd.read_csv('/content/Copy of sonar data.csv' , header=None)\r\n+\r\n+sonar_data.head()\r\n+\r\n+# number of rows and column\r\n+sonar_data.shape\r\n+\r\n+sonar_data.describe()\r\n+\r\n+sonar_data[60].value_counts()\r\n+\r\n+\"\"\"M-->mine\r\n+\r\n+R-->rock\r\n+\"\"\"\r\n+\r\n+sonar_data.groupby(60).mean()\r\n+\r\n+#seperate 60th column\r\n+X=sonar_data.drop(columns=60,axis=1)##axis = 0 for rows\r\n+Y=sonar_data[60]\r\n+\r\n+print(X)\r\n+print(Y)\r\n+\r\n+\"\"\"Training and test data\"\"\"\r\n+\r\n+X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size = 0.1,stratify=Y,random_state=1)\r",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: x_train\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: x_test\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: y_train\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: y_test",
    "line_number": 48,
    "enriched": "File: rock_or_mine.py\nCode: @@ -0,0 +1,94 @@\n+# -*- coding: utf-8 -*-\r\n+\"\"\"Project-1\r\n+\r\n+Automatically generated by Colaboratory.\r\n+\r\n+Original file is located at\r\n+    https://colab.research.google.com/drive/1uJlXbRnd1mLtAv8fp2wDPVeWG1QHKbUw\r\n+\r\n+Importing libraries\r\n+\"\"\"\r\n+\r\n+import numpy as np\r\n+import pandas as pd\r\n+from sklearn.model_selection import train_test_split\r\n+from sklearn.linear_model import LogisticRegression\r\n+from sklearn.metrics import accuracy_score\r\n+\r\n+\"\"\"Data collection + processing\"\"\"\r\n+\r\n+#loading the dataset to a panda df\r\n+sonar_data = pd.read_csv('/content/Copy of sonar data.csv' , header=None)\r\n+\r\n+sonar_data.head()\r\n+\r\n+# number of rows and column\r\n+sonar_data.shape\r\n+\r\n+sonar_data.describe()\r\n+\r\n+sonar_data[60].value_counts()\r\n+\r\n+\"\"\"M-->mine\r\n+\r\n+R-->rock\r\n+\"\"\"\r\n+\r\n+sonar_data.groupby(60).mean()\r\n+\r\n+#seperate 60th column\r\n+X=sonar_data.drop(columns=60,axis=1)##axis = 0 for rows\r\n+Y=sonar_data[60]\r\n+\r\n+print(X)\r\n+print(Y)\r\n+\r\n+\"\"\"Training and test data\"\"\"\r\n+\r\n+X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size = 0.1,stratify=Y,random_state=1)\r\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `X_train`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `X_test`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `Y_train`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `Y_test`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "rock_or_mine.py",
    "pr_number": 7554,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002706351,
    "comment_created_at": "2022-10-23T12:56:26Z"
  },
  {
    "code": "@@ -0,0 +1,37 @@\n+def countAndSay(n: int) -> str:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: countandsay\n\nplease provide descriptive name for the parameter: n",
    "line_number": 1,
    "enriched": "File: strings/count_and_say.py\nCode: @@ -0,0 +1,37 @@\n+def countAndSay(n: int) -> str:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `countAndSay`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "strings/count_and_say.py",
    "pr_number": 9926,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1348960279,
    "comment_created_at": "2023-10-06T16:35:47Z"
  },
  {
    "code": "@@ -0,0 +1,66 @@\n+# of nCr in O(1) time.\n+N = 1000001\n+\n+# array to store inverse of 1 to N\n+factorialNumInverse = [None] * (N + 1)",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: factorialnuminverse",
    "line_number": 5,
    "enriched": "File: maths/factorial_inverse_to_find_bionomial.py\nCode: @@ -0,0 +1,66 @@\n+# of nCr in O(1) time.\n+N = 1000001\n+\n+# array to store inverse of 1 to N\n+factorialNumInverse = [None] * (N + 1)\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `factorialNumInverse`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/factorial_inverse_to_find_bionomial.py",
    "pr_number": 7442,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000130574,
    "comment_created_at": "2022-10-20T04:28:26Z"
  },
  {
    "code": "@@ -0,0 +1,90 @@\n+def text_justification(words: str, maxWidth: int) -> list:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: maxwidth",
    "line_number": 1,
    "enriched": "File: strings/text_justification.py\nCode: @@ -0,0 +1,90 @@\n+def text_justification(words: str, maxWidth: int) -> list:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `maxWidth`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "strings/text_justification.py",
    "pr_number": 7342,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996500969,
    "comment_created_at": "2022-10-16T22:00:15Z"
  },
  {
    "code": "@@ -87,6 +87,19 @@ def run_linear_regression(data_x, data_y):\n     return theta\n \n \n+def mean_absolute_error(pred_X, org_X):",
    "comment": "please provide more descriptive variable names and use snake_case",
    "line_number": 90,
    "enriched": "File: machine_learning/linear_regression.py\nCode: @@ -87,6 +87,19 @@ def run_linear_regression(data_x, data_y):\n     return theta\n \n \n+def mean_absolute_error(pred_X, org_X):\nComment: Please provide more descriptive variable names and use `snake_case`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "machine_learning/linear_regression.py",
    "pr_number": 7003,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 992624036,
    "comment_created_at": "2022-10-11T17:50:00Z"
  },
  {
    "code": "@@ -0,0 +1,95 @@\n+def baseNeg2(n: int) -> str:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: baseneg2\n\nplease provide descriptive name for the parameter: n",
    "line_number": 1,
    "enriched": "File: bit_manipulation/convert_to_base_-2.py\nCode: @@ -0,0 +1,95 @@\n+def baseNeg2(n: int) -> str:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `baseNeg2`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "bit_manipulation/convert_to_base_-2.py",
    "pr_number": 9590,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1344069801,
    "comment_created_at": "2023-10-03T13:03:20Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+from typing import List\n+import doctest\n+\n+class Solution:\n+    def productExceptSelf(self, nums: List[int]) -> List[int]:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: productexceptself",
    "line_number": 5,
    "enriched": "File: data_structures/arrays/ProductOfArrayExceptSelf.py\nCode: @@ -0,0 +1,94 @@\n+from typing import List\n+import doctest\n+\n+class Solution:\n+    def productExceptSelf(self, nums: List[int]) -> List[int]:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `productExceptSelf`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/arrays/ProductOfArrayExceptSelf.py",
    "pr_number": 13174,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2403974392,
    "comment_created_at": "2025-10-04T13:27:20Z"
  },
  {
    "code": "@@ -0,0 +1,27 @@\n+\n+import numpy as np\n+from sklearn.model_selection import train_test_split\n+from sklearn.linear_model import LinearRegression\n+from sklearn.metrics import mean_squared_error\n+\n+# Generate some sample data\n+np.random.seed(0)\n+X = np.random.rand(100, 1)  # Feature (input)\n+y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)  # Target (output) with some noise\n+\n+# Split the data into training and testing sets\n+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: x_train\n\nvariable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: x_test",
    "line_number": 13,
    "enriched": "File: machine_learning/train a basic linear regression model.py\nCode: @@ -0,0 +1,27 @@\n+\n+import numpy as np\n+from sklearn.model_selection import train_test_split\n+from sklearn.linear_model import LinearRegression\n+from sklearn.metrics import mean_squared_error\n+\n+# Generate some sample data\n+np.random.seed(0)\n+X = np.random.rand(100, 1)  # Feature (input)\n+y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)  # Target (output) with some noise\n+\n+# Split the data into training and testing sets\n+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `X_train`\n\nVariable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `X_test`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/train a basic linear regression model.py",
    "pr_number": 10098,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349678949,
    "comment_created_at": "2023-10-08T11:09:41Z"
  },
  {
    "code": "@@ -3,29 +3,57 @@\n \n class SegmentTree:\n     def __init__(self, a):\n-        self.N = len(a)\n+        self.A = a\n+        self.N = len(self.A)\n         self.st = [0] * (\n             4 * self.N\n         )  # approximate the overall size of segment tree with array N",
    "comment": "i gotta say that overall i am not a fan of these single variable names and uppercase variable names!!!\r\nthis looks like it was literally ported from some other language with minimal modifications.\r\n\r\na has no data type and no self-documenting name and the class version is uppercase.  if find it unreadable.",
    "line_number": 10,
    "enriched": "File: data_structures/binary_tree/segment_tree.py\nCode: @@ -3,29 +3,57 @@\n \n class SegmentTree:\n     def __init__(self, a):\n-        self.N = len(a)\n+        self.A = a\n+        self.N = len(self.A)\n         self.st = [0] * (\n             4 * self.N\n         )  # approximate the overall size of segment tree with array N\nComment: I gotta say that overall I am not a fan of these single variable names and uppercase variable names!!!\r\nThis looks like it was literally ported from some other language with minimal modifications.\r\n\r\n`a` has no data type and no self-documenting name and the class version is uppercase.  If find it unreadable.\r\n\r\n\r\n```suggestion\r\n        # approximate the overall size of the segment tree with array N\r\n        self.st = [0] * (4 * self.N)\r\n```",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "data_structures/binary_tree/segment_tree.py",
    "pr_number": 9975,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349563442,
    "comment_created_at": "2023-10-07T19:11:06Z"
  },
  {
    "code": "@@ -0,0 +1,51 @@\n+\"\"\"\n+author: Aayush Soni\n+Given n pairs of parentheses, write a function to generate all\n+combinations of well-formed parentheses.\n+Input: n = 2\n+Output: [\"(())\",\"()()\"]\n+Leetcode link: https://leetcode.com/problems/generate-parentheses/description/\n+\"\"\"\n+\n+\n+def generateParenthesis(n: int) -> list[str]:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: generateparenthesis\n\nplease provide descriptive name for the parameter: n",
    "line_number": 11,
    "enriched": "File: backtracking/generate_parentheses.py\nCode: @@ -0,0 +1,51 @@\n+\"\"\"\n+author: Aayush Soni\n+Given n pairs of parentheses, write a function to generate all\n+combinations of well-formed parentheses.\n+Input: n = 2\n+Output: [\"(())\",\"()()\"]\n+Leetcode link: https://leetcode.com/problems/generate-parentheses/description/\n+\"\"\"\n+\n+\n+def generateParenthesis(n: int) -> list[str]:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `generateParenthesis`\n\nPlease provide descriptive name for the parameter: `n`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "backtracking/generate_parentheses.py",
    "pr_number": 10903,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1370253928,
    "comment_created_at": "2023-10-24T14:13:37Z"
  },
  {
    "code": "@@ -0,0 +1,85 @@\n+#!/usr/bin/python3            \n+#To find the python interpreter, when we execute it without python3\n+\n+\"\"\"\n+To use:\n+    ./codechef_rank.py <contest-name>\n+\"\"\"\n+\n+import requests\n+import json\n+import sys                                      #To get the passed arguments in terminal\n+\n+id= '<codechef_id>'                              #Replace with your codechef id\n+\n+\n+\n+main_url= \"https://www.codechef.com/api/rankings/\"\n+contest_name= sys.argv[1]\n+# contest_name= input(\"Enter Contest Name: \")\n+# contest_name=\"SEP221C\"\n+# print(contest_name)\n+\n+url= main_url+contest_name\n+\n+dict={}\n+headers = {\n+    'authority': 'www.codechef.com',\n+    'accept': 'application/json, text/plain, */*',\n+    'accept-language': 'en-US,en;q=0.9',\n+    # Requests sorts cookies= alphabetically\n+    'cookie': '_fbp=fb.1.1655306318354.66392898; pg_mm2_cookie_a=85654c31-1e1e-48ec-9f8b-0e71d258436a; twk_uuid_627c0f75b0d10b6f3e71c35d=%7B%22uuid%22%3A%221.H3NO5nd4jOeziibZm2mRXYSI3644LExvXsLy2XAJpeX6oKXKytIrDYkyJ5vtWMPxtdPxOmdp3K4GQf06bf07sXMb0lg4y6VSxlwRr20XGemYXnB479arAdNf5nK9agSM1Md8X1QD9MJkd5ss%22%2C%22version%22%3A3%2C%22domain%22%3A%22codechef.com%22%2C%22ts%22%3A1658326231790%7D; twk_uuid_62397c18a34c2456412c3b26=%7B%22uuid%22%3A%221.H3NMbQzCA5paUeWeHE3S0JdRNNp3WPDuQiFonJ8fJcBrCBAVhTuKUuCbnUKDAgn6QnnQCk4SSwVYleQBJh3z5Upe5NDi9N1RJ0vwflOcLcDwPvkQv50vgNeCcKHWYawcCZU7Rm3m7IMpr6Jl%22%2C%22version%22%3A3%2C%22domain%22%3A%22codechef.com%22%2C%22ts%22%3A1658577496450%7D; _gcl_au=1.1.2141587503.1663165531; FCNEC=%5B%5B%22AKsRol8OKyH-AI_Th8_GaKuT9ARaiyCq5e15MDMmxkCXSOaOOQc2UAg2I7F5xFYcIEao5-R20KCVuy_ekbbJy9ZIXib62nHbQUKxH9JgmNqqvIcNRLtM4hizhC_y2xN4sebGbvmvaBJ_J0P4e_paYSlHUuZgbAYYKg%3D%3D%22%5D%2Cnull%2C%5B%5D%5D; __gads=ID=f702a7a3ed66e141-22854ff149d70012:T=1655306346:RT=1663955229:S=ALNI_MbdnlQ1SgroV8Of5xn4KSpxKrSbKQ; SESS93b6022d778ee317bf48f7dbffe03173=db040399023abeee64d5a77e3d26f067; uid=3137718; __gpi=UID=000006ad96536525:T=1655306346:RT=1666190266:S=ALNI_MYP3iUpXkfMGriNtcMwBbc3u6ExUw; _gid=GA1.2.808286290.1666497045; _clck=17nqn5c|1|f5y|0; _gat_UA-141612136-1=1; _ga_C8RQQ7NY18=GS1.1.1666536681.133.1.1666537079.0.0.0; _ga=GA1.2.306524194.1655306320; _clsk=1qfm2v3|1666537080268|6|0|i.clarity.ms/collect',\n+    'referer': 'https://www.codechef.com/rankings/START43d?filterBy=Institution%3DMaharaja%20Agrasen%20Institute%20of%20Technology%2C%20New%20Delhi&itemsPerPage=100&order=asc&page=1&sortBy=rank',\n+    'sec-ch-ua': '\"Chromium\";v=\"106\", \"Google Chrome\";v=\"106\", \"Not;A=Brand\";v=\"99\"',\n+    'sec-ch-ua-mobile': '?1',\n+    'sec-ch-ua-platform': '\"Android\"',\n+    'sec-fetch-dest': 'empty',\n+    'sec-fetch-mode': 'cors',\n+    'sec-fetch-site': 'same-origin',\n+    'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Mobile Safari/537.36',\n+    'x-csrf-token': '9032eae6fad31fa3844a43d624be7d73fa8304fd3920bea6a5c177a8e25721be',\n+    'x-requested-with': 'XMLHttpRequest',\n+}\n+\n+params = {\n+    'filterBy': 'Institution=Maharaja Agrasen Institute of Technology, New Delhi',\n+    'itemsPerPage': '100',\n+    'order': 'asc',\n+    'page': '1',\n+    'sortBy': 'rank',\n+}\n+\n+# print(url)\n+r = requests.get(url, params=params, headers=headers).json()\n+# print(r)\n+users= r['list']\n+c=1;\n+for i in users:\n+  v= []\n+  rankVar= i['rank']",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: rankvar",
    "line_number": 59,
    "enriched": "File: web_programming/CodechefRank.py\nCode: @@ -0,0 +1,85 @@\n+#!/usr/bin/python3            \n+#To find the python interpreter, when we execute it without python3\n+\n+\"\"\"\n+To use:\n+    ./codechef_rank.py <contest-name>\n+\"\"\"\n+\n+import requests\n+import json\n+import sys                                      #To get the passed arguments in terminal\n+\n+id= '<codechef_id>'                              #Replace with your codechef id\n+\n+\n+\n+main_url= \"https://www.codechef.com/api/rankings/\"\n+contest_name= sys.argv[1]\n+# contest_name= input(\"Enter Contest Name: \")\n+# contest_name=\"SEP221C\"\n+# print(contest_name)\n+\n+url= main_url+contest_name\n+\n+dict={}\n+headers = {\n+    'authority': 'www.codechef.com',\n+    'accept': 'application/json, text/plain, */*',\n+    'accept-language': 'en-US,en;q=0.9',\n+    # Requests sorts cookies= alphabetically\n+    'cookie': '_fbp=fb.1.1655306318354.66392898; pg_mm2_cookie_a=85654c31-1e1e-48ec-9f8b-0e71d258436a; twk_uuid_627c0f75b0d10b6f3e71c35d=%7B%22uuid%22%3A%221.H3NO5nd4jOeziibZm2mRXYSI3644LExvXsLy2XAJpeX6oKXKytIrDYkyJ5vtWMPxtdPxOmdp3K4GQf06bf07sXMb0lg4y6VSxlwRr20XGemYXnB479arAdNf5nK9agSM1Md8X1QD9MJkd5ss%22%2C%22version%22%3A3%2C%22domain%22%3A%22codechef.com%22%2C%22ts%22%3A1658326231790%7D; twk_uuid_62397c18a34c2456412c3b26=%7B%22uuid%22%3A%221.H3NMbQzCA5paUeWeHE3S0JdRNNp3WPDuQiFonJ8fJcBrCBAVhTuKUuCbnUKDAgn6QnnQCk4SSwVYleQBJh3z5Upe5NDi9N1RJ0vwflOcLcDwPvkQv50vgNeCcKHWYawcCZU7Rm3m7IMpr6Jl%22%2C%22version%22%3A3%2C%22domain%22%3A%22codechef.com%22%2C%22ts%22%3A1658577496450%7D; _gcl_au=1.1.2141587503.1663165531; FCNEC=%5B%5B%22AKsRol8OKyH-AI_Th8_GaKuT9ARaiyCq5e15MDMmxkCXSOaOOQc2UAg2I7F5xFYcIEao5-R20KCVuy_ekbbJy9ZIXib62nHbQUKxH9JgmNqqvIcNRLtM4hizhC_y2xN4sebGbvmvaBJ_J0P4e_paYSlHUuZgbAYYKg%3D%3D%22%5D%2Cnull%2C%5B%5D%5D; __gads=ID=f702a7a3ed66e141-22854ff149d70012:T=1655306346:RT=1663955229:S=ALNI_MbdnlQ1SgroV8Of5xn4KSpxKrSbKQ; SESS93b6022d778ee317bf48f7dbffe03173=db040399023abeee64d5a77e3d26f067; uid=3137718; __gpi=UID=000006ad96536525:T=1655306346:RT=1666190266:S=ALNI_MYP3iUpXkfMGriNtcMwBbc3u6ExUw; _gid=GA1.2.808286290.1666497045; _clck=17nqn5c|1|f5y|0; _gat_UA-141612136-1=1; _ga_C8RQQ7NY18=GS1.1.1666536681.133.1.1666537079.0.0.0; _ga=GA1.2.306524194.1655306320; _clsk=1qfm2v3|1666537080268|6|0|i.clarity.ms/collect',\n+    'referer': 'https://www.codechef.com/rankings/START43d?filterBy=Institution%3DMaharaja%20Agrasen%20Institute%20of%20Technology%2C%20New%20Delhi&itemsPerPage=100&order=asc&page=1&sortBy=rank',\n+    'sec-ch-ua': '\"Chromium\";v=\"106\", \"Google Chrome\";v=\"106\", \"Not;A=Brand\";v=\"99\"',\n+    'sec-ch-ua-mobile': '?1',\n+    'sec-ch-ua-platform': '\"Android\"',\n+    'sec-fetch-dest': 'empty',\n+    'sec-fetch-mode': 'cors',\n+    'sec-fetch-site': 'same-origin',\n+    'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Mobile Safari/537.36',\n+    'x-csrf-token': '9032eae6fad31fa3844a43d624be7d73fa8304fd3920bea6a5c177a8e25721be',\n+    'x-requested-with': 'XMLHttpRequest',\n+}\n+\n+params = {\n+    'filterBy': 'Institution=Maharaja Agrasen Institute of Technology, New Delhi',\n+    'itemsPerPage': '100',\n+    'order': 'asc',\n+    'page': '1',\n+    'sortBy': 'rank',\n+}\n+\n+# print(url)\n+r = requests.get(url, params=params, headers=headers).json()\n+# print(r)\n+users= r['list']\n+c=1;\n+for i in users:\n+  v= []\n+  rankVar= i['rank']\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `rankVar`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "web_programming/CodechefRank.py",
    "pr_number": 7568,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002728366,
    "comment_created_at": "2022-10-23T15:31:18Z"
  },
  {
    "code": "@@ -0,0 +1,32 @@\n+\"\"\"\n+Dynamic Programming to solve Brust Balloons.py\n+Tabulation\n+\"\"\"\n+def maxCoins(self, nums: List[int]) -> int:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: maxcoins",
    "line_number": 5,
    "enriched": "File: dynamic_programming/brust_balloons.py\nCode: @@ -0,0 +1,32 @@\n+\"\"\"\n+Dynamic Programming to solve Brust Balloons.py\n+Tabulation\n+\"\"\"\n+def maxCoins(self, nums: List[int]) -> int:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `maxCoins`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/brust_balloons.py",
    "pr_number": 6904,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 991104061,
    "comment_created_at": "2022-10-10T09:48:10Z"
  },
  {
    "code": "@@ -0,0 +1,332 @@\n+import math\n+import doctest\n+\n+\n+def number_theoretic_transform(\n+    polynomial_coeffs: list, prime_modulus: int, primitive_root: int\n+) -> list:\n+    \"\"\"\n+    Compute the number-theoretic transform of a polynomial.\n+\n+    Args:\n+    - polynomial_coeffs: List of integers representing polynomial coefficients.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+    - primitive_root: Primitive root of the prime modulus.\n+\n+    Returns:\n+    - List of integers representing the polynomial in point-value form.\n+\n+    Doctest:\n+    >>> number_theoretic_transform([1, 2, 3, 4], 998244353, 3)\n+    [10, 173167434, 998244351, 825076915]\n+    >>> number_theoretic_transform([4, 3, 2, 1], 998244353, 3)\n+    [10, 825076919, 2, 173167438]\n+    \"\"\"\n+    n = len(polynomial_coeffs)\n+    assert (prime_modulus - 1) % n == 0\n+    w = pow(primitive_root, (prime_modulus - 1) // n, prime_modulus)\n+    return recursive_ntt(polynomial_coeffs, w, prime_modulus)\n+\n+\n+def recursive_ntt_transform(\n+    polynomial_coeffs_recursive: list, root_of_unity: int, prime_modulus: int\n+) -> list:\n+    \"\"\"\n+    Recursive function for number-theoretic transform.\n+\n+    Args:\n+    - polynomial_coeffs_recursive: List of integers representing polynomial coefficients during recursive steps.\n+    - root_of_unity: Primitive nth root of unity modulo the prime modulus.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+\n+    Returns:\n+    - List of integers representing the polynomial in point-value form.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> transformed = number_theoretic_transform(a, 998244353, 3)\n+    >>> recursive_ntt_transform(a, 3, 998244353)\n+    [10, 998244345, 998244351, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> recursive_ntt_transform(b, 3, 998244353)\n+    [10, 8, 2, 998244349]\n+    \"\"\"\n+    n = len(polynomial_coeffs_recursive)\n+    if n == 1:\n+        return polynomial_coeffs_recursive\n+    a0 = polynomial_coeffs_recursive[::2]\n+    a1 = polynomial_coeffs_recursive[1::2]\n+    f0 = recursive_ntt_transform(a0, root_of_unity**2 % prime_modulus, prime_modulus)\n+    f1 = recursive_ntt_transform(a1, root_of_unity**2 % prime_modulus, prime_modulus)\n+    x = 1\n+    f = [0] * n\n+    for i in range(n // 2):\n+        f[i] = (f0[i] + x * f1[i]) % prime_modulus\n+        f[i + n // 2] = (f0[i] - x * f1[i]) % prime_modulus\n+        x = x * root_of_unity % prime_modulus\n+    return f\n+\n+\n+def inverse_ntt(point_values: list, prime_modulus: int, primitive_root: int) -> list:\n+    \"\"\"\n+    Compute the inverse number-theoretic transform of a polynomial.\n+\n+    Args:\n+    - point_values: List of integers representing polynomial in point-value form.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+    - primitive_root: Primitive root of the prime modulus.\n+\n+    Returns:\n+    - List of integers representing polynomial coefficients.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> transformed = number_theoretic_transform(a, 998244353, 3)\n+    >>> inverse_ntt(transformed, 998244353, 3)\n+    [1, 2, 3, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> inverse_ntt(transformed_b, 998244353, 3)\n+    [4, 3, 2, 1]\n+    \"\"\"\n+    n = len(point_values)\n+    assert (prime_modulus - 1) % n == 0\n+    w = pow(primitive_root, (prime_modulus - 1) // n, prime_modulus)\n+    w_inv = pow(w, prime_modulus - 2, prime_modulus)\n+    return [\n+        x * pow(n, prime_modulus - 2, prime_modulus) % prime_modulus\n+        for x in recursive_inverse_ntt(point_values, w_inv, prime_modulus)\n+    ]\n+\n+\n+def recursive_inverse_transform(\n+    point_values_recursive: list, root_of_unity: int, prime_modulus: int\n+) -> list:\n+    \"\"\"\n+    Recursive function for inverse number-theoretic transform.\n+\n+    Args:\n+    - point_values_recursive: List of integers representing polynomial in point-value form during recursive steps.\n+    - root_of_unity: Primitive nth root of unity modulo the prime modulus.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+\n+    Returns:\n+    - List of integers representing polynomial coefficients.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> transformed = number_theoretic_transform(a, 998244353, 3)\n+    >>> w = pow(3, (998244353 - 1) // 4, 998244353)\n+    >>> recursive_inverse_transform(transformed, w, 998244353)\n+    [1, 2, 3, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> w_b = pow(3, (998244353 - 1) // 4, 998244353)\n+    >>> recursive_inverse_transform(transformed_b, w_b, 998244353)\n+    [4, 3, 2, 1]\n+    \"\"\"\n+    n = len(point_values_recursive)\n+    if n == 1:\n+        return point_values_recursive\n+    f0 = point_values_recursive[::2]\n+    f1 = point_values_recursive[1::2]\n+    a0 = recursive_inverse_transform(\n+        f0, root_of_unity**2 % prime_modulus, prime_modulus\n+    )\n+    a1 = recursive_inverse_transform(\n+        f1, root_of_unity**2 % prime_modulus, prime_modulus\n+    )\n+    x = 1\n+    a = [0] * n\n+    for i in range(n // 2):\n+        a[i] = (a0[i] + x * a1[i]) % prime_modulus\n+        a[i + n // 2] = (a0[i] - x * a1[i]) % prime_modulus\n+        x = x * root_of_unity % prime_modulus\n+    return a\n+\n+\n+# Helper function to compute the convolution of two polynomials in point-value form\n+def convolve_polynomials(\n+    a_point_values: list, b_point_values: list, prime_modulus: int\n+) -> list:\n+    \"\"\"\n+    Compute the convolution of two polynomials in point-value form.\n+\n+    Args:\n+    - a_point_values: List of integers representing polynomial A in point-value form.\n+    - b_point_values: List of integers representing polynomial B in point-value form.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+\n+    Returns:\n+    - List of integers representing the convolution of A and B in point-value form.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_a = number_theoretic_transform(a, 998244353, 3)\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> convolve_polynomials(transformed_a, transformed_b, 998244353)\n+    [4, 11, 20, 30, 20, 11, 4]\n+    \"\"\"\n+    return [(a * b) % prime_modulus for a, b in zip(a_point_values, b_point_values)]\n+\n+\n+def multiply_numbers(x: int, y: int, p: int, g: int) -> int:",
    "comment": "please provide descriptive name for the parameter: x\n\nplease provide descriptive name for the parameter: y\n\nplease provide descriptive name for the parameter: p\n\nplease provide descriptive name for the parameter: g",
    "line_number": 175,
    "enriched": "File: arithmetic_analysis/sch\u00f6nhage_strassen.py\nCode: @@ -0,0 +1,332 @@\n+import math\n+import doctest\n+\n+\n+def number_theoretic_transform(\n+    polynomial_coeffs: list, prime_modulus: int, primitive_root: int\n+) -> list:\n+    \"\"\"\n+    Compute the number-theoretic transform of a polynomial.\n+\n+    Args:\n+    - polynomial_coeffs: List of integers representing polynomial coefficients.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+    - primitive_root: Primitive root of the prime modulus.\n+\n+    Returns:\n+    - List of integers representing the polynomial in point-value form.\n+\n+    Doctest:\n+    >>> number_theoretic_transform([1, 2, 3, 4], 998244353, 3)\n+    [10, 173167434, 998244351, 825076915]\n+    >>> number_theoretic_transform([4, 3, 2, 1], 998244353, 3)\n+    [10, 825076919, 2, 173167438]\n+    \"\"\"\n+    n = len(polynomial_coeffs)\n+    assert (prime_modulus - 1) % n == 0\n+    w = pow(primitive_root, (prime_modulus - 1) // n, prime_modulus)\n+    return recursive_ntt(polynomial_coeffs, w, prime_modulus)\n+\n+\n+def recursive_ntt_transform(\n+    polynomial_coeffs_recursive: list, root_of_unity: int, prime_modulus: int\n+) -> list:\n+    \"\"\"\n+    Recursive function for number-theoretic transform.\n+\n+    Args:\n+    - polynomial_coeffs_recursive: List of integers representing polynomial coefficients during recursive steps.\n+    - root_of_unity: Primitive nth root of unity modulo the prime modulus.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+\n+    Returns:\n+    - List of integers representing the polynomial in point-value form.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> transformed = number_theoretic_transform(a, 998244353, 3)\n+    >>> recursive_ntt_transform(a, 3, 998244353)\n+    [10, 998244345, 998244351, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> recursive_ntt_transform(b, 3, 998244353)\n+    [10, 8, 2, 998244349]\n+    \"\"\"\n+    n = len(polynomial_coeffs_recursive)\n+    if n == 1:\n+        return polynomial_coeffs_recursive\n+    a0 = polynomial_coeffs_recursive[::2]\n+    a1 = polynomial_coeffs_recursive[1::2]\n+    f0 = recursive_ntt_transform(a0, root_of_unity**2 % prime_modulus, prime_modulus)\n+    f1 = recursive_ntt_transform(a1, root_of_unity**2 % prime_modulus, prime_modulus)\n+    x = 1\n+    f = [0] * n\n+    for i in range(n // 2):\n+        f[i] = (f0[i] + x * f1[i]) % prime_modulus\n+        f[i + n // 2] = (f0[i] - x * f1[i]) % prime_modulus\n+        x = x * root_of_unity % prime_modulus\n+    return f\n+\n+\n+def inverse_ntt(point_values: list, prime_modulus: int, primitive_root: int) -> list:\n+    \"\"\"\n+    Compute the inverse number-theoretic transform of a polynomial.\n+\n+    Args:\n+    - point_values: List of integers representing polynomial in point-value form.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+    - primitive_root: Primitive root of the prime modulus.\n+\n+    Returns:\n+    - List of integers representing polynomial coefficients.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> transformed = number_theoretic_transform(a, 998244353, 3)\n+    >>> inverse_ntt(transformed, 998244353, 3)\n+    [1, 2, 3, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> inverse_ntt(transformed_b, 998244353, 3)\n+    [4, 3, 2, 1]\n+    \"\"\"\n+    n = len(point_values)\n+    assert (prime_modulus - 1) % n == 0\n+    w = pow(primitive_root, (prime_modulus - 1) // n, prime_modulus)\n+    w_inv = pow(w, prime_modulus - 2, prime_modulus)\n+    return [\n+        x * pow(n, prime_modulus - 2, prime_modulus) % prime_modulus\n+        for x in recursive_inverse_ntt(point_values, w_inv, prime_modulus)\n+    ]\n+\n+\n+def recursive_inverse_transform(\n+    point_values_recursive: list, root_of_unity: int, prime_modulus: int\n+) -> list:\n+    \"\"\"\n+    Recursive function for inverse number-theoretic transform.\n+\n+    Args:\n+    - point_values_recursive: List of integers representing polynomial in point-value form during recursive steps.\n+    - root_of_unity: Primitive nth root of unity modulo the prime modulus.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+\n+    Returns:\n+    - List of integers representing polynomial coefficients.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> transformed = number_theoretic_transform(a, 998244353, 3)\n+    >>> w = pow(3, (998244353 - 1) // 4, 998244353)\n+    >>> recursive_inverse_transform(transformed, w, 998244353)\n+    [1, 2, 3, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> w_b = pow(3, (998244353 - 1) // 4, 998244353)\n+    >>> recursive_inverse_transform(transformed_b, w_b, 998244353)\n+    [4, 3, 2, 1]\n+    \"\"\"\n+    n = len(point_values_recursive)\n+    if n == 1:\n+        return point_values_recursive\n+    f0 = point_values_recursive[::2]\n+    f1 = point_values_recursive[1::2]\n+    a0 = recursive_inverse_transform(\n+        f0, root_of_unity**2 % prime_modulus, prime_modulus\n+    )\n+    a1 = recursive_inverse_transform(\n+        f1, root_of_unity**2 % prime_modulus, prime_modulus\n+    )\n+    x = 1\n+    a = [0] * n\n+    for i in range(n // 2):\n+        a[i] = (a0[i] + x * a1[i]) % prime_modulus\n+        a[i + n // 2] = (a0[i] - x * a1[i]) % prime_modulus\n+        x = x * root_of_unity % prime_modulus\n+    return a\n+\n+\n+# Helper function to compute the convolution of two polynomials in point-value form\n+def convolve_polynomials(\n+    a_point_values: list, b_point_values: list, prime_modulus: int\n+) -> list:\n+    \"\"\"\n+    Compute the convolution of two polynomials in point-value form.\n+\n+    Args:\n+    - a_point_values: List of integers representing polynomial A in point-value form.\n+    - b_point_values: List of integers representing polynomial B in point-value form.\n+    - prime_modulus: Prime modulus used in the NTT transformations.\n+\n+    Returns:\n+    - List of integers representing the convolution of A and B in point-value form.\n+\n+    Doctest:\n+    >>> a = [1, 2, 3, 4]\n+    >>> b = [4, 3, 2, 1]\n+    >>> transformed_a = number_theoretic_transform(a, 998244353, 3)\n+    >>> transformed_b = number_theoretic_transform(b, 998244353, 3)\n+    >>> convolve_polynomials(transformed_a, transformed_b, 998244353)\n+    [4, 11, 20, 30, 20, 11, 4]\n+    \"\"\"\n+    return [(a * b) % prime_modulus for a, b in zip(a_point_values, b_point_values)]\n+\n+\n+def multiply_numbers(x: int, y: int, p: int, g: int) -> int:\nComment: Please provide descriptive name for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `y`\n\nPlease provide descriptive name for the parameter: `p`\n\nPlease provide descriptive name for the parameter: `g`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "arithmetic_analysis/sch\u00f6nhage_strassen.py",
    "pr_number": 9554,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1343478095,
    "comment_created_at": "2023-10-03T04:36:45Z"
  },
  {
    "code": "@@ -0,0 +1,55 @@\n+\"\"\"\n+Hinge Loss\n+\n+Description:\n+Compute the Hinge loss used for training SVM (Support Vector Machine).\n+\n+Formula:\n+loss = max(0, 1 - true * pred)\n+\n+Reference: https://en.wikipedia.org/wiki/Hinge_loss\n+\n+Author: Poojan Smart\n+Email: smrtpoojan@gmail.com\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def hinge_loss(y_true: np.ndarray, pred: np.ndarray) -> float:",
    "comment": "please rename for consistency",
    "line_number": 19,
    "enriched": "File: machine_learning/loss_functions/hinge_loss.py\nCode: @@ -0,0 +1,55 @@\n+\"\"\"\n+Hinge Loss\n+\n+Description:\n+Compute the Hinge loss used for training SVM (Support Vector Machine).\n+\n+Formula:\n+loss = max(0, 1 - true * pred)\n+\n+Reference: https://en.wikipedia.org/wiki/Hinge_loss\n+\n+Author: Poojan Smart\n+Email: smrtpoojan@gmail.com\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def hinge_loss(y_true: np.ndarray, pred: np.ndarray) -> float:\nComment: ```suggestion\r\ndef hinge_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\r\n```\r\nPlease rename for consistency",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "machine_learning/loss_functions/hinge_loss.py",
    "pr_number": 10628,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1361577523,
    "comment_created_at": "2023-10-17T06:40:41Z"
  },
  {
    "code": "@@ -0,0 +1,82 @@\n+import cv2\n+import matplotlib.pyplot as plt\n+\n+config_file = \"ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt\"\n+frozen_model = \"frozen_inference_graph.pb\"\n+model = cv2.dnn_DetectionModel(frozen_model, config_file)\n+\n+classLabels = []",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: classlabels",
    "line_number": 8,
    "enriched": "File: Object Detection/ObjectDection.py\nCode: @@ -0,0 +1,82 @@\n+import cv2\n+import matplotlib.pyplot as plt\n+\n+config_file = \"ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt\"\n+frozen_model = \"frozen_inference_graph.pb\"\n+model = cv2.dnn_DetectionModel(frozen_model, config_file)\n+\n+classLabels = []\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `classLabels`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "Object Detection/ObjectDection.py",
    "pr_number": 11676,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1784355394,
    "comment_created_at": "2024-10-02T11:47:52Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+# Predicting Customer Churn rate using Decision Tree\n+# Dataset from Kaggle: https://www.kaggle.com/code/korfanakis/predicting-customer-churn-with-machine-learning/input\n+import sys\n+import numpy as np\n+import pandas as pd\n+import matplotlib.pyplot as plt\n+from sklearn.tree import DecisionTreeClassifier\n+import sklearn.tree as tree\n+from sklearn import metrics\n+from sklearn.model_selection import train_test_split\n+\n+df = pd.read_csv(\"churn_modelling.csv\")\n+\n+# prints the first 5 rows of the dataset\n+# print(df.head())\n+\n+# Sorting the dependent and independent values\n+x = df[['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts',\n+        'HasCrCard', 'IsActiveMember', 'EstimatedSalary']].values\n+y = df['Exited']\n+\n+# Splitting the dataset into training and testing data\n+x_test, x_train, y_test, y_train = train_test_split(\n+    x, y, test_size=0.3, random_state=3)\n+\n+# Creating the decision tree classifier\n+decTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4)",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: dectree",
    "line_number": 27,
    "enriched": "File: machine_learning/decision_tree_churn/churn_cal.py\nCode: @@ -0,0 +1,42 @@\n+# Predicting Customer Churn rate using Decision Tree\n+# Dataset from Kaggle: https://www.kaggle.com/code/korfanakis/predicting-customer-churn-with-machine-learning/input\n+import sys\n+import numpy as np\n+import pandas as pd\n+import matplotlib.pyplot as plt\n+from sklearn.tree import DecisionTreeClassifier\n+import sklearn.tree as tree\n+from sklearn import metrics\n+from sklearn.model_selection import train_test_split\n+\n+df = pd.read_csv(\"churn_modelling.csv\")\n+\n+# prints the first 5 rows of the dataset\n+# print(df.head())\n+\n+# Sorting the dependent and independent values\n+x = df[['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts',\n+        'HasCrCard', 'IsActiveMember', 'EstimatedSalary']].values\n+y = df['Exited']\n+\n+# Splitting the dataset into training and testing data\n+x_test, x_train, y_test, y_train = train_test_split(\n+    x, y, test_size=0.3, random_state=3)\n+\n+# Creating the decision tree classifier\n+decTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4)\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `decTree`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/decision_tree_churn/churn_cal.py",
    "pr_number": 10243,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1353052293,
    "comment_created_at": "2023-10-10T17:40:28Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+import math,sys\n+\n+def minimum_squares_to_represent_a_number(number: int) -> int:\n+    \"\"\"\n+    Count the number of minimum squares to represent a number\n+    >>> minimum_squares_to_represent_a_number(25)\n+    1\n+    >>> minimum_squares_to_represent_a_number(37)\n+    2\n+    >>> minimum_squares_to_represent_a_number(21)\n+    3\n+    >>> minimum_squares_to_represent_a_number(58)\n+    2\n+    >>> minimum_squares_to_represent_a_number(-1)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: the value of input must be positive\n+    \"\"\"\n+    if number < 0:\n+        raise ValueError(\"the value of input must be positive\")\n+    dp = [-1 for x in range(number+1)]\n+    dp[0] = 0\n+    for i in range(1,number+1):\n+        ans = sys.maxsize\n+        root = int(math.sqrt(i))\n+        for j in range(1,root+1):\n+            currAns = 1 + dp[i-(j**2)]",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: currans",
    "line_number": 27,
    "enriched": "File: dynamic_programming/minimums_squares_to_represent_a_number.py\nCode: @@ -0,0 +1,35 @@\n+import math,sys\n+\n+def minimum_squares_to_represent_a_number(number: int) -> int:\n+    \"\"\"\n+    Count the number of minimum squares to represent a number\n+    >>> minimum_squares_to_represent_a_number(25)\n+    1\n+    >>> minimum_squares_to_represent_a_number(37)\n+    2\n+    >>> minimum_squares_to_represent_a_number(21)\n+    3\n+    >>> minimum_squares_to_represent_a_number(58)\n+    2\n+    >>> minimum_squares_to_represent_a_number(-1)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: the value of input must be positive\n+    \"\"\"\n+    if number < 0:\n+        raise ValueError(\"the value of input must be positive\")\n+    dp = [-1 for x in range(number+1)]\n+    dp[0] = 0\n+    for i in range(1,number+1):\n+        ans = sys.maxsize\n+        root = int(math.sqrt(i))\n+        for j in range(1,root+1):\n+            currAns = 1 + dp[i-(j**2)]\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `currAns`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/minimums_squares_to_represent_a_number.py",
    "pr_number": 7595,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1003217976,
    "comment_created_at": "2022-10-24T11:45:29Z"
  },
  {
    "code": "@@ -0,0 +1,98 @@\n+def calculate_Pi(limit) -> str:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: calculate_pi\n\nplease provide type hint for the parameter: limit",
    "line_number": 1,
    "enriched": "File: maths/pi_generator.py\nCode: @@ -0,0 +1,98 @@\n+def calculate_Pi(limit) -> str:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `calculate_Pi`\n\nPlease provide type hint for the parameter: `limit`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/pi_generator.py",
    "pr_number": 8666,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1169158965,
    "comment_created_at": "2023-04-17T18:56:07Z"
  },
  {
    "code": "@@ -0,0 +1,29 @@\n+\"\"\"\n+# Definition for a Node.\n+class Node:\n+    def __init__(self, val = 0, neighbors = None):\n+        self.val = val\n+        self.neighbors = neighbors if neighbors is not None else []\n+\"\"\"\n+# Input: adjList = [[2,4],[1,3],[2,4],[1,3]]\n+# Output: [[2,4],[1,3],[2,4],[1,3]]\n+\n+class Solution:\n+    def cloneGraph(self, node: 'Node') -> 'Node':",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: clonegraph\n\nas there is no test file in this pull request nor any test function or class in the file graphs/clone_graph.py, please provide doctest for the function clonegraph",
    "line_number": 12,
    "enriched": "File: graphs/clone_graph.py\nCode: @@ -0,0 +1,29 @@\n+\"\"\"\n+# Definition for a Node.\n+class Node:\n+    def __init__(self, val = 0, neighbors = None):\n+        self.val = val\n+        self.neighbors = neighbors if neighbors is not None else []\n+\"\"\"\n+# Input: adjList = [[2,4],[1,3],[2,4],[1,3]]\n+# Output: [[2,4],[1,3],[2,4],[1,3]]\n+\n+class Solution:\n+    def cloneGraph(self, node: 'Node') -> 'Node':\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `cloneGraph`\n\nAs there is no test file in this pull request nor any test function or class in the file `graphs/clone_graph.py`, please provide doctest for the function `cloneGraph`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "graphs/clone_graph.py",
    "pr_number": 7611,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004003931,
    "comment_created_at": "2022-10-25T05:11:45Z"
  },
  {
    "code": "@@ -0,0 +1,129 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 7, 2022\n+\n+Task:\n+You are given a tree root of a binary tree with n nodes, where each node has\n+node.data coins. There are exactly n coins in whole tree. \n+\n+In one move, we may choose two adjacent nodes and move one coin from one node\n+to another. A move may be from parent to child, or from child to parent.\n+\n+Return the minimum number of moves required to make every node have exactly one coin.\n+\n+Example 1:\n+\n+   3\n+  / \\\n+ 0   0\n+\n+Result: 2\n+\n+Example 2:\n+\n+   0\n+  / \\\n+ 3   0\n+\n+Result 3\n+\n+leetcode: https://leetcode.com/problems/distribute-coins-in-binary-tree/\n+\n+Implementation notes:\n+User depth-first search approach.\n+\n+Let n is the number of nodes in tree\n+Runtime: O(n)\n+Space: O(1)\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+from collections import namedtuple\n+\n+\n+@dataclass\n+class TreeNode:\n+    data: float\n+    left: TreeNode | None = None\n+    right: TreeNode | None = None\n+\n+CoinsDistribResult = namedtuple(\"CoinsDistribResult\", 'moves excess')\n+\n+def distributeCoins(root: TreeNode | None) -> int:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: distributecoins",
    "line_number": 54,
    "enriched": "File: data_structures/binary_tree/distribute_coins.py\nCode: @@ -0,0 +1,129 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 7, 2022\n+\n+Task:\n+You are given a tree root of a binary tree with n nodes, where each node has\n+node.data coins. There are exactly n coins in whole tree. \n+\n+In one move, we may choose two adjacent nodes and move one coin from one node\n+to another. A move may be from parent to child, or from child to parent.\n+\n+Return the minimum number of moves required to make every node have exactly one coin.\n+\n+Example 1:\n+\n+   3\n+  / \\\n+ 0   0\n+\n+Result: 2\n+\n+Example 2:\n+\n+   0\n+  / \\\n+ 3   0\n+\n+Result 3\n+\n+leetcode: https://leetcode.com/problems/distribute-coins-in-binary-tree/\n+\n+Implementation notes:\n+User depth-first search approach.\n+\n+Let n is the number of nodes in tree\n+Runtime: O(n)\n+Space: O(1)\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+from collections import namedtuple\n+\n+\n+@dataclass\n+class TreeNode:\n+    data: float\n+    left: TreeNode | None = None\n+    right: TreeNode | None = None\n+\n+CoinsDistribResult = namedtuple(\"CoinsDistribResult\", 'moves excess')\n+\n+def distributeCoins(root: TreeNode | None) -> int:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `distributeCoins`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/binary_tree/distribute_coins.py",
    "pr_number": 7975,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1015048256,
    "comment_created_at": "2022-11-07T06:40:03Z"
  },
  {
    "code": "@@ -0,0 +1,201 @@\n+#!/usr/bin/env python\n+# coding: utf-8\n+\n+# In[5]:\n+\n+\n+import pandas as pd\n+from category_encoders import OrdinalEncoder\n+from lightgbm import LGBMRegressor\n+from sklearn.model_selection import train_test_split\n+from sklearn.ensemble import ExtraTreesRegressor\n+\n+\n+# # Building Supervized Model\n+\n+# In[10]:\n+\n+\n+from shapash.data.data_loader import data_loading\n+house_df, house_dict = data_loading('house_prices')\n+\n+\n+# In[11]:\n+\n+\n+y_df=house_df['SalePrice'].to_frame()\n+X_df=house_df[house_df.columns.difference(['SalePrice'])]",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: x_df",
    "line_number": 27,
    "enriched": "File: machine_learning/shapash.py\nCode: @@ -0,0 +1,201 @@\n+#!/usr/bin/env python\n+# coding: utf-8\n+\n+# In[5]:\n+\n+\n+import pandas as pd\n+from category_encoders import OrdinalEncoder\n+from lightgbm import LGBMRegressor\n+from sklearn.model_selection import train_test_split\n+from sklearn.ensemble import ExtraTreesRegressor\n+\n+\n+# # Building Supervized Model\n+\n+# In[10]:\n+\n+\n+from shapash.data.data_loader import data_loading\n+house_df, house_dict = data_loading('house_prices')\n+\n+\n+# In[11]:\n+\n+\n+y_df=house_df['SalePrice'].to_frame()\n+X_df=house_df[house_df.columns.difference(['SalePrice'])]\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `X_df`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/shapash.py",
    "pr_number": 7351,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996620543,
    "comment_created_at": "2022-10-17T05:17:07Z"
  },
  {
    "code": "@@ -0,0 +1,105 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : December 12, 2022\n+\n+Task:\n+Given a string s and a dictionary of strings wordDict,\n+return true if s can be segmented into a space-separated\n+sequence of one or more dictionary words.\n+\n+Note that the same word in the dictionary may be reused\n+multiple times in the segmentation.\n+\n+Implementation notes: Trie + Dynamic programming up -> down.\n+The Trie keeps all wordDict words. It will be useful for scanning\n+available words for the current position in the string.\n+\n+Leetcode:\n+https://leetcode.com/problems/word-break/description/\n+\n+Runtime: O(n * n)\n+Space: O(n)\n+\"\"\"\n+\n+from functools import lru_cache\n+\n+def wordBreak(s: str, word_dict: list[str]) -> bool:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: wordbreak\n\nplease provide descriptive name for the parameter: s",
    "line_number": 26,
    "enriched": "File: dynamic_programming/word_break.py\nCode: @@ -0,0 +1,105 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : December 12, 2022\n+\n+Task:\n+Given a string s and a dictionary of strings wordDict,\n+return true if s can be segmented into a space-separated\n+sequence of one or more dictionary words.\n+\n+Note that the same word in the dictionary may be reused\n+multiple times in the segmentation.\n+\n+Implementation notes: Trie + Dynamic programming up -> down.\n+The Trie keeps all wordDict words. It will be useful for scanning\n+available words for the current position in the string.\n+\n+Leetcode:\n+https://leetcode.com/problems/word-break/description/\n+\n+Runtime: O(n * n)\n+Space: O(n)\n+\"\"\"\n+\n+from functools import lru_cache\n+\n+def wordBreak(s: str, word_dict: list[str]) -> bool:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `wordBreak`\n\nPlease provide descriptive name for the parameter: `s`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/word_break.py",
    "pr_number": 8039,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1053197917,
    "comment_created_at": "2022-12-20T11:10:42Z"
  },
  {
    "code": "@@ -0,0 +1,31 @@\n+from collections.abc import Sequence\n+\n+\n+def max_subsequence_sum(nums: Sequence[int]) -> int:\n+    \"\"\"Return the maximum possible sum amongst all non - empty subsequences.\n+\n+    Raises:\n+      ValueError: when nums is empty.\n+\n+    >>> max_subsequence_sum([1,2,3,4,-2])\n+    10\n+    >>> max_subsequence_sum([-2, -3, -1, -4, -6])\n+    -1\n+    \"\"\"\n+    if not nums:\n+        raise ValueError(\"Input sequence should not be empty\")\n+\n+    ans = nums[0]\n+    nums_len = len(nums)\n+\n+    for i in range(1, nums_len):",
    "comment": "you only use num_lens once, so this variable isn't partially necessary",
    "line_number": 21,
    "enriched": "File: other/maximum_subsequence.py\nCode: @@ -0,0 +1,31 @@\n+from collections.abc import Sequence\n+\n+\n+def max_subsequence_sum(nums: Sequence[int]) -> int:\n+    \"\"\"Return the maximum possible sum amongst all non - empty subsequences.\n+\n+    Raises:\n+      ValueError: when nums is empty.\n+\n+    >>> max_subsequence_sum([1,2,3,4,-2])\n+    10\n+    >>> max_subsequence_sum([-2, -3, -1, -4, -6])\n+    -1\n+    \"\"\"\n+    if not nums:\n+        raise ValueError(\"Input sequence should not be empty\")\n+\n+    ans = nums[0]\n+    nums_len = len(nums)\n+\n+    for i in range(1, nums_len):\nComment: ```suggestion\r\n    for i in range(1, len(nums)):\r\n```\r\nYou only use `num_lens` once, so this variable isn't partially necessary",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "other/maximum_subsequence.py",
    "pr_number": 7811,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008767920,
    "comment_created_at": "2022-10-29T23:09:09Z"
  },
  {
    "code": "@@ -0,0 +1,38 @@\n+import cv2\n+import mediapipe as mp\n+import time\n+\n+cap = cv2.VideoCapture(0)\n+\n+mpHands = mp.solutions.hands",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: mphands",
    "line_number": 7,
    "enriched": "File: computer_vision/handtracking.py\nCode: @@ -0,0 +1,38 @@\n+import cv2\n+import mediapipe as mp\n+import time\n+\n+cap = cv2.VideoCapture(0)\n+\n+mpHands = mp.solutions.hands\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `mpHands`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "computer_vision/handtracking.py",
    "pr_number": 10131,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349745277,
    "comment_created_at": "2023-10-08T16:48:29Z"
  },
  {
    "code": "@@ -0,0 +1,47 @@\n+import sys\n+\n+\n+def array_equalization(vector: list[int], k: int) -> int:",
    "comment": "self-documenting variable names, please.",
    "line_number": 4,
    "enriched": "File: matrix/matrix_equalization.py\nCode: @@ -0,0 +1,47 @@\n+import sys\n+\n+\n+def array_equalization(vector: list[int], k: int) -> int:\nComment: Self-documenting variable names, please.\r\n```suggestion\r\ndef array_equalization(vector: list[int], step_size: int) -> int:\r\n```",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "matrix/matrix_equalization.py",
    "pr_number": 11360,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1578122014,
    "comment_created_at": "2024-04-24T15:45:15Z"
  },
  {
    "code": "@@ -1,50 +1,97 @@\n \"\"\"\n Numerical integration or quadrature for a smooth function f with known values at x_i\n \n-This method is the classical approach of suming 'Equally Spaced Abscissas'\n+This method is the classical approach of summing 'Equally Spaced Abscissas'\n+\"\"\"\n \n-method 1:\n-\"extended trapezoidal rule\"\n+from collections.abc import Callable, Iterator\n \n-\"\"\"\n \n+def trapezoidal_rule(\n+    f: Callable[[float], float], boundary: list[float], steps: int\n+) -> float:\n+    \"\"\"\n+    \"extended trapezoidal rule\"\n+    int(f) = dx/2 * (f1 + 2f2 + ... + fn)\n+\n+    >>> def func(x): return x ** 2\n+    >>> abs(trapezoidal_rule(func, [0, 1], 10) - 0.335) < 1e-9\n+    True\n+\n+    >>> def func(x): return 1\n+    >>> abs(trapezoidal_rule(func, [0, 10], 100) - 10.0) < 1e-9\n+    True\n+\n+    >>> def func(x): return x\n+    >>> trapezoidal_rule(func, [0, 1], 1)\n+    0.5\n \n-def method_1(boundary, steps):\n-    # \"extended trapezoidal rule\"\n-    # int(f) = dx/2 * (f1 + 2f2 + ... + fn)\n+    >>> trapezoidal_rule(func, [], 10)  # Empty boundary list\n+    Traceback (most recent call last):\n+        ...\n+    IndexError: list index out of range\n+\n+    >>> trapezoidal_rule(func, [0, 1], 0)  # Steps as zero\n+    Traceback (most recent call last):\n+        ...\n+    ZeroDivisionError: division by zero\n+    >>> trapezoidal_rule(func, ['0', '1'], 10)  # Boundary values as strings\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: unsupported operand type(s) for -: 'str' and 'str'\n+\n+    Parameters:\n+    - f (Callable[[float], float]): The function to be integrated.\n+    - boundary (list of float): A two-element list specifying the lower and upper bounds\n+     of the integration interval.\n+    - steps (int): The number of steps (trapezoids) to divide the interval into.\n+\n+    Returns:\n+    - float: The estimated value of the integral over the specified interval.\n+    \"\"\"\n     h = (boundary[1] - boundary[0]) / steps\n     a = boundary[0]\n     b = boundary[1]\n     x_i = make_points(a, b, h)\n     y = 0.0\n     y += (h / 2.0) * f(a)\n     for i in x_i:\n-        # print(i)\n         y += h * f(i)\n     y += (h / 2.0) * f(b)\n     return y\n \n \n-def make_points(a, b, h):\n-    x = a + h\n-    while x < (b - h):\n-        yield x\n-        x = x + h\n+def make_points(a: float, b: float, h: float) -> Iterator[float]:",
    "comment": "could we rename these variables for clarity? rather than single-letter variable names, we could have something like lower_bound, upper_bound, and step_size.",
    "line_number": 64,
    "enriched": "File: maths/trapezoidal_rule.py\nCode: @@ -1,50 +1,97 @@\n \"\"\"\n Numerical integration or quadrature for a smooth function f with known values at x_i\n \n-This method is the classical approach of suming 'Equally Spaced Abscissas'\n+This method is the classical approach of summing 'Equally Spaced Abscissas'\n+\"\"\"\n \n-method 1:\n-\"extended trapezoidal rule\"\n+from collections.abc import Callable, Iterator\n \n-\"\"\"\n \n+def trapezoidal_rule(\n+    f: Callable[[float], float], boundary: list[float], steps: int\n+) -> float:\n+    \"\"\"\n+    \"extended trapezoidal rule\"\n+    int(f) = dx/2 * (f1 + 2f2 + ... + fn)\n+\n+    >>> def func(x): return x ** 2\n+    >>> abs(trapezoidal_rule(func, [0, 1], 10) - 0.335) < 1e-9\n+    True\n+\n+    >>> def func(x): return 1\n+    >>> abs(trapezoidal_rule(func, [0, 10], 100) - 10.0) < 1e-9\n+    True\n+\n+    >>> def func(x): return x\n+    >>> trapezoidal_rule(func, [0, 1], 1)\n+    0.5\n \n-def method_1(boundary, steps):\n-    # \"extended trapezoidal rule\"\n-    # int(f) = dx/2 * (f1 + 2f2 + ... + fn)\n+    >>> trapezoidal_rule(func, [], 10)  # Empty boundary list\n+    Traceback (most recent call last):\n+        ...\n+    IndexError: list index out of range\n+\n+    >>> trapezoidal_rule(func, [0, 1], 0)  # Steps as zero\n+    Traceback (most recent call last):\n+        ...\n+    ZeroDivisionError: division by zero\n+    >>> trapezoidal_rule(func, ['0', '1'], 10)  # Boundary values as strings\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: unsupported operand type(s) for -: 'str' and 'str'\n+\n+    Parameters:\n+    - f (Callable[[float], float]): The function to be integrated.\n+    - boundary (list of float): A two-element list specifying the lower and upper bounds\n+     of the integration interval.\n+    - steps (int): The number of steps (trapezoids) to divide the interval into.\n+\n+    Returns:\n+    - float: The estimated value of the integral over the specified interval.\n+    \"\"\"\n     h = (boundary[1] - boundary[0]) / steps\n     a = boundary[0]\n     b = boundary[1]\n     x_i = make_points(a, b, h)\n     y = 0.0\n     y += (h / 2.0) * f(a)\n     for i in x_i:\n-        # print(i)\n         y += h * f(i)\n     y += (h / 2.0) * f(b)\n     return y\n \n \n-def make_points(a, b, h):\n-    x = a + h\n-    while x < (b - h):\n-        yield x\n-        x = x + h\n+def make_points(a: float, b: float, h: float) -> Iterator[float]:\nComment: Could we rename these variables for clarity? Rather than single-letter variable names, we could have something like `lower_bound`, `upper_bound`, and `step_size`.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "maths/trapezoidal_rule.py",
    "pr_number": 11491,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1705143509,
    "comment_created_at": "2024-08-06T08:39:36Z"
  },
  {
    "code": "@@ -0,0 +1,19 @@\n+ \n+\n+import math\n+def arc(angle: int, r: int) -> int:",
    "comment": "please provide descriptive name for the parameter: r",
    "line_number": 4,
    "enriched": "File: maths/arc_length.py\nCode: @@ -0,0 +1,19 @@\n+ \n+\n+import math\n+def arc(angle: int, r: int) -> int:\nComment: Please provide descriptive name for the parameter: `r`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/arc_length.py",
    "pr_number": 7609,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1003987638,
    "comment_created_at": "2022-10-25T04:34:20Z"
  },
  {
    "code": "@@ -0,0 +1,44 @@\n+def threeSum(nums: list[int]) -> list[list[int]]:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: threesum",
    "line_number": 1,
    "enriched": "File: maths/three_sum.py\nCode: @@ -0,0 +1,44 @@\n+def threeSum(nums: list[int]) -> list[list[int]]:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `threeSum`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/three_sum.py",
    "pr_number": 9177,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342071337,
    "comment_created_at": "2023-10-01T04:24:42Z"
  },
  {
    "code": "@@ -0,0 +1,29 @@\n+rect1 = {\n+    \"x\": 10,\n+    \"y\": 10,\n+    \"height\": 30,\n+    \"width\": 50\n+}\n+\n+rect2 = {\n+    \"x\": 20,\n+    \"y\": 30,\n+    \"height\": 40,\n+    \"width\": 30\n+}\n+\n+def checkCollision(rect1, rect2) -> bool:",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: checkcollision\n\nplease provide type hint for the parameter: rect1\n\nplease provide type hint for the parameter: rect2",
    "line_number": 15,
    "enriched": "File: maths/collision_between_rectangles.py\nCode: @@ -0,0 +1,29 @@\n+rect1 = {\n+    \"x\": 10,\n+    \"y\": 10,\n+    \"height\": 30,\n+    \"width\": 50\n+}\n+\n+rect2 = {\n+    \"x\": 20,\n+    \"y\": 30,\n+    \"height\": 40,\n+    \"width\": 30\n+}\n+\n+def checkCollision(rect1, rect2) -> bool:\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `checkCollision`\n\nPlease provide type hint for the parameter: `rect1`\n\nPlease provide type hint for the parameter: `rect2`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/collision_between_rectangles.py",
    "pr_number": 7830,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008653421,
    "comment_created_at": "2022-10-29T07:29:59Z"
  },
  {
    "code": "@@ -0,0 +1,214 @@\n+\"\"\"https://en.wikipedia.org/wiki/Advanced_Encryption_Standard\"\"\"\n+\n+Sbox = (",
    "comment": "variable and function names should follow the [snake_case](https://en.wikipedia.org/wiki/snake_case) naming convention. please update the following name accordingly: sbox",
    "line_number": 3,
    "enriched": "File: ciphers/aes128.py\nCode: @@ -0,0 +1,214 @@\n+\"\"\"https://en.wikipedia.org/wiki/Advanced_Encryption_Standard\"\"\"\n+\n+Sbox = (\nComment: Variable and function names should follow the [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) naming convention. Please update the following name accordingly: `Sbox`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "ciphers/aes128.py",
    "pr_number": 10812,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367913178,
    "comment_created_at": "2023-10-22T14:12:38Z"
  },
  {
    "code": "@@ -22,14 +22,40 @@ def __init__(self, value: T) -> None:\n \n     @property\n     def value(self) -> T:\n-        \"\"\"Return the value of the node.\"\"\"\n+        \"\"\"\n+        Return the value of the node.\n+\n+        >>> rhn = RandomizedHeapNode(10)\n+        >>> rhn.value\n+        10\n+        >>> rhn = RandomizedHeapNode(-10)\n+        >>> rhn.value\n+        -10\n+        \"\"\"\n         return self._value\n \n     @staticmethod\n     def merge(\n         root1: RandomizedHeapNode[T] | None, root2: RandomizedHeapNode[T] | None\n     ) -> RandomizedHeapNode[T] | None:\n-        \"\"\"Merge 2 nodes together.\"\"\"\n+        \"\"\"\n+        Merge 2 nodes together.\n+\n+        >>> rhn1 = RandomizedHeapNode(10)\n+        >>> rhn2 = RandomizedHeapNode(20)\n+        >>> RandomizedHeapNode.merge(rhn1,rhn2).value\n+        10\n+\n+        >>> rhn1 = RandomizedHeapNode(20)\n+        >>> rhn2 = RandomizedHeapNode(10)\n+        >>> RandomizedHeapNode.merge(rhn1,rhn2).value\n+        10\n+\n+        >>> rhn1 = RandomizedHeapNode(5)\n+        >>> rhn2 = RandomizedHeapNode(0)\n+        >>> RandomizedHeapNode.merge(rhn1,rhn2).value\n+        0",
    "comment": "minor formatting changes",
    "line_number": 57,
    "enriched": "File: data_structures/heap/randomized_heap.py\nCode: @@ -22,14 +22,40 @@ def __init__(self, value: T) -> None:\n \n     @property\n     def value(self) -> T:\n-        \"\"\"Return the value of the node.\"\"\"\n+        \"\"\"\n+        Return the value of the node.\n+\n+        >>> rhn = RandomizedHeapNode(10)\n+        >>> rhn.value\n+        10\n+        >>> rhn = RandomizedHeapNode(-10)\n+        >>> rhn.value\n+        -10\n+        \"\"\"\n         return self._value\n \n     @staticmethod\n     def merge(\n         root1: RandomizedHeapNode[T] | None, root2: RandomizedHeapNode[T] | None\n     ) -> RandomizedHeapNode[T] | None:\n-        \"\"\"Merge 2 nodes together.\"\"\"\n+        \"\"\"\n+        Merge 2 nodes together.\n+\n+        >>> rhn1 = RandomizedHeapNode(10)\n+        >>> rhn2 = RandomizedHeapNode(20)\n+        >>> RandomizedHeapNode.merge(rhn1,rhn2).value\n+        10\n+\n+        >>> rhn1 = RandomizedHeapNode(20)\n+        >>> rhn2 = RandomizedHeapNode(10)\n+        >>> RandomizedHeapNode.merge(rhn1,rhn2).value\n+        10\n+\n+        >>> rhn1 = RandomizedHeapNode(5)\n+        >>> rhn2 = RandomizedHeapNode(0)\n+        >>> RandomizedHeapNode.merge(rhn1,rhn2).value\n+        0\nComment: ```suggestion\r\n        >>> rhn1 = RandomizedHeapNode(10)\r\n        >>> rhn2 = RandomizedHeapNode(20)\r\n        >>> RandomizedHeapNode.merge(rhn1, rhn2).value\r\n        10\r\n\r\n        >>> rhn1 = RandomizedHeapNode(20)\r\n        >>> rhn2 = RandomizedHeapNode(10)\r\n        >>> RandomizedHeapNode.merge(rhn1, rhn2).value\r\n        10\r\n\r\n        >>> rhn1 = RandomizedHeapNode(5)\r\n        >>> rhn2 = RandomizedHeapNode(0)\r\n        >>> RandomizedHeapNode.merge(rhn1, rhn2).value\r\n        0\r\n```\r\nMinor formatting changes",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "data_structures/heap/randomized_heap.py",
    "pr_number": 11151,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1389866846,
    "comment_created_at": "2023-11-10T20:09:32Z"
  },
  {
    "code": "@@ -0,0 +1,43 @@\n+def dfs(grid: list(list(int)), row: int, col: int, visit: set()) -> int:",
    "comment": "is this what you mean?",
    "line_number": 1,
    "enriched": "File: matrix/count_paths.py\nCode: @@ -0,0 +1,43 @@\n+def dfs(grid: list(list(int)), row: int, col: int, visit: set()) -> int:\nComment: ```suggestion\ndef dfs(grid: list[list[int]], row: int, col: int, visit: set) -> int:\n```\nIs this what you mean?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "matrix/count_paths.py",
    "pr_number": 7532,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002595372,
    "comment_created_at": "2022-10-22T23:03:53Z"
  },
  {
    "code": "@@ -22,12 +22,12 @@\n \"\"\"\n \n \n-def combination_sum_iv(n: int, array: list[int], target: int) -> int:\n+def combination_sum_iv(array: list[int], target: int) -> int:",
    "comment": "i guess you can't commit a lot of changes in a single pr !!",
    "line_number": 25,
    "enriched": "File: dynamic_programming/combination_sum_iv.py\nCode: @@ -22,12 +22,12 @@\n \"\"\"\n \n \n-def combination_sum_iv(n: int, array: list[int], target: int) -> int:\n+def combination_sum_iv(array: list[int], target: int) -> int:\nComment: I guess you can't commit a lot of changes in a SINGLE PR !!",
    "subcategory": "false positive",
    "category": "false positive",
    "file_path": "dynamic_programming/combination_sum_iv.py",
    "pr_number": 11321,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1522354138,
    "comment_created_at": "2024-03-13T01:33:51Z"
  },
  {
    "code": "@@ -130,5 +130,5 @@ omit = [\".env/*\"]\n sort = \"Cover\"\n \n [tool.codespell]\n-ignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,mater,secant,som,sur,tim,zar\"\n+ignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,manuel,mater,secant,som,sur,tim,zar\"",
    "comment": "not entirely sure how codespell handles ignored words, but would this help codespell recognize \"manuel\" as a name while still catching  \"manuel\" as a typo?",
    "line_number": 133,
    "enriched": "File: pyproject.toml\nCode: @@ -130,5 +130,5 @@ omit = [\".env/*\"]\n sort = \"Cover\"\n \n [tool.codespell]\n-ignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,mater,secant,som,sur,tim,zar\"\n+ignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,manuel,mater,secant,som,sur,tim,zar\"\nComment: ```suggestion\r\nignore-words-list = \"3rt,ans,crate,damon,fo,followings,hist,iff,kwanza,Manuel,mater,secant,som,sur,tim,zar\"\r\n```\r\nNot entirely sure how codespell handles ignored words, but would this help codespell recognize \"Manuel\" as a name while still catching  \"manuel\" as a typo?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pyproject.toml",
    "pr_number": 9543,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349559334,
    "comment_created_at": "2023-10-07T18:30:59Z"
  },
  {
    "code": "@@ -36,6 +36,8 @@\n def fetch_github_info(auth_token: str) -> dict[Any, Any]:\n     \"\"\"\n     Fetch GitHub info of a user using the requests module",
    "comment": "is that enough or do we need to add more?",
    "line_number": 38,
    "enriched": "File: web_programming/fetch_github_info.py\nCode: @@ -36,6 +36,8 @@\n def fetch_github_info(auth_token: str) -> dict[Any, Any]:\n     \"\"\"\n     Fetch GitHub info of a user using the requests module\nComment: Is that enough or do we need to add more?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "web_programming/fetch_github_info.py",
    "pr_number": 11148,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1385866284,
    "comment_created_at": "2023-11-08T02:02:05Z"
  },
  {
    "code": "@@ -27,16 +24,12 @@ jobs:\n         with:\n           python-version: 3.11 \n           allow-prereleases: true\n-\n       - name: Install dependencies\n       - run: |\n           uv sync --all-extras\n           uv pip list\n-\n       - name: Lint code\n-        run: uv run ruff check , --output-format=github\n-\n-        \n+        run: uv run ruff check , --output-format=github  ",
    "comment": "what does the comma (,) do on this line?",
    "line_number": 32,
    "enriched": "File: .github/workflows/build.yml\nCode: @@ -27,16 +24,12 @@ jobs:\n         with:\n           python-version: 3.11 \n           allow-prereleases: true\n-\n       - name: Install dependencies\n       - run: |\n           uv sync --all-extras\n           uv pip list\n-\n       - name: Lint code\n-        run: uv run ruff check , --output-format=github\n-\n-        \n+        run: uv run ruff check , --output-format=github  \nComment: What does the comma (,) do on this line?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".github/workflows/build.yml",
    "pr_number": 12913,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2295088001,
    "comment_created_at": "2025-08-23T04:05:15Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+\"\"\"\n+Title : Computing the terminal velocity of an object falling\n+        through a fluid.\n+\n+Terminal velocity is defined as the highest velocity attained by an\n+object falling through a fluid. It is observed when the sum of drag force\n+and buoyancy is equal to the downward gravity force acting on the\n+object. The acceleration of the object is zero as the net force acting on\n+the object is zero.\n+\n+Vt = ((2 * m * g)/(\u03c1 * A * Cd))^0.5\n+\n+where :\n+Vt = Terminal velocity (in m/s)\n+m = Mass of the falling object (in Kg)\n+g = Acceleration due to gravity (value taken : 9.8 m/s^2)\n+\u03c1 = Density of the fluid through which the object is falling (in Kg/m^3)\n+A = Projected area of the object (in m^2)\n+Cd = Drag coefficient (dimensionless)\n+\n+Reference : https://byjus.com/physics/derivation-of-terminal-velocity/\n+\"\"\"\n+\n+\n+def terminal_velocity(\n+    mass: float, density: float, area: float, drag_coefficient: float\n+) -> float:\n+    \"\"\"\n+    >>> terminal_velocity(1, 25, 0.6, 0.77)\n+    1.3026778945578592\n+    >>> terminal_velocity(2, 100, 0.45, 0.23)\n+    1.9461345311993645\n+    >>> terminal_velocity(5, 50, 0.2, 0.5)\n+    4.427188724235731\n+    >>> terminal_velocity(-5, 50, -0.2, -2)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: mass, density, area and the drag coefficient all need to be positive\n+    >>> terminal_velocity(3, -20, -1, 2)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: mass, density, area and the drag coefficient all need to be positive\n+    >>> terminal_velocity(-2, -1, -0.44, -1)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: mass, density, area and the drag coefficient all need to be positive\n+    \"\"\"\n+    if mass <= 0 or density <= 0 or area <= 0 or drag_coefficient <= 0:\n+        raise ValueError(\n+            \"mass, density, area and the drag coefficient all need to be positive\"\n+        )\n+    return ((2 * mass * 9.8) / (density * area * drag_coefficient)) ** 0.5",
    "comment": "instead of using a hard-coded 9.8 for the acceleration due to gravity, could you import the constant from scipy.constants? the constant value in this library is more accurate.\r\n\r\nhttps://docs.scipy.org/doc/scipy/reference/constants.html",
    "line_number": 52,
    "enriched": "File: physics/terminal_velocity.py\nCode: @@ -0,0 +1,58 @@\n+\"\"\"\n+Title : Computing the terminal velocity of an object falling\n+        through a fluid.\n+\n+Terminal velocity is defined as the highest velocity attained by an\n+object falling through a fluid. It is observed when the sum of drag force\n+and buoyancy is equal to the downward gravity force acting on the\n+object. The acceleration of the object is zero as the net force acting on\n+the object is zero.\n+\n+Vt = ((2 * m * g)/(\u03c1 * A * Cd))^0.5\n+\n+where :\n+Vt = Terminal velocity (in m/s)\n+m = Mass of the falling object (in Kg)\n+g = Acceleration due to gravity (value taken : 9.8 m/s^2)\n+\u03c1 = Density of the fluid through which the object is falling (in Kg/m^3)\n+A = Projected area of the object (in m^2)\n+Cd = Drag coefficient (dimensionless)\n+\n+Reference : https://byjus.com/physics/derivation-of-terminal-velocity/\n+\"\"\"\n+\n+\n+def terminal_velocity(\n+    mass: float, density: float, area: float, drag_coefficient: float\n+) -> float:\n+    \"\"\"\n+    >>> terminal_velocity(1, 25, 0.6, 0.77)\n+    1.3026778945578592\n+    >>> terminal_velocity(2, 100, 0.45, 0.23)\n+    1.9461345311993645\n+    >>> terminal_velocity(5, 50, 0.2, 0.5)\n+    4.427188724235731\n+    >>> terminal_velocity(-5, 50, -0.2, -2)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: mass, density, area and the drag coefficient all need to be positive\n+    >>> terminal_velocity(3, -20, -1, 2)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: mass, density, area and the drag coefficient all need to be positive\n+    >>> terminal_velocity(-2, -1, -0.44, -1)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: mass, density, area and the drag coefficient all need to be positive\n+    \"\"\"\n+    if mass <= 0 or density <= 0 or area <= 0 or drag_coefficient <= 0:\n+        raise ValueError(\n+            \"mass, density, area and the drag coefficient all need to be positive\"\n+        )\n+    return ((2 * mass * 9.8) / (density * area * drag_coefficient)) ** 0.5\nComment: Instead of using a hard-coded `9.8` for the acceleration due to gravity, could you import the constant from `scipy.constants`? The constant value in this library is more accurate.\r\n\r\nhttps://docs.scipy.org/doc/scipy/reference/constants.html",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "physics/terminal_velocity.py",
    "pr_number": 10237,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368133264,
    "comment_created_at": "2023-10-23T05:00:02Z"
  },
  {
    "code": "@@ -0,0 +1,47 @@\n+\"\"\" A NAND Gate is a logic gate in boolean algebra which results to False(0)\n+    if both the inputs are 1, and True(1) if any of the input is 0.\n+   Following is the truth table of a NAND Gate:\n+   |   Input 1   |   Input 2   |    Output   |\n+   |      0      |      0      |      1      |\n+   |      0      |      1      |      1      |\n+   |      1      |      0      |      1      |\n+   |      1      |      1      |      0      |\n+\n+Following is the code implementation of the NAND Gate\"\"\"\n+\n+\n+def nand_gate(input_1: int, input_2: int) -> int:\n+    \"\"\"\n+    >>> nand_gate(0, 0)\n+    1\n+    >>> nand_gate(0, 1)\n+    1\n+    >>> nand_gate(1, 0)\n+    1\n+    >>> nand_gate(1, 1)\n+    0\n+    >>> nand_gate(0.0, 0.0)\n+    1\n+    >>> nand_gate(0, -7)\n+    0\n+    \"\"\"\n+    return 1 - int(input_1 == input_2 == 1)\n+\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file boolean_algebra/nandgate.py, please provide doctest for the function main",
    "line_number": 31,
    "enriched": "File: boolean_algebra/nandgate.py\nCode: @@ -0,0 +1,47 @@\n+\"\"\" A NAND Gate is a logic gate in boolean algebra which results to False(0)\n+    if both the inputs are 1, and True(1) if any of the input is 0.\n+   Following is the truth table of a NAND Gate:\n+   |   Input 1   |   Input 2   |    Output   |\n+   |      0      |      0      |      1      |\n+   |      0      |      1      |      1      |\n+   |      1      |      0      |      1      |\n+   |      1      |      1      |      0      |\n+\n+Following is the code implementation of the NAND Gate\"\"\"\n+\n+\n+def nand_gate(input_1: int, input_2: int) -> int:\n+    \"\"\"\n+    >>> nand_gate(0, 0)\n+    1\n+    >>> nand_gate(0, 1)\n+    1\n+    >>> nand_gate(1, 0)\n+    1\n+    >>> nand_gate(1, 1)\n+    0\n+    >>> nand_gate(0.0, 0.0)\n+    1\n+    >>> nand_gate(0, -7)\n+    0\n+    \"\"\"\n+    return 1 - int(input_1 == input_2 == 1)\n+\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `boolean_algebra/nandgate.py`, please provide doctest for the function `main`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "boolean_algebra/nandgate.py",
    "pr_number": 7286,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996399081,
    "comment_created_at": "2022-10-16T06:55:21Z"
  },
  {
    "code": "@@ -0,0 +1,77 @@\n+# Reference - https://en.wikipedia.org/wiki/Fixed-priority_pre-emptive_scheduling\n+\n+from __future__ import annotations\n+\n+def calculate_waiting_times(duration_times: list[int], priorities: list[int]) -> list[int]:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file scheduling/fixed_priority_pre_emptive_scheduling.py, please provide doctest for the function calculate_waiting_times",
    "line_number": 5,
    "enriched": "File: scheduling/fixed_priority_pre_emptive_scheduling.py\nCode: @@ -0,0 +1,77 @@\n+# Reference - https://en.wikipedia.org/wiki/Fixed-priority_pre-emptive_scheduling\n+\n+from __future__ import annotations\n+\n+def calculate_waiting_times(duration_times: list[int], priorities: list[int]) -> list[int]:\nComment: As there is no test file in this pull request nor any test function or class in the file `scheduling/fixed_priority_pre_emptive_scheduling.py`, please provide doctest for the function `calculate_waiting_times`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "scheduling/fixed_priority_pre_emptive_scheduling.py",
    "pr_number": 10536,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359879648,
    "comment_created_at": "2023-10-15T13:34:36Z"
  },
  {
    "code": "@@ -0,0 +1,101 @@\n+\"\"\"\n+Snake-Water-Gun Game\n+\n+This program allows a user to play the classic Snake-Water-Gun game against the computer.\n+The user can play multiple rounds, and scores are displayed at the end.\n+\n+Fixes Issue: #12987\n+\"\"\"\n+\n+import random\n+from typing import Literal\n+\n+\n+def snake_water_gun(\n+    user_choice: Literal[\"s\", \"w\", \"g\"], computer_choice: Literal[\"s\", \"w\", \"g\"]\n+) -> str:\n+    \"\"\"\n+    Determines the winner between user and computer.\n+\n+    Parameters:\n+    user_choice (str): 's' for Snake, 'w' for Water, 'g' for Gun\n+    computer_choice (str): 's' for Snake, 'w' for Water, 'g' for Gun\n+\n+    Returns:\n+    str: Result message - 'Draw', 'You win!', or 'Computer wins!'\n+\n+    >>> snake_water_gun('s', 'w')\n+    'You win!'\n+    >>> snake_water_gun('g', 's')\n+    'You win!'\n+    >>> snake_water_gun('w', 'w')\n+    'Draw'\n+    \"\"\"\n+    if user_choice == computer_choice:\n+        return \"Draw\"\n+    if (user_choice, computer_choice) in [(\"s\", \"w\"), (\"w\", \"g\"), (\"g\", \"s\")]:\n+        return \"You win!\"\n+    return \"Computer wins!\"\n+\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file snake-game/snake_game.py, please provide doctest for the function main",
    "line_number": 41,
    "enriched": "File: snake-game/snake_game.py\nCode: @@ -0,0 +1,101 @@\n+\"\"\"\n+Snake-Water-Gun Game\n+\n+This program allows a user to play the classic Snake-Water-Gun game against the computer.\n+The user can play multiple rounds, and scores are displayed at the end.\n+\n+Fixes Issue: #12987\n+\"\"\"\n+\n+import random\n+from typing import Literal\n+\n+\n+def snake_water_gun(\n+    user_choice: Literal[\"s\", \"w\", \"g\"], computer_choice: Literal[\"s\", \"w\", \"g\"]\n+) -> str:\n+    \"\"\"\n+    Determines the winner between user and computer.\n+\n+    Parameters:\n+    user_choice (str): 's' for Snake, 'w' for Water, 'g' for Gun\n+    computer_choice (str): 's' for Snake, 'w' for Water, 'g' for Gun\n+\n+    Returns:\n+    str: Result message - 'Draw', 'You win!', or 'Computer wins!'\n+\n+    >>> snake_water_gun('s', 'w')\n+    'You win!'\n+    >>> snake_water_gun('g', 's')\n+    'You win!'\n+    >>> snake_water_gun('w', 'w')\n+    'Draw'\n+    \"\"\"\n+    if user_choice == computer_choice:\n+        return \"Draw\"\n+    if (user_choice, computer_choice) in [(\"s\", \"w\"), (\"w\", \"g\"), (\"g\", \"s\")]:\n+        return \"You win!\"\n+    return \"Computer wins!\"\n+\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `snake-game/snake_game.py`, please provide doctest for the function `main`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "snake-game/snake_game.py",
    "pr_number": 13126,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2400961736,
    "comment_created_at": "2025-10-03T06:45:43Z"
  },
  {
    "code": "@@ -0,0 +1,46 @@\n+# CatBoost Classifier Example\n+from matplotlib import pyplot as plt\n+from sklearn.datasets import load_iris\n+from sklearn.metrics import plot_confusion_matrix\n+from sklearn.model_selection import train_test_split\n+from catboost import CatBoostClassifier\n+\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/catboostclassifier.py, please provide doctest for the function main",
    "line_number": 9,
    "enriched": "File: machine_learning/catboostclassifier.py\nCode: @@ -0,0 +1,46 @@\n+# CatBoost Classifier Example\n+from matplotlib import pyplot as plt\n+from sklearn.datasets import load_iris\n+from sklearn.metrics import plot_confusion_matrix\n+from sklearn.model_selection import train_test_split\n+from catboost import CatBoostClassifier\n+\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/catboostclassifier.py`, please provide doctest for the function `main`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/catboostclassifier.py",
    "pr_number": 7110,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994583402,
    "comment_created_at": "2022-10-13T12:33:07Z"
  },
  {
    "code": "@@ -0,0 +1,119 @@\n+\"\"\"\n+Illustrate how to invert a binary tree.\n+Author: Pranav Gor\n+https://www.geeksforgeeks.org/write-an-efficient-c-function-to-convert-a-tree-into-its-mirror-tree/\n+\"\"\"\n+\n+\n+class Node:\n+    \"\"\"Defining a BinaryTree Node\"\"\"\n+\n+    def __init__(self, data: int) -> None:\n+        self.data = data\n+        self.left: Node | None = None\n+        self.right: Node | None = None\n+\n+\n+def insert(node: Node | None, value: int) -> Node | None:\n+    \"\"\"\n+    >>> root = Node(12345)\n+    >>> root_after_insertion = insert(root, 67890)\n+    >>> root.left == root_after_insertion.left\n+    True\n+    >>> root.right == root_after_insertion.right\n+    True\n+    >>> root.data == root_after_insertion.data\n+    True\n+    >>> root = insert(None,12345)\n+    >>> root.data\n+    12345\n+    \"\"\"\n+\n+    # If the tree is empty, set the new as root.\n+    if node is None:\n+        node = Node(value)\n+        return node\n+\n+    # Since tree is not empty, we insert the new node\n+    # If the value of the new node < that of the current node,\n+    # add it to the left subtree and proceed recursively.\n+    if value < node.data:\n+        node.left = insert(node.left, value)\n+\n+    # Else, if the value of the new node >= that of the current node,\n+    # add it to the right subtree and proceed recursively.\n+    else:\n+        node.right = insert(node.right, value)\n+    return node\n+\n+\n+def make_tree() -> Node | None:\n+    \"\"\"\n+    # Creating a binary search tree\n+    >>> root = make_tree()\n+    >>> root.data\n+    5\n+    >>> root.left.data\n+    3\n+    >>> root.right.data\n+    7\n+    \"\"\"\n+    root = Node(5)\n+    insert(root, 3)\n+    insert(root, 7)\n+    insert(root, 4)\n+    insert(root, 6)\n+    insert(root, 2)\n+    insert(root, 8)\n+    insert(root, 1)\n+    insert(root, 9)\n+    return root\n+\n+\n+def invert(root: Node | None) -> Node | None:  # if node is None, return None\n+    \"\"\"\n+    >>> root = make_tree()\n+    >>> root = invert(root)\n+    >>> root.data\n+    5\n+    >>> seven = root.left\n+    >>> seven.data\n+    7\n+    >>> three = root.right\n+    >>> three.data\n+    3\n+    >>> eight = seven.left\n+    >>> eight.data\n+    8\n+    >>> six = seven.right\n+    >>> six.data\n+    6\n+    >>> four = three.left\n+    >>> four.data\n+    4\n+    >>> two = three.right\n+    >>> two.data\n+    2\n+    \"\"\"\n+\n+    # If current node exists, we swap the left and right child\n+    # and recursively invert the left and right subtree to invert the entire tree.\n+    if root:\n+        root.left, root.right = root.right, root.left\n+        invert(root.left)\n+        invert(root.right)\n+\n+    return root\n+\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/binary_tree/invert_binary_tree.py, please provide doctest for the function main",
    "line_number": 109,
    "enriched": "File: data_structures/binary_tree/invert_binary_tree.py\nCode: @@ -0,0 +1,119 @@\n+\"\"\"\n+Illustrate how to invert a binary tree.\n+Author: Pranav Gor\n+https://www.geeksforgeeks.org/write-an-efficient-c-function-to-convert-a-tree-into-its-mirror-tree/\n+\"\"\"\n+\n+\n+class Node:\n+    \"\"\"Defining a BinaryTree Node\"\"\"\n+\n+    def __init__(self, data: int) -> None:\n+        self.data = data\n+        self.left: Node | None = None\n+        self.right: Node | None = None\n+\n+\n+def insert(node: Node | None, value: int) -> Node | None:\n+    \"\"\"\n+    >>> root = Node(12345)\n+    >>> root_after_insertion = insert(root, 67890)\n+    >>> root.left == root_after_insertion.left\n+    True\n+    >>> root.right == root_after_insertion.right\n+    True\n+    >>> root.data == root_after_insertion.data\n+    True\n+    >>> root = insert(None,12345)\n+    >>> root.data\n+    12345\n+    \"\"\"\n+\n+    # If the tree is empty, set the new as root.\n+    if node is None:\n+        node = Node(value)\n+        return node\n+\n+    # Since tree is not empty, we insert the new node\n+    # If the value of the new node < that of the current node,\n+    # add it to the left subtree and proceed recursively.\n+    if value < node.data:\n+        node.left = insert(node.left, value)\n+\n+    # Else, if the value of the new node >= that of the current node,\n+    # add it to the right subtree and proceed recursively.\n+    else:\n+        node.right = insert(node.right, value)\n+    return node\n+\n+\n+def make_tree() -> Node | None:\n+    \"\"\"\n+    # Creating a binary search tree\n+    >>> root = make_tree()\n+    >>> root.data\n+    5\n+    >>> root.left.data\n+    3\n+    >>> root.right.data\n+    7\n+    \"\"\"\n+    root = Node(5)\n+    insert(root, 3)\n+    insert(root, 7)\n+    insert(root, 4)\n+    insert(root, 6)\n+    insert(root, 2)\n+    insert(root, 8)\n+    insert(root, 1)\n+    insert(root, 9)\n+    return root\n+\n+\n+def invert(root: Node | None) -> Node | None:  # if node is None, return None\n+    \"\"\"\n+    >>> root = make_tree()\n+    >>> root = invert(root)\n+    >>> root.data\n+    5\n+    >>> seven = root.left\n+    >>> seven.data\n+    7\n+    >>> three = root.right\n+    >>> three.data\n+    3\n+    >>> eight = seven.left\n+    >>> eight.data\n+    8\n+    >>> six = seven.right\n+    >>> six.data\n+    6\n+    >>> four = three.left\n+    >>> four.data\n+    4\n+    >>> two = three.right\n+    >>> two.data\n+    2\n+    \"\"\"\n+\n+    # If current node exists, we swap the left and right child\n+    # and recursively invert the left and right subtree to invert the entire tree.\n+    if root:\n+        root.left, root.right = root.right, root.left\n+        invert(root.left)\n+        invert(root.right)\n+\n+    return root\n+\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/binary_tree/invert_binary_tree.py`, please provide doctest for the function `main`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/binary_tree/invert_binary_tree.py",
    "pr_number": 7774,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1007725767,
    "comment_created_at": "2022-10-28T07:10:42Z"
  },
  {
    "code": "@@ -0,0 +1,77 @@\n+\"\"\"\n+________________________________________________________________________________________\n+Magnetic flux (\u03a6) is a scalar quantity that measures the number of magnetic field\n+lines (B) that pass through a closed area (A). Furthermore, the magnetic flux depends\n+on the angle formed between the magnetic field and the normal line (N) in area A.\n+Check out the formula used to calculate this flux:\n+ ------------\n+ | \u03a6 = B.A.cos(\u03b8) |\n+ ------------\n+\n+\u03a6 = magnetic flux (weber (Wb) or tesla square meter (T.m\u00b2))\n+B = magnetic field (tesla (T))\n+A = area (square meter (m\u00b2))\n+\u03b8 = angle between magnetic field and normal line (degrees (\u00b0))\n+\n+(Description adapted from https://en.wikipedia.org/wiki/Ideal_gas_law )\n+\"\"\"\n+\n+from math import cos, radians\n+\n+def check_args(magnetic_field: float, area: float, angle: float) -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file physics/magnetic_flux.py, please provide doctest for the function check_args",
    "line_number": 21,
    "enriched": "File: physics/magnetic_flux.py\nCode: @@ -0,0 +1,77 @@\n+\"\"\"\n+________________________________________________________________________________________\n+Magnetic flux (\u03a6) is a scalar quantity that measures the number of magnetic field\n+lines (B) that pass through a closed area (A). Furthermore, the magnetic flux depends\n+on the angle formed between the magnetic field and the normal line (N) in area A.\n+Check out the formula used to calculate this flux:\n+ ------------\n+ | \u03a6 = B.A.cos(\u03b8) |\n+ ------------\n+\n+\u03a6 = magnetic flux (weber (Wb) or tesla square meter (T.m\u00b2))\n+B = magnetic field (tesla (T))\n+A = area (square meter (m\u00b2))\n+\u03b8 = angle between magnetic field and normal line (degrees (\u00b0))\n+\n+(Description adapted from https://en.wikipedia.org/wiki/Ideal_gas_law )\n+\"\"\"\n+\n+from math import cos, radians\n+\n+def check_args(magnetic_field: float, area: float, angle: float) -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `physics/magnetic_flux.py`, please provide doctest for the function `check_args`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "physics/magnetic_flux.py",
    "pr_number": 13196,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2404234907,
    "comment_created_at": "2025-10-05T01:51:19Z"
  },
  {
    "code": "@@ -20,31 +20,28 @@\n class Tableau:\n     \"\"\"Operate on simplex tableaus\n \n-    >>> t = Tableau(np.array([[-1,-1,0,0,-1],[1,3,1,0,4],[3,1,0,1,4.]]), 2)\n+    >>> Tableau(np.array([[-1,-1,0,0,-1],[1,3,1,0,4],[3,1,0,1,4.]]), 2, 2)\n     Traceback (most recent call last):\n     ...\n     ValueError: RHS must be > 0\n     \"\"\"\n \n-    def __init__(self, tableau: np.ndarray, n_vars: int) -> None:\n+    def __init__(self, tableau: np.ndarray, n_vars: int, n_art_vars: int) -> None:",
    "comment": "what does art mean in this context and why does the reader have to guess?\r\nplease use real words even it if makes the variable longer.  https://word.tips/words-start-with/art",
    "line_number": 29,
    "enriched": "File: linear_programming/simplex.py\nCode: @@ -20,31 +20,28 @@\n class Tableau:\n     \"\"\"Operate on simplex tableaus\n \n-    >>> t = Tableau(np.array([[-1,-1,0,0,-1],[1,3,1,0,4],[3,1,0,1,4.]]), 2)\n+    >>> Tableau(np.array([[-1,-1,0,0,-1],[1,3,1,0,4],[3,1,0,1,4.]]), 2, 2)\n     Traceback (most recent call last):\n     ...\n     ValueError: RHS must be > 0\n     \"\"\"\n \n-    def __init__(self, tableau: np.ndarray, n_vars: int) -> None:\n+    def __init__(self, tableau: np.ndarray, n_vars: int, n_art_vars: int) -> None:\nComment: What does `art` mean in this context and why does the reader have to guess?\r\nPlease use real words even it if makes the variable longer.  https://word.tips/words-start-with/art",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "linear_programming/simplex.py",
    "pr_number": 8843,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1271280188,
    "comment_created_at": "2023-07-22T10:39:23Z"
  },
  {
    "code": "@@ -0,0 +1,46 @@\n+\n+\n+import numpy as np\n+",
    "comment": "as there is no test file in this pull request nor any test function or class in the file sorts/hilbert_sort.py, please provide doctest for the function hilbert_sort",
    "line_number": 4,
    "enriched": "File: sorts/hilbert_sort.py\nCode: @@ -0,0 +1,46 @@\n+\n+\n+import numpy as np\n+\nComment: As there is no test file in this pull request nor any test function or class in the file `sorts/hilbert_sort.py`, please provide doctest for the function `hilbert_sort`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "sorts/hilbert_sort.py",
    "pr_number": 8176,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1132339875,
    "comment_created_at": "2023-03-10T12:57:26Z"
  },
  {
    "code": "@@ -0,0 +1,82 @@\n+def hex_char(value: int) -> str:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file conversions/octal_to_hexadecimal.py, please provide doctest for the function hex_char",
    "line_number": 1,
    "enriched": "File: conversions/octal_to_hexadecimal.py\nCode: @@ -0,0 +1,82 @@\n+def hex_char(value: int) -> str:\nComment: As there is no test file in this pull request nor any test function or class in the file `conversions/octal_to_hexadecimal.py`, please provide doctest for the function `hex_char`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "conversions/octal_to_hexadecimal.py",
    "pr_number": 10532,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359864263,
    "comment_created_at": "2023-10-15T12:01:07Z"
  },
  {
    "code": "@@ -0,0 +1,241 @@\n+import re\n+\n+\"\"\"\n+// Normalization enhances data accuracy and integrity, simplifying database navigation\n+\n+// This program takes the input string and normalize it.\n+// This program removes whitespaces, numbers, punctuations from the string\n+\"\"\"\n+\n+# English stop words\n+stop_words = [\n+    \"a\",\n+    \"an\",\n+    \"needn\",\n+    \"couldn't\",\n+    \"very\",\n+    \"yours\",\n+    \"all\",\n+    \"be\",\n+    \"should\",\n+    \"are\",\n+    \"his\",\n+    \"with\",\n+    \"once\",\n+    \"which\",\n+    \"been\",\n+    \"between\",\n+    \"your\",\n+    \"only\",\n+    \"here\",\n+    \"does\",\n+    \"why\",\n+    \"them\",\n+    \"shan't\",\n+    \"herself\",\n+    \"have\",\n+    \"shouldn\",\n+    \"doing\",\n+    \"out\",\n+    \"our\",\n+    \"myself\",\n+    \"there\",\n+    \"most\",\n+    \"d\",\n+    \"from\",\n+    \"hadn't\",\n+    \"themselves\",\n+    \"that'll\",\n+    \"for\",\n+    \"she's\",\n+    \"can\",\n+    \"its\",\n+    \"below\",\n+    \"the\",\n+    \"to\",\n+    \"any\",\n+    \"it\",\n+    \"after\",\n+    \"down\",\n+    \"into\",\n+    \"we\",\n+    \"no\",\n+    \"other\",\n+    \"who\",\n+    \"wasn\",\n+    \"wasn't\",\n+    \"own\",\n+    \"did\",\n+    \"couldn\",\n+    \"few\",\n+    \"ain\",\n+    \"not\",\n+    \"up\",\n+    \"so\",\n+    \"aren't\",\n+    \"didn't\",\n+    \"same\",\n+    \"it's\",\n+    \"more\",\n+    \"hadn\",\n+    \"haven't\",\n+    \"will\",\n+    \"needn't\",\n+    \"ll\",\n+    \"my\",\n+    \"just\",\n+    \"because\",\n+    \"doesn't\",\n+    \"s\",\n+    \"him\",\n+    \"her\",\n+    \"theirs\",\n+    \"their\",\n+    \"too\",\n+    \"hers\",\n+    \"until\",\n+    \"m\",\n+    \"over\",\n+    \"under\",\n+    \"you'll\",\n+    \"isn't\",\n+    \"what\",\n+    \"about\",\n+    \"mightn't\",\n+    \"yourself\",\n+    \"that\",\n+    \"me\",\n+    \"ours\",\n+    \"then\",\n+    \"mustn\",\n+    \"of\",\n+    \"re\",\n+    \"don\",\n+    \"than\",\n+    \"won't\",\n+    \"as\",\n+    \"hasn't\",\n+    \"or\",\n+    \"being\",\n+    \"in\",\n+    \"such\",\n+    \"you're\",\n+    \"through\",\n+    \"won\",\n+    \"ourselves\",\n+    \"she\",\n+    \"nor\",\n+    \"himself\",\n+    \"do\",\n+    \"t\",\n+    \"both\",\n+    \"again\",\n+    \"each\",\n+    \"off\",\n+    \"at\",\n+    \"o\",\n+    \"shan\",\n+    \"am\",\n+    \"this\",\n+    \"aren\",\n+    \"now\",\n+    \"you've\",\n+    \"wouldn't\",\n+    \"mightn\",\n+    \"shouldn't\",\n+    \"before\",\n+    \"haven\",\n+    \"they\",\n+    \"when\",\n+    \"some\",\n+    \"don't\",\n+    \"you'd\",\n+    \"should've\",\n+    \"where\",\n+    \"but\",\n+    \"yourselves\",\n+    \"by\",\n+    \"you\",\n+    \"and\",\n+    \"has\",\n+    \"isn\",\n+    \"on\",\n+    \"didn\",\n+    \"against\",\n+    \"having\",\n+    \"further\",\n+    \"how\",\n+    \"he\",\n+    \"wouldn\",\n+    \"weren\",\n+    \"was\",\n+    \"is\",\n+    \"itself\",\n+    \"those\",\n+    \"mustn't\",\n+    \"i\",\n+    \"were\",\n+    \"doesn\",\n+    \"these\",\n+    \"above\",\n+    \"y\",\n+    \"ma\",\n+    \"ve\",\n+    \"while\",\n+    \"whom\",\n+    \"had\",\n+    \"during\",\n+    \"if\",\n+    \"weren't\",\n+    \"has\",\n+]\n+\n+\n+def normalize_string(text: str) -> list:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file strings/normalize_string.py, please provide doctest for the function normalize_string",
    "line_number": 194,
    "enriched": "File: strings/normalize_string.py\nCode: @@ -0,0 +1,241 @@\n+import re\n+\n+\"\"\"\n+// Normalization enhances data accuracy and integrity, simplifying database navigation\n+\n+// This program takes the input string and normalize it.\n+// This program removes whitespaces, numbers, punctuations from the string\n+\"\"\"\n+\n+# English stop words\n+stop_words = [\n+    \"a\",\n+    \"an\",\n+    \"needn\",\n+    \"couldn't\",\n+    \"very\",\n+    \"yours\",\n+    \"all\",\n+    \"be\",\n+    \"should\",\n+    \"are\",\n+    \"his\",\n+    \"with\",\n+    \"once\",\n+    \"which\",\n+    \"been\",\n+    \"between\",\n+    \"your\",\n+    \"only\",\n+    \"here\",\n+    \"does\",\n+    \"why\",\n+    \"them\",\n+    \"shan't\",\n+    \"herself\",\n+    \"have\",\n+    \"shouldn\",\n+    \"doing\",\n+    \"out\",\n+    \"our\",\n+    \"myself\",\n+    \"there\",\n+    \"most\",\n+    \"d\",\n+    \"from\",\n+    \"hadn't\",\n+    \"themselves\",\n+    \"that'll\",\n+    \"for\",\n+    \"she's\",\n+    \"can\",\n+    \"its\",\n+    \"below\",\n+    \"the\",\n+    \"to\",\n+    \"any\",\n+    \"it\",\n+    \"after\",\n+    \"down\",\n+    \"into\",\n+    \"we\",\n+    \"no\",\n+    \"other\",\n+    \"who\",\n+    \"wasn\",\n+    \"wasn't\",\n+    \"own\",\n+    \"did\",\n+    \"couldn\",\n+    \"few\",\n+    \"ain\",\n+    \"not\",\n+    \"up\",\n+    \"so\",\n+    \"aren't\",\n+    \"didn't\",\n+    \"same\",\n+    \"it's\",\n+    \"more\",\n+    \"hadn\",\n+    \"haven't\",\n+    \"will\",\n+    \"needn't\",\n+    \"ll\",\n+    \"my\",\n+    \"just\",\n+    \"because\",\n+    \"doesn't\",\n+    \"s\",\n+    \"him\",\n+    \"her\",\n+    \"theirs\",\n+    \"their\",\n+    \"too\",\n+    \"hers\",\n+    \"until\",\n+    \"m\",\n+    \"over\",\n+    \"under\",\n+    \"you'll\",\n+    \"isn't\",\n+    \"what\",\n+    \"about\",\n+    \"mightn't\",\n+    \"yourself\",\n+    \"that\",\n+    \"me\",\n+    \"ours\",\n+    \"then\",\n+    \"mustn\",\n+    \"of\",\n+    \"re\",\n+    \"don\",\n+    \"than\",\n+    \"won't\",\n+    \"as\",\n+    \"hasn't\",\n+    \"or\",\n+    \"being\",\n+    \"in\",\n+    \"such\",\n+    \"you're\",\n+    \"through\",\n+    \"won\",\n+    \"ourselves\",\n+    \"she\",\n+    \"nor\",\n+    \"himself\",\n+    \"do\",\n+    \"t\",\n+    \"both\",\n+    \"again\",\n+    \"each\",\n+    \"off\",\n+    \"at\",\n+    \"o\",\n+    \"shan\",\n+    \"am\",\n+    \"this\",\n+    \"aren\",\n+    \"now\",\n+    \"you've\",\n+    \"wouldn't\",\n+    \"mightn\",\n+    \"shouldn't\",\n+    \"before\",\n+    \"haven\",\n+    \"they\",\n+    \"when\",\n+    \"some\",\n+    \"don't\",\n+    \"you'd\",\n+    \"should've\",\n+    \"where\",\n+    \"but\",\n+    \"yourselves\",\n+    \"by\",\n+    \"you\",\n+    \"and\",\n+    \"has\",\n+    \"isn\",\n+    \"on\",\n+    \"didn\",\n+    \"against\",\n+    \"having\",\n+    \"further\",\n+    \"how\",\n+    \"he\",\n+    \"wouldn\",\n+    \"weren\",\n+    \"was\",\n+    \"is\",\n+    \"itself\",\n+    \"those\",\n+    \"mustn't\",\n+    \"i\",\n+    \"were\",\n+    \"doesn\",\n+    \"these\",\n+    \"above\",\n+    \"y\",\n+    \"ma\",\n+    \"ve\",\n+    \"while\",\n+    \"whom\",\n+    \"had\",\n+    \"during\",\n+    \"if\",\n+    \"weren't\",\n+    \"has\",\n+]\n+\n+\n+def normalize_string(text: str) -> list:\nComment: As there is no test file in this pull request nor any test function or class in the file `strings/normalize_string.py`, please provide doctest for the function `normalize_string`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "strings/normalize_string.py",
    "pr_number": 10507,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359840869,
    "comment_created_at": "2023-10-15T09:24:14Z"
  },
  {
    "code": "@@ -10,8 +10,8 @@\n \"\"\"\n \n \n-# Acceleration Constant on Earth (unit m/s^2)\n-g = 9.80665\n+# Acceleration Constant on Earth (unit m/s^2) imported from scipy\n+from scipy.constants import g",
    "comment": "please revert this change.  let's not create a dependency on all of scipy just to set a constant.",
    "line_number": 14,
    "enriched": "File: physics/archimedes_principle.py\nCode: @@ -10,8 +10,8 @@\n \"\"\"\n \n \n-# Acceleration Constant on Earth (unit m/s^2)\n-g = 9.80665\n+# Acceleration Constant on Earth (unit m/s^2) imported from scipy\n+from scipy.constants import g\nComment: Please revert this change.  Let's not create a dependency on all of scipy just to set a constant.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "physics/archimedes_principle.py",
    "pr_number": 10479,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359781399,
    "comment_created_at": "2023-10-15T06:01:02Z"
  },
  {
    "code": "@@ -0,0 +1,99 @@\n+\"\"\"\n+Author  : ilyas dahhou\n+Date    : Oct 7, 2023\n+\n+Task:\n+Given an input string (s) and a pattern (p), implement wildcard\n+pattern matching with support for '?' and '*' where:\n+\n+'?' matches any single character.\n+'*' matches any sequence of characters (including the empty sequence).\n+The matching should cover the entire input string (not partial).\n+\n+Implementation notes:\n+implementation Dynamic Programming up bottom approach.\n+\n+Runtime complexity:O(m * n)\n+\n+The implementation was tested on the\n+leetcode: https://leetcode.com/problems/wildcard-matching/\n+\n+\n+wildcard matching\n+Dynamic Programming: top -> down.\n+\n+\"\"\"\n+import datetime\n+\n+import pytz\n+\n+tz_maroc = pytz.timezone(\"Africa/Casablanca\")\n+\n+now_maroc = datetime.datetime.now(tz=tz_maroc)\n+\n+\n+def is_match(s: str, p: str) -> bool:",
    "comment": "please provide descriptive name for the parameter: s\n\nplease provide descriptive name for the parameter: p",
    "line_number": 35,
    "enriched": "File: dynamic_programming/wildcard_matching.py\nCode: @@ -0,0 +1,99 @@\n+\"\"\"\n+Author  : ilyas dahhou\n+Date    : Oct 7, 2023\n+\n+Task:\n+Given an input string (s) and a pattern (p), implement wildcard\n+pattern matching with support for '?' and '*' where:\n+\n+'?' matches any single character.\n+'*' matches any sequence of characters (including the empty sequence).\n+The matching should cover the entire input string (not partial).\n+\n+Implementation notes:\n+implementation Dynamic Programming up bottom approach.\n+\n+Runtime complexity:O(m * n)\n+\n+The implementation was tested on the\n+leetcode: https://leetcode.com/problems/wildcard-matching/\n+\n+\n+wildcard matching\n+Dynamic Programming: top -> down.\n+\n+\"\"\"\n+import datetime\n+\n+import pytz\n+\n+tz_maroc = pytz.timezone(\"Africa/Casablanca\")\n+\n+now_maroc = datetime.datetime.now(tz=tz_maroc)\n+\n+\n+def is_match(s: str, p: str) -> bool:\nComment: Please provide descriptive name for the parameter: `s`\n\nPlease provide descriptive name for the parameter: `p`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/wildcard_matching.py",
    "pr_number": 10052,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349568581,
    "comment_created_at": "2023-10-07T20:03:48Z"
  },
  {
    "code": "@@ -0,0 +1,82 @@\n+\"\"\"",
    "comment": "hi, you are also changing this file. can you create the different pull request for different files.",
    "line_number": 1,
    "enriched": "File: data_structures/binary_tree/floor_ceil_in_bst.py\nCode: @@ -0,0 +1,82 @@\n+\"\"\"\nComment: Hi, You are also changing this file. Can you create the different pull request for different files.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "data_structures/binary_tree/floor_ceil_in_bst.py",
    "pr_number": 10725,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367935118,
    "comment_created_at": "2023-10-22T16:44:06Z"
  },
  {
    "code": "@@ -0,0 +1,78 @@\n+from sympy.abc import x\n+from sympy import diff\n+def bisection_method(function: str, a: float, b: float) -> float:",
    "comment": "please provide descriptive name for the parameter: a\n\nplease provide descriptive name for the parameter: b",
    "line_number": 3,
    "enriched": "File: arithmetic_analysis/bisection_method.py\nCode: @@ -0,0 +1,78 @@\n+from sympy.abc import x\n+from sympy import diff\n+def bisection_method(function: str, a: float, b: float) -> float:\nComment: Please provide descriptive name for the parameter: `a`\n\nPlease provide descriptive name for the parameter: `b`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "arithmetic_analysis/bisection_method.py",
    "pr_number": 8142,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1109076375,
    "comment_created_at": "2023-02-16T22:15:31Z"
  },
  {
    "code": "@@ -0,0 +1,31 @@\n+# All permutations:\n+def permute(nums: list[int]) -> list[list[int]]:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/arrays/permutations.py, please provide doctest for the function permute",
    "line_number": 2,
    "enriched": "File: data_structures/Arrays/permutations.py\nCode: @@ -0,0 +1,31 @@\n+# All permutations:\n+def permute(nums: list[int]) -> list[list[int]]:\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/Arrays/permutations.py`, please provide doctest for the function `permute`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/Arrays/permutations.py",
    "pr_number": 7614,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004042079,
    "comment_created_at": "2022-10-25T06:08:29Z"
  },
  {
    "code": "@@ -0,0 +1,55 @@\n+def camel_to_snake_case(input_str: str) -> str:\r\n+    \"\"\"\r\n+    Transforms a camelCase (or PascalCase) string to snake_case\r\n+\r\n+    >>> camel_to_snake_case(\"someRandomString\")\r\n+    'some_random_string'\r\n+\r\n+    >>> camel_to_snake_case(\"SomeRandomString\")\r\n+    'some_random_string'\r\n+\r\n+    >>> camel_to_snake_case(\"123someRandom123String123\")\r\n+    '123_some_random_123_string_123'\r\n+\r\n+    >>> camel_to_snake_case(\"123SomeRandom123String123\")\r\n+    '123_some_random_123_string_123'\r\n+\r\n+    >>> camel_to_snake_case(123)\r\n+    Traceback (most recent call last):\r\n+        ...\r\n+    ValueError: Expected string as input, found <class 'int'>\r\n+\r\n+    \"\"\"\r\n+\r\n+    import re\r\n+\r\n+    # check for invalid input type\r\n+    if not isinstance(input_str, str):\r\n+        msg = f\"Expected string as input, found {type(input_str)}\"\r\n+        raise ValueError(msg)\r\n+\r\n+    # Replace all characters that are not letters or numbers with the underscore\r\n+    snake_str = re.sub(r\"[^a-zA-Z0-9]\", \"_\", input_str)\r\n+\r\n+    # Find where lowercase meets uppercase. Insert underscore between them\r\n+    snake_str = re.sub(r\"([a-z])([A-Z])\", r\"\\1_\\2\", snake_str).lower()\r\n+\r\n+    # Find the sequence of digits at the beginning\r\n+    snake_str = re.sub(r\"^(\\d+)\", r\"\\1_\", snake_str)\r\n+\r\n+    # Find the sequence of digits at the end\r\n+    snake_str = re.sub(r\"(\\d+)$\", r\"_\\1\", snake_str)\r\n+\r\n+    # Find where letter meets digits\r\n+    snake_str = re.sub(r\"([a-z])(\\d+)\", r\"\\1_\\2\", snake_str)\r\n+\r\n+    # Find where digits meet letter\r\n+    snake_str = re.sub(r\"(\\d+)([a-z])\", r\"\\1_\\2\", snake_str)\r",
    "comment": "can this be done without using a regex?",
    "line_number": 47,
    "enriched": "File: strings/camel_case_to_snake_case.py\nCode: @@ -0,0 +1,55 @@\n+def camel_to_snake_case(input_str: str) -> str:\r\n+    \"\"\"\r\n+    Transforms a camelCase (or PascalCase) string to snake_case\r\n+\r\n+    >>> camel_to_snake_case(\"someRandomString\")\r\n+    'some_random_string'\r\n+\r\n+    >>> camel_to_snake_case(\"SomeRandomString\")\r\n+    'some_random_string'\r\n+\r\n+    >>> camel_to_snake_case(\"123someRandom123String123\")\r\n+    '123_some_random_123_string_123'\r\n+\r\n+    >>> camel_to_snake_case(\"123SomeRandom123String123\")\r\n+    '123_some_random_123_string_123'\r\n+\r\n+    >>> camel_to_snake_case(123)\r\n+    Traceback (most recent call last):\r\n+        ...\r\n+    ValueError: Expected string as input, found <class 'int'>\r\n+\r\n+    \"\"\"\r\n+\r\n+    import re\r\n+\r\n+    # check for invalid input type\r\n+    if not isinstance(input_str, str):\r\n+        msg = f\"Expected string as input, found {type(input_str)}\"\r\n+        raise ValueError(msg)\r\n+\r\n+    # Replace all characters that are not letters or numbers with the underscore\r\n+    snake_str = re.sub(r\"[^a-zA-Z0-9]\", \"_\", input_str)\r\n+\r\n+    # Find where lowercase meets uppercase. Insert underscore between them\r\n+    snake_str = re.sub(r\"([a-z])([A-Z])\", r\"\\1_\\2\", snake_str).lower()\r\n+\r\n+    # Find the sequence of digits at the beginning\r\n+    snake_str = re.sub(r\"^(\\d+)\", r\"\\1_\", snake_str)\r\n+\r\n+    # Find the sequence of digits at the end\r\n+    snake_str = re.sub(r\"(\\d+)$\", r\"_\\1\", snake_str)\r\n+\r\n+    # Find where letter meets digits\r\n+    snake_str = re.sub(r\"([a-z])(\\d+)\", r\"\\1_\\2\", snake_str)\r\n+\r\n+    # Find where digits meet letter\r\n+    snake_str = re.sub(r\"(\\d+)([a-z])\", r\"\\1_\\2\", snake_str)\r\nComment: Can this be done without using a regex?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "strings/camel_case_to_snake_case.py",
    "pr_number": 9727,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346546509,
    "comment_created_at": "2023-10-04T22:35:00Z"
  },
  {
    "code": "@@ -0,0 +1,55 @@\n+def get_matrix_input() -> list[list[float]]:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/gaussian_elimination.py, please provide doctest for the function get_matrix_input",
    "line_number": 1,
    "enriched": "File: maths/gaussian_elimination.py\nCode: @@ -0,0 +1,55 @@\n+def get_matrix_input() -> list[list[float]]:\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/gaussian_elimination.py`, please provide doctest for the function `get_matrix_input`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/gaussian_elimination.py",
    "pr_number": 9336,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342186266,
    "comment_created_at": "2023-10-01T20:48:39Z"
  },
  {
    "code": "@@ -0,0 +1,108 @@\n+\"\"\"\n+Done and sourced by https://github.com/gobmj\n+Find the perimeter of various geometric shapes\n+\"\"\"\n+from math import pi, sqrt\n+\n+\n+def perimeter_cube(side_length: float) -> float:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/perimeter.py, please provide doctest for the function perimeter_cube",
    "line_number": 8,
    "enriched": "File: maths/perimeter.py\nCode: @@ -0,0 +1,108 @@\n+\"\"\"\n+Done and sourced by https://github.com/gobmj\n+Find the perimeter of various geometric shapes\n+\"\"\"\n+from math import pi, sqrt\n+\n+\n+def perimeter_cube(side_length: float) -> float:\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/perimeter.py`, please provide doctest for the function `perimeter_cube`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/perimeter.py",
    "pr_number": 7635,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004371955,
    "comment_created_at": "2022-10-25T11:33:24Z"
  },
  {
    "code": "@@ -0,0 +1,30 @@\n+def max_sum_subarray(arr: list[int], k: int) -> int:",
    "comment": "please provide descriptive name for the parameter: k",
    "line_number": 1,
    "enriched": "File: data_structures/arrays/sliding_window.py\nCode: @@ -0,0 +1,30 @@\n+def max_sum_subarray(arr: list[int], k: int) -> int:\nComment: Please provide descriptive name for the parameter: `k`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "data_structures/arrays/sliding_window.py",
    "pr_number": 11816,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1788970733,
    "comment_created_at": "2024-10-06T08:35:01Z"
  },
  {
    "code": "@@ -0,0 +1,50 @@\n+# https://en.wikipedia.org/wiki/555_timer_IC#Astable\n+from __future__ import annotations\n+\n+\n+def astable_mode(",
    "comment": "as there is no test file in this pull request nor any test function or class in the file electronics/ic_555_timer.py, please provide doctest for the function astable_mode",
    "line_number": 5,
    "enriched": "File: electronics/IC_555_Timer.py\nCode: @@ -0,0 +1,50 @@\n+# https://en.wikipedia.org/wiki/555_timer_IC#Astable\n+from __future__ import annotations\n+\n+\n+def astable_mode(\nComment: As there is no test file in this pull request nor any test function or class in the file `electronics/IC_555_Timer.py`, please provide doctest for the function `astable_mode`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "electronics/IC_555_Timer.py",
    "pr_number": 10456,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359492318,
    "comment_created_at": "2023-10-14T17:06:09Z"
  },
  {
    "code": "@@ -0,0 +1,54 @@\n+#!/usr/bin/env python3\n+\n+from functools import lru_cache\n+\n+\n+def tsp(distances: list[list[int]]) -> int:\n+    \"\"\"\n+    Solves the Travelling Salesman Problem (TSP) using dynamic programming and bitmasking.\n+\n+    Args:\n+        distances: A 2D list where distances[i][j] represents the distance between city i and city j.\n+\n+    Returns:\n+        The minimum cost to complete the tour visiting all cities.\n+\n+    Raises:\n+        ValueError: If any distance is negative.\n+\n+    >>> tsp([[0, 10, 15, 20], [10, 0, 35, 25], [15, 35, 0, 30], [20, 25, 30, 0]])\n+    80\n+    >>> tsp([[0, 29, 20, 21], [29, 0, 15, 17], [20, 15, 0, 28], [21, 17, 28, 0]])\n+    69\n+    >>> tsp([[0, 10, -15, 20], [10, 0, 35, 25], [15, 35, 0, 30], [20, 25, 30, 0]])  # doctest: +ELLIPSIS\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Distance cannot be negative\n+    \"\"\"\n+    n = len(distances)\n+    if any(distances[i][j] < 0 for i in range(n) for j in range(n)):\n+        raise ValueError(\"Distance cannot be negative\")\n+\n+    visited_all = (1 << n) - 1\n+\n+    @lru_cache(None)\n+    def visit(city: int, mask: int) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/travelling_salesman_problem.py, please provide doctest for the function visit",
    "line_number": 35,
    "enriched": "File: dynamic_programming/travelling_salesman_problem.py\nCode: @@ -0,0 +1,54 @@\n+#!/usr/bin/env python3\n+\n+from functools import lru_cache\n+\n+\n+def tsp(distances: list[list[int]]) -> int:\n+    \"\"\"\n+    Solves the Travelling Salesman Problem (TSP) using dynamic programming and bitmasking.\n+\n+    Args:\n+        distances: A 2D list where distances[i][j] represents the distance between city i and city j.\n+\n+    Returns:\n+        The minimum cost to complete the tour visiting all cities.\n+\n+    Raises:\n+        ValueError: If any distance is negative.\n+\n+    >>> tsp([[0, 10, 15, 20], [10, 0, 35, 25], [15, 35, 0, 30], [20, 25, 30, 0]])\n+    80\n+    >>> tsp([[0, 29, 20, 21], [29, 0, 15, 17], [20, 15, 0, 28], [21, 17, 28, 0]])\n+    69\n+    >>> tsp([[0, 10, -15, 20], [10, 0, 35, 25], [15, 35, 0, 30], [20, 25, 30, 0]])  # doctest: +ELLIPSIS\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Distance cannot be negative\n+    \"\"\"\n+    n = len(distances)\n+    if any(distances[i][j] < 0 for i in range(n) for j in range(n)):\n+        raise ValueError(\"Distance cannot be negative\")\n+\n+    visited_all = (1 << n) - 1\n+\n+    @lru_cache(None)\n+    def visit(city: int, mask: int) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/travelling_salesman_problem.py`, please provide doctest for the function `visit`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "dynamic_programming/travelling_salesman_problem.py",
    "pr_number": 11939,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1795643764,
    "comment_created_at": "2024-10-10T15:07:44Z"
  },
  {
    "code": "@@ -0,0 +1,112 @@\n+\"\"\"\n+https://en.wikipedia.org/wiki/Kruskal%27s_algorithm\n+\"\"\"\n+import doctest\n+from collections.abc import Iterable\n+from dataclasses import dataclass\n+\n+\n+@dataclass\n+class Edge:\n+    \"\"\"\n+    Represents an edge in a graph.\n+\n+    Attributes:\n+        start (str): The starting vertex of the edge.\n+        end (str): The ending vertex of the edge.\n+        weight (int): The weight of the edge.\n+    \"\"\"\n+\n+    start: str\n+    end: str\n+    weight: int\n+\n+\n+class DisjointSet:\n+    \"\"\"\n+    Implements a disjoint-set (union-find) data structure.\n+\n+    This data structure is useful for tracking the partitioning of a set into disjoint,\n+    non-overlapping subsets. It offers efficient operations for union and finding the\n+    representative of a set.\n+\n+    Attributes:\n+        parent (dict[str, str]): Maps each element to its parent element.\n+        rank (dict[str, int]): Maps each element to its rank in the tree.\n+\n+    Methods:\n+        find(item): Finds the representative of the set containing 'item'.\n+        union(set1, set2): Merges the sets containing 'set1' and 'set2'.\n+    \"\"\"\n+\n+    def __init__(self, vertices: Iterable[str]) -> None:\n+        \"\"\"Initializes the disjoint set with each vertex in its own set.\"\"\"\n+        self.parent: dict[str, str] = {v: v for v in vertices}\n+        self.rank: dict[str, int] = {v: 0 for v in vertices}\n+\n+    def find(self, item: str) -> str:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file greedy_methods/kruskal.py, please provide doctest for the function find",
    "line_number": 47,
    "enriched": "File: greedy_methods/kruskal.py\nCode: @@ -0,0 +1,112 @@\n+\"\"\"\n+https://en.wikipedia.org/wiki/Kruskal%27s_algorithm\n+\"\"\"\n+import doctest\n+from collections.abc import Iterable\n+from dataclasses import dataclass\n+\n+\n+@dataclass\n+class Edge:\n+    \"\"\"\n+    Represents an edge in a graph.\n+\n+    Attributes:\n+        start (str): The starting vertex of the edge.\n+        end (str): The ending vertex of the edge.\n+        weight (int): The weight of the edge.\n+    \"\"\"\n+\n+    start: str\n+    end: str\n+    weight: int\n+\n+\n+class DisjointSet:\n+    \"\"\"\n+    Implements a disjoint-set (union-find) data structure.\n+\n+    This data structure is useful for tracking the partitioning of a set into disjoint,\n+    non-overlapping subsets. It offers efficient operations for union and finding the\n+    representative of a set.\n+\n+    Attributes:\n+        parent (dict[str, str]): Maps each element to its parent element.\n+        rank (dict[str, int]): Maps each element to its rank in the tree.\n+\n+    Methods:\n+        find(item): Finds the representative of the set containing 'item'.\n+        union(set1, set2): Merges the sets containing 'set1' and 'set2'.\n+    \"\"\"\n+\n+    def __init__(self, vertices: Iterable[str]) -> None:\n+        \"\"\"Initializes the disjoint set with each vertex in its own set.\"\"\"\n+        self.parent: dict[str, str] = {v: v for v in vertices}\n+        self.rank: dict[str, int] = {v: 0 for v in vertices}\n+\n+    def find(self, item: str) -> str:\nComment: As there is no test file in this pull request nor any test function or class in the file `greedy_methods/kruskal.py`, please provide doctest for the function `find`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "greedy_methods/kruskal.py",
    "pr_number": 11178,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1405188269,
    "comment_created_at": "2023-11-25T17:58:22Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+# import the necessary packages\n+from skimage.feature import peak_local_max\n+from skimage.segmentation import watershed\n+from scipy import ndimage\n+import numpy as np\n+import imutils\n+import cv2\n+\n+\n+def watershed_image(path_image: str) -> None :",
    "comment": "as there is no test file in this pull request nor any test function or class in the file digital_image_processing/edge_detection/watershed.py, please provide doctest for the function watershed_image",
    "line_number": 10,
    "enriched": "File: digital_image_processing/edge_detection/watershed.py\nCode: @@ -0,0 +1,58 @@\n+# import the necessary packages\n+from skimage.feature import peak_local_max\n+from skimage.segmentation import watershed\n+from scipy import ndimage\n+import numpy as np\n+import imutils\n+import cv2\n+\n+\n+def watershed_image(path_image: str) -> None :\nComment: As there is no test file in this pull request nor any test function or class in the file `digital_image_processing/edge_detection/watershed.py`, please provide doctest for the function `watershed_image`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "digital_image_processing/edge_detection/watershed.py",
    "pr_number": 11044,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375116904,
    "comment_created_at": "2023-10-27T23:26:10Z"
  },
  {
    "code": "@@ -52,7 +52,58 @@ def decimal_to_binary(num: int) -> str:\n     return \"0b\" + \"\".join(str(e) for e in binary)\n \n \n+def decimal_to_binary_recursive(decimal: int) -> str:\n+    \"\"\"\n+    Take a positive integer value and return its binary equivalent.\n+    >>> decimal_to_binary_recursive(1000)\n+    '1111101000'\n+    >>> decimal_to_binary_recursive(\"72\")\n+    '1001000'\n+    >>> decimal_to_binary_recursive(\"number\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: invalid literal for int() with base 10: 'number'\n+    \"\"\"\n+    decimal = int(decimal)\n+    if decimal in (0, 1):  # Exit cases for the recursion\n+        return str(decimal)\n+    div, mod = divmod(decimal, 2)\n+    return decimal_to_binary_recursive(div) + str(mod)\n+\n+\n+def decimal_to_binary_recursive_main(number: str) -> str:",
    "comment": "for simplicity, can we rename the main function decimal_to_binary_recursive rather than decimal_to_binary_recursive_main? we can then rename the recursive helper function to decimal_to_binary_recursive_helper or decimal_to_binary_recursive_no_prefix or something like that",
    "line_number": 74,
    "enriched": "File: conversions/decimal_to_binary.py\nCode: @@ -52,7 +52,58 @@ def decimal_to_binary(num: int) -> str:\n     return \"0b\" + \"\".join(str(e) for e in binary)\n \n \n+def decimal_to_binary_recursive(decimal: int) -> str:\n+    \"\"\"\n+    Take a positive integer value and return its binary equivalent.\n+    >>> decimal_to_binary_recursive(1000)\n+    '1111101000'\n+    >>> decimal_to_binary_recursive(\"72\")\n+    '1001000'\n+    >>> decimal_to_binary_recursive(\"number\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: invalid literal for int() with base 10: 'number'\n+    \"\"\"\n+    decimal = int(decimal)\n+    if decimal in (0, 1):  # Exit cases for the recursion\n+        return str(decimal)\n+    div, mod = divmod(decimal, 2)\n+    return decimal_to_binary_recursive(div) + str(mod)\n+\n+\n+def decimal_to_binary_recursive_main(number: str) -> str:\nComment: For simplicity, can we rename the main function `decimal_to_binary_recursive` rather than `decimal_to_binary_recursive_main`? We can then rename the recursive helper function to `decimal_to_binary_recursive_helper` or `decimal_to_binary_recursive_no_prefix` or something like that",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "conversions/decimal_to_binary.py",
    "pr_number": 8999,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1299456574,
    "comment_created_at": "2023-08-20T23:55:50Z"
  },
  {
    "code": "@@ -0,0 +1,33 @@\n+# Reference: https://www.investopedia.com/terms/p/presentvalue.asp\n+\n+# Algorithm that calculates the present value of a stream of yearly cash flows given...\n+# 1. The discount rate (as a decimal, not a percent)\n+# 2. An array of cash flows, with the index of the cash flow being the associated year\n+\n+# Note: This algorithm assumes that cash flows are paid at the end of the specified year\n+\n+from typing import list, tuple\n+\n+\n+def present_value(discount_rate: float, cash_flows: list[float]) -> tuple[float, str]:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file financial/present_value.py, please provide doctest for the function present_value",
    "line_number": 12,
    "enriched": "File: financial/present_value.py\nCode: @@ -0,0 +1,33 @@\n+# Reference: https://www.investopedia.com/terms/p/presentvalue.asp\n+\n+# Algorithm that calculates the present value of a stream of yearly cash flows given...\n+# 1. The discount rate (as a decimal, not a percent)\n+# 2. An array of cash flows, with the index of the cash flow being the associated year\n+\n+# Note: This algorithm assumes that cash flows are paid at the end of the specified year\n+\n+from typing import list, tuple\n+\n+\n+def present_value(discount_rate: float, cash_flows: list[float]) -> tuple[float, str]:\nComment: As there is no test file in this pull request nor any test function or class in the file `financial/present_value.py`, please provide doctest for the function `present_value`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "financial/present_value.py",
    "pr_number": 8700,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1180947821,
    "comment_created_at": "2023-04-29T03:43:11Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+import math\n+import random\n+\n+def gcd(a, b):\n+    \"\"\"Computes the greatest common divisor using Euclidean algorithm.\"\"\"\n+    while b:",
    "comment": "this line should be while b == 0:, not while b: to make sure b is zero.",
    "line_number": 6,
    "enriched": "File: algorithms/cryptography/shor_algorithm.py\nCode: @@ -0,0 +1,56 @@\n+import math\n+import random\n+\n+def gcd(a, b):\n+    \"\"\"Computes the greatest common divisor using Euclidean algorithm.\"\"\"\n+    while b:\nComment: This line should be `while b == 0:`, not `while b:` to make sure `b` is zero.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "algorithms/cryptography/shor_algorithm.py",
    "pr_number": 12545,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1954891619,
    "comment_created_at": "2025-02-13T17:01:41Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+import math\n+\n+\n+def is_power_of_four_logarithm(num: int) -> bool:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/is_power_of_four_logarithm.py, please provide doctest for the function is_power_of_four_logarithm",
    "line_number": 4,
    "enriched": "File: maths/is_power_of_four_logarithm.py\nCode: @@ -0,0 +1,35 @@\n+import math\n+\n+\n+def is_power_of_four_logarithm(num: int) -> bool:\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/is_power_of_four_logarithm.py`, please provide doctest for the function `is_power_of_four_logarithm`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/is_power_of_four_logarithm.py",
    "pr_number": 9338,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342206523,
    "comment_created_at": "2023-10-02T00:05:05Z"
  },
  {
    "code": "@@ -0,0 +1,26 @@\n+''' A NOR Gate is a logic gate in boolean algebra which results to false(0)\n+    if any of the input is 1, and True(1) if  both the inputs are 0.\n+   Following is the truth table of an NOR Gate:\n+   | Input 1 | Input 2 |  Output |\n+   |      0      |     0      |      1      |\n+   |      0      |     1      |      0      |\n+   |      1      |     0      |      0      |\n+   |      1      |     1      |      0      |\n+'''\n+'''Following is the code implementation of the NOR Gate'''\n+\n+def nor_gate(input_1: int,input_2: int) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file boolean_algebra/norgate.py, please provide doctest for the function nor_gate",
    "line_number": 12,
    "enriched": "File: boolean_algebra/norgate.py\nCode: @@ -0,0 +1,26 @@\n+''' A NOR Gate is a logic gate in boolean algebra which results to false(0)\n+    if any of the input is 1, and True(1) if  both the inputs are 0.\n+   Following is the truth table of an NOR Gate:\n+   | Input 1 | Input 2 |  Output |\n+   |      0      |     0      |      1      |\n+   |      0      |     1      |      0      |\n+   |      1      |     0      |      0      |\n+   |      1      |     1      |      0      |\n+'''\n+'''Following is the code implementation of the NOR Gate'''\n+\n+def nor_gate(input_1: int,input_2: int) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `boolean_algebra/norgate.py`, please provide doctest for the function `nor_gate`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "boolean_algebra/norgate.py",
    "pr_number": 7133,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994830638,
    "comment_created_at": "2022-10-13T15:51:27Z"
  },
  {
    "code": "@@ -0,0 +1,138 @@\n+from __future__ import annotations\n+\n+\n+class Node:\n+    def __init__(self, number: int) -> None:\n+        self.data = number\n+        self.left = None\n+        self.right = None\n+\n+class BinaryTree:\n+    def __init__(self, root=None) -> None:",
    "comment": "please provide type hint for the parameter: root",
    "line_number": 11,
    "enriched": "File: data_structures/binary_tree/transform_bst_sum_tree.py\nCode: @@ -0,0 +1,138 @@\n+from __future__ import annotations\n+\n+\n+class Node:\n+    def __init__(self, number: int) -> None:\n+        self.data = number\n+        self.left = None\n+        self.right = None\n+\n+class BinaryTree:\n+    def __init__(self, root=None) -> None:\nComment: Please provide type hint for the parameter: `root`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "data_structures/binary_tree/transform_bst_sum_tree.py",
    "pr_number": 10110,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349695777,
    "comment_created_at": "2023-10-08T13:15:58Z"
  },
  {
    "code": "@@ -0,0 +1,33 @@\n+import doctest\n+\n+\n+''' \n+\n+\n+\n+Finding the kinetic energy of an object,by taking its mass and velocity as input\n+\n+\n+Description : In physics, the kinetic energy of an object is the energy that it possesses due to its motion.\n+It is defined as the work needed to accelerate a body of a given mass from rest to its stated velocity. \n+Having gained this energy during its acceleration, the body maintains this kinetic energy unless its speed changes. \n+The same amount of work is done by the body when decelerating from its current speed to a state of rest. \n+Formally, a kinetic energy is any term in a system's Lagrangian which includes a derivative with respect to time. \n+\n+In classical mechanics, the kinetic energy of a non-rotating object of mass m traveling at a speed v is \u00bdmv\u00b2.\n+In relativistic mechanics, this is a good approximation only when v is much less than the speed of light.\n+The standard unit of kinetic energy is the joule, while the English unit of kinetic energy is the foot-pound.\n+\n+Reference : \"https://en.m.wikipedia.org/wiki/Kinetic_energy\"\n+\n+'''\n+\n+def kinetic_energy(mass : float, velocity : float) -> float:              #function will accept mass and velocity as parameters and return kinetic energy ",
    "comment": "as there is no test file in this pull request nor any test function or class in the file physics/kinetic_energy.py, please provide doctest for the function kinetic_energy",
    "line_number": 25,
    "enriched": "File: physics/kinetic_energy.py\nCode: @@ -0,0 +1,33 @@\n+import doctest\n+\n+\n+''' \n+\n+\n+\n+Finding the kinetic energy of an object,by taking its mass and velocity as input\n+\n+\n+Description : In physics, the kinetic energy of an object is the energy that it possesses due to its motion.\n+It is defined as the work needed to accelerate a body of a given mass from rest to its stated velocity. \n+Having gained this energy during its acceleration, the body maintains this kinetic energy unless its speed changes. \n+The same amount of work is done by the body when decelerating from its current speed to a state of rest. \n+Formally, a kinetic energy is any term in a system's Lagrangian which includes a derivative with respect to time. \n+\n+In classical mechanics, the kinetic energy of a non-rotating object of mass m traveling at a speed v is \u00bdmv\u00b2.\n+In relativistic mechanics, this is a good approximation only when v is much less than the speed of light.\n+The standard unit of kinetic energy is the joule, while the English unit of kinetic energy is the foot-pound.\n+\n+Reference : \"https://en.m.wikipedia.org/wiki/Kinetic_energy\"\n+\n+'''\n+\n+def kinetic_energy(mass : float, velocity : float) -> float:              #function will accept mass and velocity as parameters and return kinetic energy \nComment: As there is no test file in this pull request nor any test function or class in the file `physics/kinetic_energy.py`, please provide doctest for the function `kinetic_energy`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "physics/kinetic_energy.py",
    "pr_number": 7605,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1003538558,
    "comment_created_at": "2022-10-24T16:48:06Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+from math import ceil, sqrt\n+from __future__ import annotations\n+\n+\n+def primeproduct(n: int, x: list = []) -> list[int]:",
    "comment": "please provide descriptive name for the parameter: n\n\nplease provide descriptive name for the parameter: x",
    "line_number": 5,
    "enriched": "File: maths/prime_factorization_fast.py\nCode: @@ -0,0 +1,61 @@\n+from math import ceil, sqrt\n+from __future__ import annotations\n+\n+\n+def primeproduct(n: int, x: list = []) -> list[int]:\nComment: Please provide descriptive name for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "maths/prime_factorization_fast.py",
    "pr_number": 8920,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1283299337,
    "comment_created_at": "2023-08-03T14:34:14Z"
  },
  {
    "code": "@@ -0,0 +1,131 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval: int | str | float = None) -> None:\n+        \"\"\"\n+        An empty left and right branches will be created for every value inserted,\n+        to perform better in recursive methods\n+        \"\"\"\n+        self.value = initval\n+        if self.value:\n+            self.left = Tree()\n+            self.right = Tree()\n+        else:\n+            self.left = None\n+            self.right = None\n+        return\n+\n+    # Empty nodes are None valued\n+    def isempty(self) -> bool:\n+        \"\"\"\n+        Returns true if tree is empty else False\n+        >>> isempty([])\n+        True\n+        >>> isempty([1,2,3])\n+        False\n+        \"\"\"\n+        return self.value == None\n+\n+    def isleaf(self) -> bool:\n+        \"\"\"\n+        Returns true if leaf is a node\n+        Suppose in [1,2,3,4,5,6,7,8,9,10]\n+        4 had empty left and right branches.\n+        >>> isleaf(4)\n+        True\n+        \"\"\"\n+        return self.value != None and self.left.isempty() and self.right.isempty()\n+\n+    # Inorder Traversal\n+    def inorder(self) -> list:\n+        \"\"\"\n+        Returns a list sorted in increasing order from the Binary Search Tree.\n+        >>> tree_sort([23235,82107,35775,91961,4323,40556,76603,64302,27316,74372])\n+        [4323, 23235, 27316, 35775, 40556, 64302, 74372, 76603, 82107, 91961]\n+        >>> tree_sort([50,52,54,74,93,100,114,124,130,143])\n+        [50, 52, 54, 74, 93, 100, 114, 124, 130, 143]\n+        \"\"\"\n+        # Corner Case\n+        if self.isempty():\n+            return []\n+        else:\n+            return self.left.inorder() + [self.value] + self.right.inorder()\n+\n+    # Display Tree\n+    def __str__(self) -> str:\n+        \"\"\"\n+        Prints the sorted tree\n+        Suppose tree is t.inorder() = [1,2,3]\n+        >>> print(t)\n+        [1,2,3]\n+        \"\"\"\n+        return str(self.inorder())\n+\n+    # Insert new element\n+    def insert(self, v: int | str | float) -> None:",
    "comment": "please provide descriptive name for the parameter: v",
    "line_number": 82,
    "enriched": "File: sorts/tree_sort_2.py\nCode: @@ -0,0 +1,131 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval: int | str | float = None) -> None:\n+        \"\"\"\n+        An empty left and right branches will be created for every value inserted,\n+        to perform better in recursive methods\n+        \"\"\"\n+        self.value = initval\n+        if self.value:\n+            self.left = Tree()\n+            self.right = Tree()\n+        else:\n+            self.left = None\n+            self.right = None\n+        return\n+\n+    # Empty nodes are None valued\n+    def isempty(self) -> bool:\n+        \"\"\"\n+        Returns true if tree is empty else False\n+        >>> isempty([])\n+        True\n+        >>> isempty([1,2,3])\n+        False\n+        \"\"\"\n+        return self.value == None\n+\n+    def isleaf(self) -> bool:\n+        \"\"\"\n+        Returns true if leaf is a node\n+        Suppose in [1,2,3,4,5,6,7,8,9,10]\n+        4 had empty left and right branches.\n+        >>> isleaf(4)\n+        True\n+        \"\"\"\n+        return self.value != None and self.left.isempty() and self.right.isempty()\n+\n+    # Inorder Traversal\n+    def inorder(self) -> list:\n+        \"\"\"\n+        Returns a list sorted in increasing order from the Binary Search Tree.\n+        >>> tree_sort([23235,82107,35775,91961,4323,40556,76603,64302,27316,74372])\n+        [4323, 23235, 27316, 35775, 40556, 64302, 74372, 76603, 82107, 91961]\n+        >>> tree_sort([50,52,54,74,93,100,114,124,130,143])\n+        [50, 52, 54, 74, 93, 100, 114, 124, 130, 143]\n+        \"\"\"\n+        # Corner Case\n+        if self.isempty():\n+            return []\n+        else:\n+            return self.left.inorder() + [self.value] + self.right.inorder()\n+\n+    # Display Tree\n+    def __str__(self) -> str:\n+        \"\"\"\n+        Prints the sorted tree\n+        Suppose tree is t.inorder() = [1,2,3]\n+        >>> print(t)\n+        [1,2,3]\n+        \"\"\"\n+        return str(self.inorder())\n+\n+    # Insert new element\n+    def insert(self, v: int | str | float) -> None:\nComment: Please provide descriptive name for the parameter: `v`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "sorts/tree_sort_2.py",
    "pr_number": 7461,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000813400,
    "comment_created_at": "2022-10-20T15:44:44Z"
  },
  {
    "code": "@@ -1,131 +1,96 @@\n \"\"\"\n-Author  : Alexander Pantyukhin\n-Date    : November 2, 2022\n-\n-Task:\n-Given the root of a binary tree, determine if it is a valid binary search\n-tree (BST).\n+Given the root of a binary tree, determine if it is a valid binary search tree (BST).\n \n A valid binary search tree is defined as follows:\n-\n - The left subtree of a node contains only nodes with keys less than the node's key.\n - The right subtree of a node contains only nodes with keys greater than the node's key.\n - Both the left and right subtrees must also be binary search trees.\n \n-Implementation notes:\n-Depth-first search approach.\n-\n leetcode: https://leetcode.com/problems/validate-binary-search-tree/\n \n-Let n is the number of nodes in tree\n+If n is the number of nodes in the tree then:\n Runtime: O(n)\n Space: O(1)\n \"\"\"\n-\n from __future__ import annotations\n \n+from collections.abc import Iterator\n from dataclasses import dataclass\n \n \n @dataclass\n-class TreeNode:\n+class Node:\n     data: float\n-    left: TreeNode | None = None\n-    right: TreeNode | None = None\n-\n-\n-def is_binary_search_tree(root: TreeNode | None) -> bool:\n-    \"\"\"\n-    >>> is_binary_search_tree(TreeNode(data=2,\n-    ...                                left=TreeNode(data=1),\n-    ...                                right=TreeNode(data=3))\n-    ...                                )\n-    True\n-\n-    >>> is_binary_search_tree(TreeNode(data=0,\n-    ...                                left=TreeNode(data=-11),\n-    ...                                right=TreeNode(data=3))\n-    ...                                )\n-    True\n-\n-    >>> is_binary_search_tree(TreeNode(data=5,\n-    ...                                left=TreeNode(data=1),\n-    ...                                right=TreeNode(data=4, left=TreeNode(data=3)))\n-    ...                      )\n-    False\n+    left: Node | None = None\n+    right: Node | None = None\n \n-    >>> is_binary_search_tree(TreeNode(data='a',\n-    ...                                left=TreeNode(data=1),\n-    ...                                right=TreeNode(data=4, left=TreeNode(data=3)))\n-    ...                      )\n-    Traceback (most recent call last):\n-     ...\n-    ValueError: Each node should be type of TreeNode and data should be float.\n-\n-    >>> is_binary_search_tree(TreeNode(data=2,\n-    ...                                left=TreeNode([]),\n-    ...                                right=TreeNode(data=4, left=TreeNode(data=3)))\n-    ...                                )\n-    Traceback (most recent call last):\n-     ...\n-    ValueError: Each node should be type of TreeNode and data should be float.\n-    \"\"\"\n-\n-    # Validation\n-    def is_valid_tree(node: TreeNode | None) -> bool:\n+    def __iter__(self) -> Iterator[float]:\n         \"\"\"\n-        >>> is_valid_tree(None)\n-        True\n-        >>> is_valid_tree('abc')\n-        False\n-        >>> is_valid_tree(TreeNode(data='not a float'))\n-        False\n-        >>> is_valid_tree(TreeNode(data=1, left=TreeNode('123')))\n-        False\n+        >>> root = Node(data=2.1)\n+        >>> list(root)\n+        [2.1]\n+        >>> root.left=Node(data=2.0)\n+        >>> list(root)\n+        [2.0, 2.1]\n+        >>> root.right=Node(data=2.2)\n+        >>> list(root)\n+        [2.0, 2.1, 2.2]\n         \"\"\"\n-        if node is None:\n-            return True\n-\n-        if not isinstance(node, TreeNode):\n-            return False\n-\n-        try:\n-            float(node.data)\n-        except (TypeError, ValueError):\n-            return False\n-\n-        return is_valid_tree(node.left) and is_valid_tree(node.right)\n-\n-    if not is_valid_tree(root):\n-        raise ValueError(\n-            \"Each node should be type of TreeNode and data should be float.\"\n-        )\n-\n-    def is_binary_search_tree_recursive_check(\n-        node: TreeNode | None, left_bound: float, right_bound: float\n-    ) -> bool:\n+        if self.left:\n+            yield from self.left\n+        yield self.data\n+        if self.right:\n+            yield from self.right\n+\n+    @property\n+    def is_sorted(self) -> bool:",
    "comment": "the name of this function no longer matches the name of the file, and that might confuse some readers. i feel that either this function should be renamed to is_bst or the file should be renamed to is_sorted_binary_tree.",
    "line_number": 46,
    "enriched": "File: data_structures/binary_tree/is_bst.py\nCode: @@ -1,131 +1,96 @@\n \"\"\"\n-Author  : Alexander Pantyukhin\n-Date    : November 2, 2022\n-\n-Task:\n-Given the root of a binary tree, determine if it is a valid binary search\n-tree (BST).\n+Given the root of a binary tree, determine if it is a valid binary search tree (BST).\n \n A valid binary search tree is defined as follows:\n-\n - The left subtree of a node contains only nodes with keys less than the node's key.\n - The right subtree of a node contains only nodes with keys greater than the node's key.\n - Both the left and right subtrees must also be binary search trees.\n \n-Implementation notes:\n-Depth-first search approach.\n-\n leetcode: https://leetcode.com/problems/validate-binary-search-tree/\n \n-Let n is the number of nodes in tree\n+If n is the number of nodes in the tree then:\n Runtime: O(n)\n Space: O(1)\n \"\"\"\n-\n from __future__ import annotations\n \n+from collections.abc import Iterator\n from dataclasses import dataclass\n \n \n @dataclass\n-class TreeNode:\n+class Node:\n     data: float\n-    left: TreeNode | None = None\n-    right: TreeNode | None = None\n-\n-\n-def is_binary_search_tree(root: TreeNode | None) -> bool:\n-    \"\"\"\n-    >>> is_binary_search_tree(TreeNode(data=2,\n-    ...                                left=TreeNode(data=1),\n-    ...                                right=TreeNode(data=3))\n-    ...                                )\n-    True\n-\n-    >>> is_binary_search_tree(TreeNode(data=0,\n-    ...                                left=TreeNode(data=-11),\n-    ...                                right=TreeNode(data=3))\n-    ...                                )\n-    True\n-\n-    >>> is_binary_search_tree(TreeNode(data=5,\n-    ...                                left=TreeNode(data=1),\n-    ...                                right=TreeNode(data=4, left=TreeNode(data=3)))\n-    ...                      )\n-    False\n+    left: Node | None = None\n+    right: Node | None = None\n \n-    >>> is_binary_search_tree(TreeNode(data='a',\n-    ...                                left=TreeNode(data=1),\n-    ...                                right=TreeNode(data=4, left=TreeNode(data=3)))\n-    ...                      )\n-    Traceback (most recent call last):\n-     ...\n-    ValueError: Each node should be type of TreeNode and data should be float.\n-\n-    >>> is_binary_search_tree(TreeNode(data=2,\n-    ...                                left=TreeNode([]),\n-    ...                                right=TreeNode(data=4, left=TreeNode(data=3)))\n-    ...                                )\n-    Traceback (most recent call last):\n-     ...\n-    ValueError: Each node should be type of TreeNode and data should be float.\n-    \"\"\"\n-\n-    # Validation\n-    def is_valid_tree(node: TreeNode | None) -> bool:\n+    def __iter__(self) -> Iterator[float]:\n         \"\"\"\n-        >>> is_valid_tree(None)\n-        True\n-        >>> is_valid_tree('abc')\n-        False\n-        >>> is_valid_tree(TreeNode(data='not a float'))\n-        False\n-        >>> is_valid_tree(TreeNode(data=1, left=TreeNode('123')))\n-        False\n+        >>> root = Node(data=2.1)\n+        >>> list(root)\n+        [2.1]\n+        >>> root.left=Node(data=2.0)\n+        >>> list(root)\n+        [2.0, 2.1]\n+        >>> root.right=Node(data=2.2)\n+        >>> list(root)\n+        [2.0, 2.1, 2.2]\n         \"\"\"\n-        if node is None:\n-            return True\n-\n-        if not isinstance(node, TreeNode):\n-            return False\n-\n-        try:\n-            float(node.data)\n-        except (TypeError, ValueError):\n-            return False\n-\n-        return is_valid_tree(node.left) and is_valid_tree(node.right)\n-\n-    if not is_valid_tree(root):\n-        raise ValueError(\n-            \"Each node should be type of TreeNode and data should be float.\"\n-        )\n-\n-    def is_binary_search_tree_recursive_check(\n-        node: TreeNode | None, left_bound: float, right_bound: float\n-    ) -> bool:\n+        if self.left:\n+            yield from self.left\n+        yield self.data\n+        if self.right:\n+            yield from self.right\n+\n+    @property\n+    def is_sorted(self) -> bool:\nComment: The name of this function no longer matches the name of the file, and that might confuse some readers. I feel that either this function should be renamed to `is_bst` or the file should be renamed to `is_sorted_binary_tree`.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "data_structures/binary_tree/is_bst.py",
    "pr_number": 10627,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1365731163,
    "comment_created_at": "2023-10-19T15:23:47Z"
  },
  {
    "code": "@@ -22,40 +22,28 @@ def solve_linear_system(matrix: np.ndarray) -> np.ndarray:\n     >>> solution = solve_linear_system(np.column_stack((A, B)))\n     >>> np.allclose(solution, np.array([2., 3., -1.]))\n     True\n-    >>> solve_linear_system(np.array([[0, 0], [0, 0]],  dtype=float))\n-    array([nan, nan])\n+    >>> solve_linear_system(np.array([[0, 0, 0], [0, 0, 0]],  dtype=float))\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix is not correct\n     \"\"\"\n     ab = np.copy(matrix)\n     num_of_rows = ab.shape[0]\n     num_of_columns = ab.shape[1] - 1\n     x_lst: list[float] = []\n \n-    # Lead element search\n-    for column_num in range(num_of_rows):\n-        for i in range(column_num, num_of_columns):\n-            if abs(ab[i][column_num]) > abs(ab[column_num][column_num]):\n-                ab[[column_num, i]] = ab[[i, column_num]]\n-                if ab[column_num, column_num] == 0.0:\n-                    raise ValueError(\"Matrix is not correct\")\n-            else:\n-                pass\n-        if column_num != 0:\n-            for i in range(column_num, num_of_rows):\n-                ab[i, :] -= (\n-                    ab[i, column_num - 1]\n-                    / ab[column_num - 1, column_num - 1]\n-                    * ab[column_num - 1, :]\n-                )\n+    assert num_of_rows == num_of_columns\n \n-    # Upper triangular matrix\n     for column_num in range(num_of_rows):\n+        # Lead element search\n         for i in range(column_num, num_of_columns):\n             if abs(ab[i][column_num]) > abs(ab[column_num][column_num]):\n                 ab[[column_num, i]] = ab[[i, column_num]]\n-                if ab[column_num, column_num] == 0.0:\n-                    raise ValueError(\"Matrix is not correct\")\n-            else:\n-                pass\n+\n+        # Upper triangular matrix\n+        if ab[column_num, column_num] == 0.0:\n+            raise ValueError(\"Matrix is not correct\")",
    "comment": "can we have the error say \"matrix is singular\" instead? \"not correct\" is very vague. (this will need to be updated in the function docstring as well.)",
    "line_number": 45,
    "enriched": "File: linear_algebra/src/gaussian_elimination_pivoting.py\nCode: @@ -22,40 +22,28 @@ def solve_linear_system(matrix: np.ndarray) -> np.ndarray:\n     >>> solution = solve_linear_system(np.column_stack((A, B)))\n     >>> np.allclose(solution, np.array([2., 3., -1.]))\n     True\n-    >>> solve_linear_system(np.array([[0, 0], [0, 0]],  dtype=float))\n-    array([nan, nan])\n+    >>> solve_linear_system(np.array([[0, 0, 0], [0, 0, 0]],  dtype=float))\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix is not correct\n     \"\"\"\n     ab = np.copy(matrix)\n     num_of_rows = ab.shape[0]\n     num_of_columns = ab.shape[1] - 1\n     x_lst: list[float] = []\n \n-    # Lead element search\n-    for column_num in range(num_of_rows):\n-        for i in range(column_num, num_of_columns):\n-            if abs(ab[i][column_num]) > abs(ab[column_num][column_num]):\n-                ab[[column_num, i]] = ab[[i, column_num]]\n-                if ab[column_num, column_num] == 0.0:\n-                    raise ValueError(\"Matrix is not correct\")\n-            else:\n-                pass\n-        if column_num != 0:\n-            for i in range(column_num, num_of_rows):\n-                ab[i, :] -= (\n-                    ab[i, column_num - 1]\n-                    / ab[column_num - 1, column_num - 1]\n-                    * ab[column_num - 1, :]\n-                )\n+    assert num_of_rows == num_of_columns\n \n-    # Upper triangular matrix\n     for column_num in range(num_of_rows):\n+        # Lead element search\n         for i in range(column_num, num_of_columns):\n             if abs(ab[i][column_num]) > abs(ab[column_num][column_num]):\n                 ab[[column_num, i]] = ab[[i, column_num]]\n-                if ab[column_num, column_num] == 0.0:\n-                    raise ValueError(\"Matrix is not correct\")\n-            else:\n-                pass\n+\n+        # Upper triangular matrix\n+        if ab[column_num, column_num] == 0.0:\n+            raise ValueError(\"Matrix is not correct\")\nComment: Can we have the error say \"Matrix is singular\" instead? \"Not correct\" is very vague. (This will need to be updated in the function docstring as well.)",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "linear_algebra/src/gaussian_elimination_pivoting.py",
    "pr_number": 11393,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1730446141,
    "comment_created_at": "2024-08-25T22:49:23Z"
  },
  {
    "code": "@@ -0,0 +1,23 @@\n+### \u2705 **Python Code Conventions (PEP 8 Overview)**\n+\n+1. **Indentation**: Use 4 spaces per indentation level.\n+2. **Line Length**: Limit lines to 79 characters.\n+3. **Blank Lines**: Use blank lines to separate functions and classes.\n+4. **Imports**:\n+\n+   * Standard libraries first, third-party next, then local.\n+   * One import per line.\n+5. **Naming Conventions**:\n+\n+   * `snake_case` for variables and functions.\n+   * `CamelCase` for classes.\n+   * `UPPER_CASE` for constants.\n+6. **Whitespace**:\n+\n+   * Avoid extra spaces inside parentheses, brackets, or before commas.\n+7. **Comments**:\n+\n+   * Use inline comments sparingly.\n+   * Use docstrings (`\"\"\"Example\"\"\"`) for modules, functions, classes.\n+8. **Python version**:\n+   * use python 3.5.2",
    "comment": "seriously?!?",
    "line_number": 23,
    "enriched": "File: rule.txt\nCode: @@ -0,0 +1,23 @@\n+### \u2705 **Python Code Conventions (PEP 8 Overview)**\n+\n+1. **Indentation**: Use 4 spaces per indentation level.\n+2. **Line Length**: Limit lines to 79 characters.\n+3. **Blank Lines**: Use blank lines to separate functions and classes.\n+4. **Imports**:\n+\n+   * Standard libraries first, third-party next, then local.\n+   * One import per line.\n+5. **Naming Conventions**:\n+\n+   * `snake_case` for variables and functions.\n+   * `CamelCase` for classes.\n+   * `UPPER_CASE` for constants.\n+6. **Whitespace**:\n+\n+   * Avoid extra spaces inside parentheses, brackets, or before commas.\n+7. **Comments**:\n+\n+   * Use inline comments sparingly.\n+   * Use docstrings (`\"\"\"Example\"\"\"`) for modules, functions, classes.\n+8. **Python version**:\n+   * use python 3.5.2\nComment: Seriously?!?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "rule.txt",
    "pr_number": 12770,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2113013506,
    "comment_created_at": "2025-05-29T01:35:40Z"
  },
  {
    "code": "@@ -0,0 +1,126 @@\n+\"\"\"\n+    Given 'n' pairs of parentheses,\n+    this program generates all combinations of well-formed parentheses.\n+\n+    Example, n = 3:\n+    [\n+        \"((()))\",\n+        \"(()())\",\n+        \"(())()\",\n+        \"()(())\",\n+        \"()()()\",\n+    ]\n+\n+    The number of valid combinations of parentheses (Catalan number) for n pairs is:\n+    C(n) = (2n)! / ((n + 1)! * n!)\n+\n+    You can find more information about Catalan numbers and their applications here:\n+    https://en.wikipedia.org/wiki/Catalan_number\n+\n+    We use backtracking to solve this problem.\n+    Time complexity: O(4^n / sqrt(n))\n+    Space complexity: O(4^n / sqrt(n))\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+\n+def helper_function_to_generate_parenthesis(\n+    num_pairs: int,\n+    current_string: str,\n+    result: list[str],\n+    num_open: int,\n+    num_close: int,\n+) -> None:\n+    \"\"\"\n+    Helper function to generate all combinations of well-formed parentheses.\n+\n+    Args:\n+        num_pairs (int): The number of pairs of parentheses to use.\n+        current_string (str): The current string of parentheses.\n+        result (list[str]): The list of all possible combinations of parentheses.\n+        num_open (int): The number of open parentheses.\n+        num_close (int): The number of close parentheses.\n+\n+    Returns:\n+        None, but the result is updated.\n+\n+    Examples:\n+        >>> result = []\n+        >>> helper_function_to_generate_parenthesis(3, \"\", result, 0, 0)\n+        >>> result\n+        ['((()))', '(()())', '(())()', '()(())', '()()()']\n+        >>> result = []\n+        >>> helper_function_to_generate_parenthesis(1, \"\", result, 0, 0)\n+        >>> result\n+        ['()']\n+        >>> result = []\n+        >>> helper_function_to_generate_parenthesis(0, \"\", result, 0, 0)\n+        >>> result\n+        ['']\n+    \"\"\"\n+\n+    if num_open == num_pairs and num_close == num_pairs:\n+        result.append(current_string)\n+        return\n+\n+    if num_open < num_pairs:\n+        helper_function_to_generate_parenthesis(\n+            num_pairs, current_string + \"(\", result, num_open + 1, num_close\n+        )\n+\n+    if num_close < num_open:\n+        helper_function_to_generate_parenthesis(\n+            num_pairs, current_string + \")\", result, num_open, num_close + 1\n+        )\n+\n+\n+def generate_parenthesis(num_pairs: int) -> list[str]:\n+    \"\"\"\n+    Generate all combinations of well-formed parentheses given the number of pairs.\n+\n+    Args:\n+        num_pairs (int): The number of pairs of parentheses to use.\n+\n+    Returns:\n+        list[str]: A list of all possible combinations of well-formed parentheses.\n+\n+    Examples:\n+        >>> generate_parenthesis(num_pairs=3)\n+        ['((()))', '(()())', '(())()', '()(())', '()()()']\n+        >>> generate_parenthesis(num_pairs=1)\n+        ['()']\n+        >>> generate_parenthesis(num_pairs=0)\n+        ['']\n+    \"\"\"\n+\n+    result: list[str] = []\n+    helper_function_to_generate_parenthesis(num_pairs, \"\", result, 0, 0)\n+    return result\n+\n+\n+def print_all_parenthesis(total_list: list[str]) -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file backtracking/generate_parenthesis.py, please provide doctest for the function print_all_parenthesis",
    "line_number": 102,
    "enriched": "File: backtracking/generate_parenthesis.py\nCode: @@ -0,0 +1,126 @@\n+\"\"\"\n+    Given 'n' pairs of parentheses,\n+    this program generates all combinations of well-formed parentheses.\n+\n+    Example, n = 3:\n+    [\n+        \"((()))\",\n+        \"(()())\",\n+        \"(())()\",\n+        \"()(())\",\n+        \"()()()\",\n+    ]\n+\n+    The number of valid combinations of parentheses (Catalan number) for n pairs is:\n+    C(n) = (2n)! / ((n + 1)! * n!)\n+\n+    You can find more information about Catalan numbers and their applications here:\n+    https://en.wikipedia.org/wiki/Catalan_number\n+\n+    We use backtracking to solve this problem.\n+    Time complexity: O(4^n / sqrt(n))\n+    Space complexity: O(4^n / sqrt(n))\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+\n+def helper_function_to_generate_parenthesis(\n+    num_pairs: int,\n+    current_string: str,\n+    result: list[str],\n+    num_open: int,\n+    num_close: int,\n+) -> None:\n+    \"\"\"\n+    Helper function to generate all combinations of well-formed parentheses.\n+\n+    Args:\n+        num_pairs (int): The number of pairs of parentheses to use.\n+        current_string (str): The current string of parentheses.\n+        result (list[str]): The list of all possible combinations of parentheses.\n+        num_open (int): The number of open parentheses.\n+        num_close (int): The number of close parentheses.\n+\n+    Returns:\n+        None, but the result is updated.\n+\n+    Examples:\n+        >>> result = []\n+        >>> helper_function_to_generate_parenthesis(3, \"\", result, 0, 0)\n+        >>> result\n+        ['((()))', '(()())', '(())()', '()(())', '()()()']\n+        >>> result = []\n+        >>> helper_function_to_generate_parenthesis(1, \"\", result, 0, 0)\n+        >>> result\n+        ['()']\n+        >>> result = []\n+        >>> helper_function_to_generate_parenthesis(0, \"\", result, 0, 0)\n+        >>> result\n+        ['']\n+    \"\"\"\n+\n+    if num_open == num_pairs and num_close == num_pairs:\n+        result.append(current_string)\n+        return\n+\n+    if num_open < num_pairs:\n+        helper_function_to_generate_parenthesis(\n+            num_pairs, current_string + \"(\", result, num_open + 1, num_close\n+        )\n+\n+    if num_close < num_open:\n+        helper_function_to_generate_parenthesis(\n+            num_pairs, current_string + \")\", result, num_open, num_close + 1\n+        )\n+\n+\n+def generate_parenthesis(num_pairs: int) -> list[str]:\n+    \"\"\"\n+    Generate all combinations of well-formed parentheses given the number of pairs.\n+\n+    Args:\n+        num_pairs (int): The number of pairs of parentheses to use.\n+\n+    Returns:\n+        list[str]: A list of all possible combinations of well-formed parentheses.\n+\n+    Examples:\n+        >>> generate_parenthesis(num_pairs=3)\n+        ['((()))', '(()())', '(())()', '()(())', '()()()']\n+        >>> generate_parenthesis(num_pairs=1)\n+        ['()']\n+        >>> generate_parenthesis(num_pairs=0)\n+        ['']\n+    \"\"\"\n+\n+    result: list[str] = []\n+    helper_function_to_generate_parenthesis(num_pairs, \"\", result, 0, 0)\n+    return result\n+\n+\n+def print_all_parenthesis(total_list: list[str]) -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `backtracking/generate_parenthesis.py`, please provide doctest for the function `print_all_parenthesis`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "backtracking/generate_parenthesis.py",
    "pr_number": 9979,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349466686,
    "comment_created_at": "2023-10-07T05:45:52Z"
  },
  {
    "code": "@@ -0,0 +1,78 @@\n+# @Author  : ojas-wani\n+# @File    : laplacian_filter.py\n+# @Date    : 10/04/2023\n+\n+import numpy as np\n+from cv2 import (\n+    BORDER_DEFAULT,\n+    COLOR_BGR2GRAY,\n+    CV_64F,\n+    cvtColor,\n+    filter2D,\n+    imread,\n+    imshow,\n+    waitKey,\n+)\n+from digital_image_processing.filters.gaussian_filter import gaussian_filter\n+\n+def my_laplacian(src: np.ndarray, ksize: int) -> np.ndarray:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file digital_image_processing/filters/laplacian_filter.py, please provide doctest for the function my_laplacian",
    "line_number": 18,
    "enriched": "File: digital_image_processing/filters/laplacian_filter.py\nCode: @@ -0,0 +1,78 @@\n+# @Author  : ojas-wani\n+# @File    : laplacian_filter.py\n+# @Date    : 10/04/2023\n+\n+import numpy as np\n+from cv2 import (\n+    BORDER_DEFAULT,\n+    COLOR_BGR2GRAY,\n+    CV_64F,\n+    cvtColor,\n+    filter2D,\n+    imread,\n+    imshow,\n+    waitKey,\n+)\n+from digital_image_processing.filters.gaussian_filter import gaussian_filter\n+\n+def my_laplacian(src: np.ndarray, ksize: int) -> np.ndarray:\nComment: As there is no test file in this pull request nor any test function or class in the file `digital_image_processing/filters/laplacian_filter.py`, please provide doctest for the function `my_laplacian`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "digital_image_processing/filters/laplacian_filter.py",
    "pr_number": 9983,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349469414,
    "comment_created_at": "2023-10-07T06:07:33Z"
  },
  {
    "code": "@@ -1,24 +1,32 @@\n-import sys\n import webbrowser\n+from sys import argv\n+from urllib.parse import quote\n \n-import requests\n from bs4 import BeautifulSoup\n-from fake_useragent import UserAgent\n+from requests import get\n \n if __name__ == \"__main__\":\n+    if len(argv) > 1:\n+        query = \"%20\".join(argv[1:])\n+    else:\n+        query = quote(str(input(\"Search: \")))\n+\n     print(\"Googling.....\")\n-    url = \"https://www.google.com/search?q=\" + \" \".join(sys.argv[1:])\n-    res = requests.get(url, headers={\"UserAgent\": UserAgent().random})\n-    # res.raise_for_status()\n-    with open(\"project1a.html\", \"wb\") as out_file:  # only for knowing the class\n-        for data in res.iter_content(10000):\n-            out_file.write(data)\n-    soup = BeautifulSoup(res.text, \"html.parser\")\n-    links = list(soup.select(\".eZt8xd\"))[:5]\n \n-    print(len(links))\n-    for link in links:\n-        if link.text == \"Maps\":\n-            webbrowser.open(link.get(\"href\"))\n-        else:\n-            webbrowser.open(f\"http://google.com{link.get('href')}\")\n+    url = f\"https://www.google.com/search?q={query}&num=2\"\n+\n+    res = get(\n+        url,\n+        headers={\n+            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\"",
    "comment": "tests are failing on\r\n* web_programming/crawl_google_results.py:21:89: e501 line too long (106 > 88 characters)\r\n\r\nwhy is this version an improvement on the original?",
    "line_number": 21,
    "enriched": "File: web_programming/crawl_google_results.py\nCode: @@ -1,24 +1,32 @@\n-import sys\n import webbrowser\n+from sys import argv\n+from urllib.parse import quote\n \n-import requests\n from bs4 import BeautifulSoup\n-from fake_useragent import UserAgent\n+from requests import get\n \n if __name__ == \"__main__\":\n+    if len(argv) > 1:\n+        query = \"%20\".join(argv[1:])\n+    else:\n+        query = quote(str(input(\"Search: \")))\n+\n     print(\"Googling.....\")\n-    url = \"https://www.google.com/search?q=\" + \" \".join(sys.argv[1:])\n-    res = requests.get(url, headers={\"UserAgent\": UserAgent().random})\n-    # res.raise_for_status()\n-    with open(\"project1a.html\", \"wb\") as out_file:  # only for knowing the class\n-        for data in res.iter_content(10000):\n-            out_file.write(data)\n-    soup = BeautifulSoup(res.text, \"html.parser\")\n-    links = list(soup.select(\".eZt8xd\"))[:5]\n \n-    print(len(links))\n-    for link in links:\n-        if link.text == \"Maps\":\n-            webbrowser.open(link.get(\"href\"))\n-        else:\n-            webbrowser.open(f\"http://google.com{link.get('href')}\")\n+    url = f\"https://www.google.com/search?q={query}&num=2\"\n+\n+    res = get(\n+        url,\n+        headers={\n+            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\"\nComment: Tests are failing on\r\n* `web_programming/crawl_google_results.py:21:89: E501 line too long (106 > 88 characters)`\r\n\r\nWhy is this version an improvement on the original?\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "web_programming/crawl_google_results.py",
    "pr_number": 7085,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994209224,
    "comment_created_at": "2022-10-13T06:27:47Z"
  },
  {
    "code": "@@ -1,7 +1,7 @@\n from itertools import combinations\n \n \n-def find_triplets_with_0_sum(nums: list[int]) -> list[list[int]]:\n+def find_triplets_with_0_sum(nums: list) -> list:",
    "comment": "# why?!?",
    "line_number": 4,
    "enriched": "File: data_structures/arrays/find_triplets_with_0_sum.py\nCode: @@ -1,7 +1,7 @@\n from itertools import combinations\n \n \n-def find_triplets_with_0_sum(nums: list[int]) -> list[list[int]]:\n+def find_triplets_with_0_sum(nums: list) -> list:\nComment: # Why?!?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "data_structures/arrays/find_triplets_with_0_sum.py",
    "pr_number": 11134,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1382505594,
    "comment_created_at": "2023-11-05T04:41:02Z"
  },
  {
    "code": "@@ -0,0 +1,308 @@\n+import imageio.v2 as imageio\n+import numpy as np\n+\n+\n+def rmse(original: np.ndarray, reference: np.ndarray) -> float:\n+    \"\"\"Simple implementation of Root Mean Squared Error\n+    for two N dimensional numpy arrays.\n+    >>> rmse(np.array([1, 2, 3]), np.array([1, 2, 3]))\n+    0.0\n+    >>> rmse(np.array([1, 2, 3]), np.array([2, 2, 2]))\n+    1.0\n+    >>> rmse(np.array([1, 2, 3]), np.array([6, 4, 2]))\n+    30.0\n+    \"\"\"\n+    return np.sqrt(((original - reference) ** 2).mean())\n+\n+\n+def normalize_image(image: np.ndarray, cap: float = 255) -> np.ndarray:\n+    \"\"\"\n+    Normalizes image in Numpy 2D array format, between ranges 0-cap,\n+    as to fit uint8 type.\n+\n+    Args:\n+        image: 2D numpy array representing image as matrix, with values in any range\n+        cap: Maximum cap amount for normalization\n+    Returns:\n+        return 2D numpy array of type uint8, corresponding to limited range matrix\n+\n+    >>> normalize_image(np.array([[1, 2, 3], [4, 5, 10]]), cap=1)\n+    array([[ 0,         0.111111,  0.222222],\n+           [ 0.333333,  0.444444,  1.      ]], dtype=float32)\n+    >>> normalize_image(np.array([[4, 4, 3], [1, 7, 2]]))\n+\n+    \"\"\"\n+    normalized = (image - np.min(image)) / (np.max(image) - np.min(image)) * cap\n+    return normalized.astype(np.uint8)\n+\n+\n+def normalize_array(array: np.ndarray, cap: float = 1) -> np.ndarray:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file computer_vision/haralick_descriptors.py, please provide doctest for the function normalize_array",
    "line_number": 39,
    "enriched": "File: computer_vision/haralick_descriptors.py\nCode: @@ -0,0 +1,308 @@\n+import imageio.v2 as imageio\n+import numpy as np\n+\n+\n+def rmse(original: np.ndarray, reference: np.ndarray) -> float:\n+    \"\"\"Simple implementation of Root Mean Squared Error\n+    for two N dimensional numpy arrays.\n+    >>> rmse(np.array([1, 2, 3]), np.array([1, 2, 3]))\n+    0.0\n+    >>> rmse(np.array([1, 2, 3]), np.array([2, 2, 2]))\n+    1.0\n+    >>> rmse(np.array([1, 2, 3]), np.array([6, 4, 2]))\n+    30.0\n+    \"\"\"\n+    return np.sqrt(((original - reference) ** 2).mean())\n+\n+\n+def normalize_image(image: np.ndarray, cap: float = 255) -> np.ndarray:\n+    \"\"\"\n+    Normalizes image in Numpy 2D array format, between ranges 0-cap,\n+    as to fit uint8 type.\n+\n+    Args:\n+        image: 2D numpy array representing image as matrix, with values in any range\n+        cap: Maximum cap amount for normalization\n+    Returns:\n+        return 2D numpy array of type uint8, corresponding to limited range matrix\n+\n+    >>> normalize_image(np.array([[1, 2, 3], [4, 5, 10]]), cap=1)\n+    array([[ 0,         0.111111,  0.222222],\n+           [ 0.333333,  0.444444,  1.      ]], dtype=float32)\n+    >>> normalize_image(np.array([[4, 4, 3], [1, 7, 2]]))\n+\n+    \"\"\"\n+    normalized = (image - np.min(image)) / (np.max(image) - np.min(image)) * cap\n+    return normalized.astype(np.uint8)\n+\n+\n+def normalize_array(array: np.ndarray, cap: float = 1) -> np.ndarray:\nComment: As there is no test file in this pull request nor any test function or class in the file `computer_vision/haralick_descriptors.py`, please provide doctest for the function `normalize_array`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "computer_vision/haralick_descriptors.py",
    "pr_number": 7418,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 998947388,
    "comment_created_at": "2022-10-19T05:09:46Z"
  },
  {
    "code": "@@ -0,0 +1,81 @@\n+'''\n+You are given a rows x cols matrix grid representing a field of\n+cherries where grid[i][j] represents the\n+number of cherries that you can\n+collect from the (i, j) cell.\n+\n+You have two robots that can\n+collect cherries for you:\n+\n+Robot #1 is located at the top-left \n+corner (0, 0), and\n+Robot #2 is located at the\n+top-right corner (0, cols - 1).\n+\n+Return the maximum number of cherries\n+collection using both robots by\n+following the rules below:\n+\n+1. From a cell (i, j), robots can move to cell\n+   (i + 1, j - 1), (i + 1, j), or (i + 1, j + 1).\n+2. When any robot passes through a cell, \n+   picks up all cherries,\n+   and the cell becomes an empty cell.\n+3. When both robots stay in the same cell,\n+   only one takes the cherries.\n+4. Both robots cannot move outside\n+   of the grid at any moment.\n+5. Both robots should reach the bottom row in grid.\n+\n+Problem Statement:-\n+   https://leetcode.com/problems/cherry-pickup-ii\n+\n+'''\n+\n+\t\t\n+from typing import List\n+from collections import defaultdict\n+\n+\n+class Solution:\n+    def cherrypickup(self, grid: list[list[int]]) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/cherry_pickup_ii.py, please provide doctest for the function cherrypickup",
    "line_number": 41,
    "enriched": "File: dynamic_programming/cherry_pickup_ii.py\nCode: @@ -0,0 +1,81 @@\n+'''\n+You are given a rows x cols matrix grid representing a field of\n+cherries where grid[i][j] represents the\n+number of cherries that you can\n+collect from the (i, j) cell.\n+\n+You have two robots that can\n+collect cherries for you:\n+\n+Robot #1 is located at the top-left \n+corner (0, 0), and\n+Robot #2 is located at the\n+top-right corner (0, cols - 1).\n+\n+Return the maximum number of cherries\n+collection using both robots by\n+following the rules below:\n+\n+1. From a cell (i, j), robots can move to cell\n+   (i + 1, j - 1), (i + 1, j), or (i + 1, j + 1).\n+2. When any robot passes through a cell, \n+   picks up all cherries,\n+   and the cell becomes an empty cell.\n+3. When both robots stay in the same cell,\n+   only one takes the cherries.\n+4. Both robots cannot move outside\n+   of the grid at any moment.\n+5. Both robots should reach the bottom row in grid.\n+\n+Problem Statement:-\n+   https://leetcode.com/problems/cherry-pickup-ii\n+\n+'''\n+\n+\t\t\n+from typing import List\n+from collections import defaultdict\n+\n+\n+class Solution:\n+    def cherrypickup(self, grid: list[list[int]]) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/cherry_pickup_ii.py`, please provide doctest for the function `cherrypickup`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/cherry_pickup_ii.py",
    "pr_number": 9934,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349056902,
    "comment_created_at": "2023-10-06T17:18:47Z"
  },
  {
    "code": "@@ -3,12 +3,20 @@\n \n \n def stock_price(symbol: str = \"AAPL\") -> str:\n-    url = f\"https://in.finance.yahoo.com/quote/{symbol}?s={symbol}\"\n-    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n-    class_ = \"My(6px) Pos(r) smartphone_Mt(6px)\"\n-    return soup.find(\"div\", class_=class_).find(\"span\").text\n+    url = f\"https://finance.yahoo.com/quote/{symbol}?p={symbol}\"\n+    yahoo_finance_source = requests.get(url, headers={\"USER-AGENT\": \"Mozilla/5.0\"}).text\n+    soup = BeautifulSoup(yahoo_finance_source, \"html.parser\")\n+    specific_fin_streamer_tag = soup.find(\"fin-streamer\", {\"data-test\": \"qsp-price\"})\n \n+    if specific_fin_streamer_tag:\n+        text = specific_fin_streamer_tag.get_text()\n+        return text\n+    else:\n+        print(\"No <fin-streamer> tag with the specified data-test attribute found.\")\n+    return \"Not Found\"",
    "comment": "also, why both print and return? why not just return the error message? or print the error message and return none?",
    "line_number": 16,
    "enriched": "File: web_programming/current_stock_price.py\nCode: @@ -3,12 +3,20 @@\n \n \n def stock_price(symbol: str = \"AAPL\") -> str:\n-    url = f\"https://in.finance.yahoo.com/quote/{symbol}?s={symbol}\"\n-    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n-    class_ = \"My(6px) Pos(r) smartphone_Mt(6px)\"\n-    return soup.find(\"div\", class_=class_).find(\"span\").text\n+    url = f\"https://finance.yahoo.com/quote/{symbol}?p={symbol}\"\n+    yahoo_finance_source = requests.get(url, headers={\"USER-AGENT\": \"Mozilla/5.0\"}).text\n+    soup = BeautifulSoup(yahoo_finance_source, \"html.parser\")\n+    specific_fin_streamer_tag = soup.find(\"fin-streamer\", {\"data-test\": \"qsp-price\"})\n \n+    if specific_fin_streamer_tag:\n+        text = specific_fin_streamer_tag.get_text()\n+        return text\n+    else:\n+        print(\"No <fin-streamer> tag with the specified data-test attribute found.\")\n+    return \"Not Found\"\nComment: ```suggestion\r\n    if specific_fin_streamer_tag:\r\n        text = specific_fin_streamer_tag.get_text()\r\n        return text\r\n    \r\n    print(\"No <fin-streamer> tag with the specified data-test attribute found.\")\r\n    return \"Not Found\"\r\n```\r\n\r\nAlso, why both print and return? Why not just return the error message? Or print the error message and return None?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "web_programming/current_stock_price.py",
    "pr_number": 8942,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1290839887,
    "comment_created_at": "2023-08-11T02:31:02Z"
  },
  {
    "code": "@@ -0,0 +1,76 @@\n+\"\"\"\n+Performs multiplication of two binary strings using the Karatsuba algorithm.\n+\n+Given two binary strings of equal length n, the goal is to compute\n+their product as integers.\n+\n+For example:\n+\"1100\" (12) \u00d7 \"1010\" (10) = 120\n+\n+Karatsuba's algorithm reduces the multiplication of two n-bit numbers\n+to at most three multiplications of n/2-bit numbers. It achieves\n+a time complexity of O(n^log\u20823) \u2248 O(n^1.585), which is faster\n+than the naive O(n\u00b2) approach.\n+\n+References:\n+https://en.wikipedia.org/wiki/Karatsuba_algorithm\n+\n+Example:\n+>>> karatsuba_multiply(\"1100\", \"1010\")\n+120\n+>>> karatsuba_multiply(\"10\", \"11\")\n+6\n+>>> karatsuba_multiply(\"101\", \"111\")\n+35\n+>>> karatsuba_multiply(\"0\", \"0\")\n+0\n+>>> karatsuba_multiply(\"1\", \"0\")\n+0\n+\"\"\"\n+\n+def karatsuba_multiply(x: str, y: str) -> int:",
    "comment": "please provide descriptive name for the parameter: x\n\nplease provide descriptive name for the parameter: y",
    "line_number": 31,
    "enriched": "File: divide_and_conquer/karatsuba_multiplication.py\nCode: @@ -0,0 +1,76 @@\n+\"\"\"\n+Performs multiplication of two binary strings using the Karatsuba algorithm.\n+\n+Given two binary strings of equal length n, the goal is to compute\n+their product as integers.\n+\n+For example:\n+\"1100\" (12) \u00d7 \"1010\" (10) = 120\n+\n+Karatsuba's algorithm reduces the multiplication of two n-bit numbers\n+to at most three multiplications of n/2-bit numbers. It achieves\n+a time complexity of O(n^log\u20823) \u2248 O(n^1.585), which is faster\n+than the naive O(n\u00b2) approach.\n+\n+References:\n+https://en.wikipedia.org/wiki/Karatsuba_algorithm\n+\n+Example:\n+>>> karatsuba_multiply(\"1100\", \"1010\")\n+120\n+>>> karatsuba_multiply(\"10\", \"11\")\n+6\n+>>> karatsuba_multiply(\"101\", \"111\")\n+35\n+>>> karatsuba_multiply(\"0\", \"0\")\n+0\n+>>> karatsuba_multiply(\"1\", \"0\")\n+0\n+\"\"\"\n+\n+def karatsuba_multiply(x: str, y: str) -> int:\nComment: Please provide descriptive name for the parameter: `x`\n\nPlease provide descriptive name for the parameter: `y`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "divide_and_conquer/karatsuba_multiplication.py",
    "pr_number": 13298,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2409295433,
    "comment_created_at": "2025-10-07T04:17:37Z"
  },
  {
    "code": "@@ -0,0 +1,49 @@\n+\"\"\"\n+TT-ENTAILS Algorithm (Propositional Logic)\n+Reference: [Russell & Norvig, Artificial Intelligence: A Modern Approach, Ch. 7](https://aima.cs.berkeley.edu/)\n+Wikipedia: [Entailment](https://en.wikipedia.org/wiki/Entailment)\n+\n+This algorithm checks if a knowledge base (KB) entails a query sentence (a)\n+using truth tables. Returns True if KB entails a, False otherwise.\n+\"\"\"\n+\n+import itertools\n+\n+def tt_entails(kb: list[str], query: str, symbols: list[str]) -> bool:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/ttentails.py, please provide doctest for the function tt_entails",
    "line_number": 12,
    "enriched": "File: machine_learning/ttentails.py\nCode: @@ -0,0 +1,49 @@\n+\"\"\"\n+TT-ENTAILS Algorithm (Propositional Logic)\n+Reference: [Russell & Norvig, Artificial Intelligence: A Modern Approach, Ch. 7](https://aima.cs.berkeley.edu/)\n+Wikipedia: [Entailment](https://en.wikipedia.org/wiki/Entailment)\n+\n+This algorithm checks if a knowledge base (KB) entails a query sentence (a)\n+using truth tables. Returns True if KB entails a, False otherwise.\n+\"\"\"\n+\n+import itertools\n+\n+def tt_entails(kb: list[str], query: str, symbols: list[str]) -> bool:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/ttentails.py`, please provide doctest for the function `tt_entails`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/ttentails.py",
    "pr_number": 12901,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2283154350,
    "comment_created_at": "2025-08-18T18:34:53Z"
  },
  {
    "code": "@@ -11,24 +11,35 @@\n \"\"\"\n \n \n-def heapify(unsorted, index, heap_size):\n+def heapify(unsorted_list: list[int], index: int, heap_size: int) -> None:\n+    \"\"\"\n+\n+    :param unsorted_list: unsorted list containing integers numbers\n+    :param index: index\n+    :param heap_size: size of the heap\n+    :return: None\n+    \"\"\"",
    "comment": "could you add some doctests for this function?",
    "line_number": 21,
    "enriched": "File: sorts/heap_sort.py\nCode: @@ -11,24 +11,35 @@\n \"\"\"\n \n \n-def heapify(unsorted, index, heap_size):\n+def heapify(unsorted_list: list[int], index: int, heap_size: int) -> None:\n+    \"\"\"\n+\n+    :param unsorted_list: unsorted list containing integers numbers\n+    :param index: index\n+    :param heap_size: size of the heap\n+    :return: None\n+    \"\"\"\nComment: Could you add some doctests for this function?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "sorts/heap_sort.py",
    "pr_number": 9949,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359599096,
    "comment_created_at": "2023-10-14T19:06:02Z"
  },
  {
    "code": "@@ -0,0 +1,92 @@\n+from typing import List, Tuple\n+\n+def equi_emf(",
    "comment": "as there is no test file in this pull request nor any test function or class in the file electronics/equivalent_emf.py, please provide doctest for the function equi_emf",
    "line_number": 3,
    "enriched": "File: electronics/equivalent_emf.py\nCode: @@ -0,0 +1,92 @@\n+from typing import List, Tuple\n+\n+def equi_emf(\nComment: As there is no test file in this pull request nor any test function or class in the file `electronics/equivalent_emf.py`, please provide doctest for the function `equi_emf`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "electronics/equivalent_emf.py",
    "pr_number": 10044,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349550089,
    "comment_created_at": "2023-10-07T17:37:16Z"
  },
  {
    "code": "@@ -17,32 +17,52 @@ def evaluate_postfix(postfix_notation: list) -> int:\n     9\n     >>> evaluate_postfix([\"4\", \"13\", \"5\", \"/\", \"+\"])\n     6\n+    >>> evaluate_postfix([\"2\", \"+\"])\n+    2\n+    >>> evaluate_postfix([\"5\", \"-\"])\n+    -5\n     >>> evaluate_postfix([])\n     0\n     \"\"\"\n     if not postfix_notation:\n         return 0\n \n     operations = {\"+\", \"-\", \"*\", \"/\"}\n+    unary_operations = {\"+\"}  # Unary operator(s)",
    "comment": "unary_operations is not currently being used.",
    "line_number": 31,
    "enriched": "File: data_structures/stacks/evaluate_postfix_notations.py\nCode: @@ -17,32 +17,52 @@ def evaluate_postfix(postfix_notation: list) -> int:\n     9\n     >>> evaluate_postfix([\"4\", \"13\", \"5\", \"/\", \"+\"])\n     6\n+    >>> evaluate_postfix([\"2\", \"+\"])\n+    2\n+    >>> evaluate_postfix([\"5\", \"-\"])\n+    -5\n     >>> evaluate_postfix([])\n     0\n     \"\"\"\n     if not postfix_notation:\n         return 0\n \n     operations = {\"+\", \"-\", \"*\", \"/\"}\n+    unary_operations = {\"+\"}  # Unary operator(s)\nComment: `unary_operations` is not currently being used.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "data_structures/stacks/evaluate_postfix_notations.py",
    "pr_number": 8758,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1208643279,
    "comment_created_at": "2023-05-28T20:18:14Z"
  },
  {
    "code": "@@ -0,0 +1,137 @@\n+\"\"\"\n+\n+\n+Source:\n+- https://courses.lumenlearning.com/suny-osuniversityphysics/chapter/14-6-bernoullis-equation/\n+\"\"\"\n+\n+# Acceleration Constant on Earth (unit m/s^2)\n+g = 9.80665\n+\n+\n+def bernoulli_static_equation(\n+    fluid_density: float, h: float, gravity: float = g, initial_pressure: float = 0",
    "comment": "please provide descriptive name for the parameter: h",
    "line_number": 13,
    "enriched": "File: physics/bernoullis_equation.py\nCode: @@ -0,0 +1,137 @@\n+\"\"\"\n+\n+\n+Source:\n+- https://courses.lumenlearning.com/suny-osuniversityphysics/chapter/14-6-bernoullis-equation/\n+\"\"\"\n+\n+# Acceleration Constant on Earth (unit m/s^2)\n+g = 9.80665\n+\n+\n+def bernoulli_static_equation(\n+    fluid_density: float, h: float, gravity: float = g, initial_pressure: float = 0\nComment: Please provide descriptive name for the parameter: `h`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "physics/bernoullis_equation.py",
    "pr_number": 7200,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996237299,
    "comment_created_at": "2022-10-15T02:01:06Z"
  },
  {
    "code": "@@ -0,0 +1,105 @@\n+from __future__ import annotations\n+from enum import Enum, unique\n+from typing import TypeVar\n+\n+# Create a generic variable that can be 'Enum', or any subclass.\n+T = TypeVar(\"T\", bound=Enum)\n+\n+@unique\n+class BinaryUnit(Enum):\n+    yotta = 80\n+    zetta = 70\n+    exa = 60\n+    peta = 50\n+    tera = 40\n+    giga = 30\n+    mega = 20\n+    kilo = 10\n+\n+@unique\n+class SIUnit(Enum):\n+    yotta = 24\n+    zetta = 21\n+    exa = 18\n+    peta = 15\n+    tera = 12\n+    giga = 9\n+    mega = 6\n+    kilo = 3\n+    hecto = 2\n+    deca = 1\n+    deci = -1\n+    centi = -2\n+    milli = -3\n+    micro = -6\n+    nano = -9\n+    pico = -12\n+    femto = -15\n+    atto = -18\n+    zepto = -21\n+    yocto = -24\n+\n+    @classmethod\n+    def get_positive(cls: type[T]) -> dict:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file conversions/infix_to_prefix_conversions.py, please provide doctest for the function get_positive",
    "line_number": 43,
    "enriched": "File: conversions/infix_to_prefix_conversions.py\nCode: @@ -0,0 +1,105 @@\n+from __future__ import annotations\n+from enum import Enum, unique\n+from typing import TypeVar\n+\n+# Create a generic variable that can be 'Enum', or any subclass.\n+T = TypeVar(\"T\", bound=Enum)\n+\n+@unique\n+class BinaryUnit(Enum):\n+    yotta = 80\n+    zetta = 70\n+    exa = 60\n+    peta = 50\n+    tera = 40\n+    giga = 30\n+    mega = 20\n+    kilo = 10\n+\n+@unique\n+class SIUnit(Enum):\n+    yotta = 24\n+    zetta = 21\n+    exa = 18\n+    peta = 15\n+    tera = 12\n+    giga = 9\n+    mega = 6\n+    kilo = 3\n+    hecto = 2\n+    deca = 1\n+    deci = -1\n+    centi = -2\n+    milli = -3\n+    micro = -6\n+    nano = -9\n+    pico = -12\n+    femto = -15\n+    atto = -18\n+    zepto = -21\n+    yocto = -24\n+\n+    @classmethod\n+    def get_positive(cls: type[T]) -> dict:\nComment: As there is no test file in this pull request nor any test function or class in the file `conversions/infix_to_prefix_conversions.py`, please provide doctest for the function `get_positive`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "conversions/infix_to_prefix_conversions.py",
    "pr_number": 9419,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342384012,
    "comment_created_at": "2023-10-02T08:01:36Z"
  },
  {
    "code": "@@ -0,0 +1,89 @@\n+import glob\n+import os\n+import random\n+from string import ascii_lowercase, digits\n+\n+import cv2\n+import numpy as np\n+\n+\n+# Params\n+IMAGE_DIR = \"\"\n+OUTPUT_DIR = \"\"\n+BRIGHTNESS_PERCENT = 50  # (between 0 and 100)\n+\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file computer_vision/brightness_augmentation.py, please provide doctest for the function main",
    "line_number": 16,
    "enriched": "File: computer_vision/brightness_augmentation.py\nCode: @@ -0,0 +1,89 @@\n+import glob\n+import os\n+import random\n+from string import ascii_lowercase, digits\n+\n+import cv2\n+import numpy as np\n+\n+\n+# Params\n+IMAGE_DIR = \"\"\n+OUTPUT_DIR = \"\"\n+BRIGHTNESS_PERCENT = 50  # (between 0 and 100)\n+\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `computer_vision/brightness_augmentation.py`, please provide doctest for the function `main`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "computer_vision/brightness_augmentation.py",
    "pr_number": 8050,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1057515947,
    "comment_created_at": "2022-12-27T08:16:05Z"
  },
  {
    "code": "@@ -0,0 +1,65 @@\n+\"\"\"\n+Author Suvan Banerjee (@suvanbanerjee)\n+Date: 2023-02-10\n+Description: This program calculates the future value of money based on the present\n+value, inflation rate, and number of years.\n+\n+--------------------------------------------\n+In economics, inflation is a general increase of the prices. This is usually measured\n+using the consumer price index (CPI).When the general price level rises, each unit of\n+ currency buys fewer goods and services; consequently, inflation corresponds\n+to a reduction in the purchasing power of money.\n+\n+Source: https://en.wikipedia.org/wiki/Inflation\n+\n+\"\"\"\n+\n+\n+\n+def calculate_future_value(\n+    present_value: float, inflation_rate: float, years: int\n+) -> float:\n+    \"\"\"\n+    Calculate the future value of money considering inflation.\n+\n+    Args:\n+        present_value (float): The present value of money.\n+        inflation_rate (float): The annual inflation rate as a percentage.\n+        years (int): The number of years into the future.\n+\n+    Returns:\n+        float: The future value of money adjusted for inflation.\n+\n+    Formula:\n+    Future Value = Present Value * (1 + Inflation Rate/100)^Years\n+\n+    >>> calculate_future_value(1000, 2, 5)\n+    1104.0808032\n+    >>> calculate_future_value(5000, 3.5, 10)\n+    7052.993803105605\n+    >>> calculate_future_value(10000, 1.5, 20)\n+    13468.550065500534\n+    \"\"\"\n+\n+    future_value = present_value * (1 + inflation_rate / 100) ** years\n+    return future_value\n+\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file financial/inflation.py, please provide doctest for the function main",
    "line_number": 48,
    "enriched": "File: financial/inflation.py\nCode: @@ -0,0 +1,65 @@\n+\"\"\"\n+Author Suvan Banerjee (@suvanbanerjee)\n+Date: 2023-02-10\n+Description: This program calculates the future value of money based on the present\n+value, inflation rate, and number of years.\n+\n+--------------------------------------------\n+In economics, inflation is a general increase of the prices. This is usually measured\n+using the consumer price index (CPI).When the general price level rises, each unit of\n+ currency buys fewer goods and services; consequently, inflation corresponds\n+to a reduction in the purchasing power of money.\n+\n+Source: https://en.wikipedia.org/wiki/Inflation\n+\n+\"\"\"\n+\n+\n+\n+def calculate_future_value(\n+    present_value: float, inflation_rate: float, years: int\n+) -> float:\n+    \"\"\"\n+    Calculate the future value of money considering inflation.\n+\n+    Args:\n+        present_value (float): The present value of money.\n+        inflation_rate (float): The annual inflation rate as a percentage.\n+        years (int): The number of years into the future.\n+\n+    Returns:\n+        float: The future value of money adjusted for inflation.\n+\n+    Formula:\n+    Future Value = Present Value * (1 + Inflation Rate/100)^Years\n+\n+    >>> calculate_future_value(1000, 2, 5)\n+    1104.0808032\n+    >>> calculate_future_value(5000, 3.5, 10)\n+    7052.993803105605\n+    >>> calculate_future_value(10000, 1.5, 20)\n+    13468.550065500534\n+    \"\"\"\n+\n+    future_value = present_value * (1 + inflation_rate / 100) ** years\n+    return future_value\n+\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `financial/inflation.py`, please provide doctest for the function `main`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "financial/inflation.py",
    "pr_number": 9398,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342335325,
    "comment_created_at": "2023-10-02T07:00:02Z"
  },
  {
    "code": "@@ -0,0 +1,80 @@\n+class Node:\r\n+    def __init__(self, value: int = 0) -> None:\r\n+        self.value = value\r\n+        self.left = None\r\n+        self.right = None\r\n+\r\n+\r\n+class PersistentSegmentTree:\r\n+    def __init__(self, arr: list[int]) -> None:\r\n+        self.n = len(arr)\r\n+        self.roots: list[Node] = []\r\n+        self.roots.append(self._build(arr, 0, self.n - 1))\r\n+\r\n+    def _build(self, arr: list[int], start: int, end: int) -> Node:\r\n+        \"\"\"\r\n+        Builds a segment tree from the provided array.\r\n+\r\n+        >>> pst = PersistentSegmentTree([1, 2, 3])\r\n+        >>> root = pst._build([1, 2, 3], 0, 2)\r\n+        >>> root.value  # Sum of the whole array\r\n+        6\r\n+        \"\"\"\r\n+        if start == end:\r\n+            return Node(arr[start])\r\n+        mid = (start + end) // 2\r\n+        node = Node()\r\n+        node.left = self._build(arr, start, mid)\r\n+        node.right = self._build(arr, mid + 1, end)\r\n+        node.value = node.left.value + node.right.value\r\n+        return node\r\n+\r\n+    def update(self, version: int, index: int, value: int) -> int:\r\n+        \"\"\"\r\n+        Updates the segment tree with a new value at the specified index.\r\n+\r\n+        >>> pst = PersistentSegmentTree([1, 2, 3])\r\n+        >>> version_1 = pst.update(0, 1, 5)\r\n+        >>> pst.query(version_1, 0, 2)  # Query sum from index 0 to 2\r\n+        9\r\n+        \"\"\"\r\n+        new_root = self._update(self.roots[version], 0, self.n - 1, index, value)\r\n+        self.roots.append(new_root)\r\n+        return len(self.roots) - 1  # return the index of the new version\r\n+\r\n+    def _update(self, node: Node, start: int, end: int, index: int, value: int) -> Node:\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/persistent_segment_tree.py, please provide doctest for the function _update",
    "line_number": 45,
    "enriched": "File: data_structures/persistent_segment_tree.py\nCode: @@ -0,0 +1,80 @@\n+class Node:\r\n+    def __init__(self, value: int = 0) -> None:\r\n+        self.value = value\r\n+        self.left = None\r\n+        self.right = None\r\n+\r\n+\r\n+class PersistentSegmentTree:\r\n+    def __init__(self, arr: list[int]) -> None:\r\n+        self.n = len(arr)\r\n+        self.roots: list[Node] = []\r\n+        self.roots.append(self._build(arr, 0, self.n - 1))\r\n+\r\n+    def _build(self, arr: list[int], start: int, end: int) -> Node:\r\n+        \"\"\"\r\n+        Builds a segment tree from the provided array.\r\n+\r\n+        >>> pst = PersistentSegmentTree([1, 2, 3])\r\n+        >>> root = pst._build([1, 2, 3], 0, 2)\r\n+        >>> root.value  # Sum of the whole array\r\n+        6\r\n+        \"\"\"\r\n+        if start == end:\r\n+            return Node(arr[start])\r\n+        mid = (start + end) // 2\r\n+        node = Node()\r\n+        node.left = self._build(arr, start, mid)\r\n+        node.right = self._build(arr, mid + 1, end)\r\n+        node.value = node.left.value + node.right.value\r\n+        return node\r\n+\r\n+    def update(self, version: int, index: int, value: int) -> int:\r\n+        \"\"\"\r\n+        Updates the segment tree with a new value at the specified index.\r\n+\r\n+        >>> pst = PersistentSegmentTree([1, 2, 3])\r\n+        >>> version_1 = pst.update(0, 1, 5)\r\n+        >>> pst.query(version_1, 0, 2)  # Query sum from index 0 to 2\r\n+        9\r\n+        \"\"\"\r\n+        new_root = self._update(self.roots[version], 0, self.n - 1, index, value)\r\n+        self.roots.append(new_root)\r\n+        return len(self.roots) - 1  # return the index of the new version\r\n+\r\n+    def _update(self, node: Node, start: int, end: int, index: int, value: int) -> Node:\r\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/persistent_segment_tree.py`, please provide doctest for the function `_update`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/persistent_segment_tree.py",
    "pr_number": 12176,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1807283239,
    "comment_created_at": "2024-10-19T10:30:31Z"
  },
  {
    "code": "@@ -1,17 +1,23 @@\n-def bubble_sort(collection):\n-    \"\"\"Pure implementation of bubble sort algorithm in Python\n-\n+#!/usr/bin/python3\n+def bubble_sort(collection: list, isascending: bool = True):\n+    \"\"\"\n+    This Is a Pure implementation of bubble sort algorithm in Python\n     :param collection: some mutable ordered collection with heterogeneous\n     comparable items inside\n-    :return: the same collection ordered by ascending\n+    :param isAccending: a boolean that determines if the output should be\\\n+    Sorted in an Ascending or Descending manner (Defaults to Acceding\\\n+    if not specified)\n+    :return: the same collection ordered by the isAccending param\n \n     Examples:\n     >>> bubble_sort([0, 5, 2, 3, 2])\n     [0, 2, 2, 3, 5]\n-    >>> bubble_sort([0, 5, 2, 3, 2]) == sorted([0, 5, 2, 3, 2])\n-    True\n-    >>> bubble_sort([]) == sorted([])",
    "comment": "why delete this test???",
    "line_number": 13,
    "enriched": "File: sorts/bubble_sort.py\nCode: @@ -1,17 +1,23 @@\n-def bubble_sort(collection):\n-    \"\"\"Pure implementation of bubble sort algorithm in Python\n-\n+#!/usr/bin/python3\n+def bubble_sort(collection: list, isascending: bool = True):\n+    \"\"\"\n+    This Is a Pure implementation of bubble sort algorithm in Python\n     :param collection: some mutable ordered collection with heterogeneous\n     comparable items inside\n-    :return: the same collection ordered by ascending\n+    :param isAccending: a boolean that determines if the output should be\\\n+    Sorted in an Ascending or Descending manner (Defaults to Acceding\\\n+    if not specified)\n+    :return: the same collection ordered by the isAccending param\n \n     Examples:\n     >>> bubble_sort([0, 5, 2, 3, 2])\n     [0, 2, 2, 3, 5]\n-    >>> bubble_sort([0, 5, 2, 3, 2]) == sorted([0, 5, 2, 3, 2])\n-    True\n-    >>> bubble_sort([]) == sorted([])\nComment: Why delete this test???",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "sorts/bubble_sort.py",
    "pr_number": 8816,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1233336002,
    "comment_created_at": "2023-06-18T16:34:56Z"
  },
  {
    "code": "@@ -0,0 +1,438 @@\n+\"\"\"\n+Created on Mon Feb 26 14:29:11 2018\n+\n+@author: Christian Bender\n+@license: MIT-license\n+\n+This module contains some useful classes and functions for dealing\n+with linear algebra in python.\n+\n+Overview:\n+\n+- class Vector\n+- function zero_vector(dimension)\n+- function unit_basis_vector(dimension, pos)\n+- function axpy(scalar, vector1, vector2)\n+- function random_vector(N, a, b)\n+- class Matrix\n+- function square_zero_matrix(N)\n+- function random_matrix(W, H, a, b)\n+\"\"\"\n+from __future__ import annotations\n+\n+import math\n+import random\n+from collections.abc import Collection\n+from typing import overload\n+\n+\n+class Vector:\n+    \"\"\"\n+    This class represents a vector of arbitrary size.\n+    You need to give the vector components.\n+\n+    Overview of the methods:\n+\n+        __init__(components: Collection[float] | None): init the vector\n+        __len__(): gets the size of the vector (number of components)\n+        __str__(): returns a string representation\n+        __add__(other: Vector): vector addition\n+        __sub__(other: Vector): vector subtraction\n+        __mul__(other: float): scalar multiplication\n+        __mul__(other: Vector): dot product\n+        copy(): copies this vector and returns it\n+        component(i): gets the i-th component (0-indexed)\n+        change_component(pos: int, value: float): changes specified component\n+        euclidean_length(): returns the euclidean length of the vector\n+        angle(other: Vector, deg: bool): returns the angle between two vectors\n+        TODO: compare-operator\n+    \"\"\"\n+\n+    def __init__(self, components: Collection[float] | None = None) -> None:\n+        \"\"\"\n+        input: components or nothing\n+        simple constructor for init the vector\n+        \"\"\"\n+        if components is None:\n+            components = []\n+        self.__components = list(components)\n+\n+    def __len__(self) -> int:\n+        \"\"\"\n+        returns the size of the vector\n+        \"\"\"\n+        return len(self.__components)\n+\n+    def __str__(self) -> str:\n+        \"\"\"\n+        returns a string representation of the vector\n+        \"\"\"\n+        return \"(\" + \",\".join(map(str, self.__components)) + \")\"\n+\n+    def __add__(self, other: Vector) -> Vector:\n+        \"\"\"\n+        input: other vector\n+        assumes: other vector has the same size\n+        returns a new vector that represents the sum.\n+        \"\"\"\n+        size = len(self)\n+        if size == len(other):\n+            result = [self.__components[i] + other.component(i) for i in range(size)]\n+            return Vector(result)\n+        else:\n+            raise Exception(\"must have the same size\")\n+\n+    def __sub__(self, other: Vector) -> Vector:\n+        \"\"\"\n+        input: other vector\n+        assumes: other vector has the same size\n+        returns a new vector that represents the difference.\n+        \"\"\"\n+        size = len(self)\n+        if size == len(other):\n+            result = [self.__components[i] - other.component(i) for i in range(size)]\n+            return Vector(result)\n+        else:  # error case\n+            raise Exception(\"must have the same size\")\n+\n+    @overload\n+    def __mul__(self, other: float) -> Vector:\n+        ...\n+\n+    @overload\n+    def __mul__(self, other: Vector) -> float:\n+        ...\n+\n+    def __mul__(self, other: float | Vector) -> float | Vector:\n+        \"\"\"\n+        mul implements the scalar multiplication\n+        and the dot-product\n+        \"\"\"\n+        if isinstance(other, float) or isinstance(other, int):\n+            ans = [c * other for c in self.__components]\n+            return Vector(ans)\n+        elif isinstance(other, Vector) and len(self) == len(other):\n+            size = len(self)\n+            prods = [self.__components[i] * other.component(i) for i in range(size)]\n+            return sum(prods)\n+        else:  # error case\n+            raise Exception(\"invalid operand!\")\n+\n+    def copy(self) -> Vector:\n+        \"\"\"\n+        copies this vector and returns it.\n+        \"\"\"\n+        return Vector(self.__components)\n+\n+    def component(self, i: int) -> float:",
    "comment": "please provide descriptive name for the parameter: i",
    "line_number": 127,
    "enriched": "File: linear_algebra/lib.py\nCode: @@ -0,0 +1,438 @@\n+\"\"\"\n+Created on Mon Feb 26 14:29:11 2018\n+\n+@author: Christian Bender\n+@license: MIT-license\n+\n+This module contains some useful classes and functions for dealing\n+with linear algebra in python.\n+\n+Overview:\n+\n+- class Vector\n+- function zero_vector(dimension)\n+- function unit_basis_vector(dimension, pos)\n+- function axpy(scalar, vector1, vector2)\n+- function random_vector(N, a, b)\n+- class Matrix\n+- function square_zero_matrix(N)\n+- function random_matrix(W, H, a, b)\n+\"\"\"\n+from __future__ import annotations\n+\n+import math\n+import random\n+from collections.abc import Collection\n+from typing import overload\n+\n+\n+class Vector:\n+    \"\"\"\n+    This class represents a vector of arbitrary size.\n+    You need to give the vector components.\n+\n+    Overview of the methods:\n+\n+        __init__(components: Collection[float] | None): init the vector\n+        __len__(): gets the size of the vector (number of components)\n+        __str__(): returns a string representation\n+        __add__(other: Vector): vector addition\n+        __sub__(other: Vector): vector subtraction\n+        __mul__(other: float): scalar multiplication\n+        __mul__(other: Vector): dot product\n+        copy(): copies this vector and returns it\n+        component(i): gets the i-th component (0-indexed)\n+        change_component(pos: int, value: float): changes specified component\n+        euclidean_length(): returns the euclidean length of the vector\n+        angle(other: Vector, deg: bool): returns the angle between two vectors\n+        TODO: compare-operator\n+    \"\"\"\n+\n+    def __init__(self, components: Collection[float] | None = None) -> None:\n+        \"\"\"\n+        input: components or nothing\n+        simple constructor for init the vector\n+        \"\"\"\n+        if components is None:\n+            components = []\n+        self.__components = list(components)\n+\n+    def __len__(self) -> int:\n+        \"\"\"\n+        returns the size of the vector\n+        \"\"\"\n+        return len(self.__components)\n+\n+    def __str__(self) -> str:\n+        \"\"\"\n+        returns a string representation of the vector\n+        \"\"\"\n+        return \"(\" + \",\".join(map(str, self.__components)) + \")\"\n+\n+    def __add__(self, other: Vector) -> Vector:\n+        \"\"\"\n+        input: other vector\n+        assumes: other vector has the same size\n+        returns a new vector that represents the sum.\n+        \"\"\"\n+        size = len(self)\n+        if size == len(other):\n+            result = [self.__components[i] + other.component(i) for i in range(size)]\n+            return Vector(result)\n+        else:\n+            raise Exception(\"must have the same size\")\n+\n+    def __sub__(self, other: Vector) -> Vector:\n+        \"\"\"\n+        input: other vector\n+        assumes: other vector has the same size\n+        returns a new vector that represents the difference.\n+        \"\"\"\n+        size = len(self)\n+        if size == len(other):\n+            result = [self.__components[i] - other.component(i) for i in range(size)]\n+            return Vector(result)\n+        else:  # error case\n+            raise Exception(\"must have the same size\")\n+\n+    @overload\n+    def __mul__(self, other: float) -> Vector:\n+        ...\n+\n+    @overload\n+    def __mul__(self, other: Vector) -> float:\n+        ...\n+\n+    def __mul__(self, other: float | Vector) -> float | Vector:\n+        \"\"\"\n+        mul implements the scalar multiplication\n+        and the dot-product\n+        \"\"\"\n+        if isinstance(other, float) or isinstance(other, int):\n+            ans = [c * other for c in self.__components]\n+            return Vector(ans)\n+        elif isinstance(other, Vector) and len(self) == len(other):\n+            size = len(self)\n+            prods = [self.__components[i] * other.component(i) for i in range(size)]\n+            return sum(prods)\n+        else:  # error case\n+            raise Exception(\"invalid operand!\")\n+\n+    def copy(self) -> Vector:\n+        \"\"\"\n+        copies this vector and returns it.\n+        \"\"\"\n+        return Vector(self.__components)\n+\n+    def component(self, i: int) -> float:\nComment: Please provide descriptive name for the parameter: `i`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "linear_algebra/lib.py",
    "pr_number": 7950,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1012201216,
    "comment_created_at": "2022-11-02T19:15:37Z"
  },
  {
    "code": "@@ -0,0 +1,41 @@\n+from __future__ import annotations\n+\n+\n+def partition(array: list, low: int, high: int) -> int:\n+    \"\"\"Helper function for quicksort to partition the array.\n+    >>> array = [3, 2, 1]\n+    >>> partition(array, 0, len(array) - 1)\n+    0\n+    >>> array\n+    [1, 2, 3]\n+    >>> array = [12, 4, 5, 2, 3]\n+    >>> idx = partition(array, 0, len(array) - 1)\n+    >>> array[:idx], array[idx], array[idx+1:]\n+    ([2, 3, 4], 5, [12])\n+    \"\"\"\n+    pivot = array[high]\n+    i = low - 1  # Pointer for the smaller element\n+\n+    for j in range(low, high):\n+        if array[j] <= pivot:\n+            i += 1\n+            array[i], array[j] = array[j], array[i]\n+\n+    array[i + 1], array[high] = array[high], array[i + 1]\n+    return i + 1\n+\n+\n+def quicksort(array: list, low: int = 0, high: int | None = None) -> list:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file divide_and_conquer/quicksort.py, please provide doctest for the function quicksort",
    "line_number": 28,
    "enriched": "File: divide_and_conquer/quicksort.py\nCode: @@ -0,0 +1,41 @@\n+from __future__ import annotations\n+\n+\n+def partition(array: list, low: int, high: int) -> int:\n+    \"\"\"Helper function for quicksort to partition the array.\n+    >>> array = [3, 2, 1]\n+    >>> partition(array, 0, len(array) - 1)\n+    0\n+    >>> array\n+    [1, 2, 3]\n+    >>> array = [12, 4, 5, 2, 3]\n+    >>> idx = partition(array, 0, len(array) - 1)\n+    >>> array[:idx], array[idx], array[idx+1:]\n+    ([2, 3, 4], 5, [12])\n+    \"\"\"\n+    pivot = array[high]\n+    i = low - 1  # Pointer for the smaller element\n+\n+    for j in range(low, high):\n+        if array[j] <= pivot:\n+            i += 1\n+            array[i], array[j] = array[j], array[i]\n+\n+    array[i + 1], array[high] = array[high], array[i + 1]\n+    return i + 1\n+\n+\n+def quicksort(array: list, low: int = 0, high: int | None = None) -> list:\nComment: As there is no test file in this pull request nor any test function or class in the file `divide_and_conquer/quicksort.py`, please provide doctest for the function `quicksort`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "divide_and_conquer/quicksort.py",
    "pr_number": 12051,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1798652467,
    "comment_created_at": "2024-10-14T01:06:57Z"
  },
  {
    "code": "@@ -0,0 +1,74 @@\n+\"\"\"\n+Given an array of integers and an integer k, find the kth largest element in the array\n+\n+https://stackoverflow.com/questions/251781/how-to-find-the-kth-largest-element-in-an-unsorted-array-of-length-n-in-on\n+\"\"\"\n+\n+\n+def partition(arr, low, high):",
    "comment": "no type hints.\r\ncan we come up with a more self-documenting name than arr?",
    "line_number": 8,
    "enriched": "File: data_structures/arrays/kth_largest_element.py\nCode: @@ -0,0 +1,74 @@\n+\"\"\"\n+Given an array of integers and an integer k, find the kth largest element in the array\n+\n+https://stackoverflow.com/questions/251781/how-to-find-the-kth-largest-element-in-an-unsorted-array-of-length-n-in-on\n+\"\"\"\n+\n+\n+def partition(arr, low, high):\nComment: No type hints.\r\nCan we come up with a more self-documenting name than `arr`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "data_structures/arrays/kth_largest_element.py",
    "pr_number": 10687,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368616542,
    "comment_created_at": "2023-10-23T12:53:30Z"
  },
  {
    "code": "@@ -74,19 +74,12 @@ We want your work to be readable by others; therefore, we encourage you to note\n \n - We encourage the use of Python [f-strings](https://realpython.com/python-f-strings/#f-strings-a-new-and-improved-way-to-format-strings-in-python) where they make the code easier to read.\n \n-- Please consider running [__psf/black__](https://github.com/python/black) on your Python file(s) before submitting your pull request.  This is not yet a requirement but it does make your code more readable and automatically aligns it with much of [PEP 8](https://www.python.org/dev/peps/pep-0008/). There are other code formatters (autopep8, yapf) but the __black__ formatter is now hosted by the Python Software Foundation. To use it,\n-\n-  ```bash\n-  python3 -m pip install black  # only required the first time\n-  black .\n-  ```\n-\n-- All submissions will need to pass the test `flake8 . --ignore=E203,W503 --max-line-length=88` before they will be accepted so if possible, try this test locally on your Python file(s) before submitting your pull request.\n-\n-  ```bash\n-  python3 -m pip install flake8  # only required the first time\n-  flake8 . --ignore=E203,W503  --max-line-length=88 --show-source\n-  ```\n+- Do not forget that running `pre-commit` checks locally will help you to follow code quality standards.",
    "comment": "this is indecipherable for first-time contributors.  we would need a link to pre-commit instructions on how to install and initialized that tool.  i think that will be beyond the reach of many newbies.",
    "line_number": 77,
    "enriched": "File: CONTRIBUTING.md\nCode: @@ -74,19 +74,12 @@ We want your work to be readable by others; therefore, we encourage you to note\n \n - We encourage the use of Python [f-strings](https://realpython.com/python-f-strings/#f-strings-a-new-and-improved-way-to-format-strings-in-python) where they make the code easier to read.\n \n-- Please consider running [__psf/black__](https://github.com/python/black) on your Python file(s) before submitting your pull request.  This is not yet a requirement but it does make your code more readable and automatically aligns it with much of [PEP 8](https://www.python.org/dev/peps/pep-0008/). There are other code formatters (autopep8, yapf) but the __black__ formatter is now hosted by the Python Software Foundation. To use it,\n-\n-  ```bash\n-  python3 -m pip install black  # only required the first time\n-  black .\n-  ```\n-\n-- All submissions will need to pass the test `flake8 . --ignore=E203,W503 --max-line-length=88` before they will be accepted so if possible, try this test locally on your Python file(s) before submitting your pull request.\n-\n-  ```bash\n-  python3 -m pip install flake8  # only required the first time\n-  flake8 . --ignore=E203,W503  --max-line-length=88 --show-source\n-  ```\n+- Do not forget that running `pre-commit` checks locally will help you to follow code quality standards.\nComment: This is indecipherable for first-time contributors.  We would need a link to pre-commit instructions on how to install and initialized that tool.  I think that will be beyond the reach of many newbies.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "CONTRIBUTING.md",
    "pr_number": 7762,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008696288,
    "comment_created_at": "2022-10-29T12:51:22Z"
  },
  {
    "code": "@@ -0,0 +1,64 @@\n+class MaximalBipartiteMatching:\n+    \"\"\"\n+    This class implements finding maximal Bipartite matching using DFS.\n+\n+    Usage:\n+    >>> bipartite_graph = [\n+    ...     [0, 1, 1, 0, 0, 0],\n+    ...     [1, 0, 0, 1, 0, 0],\n+    ...     [0, 0, 1, 0, 0, 0],\n+    ...     [0, 0, 1, 1, 0, 0],\n+    ...     [0, 0, 0, 0, 0, 0],\n+    ...     [0, 0, 0, 0, 0, 1]",
    "comment": "we should need to add a couple more doctests ?",
    "line_number": 11,
    "enriched": "File: graphs/maximum_bipartite_matching.py\nCode: @@ -0,0 +1,64 @@\n+class MaximalBipartiteMatching:\n+    \"\"\"\n+    This class implements finding maximal Bipartite matching using DFS.\n+\n+    Usage:\n+    >>> bipartite_graph = [\n+    ...     [0, 1, 1, 0, 0, 0],\n+    ...     [1, 0, 0, 1, 0, 0],\n+    ...     [0, 0, 1, 0, 0, 0],\n+    ...     [0, 0, 1, 1, 0, 0],\n+    ...     [0, 0, 0, 0, 0, 0],\n+    ...     [0, 0, 0, 0, 0, 1]\nComment: We should need to add a couple more doctests ? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "graphs/maximum_bipartite_matching.py",
    "pr_number": 11165,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1398582910,
    "comment_created_at": "2023-11-20T02:02:40Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+\"\"\"\n+Categorical Cross-Entropy Loss\n+\n+This function calculates the Categorical Cross-Entropy Loss between true class\n+labels and predicted class probabilities.\n+\n+Formula:\n+Categorical Cross-Entropy Loss = -\u03a3(y_true * log(y_pred))\n+\n+Resources:\n+- [Wikipedia - Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def categorical_crossentropy(\n+    y_true: np.ndarray, y_pred: np.ndarray, epsilon: float = 1e-15\n+) -> float:\n+    \"\"\"\n+    Calculate Categorical Cross-Entropy Loss between true class labels and\n+    predicted class probabilities.\n+\n+    Parameters:\n+    - y_true: True class labels (one-hot encoded) as a NumPy array.\n+    - y_pred: Predicted class probabilities as a NumPy array.\n+    - epsilon: Small constant to avoid numerical instability.\n+\n+    Returns:\n+    - ce_loss: Categorical Cross-Entropy Loss as a floating-point number.\n+\n+    Example:\n+    >>> true_labels = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+    >>> pred_probs = np.array([[0.9, 0.1, 0.0], [0.2, 0.7, 0.1], [0.0, 0.1, 0.9]])\n+    >>> categorical_crossentropy(true_labels, pred_probs)\n+    0.18913199175146167\n+\n+    >>> y_true = np.array([[1, 0], [0, 1]])\n+    >>> y_pred = np.array([[0.9, 0.1, 0.0], [0.2, 0.7, 0.1]])\n+    >>> categorical_crossentropy(y_true, y_pred)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Input arrays must have the same length.\n+    \"\"\"\n+    if y_true.shape != y_pred.shape:\n+        raise ValueError(\"Input arrays must have the same length.\")\n+\n+    # Clip predicted probabilities to avoid log(0)\n+    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n+\n+    # Calculate categorical cross-entropy loss\n+    return -np.sum(y_true * np.log(y_pred)) / len(y_true)",
    "comment": "why the / len(true)? wouldn't that make it the _mean_ categorical cross-entropy loss?",
    "line_number": 52,
    "enriched": "File: machine_learning/loss_functions/categorical_cross_entropy.py\nCode: @@ -0,0 +1,58 @@\n+\"\"\"\n+Categorical Cross-Entropy Loss\n+\n+This function calculates the Categorical Cross-Entropy Loss between true class\n+labels and predicted class probabilities.\n+\n+Formula:\n+Categorical Cross-Entropy Loss = -\u03a3(y_true * log(y_pred))\n+\n+Resources:\n+- [Wikipedia - Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def categorical_crossentropy(\n+    y_true: np.ndarray, y_pred: np.ndarray, epsilon: float = 1e-15\n+) -> float:\n+    \"\"\"\n+    Calculate Categorical Cross-Entropy Loss between true class labels and\n+    predicted class probabilities.\n+\n+    Parameters:\n+    - y_true: True class labels (one-hot encoded) as a NumPy array.\n+    - y_pred: Predicted class probabilities as a NumPy array.\n+    - epsilon: Small constant to avoid numerical instability.\n+\n+    Returns:\n+    - ce_loss: Categorical Cross-Entropy Loss as a floating-point number.\n+\n+    Example:\n+    >>> true_labels = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+    >>> pred_probs = np.array([[0.9, 0.1, 0.0], [0.2, 0.7, 0.1], [0.0, 0.1, 0.9]])\n+    >>> categorical_crossentropy(true_labels, pred_probs)\n+    0.18913199175146167\n+\n+    >>> y_true = np.array([[1, 0], [0, 1]])\n+    >>> y_pred = np.array([[0.9, 0.1, 0.0], [0.2, 0.7, 0.1]])\n+    >>> categorical_crossentropy(y_true, y_pred)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Input arrays must have the same length.\n+    \"\"\"\n+    if y_true.shape != y_pred.shape:\n+        raise ValueError(\"Input arrays must have the same length.\")\n+\n+    # Clip predicted probabilities to avoid log(0)\n+    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n+\n+    # Calculate categorical cross-entropy loss\n+    return -np.sum(y_true * np.log(y_pred)) / len(y_true)\nComment: Why the `/ len(true)`? Wouldn't that make it the _mean_ categorical cross-entropy loss?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "machine_learning/loss_functions/categorical_cross_entropy.py",
    "pr_number": 10152,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349917308,
    "comment_created_at": "2023-10-09T06:56:49Z"
  },
  {
    "code": "@@ -0,0 +1,8 @@\n+def is_string_palindrome(words: str) -> bool:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/is_string_palindrome.py, please provide doctest for the function is_string_palindrome",
    "line_number": 1,
    "enriched": "File: maths/is_string_palindrome.py\nCode: @@ -0,0 +1,8 @@\n+def is_string_palindrome(words: str) -> bool:\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/is_string_palindrome.py`, please provide doctest for the function `is_string_palindrome`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/is_string_palindrome.py",
    "pr_number": 12169,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1807240459,
    "comment_created_at": "2024-10-19T08:16:58Z"
  },
  {
    "code": "@@ -0,0 +1,98 @@\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\n+@dataclass\n+class TrapezoidalFuzzySet:\n+    \"\"\"\n+    Represents and manipulates trapezoidal fuzzy sets.\n+    \n+    Attributes:\n+        name: The name or label of the fuzzy set.\n+        left_base: The left base of the trapezoid.\n+        left_peak: The top left vertex of the trapezoid.\n+        right_peak: The top right vertex of the trapezoid.\n+        right_base: The right base of the trapezoid.\n+        is_complement: Indicates if this is the complement of the fuzzy set.\n+\n+    Methods:\n+        membership(value): Calculates membership value for input 'value'.\n+        complement(): Creates left_base TrapezoidalFuzzySet instance representing\n+            the complement.\n+        plot(): Plots the membership function of the fuzzy set.\n+    \"\"\"\n+\n+    name: str\n+    left_base: float\n+    left_peak: float\n+    right_peak: float\n+    right_base: float\n+    is_complement: bool = False  # Flag for complement set\n+\n+    def membership(self, value: float) -> float:\n+        \"\"\"\n+        Calculates membership value for input 'value'. For complement sets,\n+        returns 1 - trapezoidal membership.\n+\n+        >>> TrapezoidalFuzzySet(\"Medium\", 0, 1, 2, 3).membership(1)\n+        1.0\n+        >>> TrapezoidalFuzzySet(\"Medium\", 0, 1, 2, 3).membership(0.5)\n+        0.5\n+        \"\"\"\n+        if value < self.left_base or value > self.right_base:\n+            membership_value = 0.0\n+        elif self.left_base <= value < self.left_peak:\n+            membership_value = (value - self.left_base) / (self.left_peak - self.left_base)\n+        elif self.left_peak <= value <= self.right_peak:\n+            membership_value = 1.0\n+        elif self.right_peak < value <= self.right_base:\n+            membership_value = (self.right_base - value) / (self.right_base - self.right_peak)\n+        \n+        # For complements, invert the membership value\n+        return membership_value if not self.is_complement else 1 - membership_value\n+\n+    def complement(self) -> TrapezoidalFuzzySet:\n+        \"\"\"\n+        Creates a new TrapezoidalFuzzySet instance representing the complement.\n+        \n+        >>> TrapezoidalFuzzySet(\"Medium\", 0, 1, 2, 3).complement().membership(1)\n+        0.0\n+        \"\"\"\n+        return TrapezoidalFuzzySet(f\"\u00ac{self.name}\", self.left_base, self.left_peak, self.right_peak, self.right_base,\n+                                   is_complement=not self.is_complement)\n+\n+    def plot(self) -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file fuzzy_logic/trapazoidal_fuzzyset.py, please provide doctest for the function plot",
    "line_number": 68,
    "enriched": "File: fuzzy_logic/trapazoidal_fuzzyset.py\nCode: @@ -0,0 +1,98 @@\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\n+@dataclass\n+class TrapezoidalFuzzySet:\n+    \"\"\"\n+    Represents and manipulates trapezoidal fuzzy sets.\n+    \n+    Attributes:\n+        name: The name or label of the fuzzy set.\n+        left_base: The left base of the trapezoid.\n+        left_peak: The top left vertex of the trapezoid.\n+        right_peak: The top right vertex of the trapezoid.\n+        right_base: The right base of the trapezoid.\n+        is_complement: Indicates if this is the complement of the fuzzy set.\n+\n+    Methods:\n+        membership(value): Calculates membership value for input 'value'.\n+        complement(): Creates left_base TrapezoidalFuzzySet instance representing\n+            the complement.\n+        plot(): Plots the membership function of the fuzzy set.\n+    \"\"\"\n+\n+    name: str\n+    left_base: float\n+    left_peak: float\n+    right_peak: float\n+    right_base: float\n+    is_complement: bool = False  # Flag for complement set\n+\n+    def membership(self, value: float) -> float:\n+        \"\"\"\n+        Calculates membership value for input 'value'. For complement sets,\n+        returns 1 - trapezoidal membership.\n+\n+        >>> TrapezoidalFuzzySet(\"Medium\", 0, 1, 2, 3).membership(1)\n+        1.0\n+        >>> TrapezoidalFuzzySet(\"Medium\", 0, 1, 2, 3).membership(0.5)\n+        0.5\n+        \"\"\"\n+        if value < self.left_base or value > self.right_base:\n+            membership_value = 0.0\n+        elif self.left_base <= value < self.left_peak:\n+            membership_value = (value - self.left_base) / (self.left_peak - self.left_base)\n+        elif self.left_peak <= value <= self.right_peak:\n+            membership_value = 1.0\n+        elif self.right_peak < value <= self.right_base:\n+            membership_value = (self.right_base - value) / (self.right_base - self.right_peak)\n+        \n+        # For complements, invert the membership value\n+        return membership_value if not self.is_complement else 1 - membership_value\n+\n+    def complement(self) -> TrapezoidalFuzzySet:\n+        \"\"\"\n+        Creates a new TrapezoidalFuzzySet instance representing the complement.\n+        \n+        >>> TrapezoidalFuzzySet(\"Medium\", 0, 1, 2, 3).complement().membership(1)\n+        0.0\n+        \"\"\"\n+        return TrapezoidalFuzzySet(f\"\u00ac{self.name}\", self.left_base, self.left_peak, self.right_peak, self.right_base,\n+                                   is_complement=not self.is_complement)\n+\n+    def plot(self) -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `fuzzy_logic/trapazoidal_fuzzyset.py`, please provide doctest for the function `plot`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "fuzzy_logic/trapazoidal_fuzzyset.py",
    "pr_number": 12331,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1822630102,
    "comment_created_at": "2024-10-30T13:27:04Z"
  },
  {
    "code": "@@ -0,0 +1,76 @@\n+\"\"\"\n+Word Ladder is a classic problem in computer science.\n+The problem is to transform a start word into an end word\n+by changing one letter at a time.\n+Each intermediate word must be a valid word from a given list of words.\n+The goal is to find a transformation sequence\n+from the start word to the end word.\n+\n+Wikipedia: https://en.wikipedia.org/wiki/Word_ladder\n+\"\"\"\n+\n+\n+def word_ladder_backtrack(\n+    begin_word: str, end_word: str, word_list: list[str]\n+) -> list[str]:\n+    \"\"\"\n+    Solve the Word Ladder problem using Backtracking and return\n+    the list of transformations from begin_word to end_word.\n+\n+    Parameters:\n+    begin_word (str): The word from which the transformation starts.\n+    end_word (str): The target word for transformation.\n+    word_list (list[str]): The list of valid words for transformation.\n+\n+    Returns:\n+    list[str]: The list of transformations from begin_word to end_word.\n+               Returns an empty list if there is no valid transformation.\n+\n+    Example:\n+    >>> word_ladder_backtrack(\"hit\", \"cog\", [\"hot\", \"dot\", \"dog\", \"lot\", \"log\", \"cog\"])\n+    ['hit', 'hot', 'dot', 'lot', 'log', 'cog']\n+\n+    >>> word_ladder_backtrack(\"hit\", \"cog\", [\"hot\", \"dot\", \"dog\", \"lot\", \"log\"])\n+    []\n+\n+    >>> word_ladder_backtrack(\"lead\", \"gold\", [\"load\", \"goad\", \"gold\", \"lead\", \"lord\"])\n+    ['lead', 'lead', 'load', 'goad', 'gold']\n+\n+    >>> word_ladder_backtrack(\"game\", \"code\", [\"came\", \"cage\", \"code\", \"cade\", \"gave\"])\n+    ['game', 'came', 'cade', 'code']\n+    \"\"\"\n+\n+    # Step 1: Convert the word_list to a set for faster lookup\n+    word_set = set(word_list)",
    "comment": "if word_list will be immediately converted from a list to a set, why make word_set a list at all? why not just require the input word list to be a set?",
    "line_number": 44,
    "enriched": "File: backtracking/word_ladder.py\nCode: @@ -0,0 +1,76 @@\n+\"\"\"\n+Word Ladder is a classic problem in computer science.\n+The problem is to transform a start word into an end word\n+by changing one letter at a time.\n+Each intermediate word must be a valid word from a given list of words.\n+The goal is to find a transformation sequence\n+from the start word to the end word.\n+\n+Wikipedia: https://en.wikipedia.org/wiki/Word_ladder\n+\"\"\"\n+\n+\n+def word_ladder_backtrack(\n+    begin_word: str, end_word: str, word_list: list[str]\n+) -> list[str]:\n+    \"\"\"\n+    Solve the Word Ladder problem using Backtracking and return\n+    the list of transformations from begin_word to end_word.\n+\n+    Parameters:\n+    begin_word (str): The word from which the transformation starts.\n+    end_word (str): The target word for transformation.\n+    word_list (list[str]): The list of valid words for transformation.\n+\n+    Returns:\n+    list[str]: The list of transformations from begin_word to end_word.\n+               Returns an empty list if there is no valid transformation.\n+\n+    Example:\n+    >>> word_ladder_backtrack(\"hit\", \"cog\", [\"hot\", \"dot\", \"dog\", \"lot\", \"log\", \"cog\"])\n+    ['hit', 'hot', 'dot', 'lot', 'log', 'cog']\n+\n+    >>> word_ladder_backtrack(\"hit\", \"cog\", [\"hot\", \"dot\", \"dog\", \"lot\", \"log\"])\n+    []\n+\n+    >>> word_ladder_backtrack(\"lead\", \"gold\", [\"load\", \"goad\", \"gold\", \"lead\", \"lord\"])\n+    ['lead', 'lead', 'load', 'goad', 'gold']\n+\n+    >>> word_ladder_backtrack(\"game\", \"code\", [\"came\", \"cage\", \"code\", \"cade\", \"gave\"])\n+    ['game', 'came', 'cade', 'code']\n+    \"\"\"\n+\n+    # Step 1: Convert the word_list to a set for faster lookup\n+    word_set = set(word_list)\nComment: If `word_list` will be immediately converted from a list to a set, why make `word_set` a list at all? Why not just require the input word list to be a set?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "backtracking/word_ladder.py",
    "pr_number": 11590,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1782123719,
    "comment_created_at": "2024-10-01T05:16:07Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+\"\"\"\n+CatBoost Regressor Example.\n+\n+This script demonstrates the usage of the CatBoost Regressor for a simple regression task.\n+CatBoost is a powerful gradient boosting library that handles categorical features automatically\n+and is highly efficient.\n+\n+Make sure to install CatBoost using:\n+    pip install catboost\n+\n+Contributed by: @AHuzail\n+\"\"\"\n+\n+import numpy as np\n+from sklearn.datasets import load_boston\n+from sklearn.model_selection import train_test_split\n+from sklearn.metrics import mean_squared_error\n+from catboost import CatBoostRegressor\n+\n+\n+def data_handling() -> tuple:\n+    \"\"\"\n+    Loads and handles the dataset, splitting it into features and targets.\n+\n+    The Boston dataset is used as a regression example.\n+    \n+    Returns:\n+        tuple: A tuple of (features, target), where both are numpy arrays.\n+\n+    Example:\n+    >>> features, target = data_handling()\n+    >>> features.shape\n+    (506, 13)\n+    >>> target.shape\n+    (506,)\n+    \"\"\"\n+    # Load Boston dataset (note: this dataset may be deprecated, replace if needed)\n+    boston = load_boston()\n+    features = boston.data\n+    target = boston.target\n+    return features, target\n+\n+\n+def catboost_regressor(features: np.ndarray, target: np.ndarray) -> CatBoostRegressor:\n+    \"\"\"\n+    Trains a CatBoostRegressor using the provided features and target values.\n+\n+    Args:\n+        features (np.ndarray): The input features for the regression model.\n+        target (np.ndarray): The target values for the regression model.\n+\n+    Returns:\n+        CatBoostRegressor: A trained CatBoost regressor model.\n+\n+    Example:\n+    >>> features, target = data_handling()\n+    >>> model = catboost_regressor(features, target)\n+    >>> isinstance(model, CatBoostRegressor)\n+    True\n+    \"\"\"\n+    regressor = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, verbose=0)\n+    regressor.fit(features, target)\n+    return regressor\n+\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/catboost_regressor.py, please provide doctest for the function main",
    "line_number": 66,
    "enriched": "File: machine_learning/catboost_regressor.py\nCode: @@ -0,0 +1,94 @@\n+\"\"\"\n+CatBoost Regressor Example.\n+\n+This script demonstrates the usage of the CatBoost Regressor for a simple regression task.\n+CatBoost is a powerful gradient boosting library that handles categorical features automatically\n+and is highly efficient.\n+\n+Make sure to install CatBoost using:\n+    pip install catboost\n+\n+Contributed by: @AHuzail\n+\"\"\"\n+\n+import numpy as np\n+from sklearn.datasets import load_boston\n+from sklearn.model_selection import train_test_split\n+from sklearn.metrics import mean_squared_error\n+from catboost import CatBoostRegressor\n+\n+\n+def data_handling() -> tuple:\n+    \"\"\"\n+    Loads and handles the dataset, splitting it into features and targets.\n+\n+    The Boston dataset is used as a regression example.\n+    \n+    Returns:\n+        tuple: A tuple of (features, target), where both are numpy arrays.\n+\n+    Example:\n+    >>> features, target = data_handling()\n+    >>> features.shape\n+    (506, 13)\n+    >>> target.shape\n+    (506,)\n+    \"\"\"\n+    # Load Boston dataset (note: this dataset may be deprecated, replace if needed)\n+    boston = load_boston()\n+    features = boston.data\n+    target = boston.target\n+    return features, target\n+\n+\n+def catboost_regressor(features: np.ndarray, target: np.ndarray) -> CatBoostRegressor:\n+    \"\"\"\n+    Trains a CatBoostRegressor using the provided features and target values.\n+\n+    Args:\n+        features (np.ndarray): The input features for the regression model.\n+        target (np.ndarray): The target values for the regression model.\n+\n+    Returns:\n+        CatBoostRegressor: A trained CatBoost regressor model.\n+\n+    Example:\n+    >>> features, target = data_handling()\n+    >>> model = catboost_regressor(features, target)\n+    >>> isinstance(model, CatBoostRegressor)\n+    True\n+    \"\"\"\n+    regressor = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, verbose=0)\n+    regressor.fit(features, target)\n+    return regressor\n+\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/catboost_regressor.py`, please provide doctest for the function `main`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/catboost_regressor.py",
    "pr_number": 11877,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1790935876,
    "comment_created_at": "2024-10-07T22:07:09Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+import numpy as np\n+import doctest\n+\n+\"\"\"\n+Mean Absolute Percentage Error (MAPE): \n+MAPE calculates the average of the absolute percentage differences between the\n+predicted and true values.\n+\n+MAPE = (\u03a3|y_true[i]-Y_pred[i]/y_true[i]|)/n\n+\n+https://stephenallwright.com/good-mape-score/\n+\n+\"\"\"\n+\n+\n+def mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/loss_functions/mean_absolute_percentage_error.py, please provide doctest for the function mean_absolute_percentage_error",
    "line_number": 16,
    "enriched": "File: machine_learning/loss_functions/mean_absolute_percentage_error.py\nCode: @@ -0,0 +1,48 @@\n+import numpy as np\n+import doctest\n+\n+\"\"\"\n+Mean Absolute Percentage Error (MAPE): \n+MAPE calculates the average of the absolute percentage differences between the\n+predicted and true values.\n+\n+MAPE = (\u03a3|y_true[i]-Y_pred[i]/y_true[i]|)/n\n+\n+https://stephenallwright.com/good-mape-score/\n+\n+\"\"\"\n+\n+\n+def mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/loss_functions/mean_absolute_percentage_error.py`, please provide doctest for the function `mean_absolute_percentage_error`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/loss_functions/mean_absolute_percentage_error.py",
    "pr_number": 10464,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359590241,
    "comment_created_at": "2023-10-14T18:50:56Z"
  },
  {
    "code": "@@ -0,0 +1,135 @@\n+'''\n+The forward-chaining algorithm PL-FC-ENTAILS? (KB, q) determines if a single proposition\n+symbol q\u2014the query\u2014is entailed by a knowledge base of definite clauses. It begins from\n+known facts (positive literals) in the knowledge base. \n+known facts (positive literals) in the knowledge base.\n+\n+Reference: https://dl.ebooksworld.ir/books/Artificial.Intelligence.A.Modern.Approach.4th.Edition.Peter.Norvig.%20Stuart.Russell.Pearson.9780134610993.EBooksWorld.ir.pdf\n+\n+'''\n+\n+import re\n+\n+def find_symbols_in_kb(knowledge_base: list[str]) -> dict:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file inference_engine_algorithms/forward_chaining.py, please provide doctest for the function find_symbols_in_kb",
    "line_number": 13,
    "enriched": "File: inference_engine_algorithms/forward_chaining.py\nCode: @@ -0,0 +1,135 @@\n+'''\n+The forward-chaining algorithm PL-FC-ENTAILS? (KB, q) determines if a single proposition\n+symbol q\u2014the query\u2014is entailed by a knowledge base of definite clauses. It begins from\n+known facts (positive literals) in the knowledge base. \n+known facts (positive literals) in the knowledge base.\n+\n+Reference: https://dl.ebooksworld.ir/books/Artificial.Intelligence.A.Modern.Approach.4th.Edition.Peter.Norvig.%20Stuart.Russell.Pearson.9780134610993.EBooksWorld.ir.pdf\n+\n+'''\n+\n+import re\n+\n+def find_symbols_in_kb(knowledge_base: list[str]) -> dict:\nComment: As there is no test file in this pull request nor any test function or class in the file `inference_engine_algorithms/forward_chaining.py`, please provide doctest for the function `find_symbols_in_kb`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "inference_engine_algorithms/forward_chaining.py",
    "pr_number": 11400,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1597368011,
    "comment_created_at": "2024-05-11T06:20:21Z"
  },
  {
    "code": "@@ -4,18 +4,27 @@\n def solve_maze(maze: list[list[int]]) -> bool:\n     \"\"\"\n     This method solves the \"rat in maze\" problem.\n-    In this problem we have some n by n matrix, a start point and an end point.\n-    We want to go from the start to the end. In this matrix zeroes represent walls\n-    and ones paths we can use.\n+    In this problem, we have an n by n binary matrix where each\n+    cell in the matrix contains a value 0 or 1.\n+    In this matrix, 1 represent a valid path that can be\n+    followed to reach the end of the matrix,\n+    while 0 is a dead end. The rat is allowed to move in 4 directions:",
    "comment": "i prefer wall/obstacle to \"dead end\".\r\naccording to the cambridge dictionary, a \"dead end\" is:\r\n\u00ab a [road](https://dictionary.cambridge.org/fr/dictionnaire/anglais/road) that is [closed](https://dictionary.cambridge.org/fr/dictionnaire/anglais/closed) at one end, and does not [lead](https://dictionary.cambridge.org/fr/dictionnaire/anglais/lead) [anywhere](https://dictionary.cambridge.org/fr/dictionnaire/anglais/anywhere): \u00bb\r\na \"dead end\" is already a road and doesn't as such depict very well what i see from the input.",
    "line_number": 11,
    "enriched": "File: backtracking/rat_in_maze.py\nCode: @@ -4,18 +4,27 @@\n def solve_maze(maze: list[list[int]]) -> bool:\n     \"\"\"\n     This method solves the \"rat in maze\" problem.\n-    In this problem we have some n by n matrix, a start point and an end point.\n-    We want to go from the start to the end. In this matrix zeroes represent walls\n-    and ones paths we can use.\n+    In this problem, we have an n by n binary matrix where each\n+    cell in the matrix contains a value 0 or 1.\n+    In this matrix, 1 represent a valid path that can be\n+    followed to reach the end of the matrix,\n+    while 0 is a dead end. The rat is allowed to move in 4 directions:\nComment: I prefer wall/obstacle to \"dead end\".\r\nAccording to the Cambridge Dictionary, a \"dead end\" is:\r\n\u00ab a [road](https://dictionary.cambridge.org/fr/dictionnaire/anglais/road) that is [closed](https://dictionary.cambridge.org/fr/dictionnaire/anglais/closed) at one end, and does not [lead](https://dictionary.cambridge.org/fr/dictionnaire/anglais/lead) [anywhere](https://dictionary.cambridge.org/fr/dictionnaire/anglais/anywhere): \u00bb\r\nA \"dead end\" is already a road and doesn't as such depict very well what I see from the input.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "backtracking/rat_in_maze.py",
    "pr_number": 9289,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1344269517,
    "comment_created_at": "2023-10-03T15:03:52Z"
  },
  {
    "code": "@@ -0,0 +1,136 @@\n+\"\"\"\n+A simple example uses the ant colony optimization algorithm\n+to solve the classic TSP problem",
    "comment": "what is tsp and why does the reader need to guess?",
    "line_number": 3,
    "enriched": "File: graphs/ant_colony_optimization_algorithms.py\nCode: @@ -0,0 +1,136 @@\n+\"\"\"\n+A simple example uses the ant colony optimization algorithm\n+to solve the classic TSP problem\nComment: What is TSP and why does the reader need to guess?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "graphs/ant_colony_optimization_algorithms.py",
    "pr_number": 11163,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1395453161,
    "comment_created_at": "2023-11-16T10:05:44Z"
  },
  {
    "code": "@@ -43,29 +43,19 @@ def points_to_polynomial(coordinates: list[list[int]]) -> str:\n \n     x = len(coordinates)\n \n-    count_of_line = 0\n-    matrix: list[list[float]] = []\n     # put the x and x to the power values in a matrix\n-    while count_of_line < x:\n-        count_in_line = 0\n-        a = coordinates[count_of_line][0]\n-        count_line: list[float] = []\n-        while count_in_line < x:\n-            count_line.append(a ** (x - (count_in_line + 1)))\n-            count_in_line += 1\n-        matrix.append(count_line)\n-        count_of_line += 1\n+    matrix: list[list[float]] = [\n+        [\n+            coordinates[count_of_line][0] ** (x - (count_in_line + 1))\n+            for count_in_line in range(x)\n+        ]\n+        for count_of_line in range(x)\n+    ]\n \n-    count_of_line = 0\n     # put the y values into a vector\n-    vector: list[float] = []\n-    while count_of_line < x:\n-        vector.append(coordinates[count_of_line][1])\n-        count_of_line += 1\n+    vector: list[float] = [coordinates[count_of_line][1] for count_of_line in range(x)]\n \n-    count = 0\n-\n-    while count < x:\n+    for count in range(x):\n         zahlen = 0",
    "comment": "1) could we refactor this inner while-loop to be a for-loop too? it'd probably require changing the control flow slightly, but it should be fine as long as we preserve the program logic. as a starting point, it seems to me that the reason why it's not trivial to convert this while-loop to a for-loop is because of this if-statement changing the value of zahlen inside the loop:\r\n    py\r\n    if count == zahlen:\r\n        zahlen += 1\r\n    \r\n    i think this condition is only there to ensure that zahlen != count. if my understanding is correct, then this would simply translate to a continue statement in a for-loop:\r\n    py\r\n    for zahlen in range(x):\r\n        if zahlen == count:\r\n            continue\r\n        ...\r\n    \r\n    from here, it should be pretty easy to convert this inner while-loop to a for-loop.\r\n2) let's change the variable names to be in english. nothing against german, but these names would be confusing when the rest of the repo is entirely in english.\r\nzahlen -> number or num\r\nbruch -> fraction or frac",
    "line_number": 59,
    "enriched": "File: linear_algebra/src/polynom_for_points.py\nCode: @@ -43,29 +43,19 @@ def points_to_polynomial(coordinates: list[list[int]]) -> str:\n \n     x = len(coordinates)\n \n-    count_of_line = 0\n-    matrix: list[list[float]] = []\n     # put the x and x to the power values in a matrix\n-    while count_of_line < x:\n-        count_in_line = 0\n-        a = coordinates[count_of_line][0]\n-        count_line: list[float] = []\n-        while count_in_line < x:\n-            count_line.append(a ** (x - (count_in_line + 1)))\n-            count_in_line += 1\n-        matrix.append(count_line)\n-        count_of_line += 1\n+    matrix: list[list[float]] = [\n+        [\n+            coordinates[count_of_line][0] ** (x - (count_in_line + 1))\n+            for count_in_line in range(x)\n+        ]\n+        for count_of_line in range(x)\n+    ]\n \n-    count_of_line = 0\n     # put the y values into a vector\n-    vector: list[float] = []\n-    while count_of_line < x:\n-        vector.append(coordinates[count_of_line][1])\n-        count_of_line += 1\n+    vector: list[float] = [coordinates[count_of_line][1] for count_of_line in range(x)]\n \n-    count = 0\n-\n-    while count < x:\n+    for count in range(x):\n         zahlen = 0\nComment: 1) Could we refactor this inner while-loop to be a for-loop too? It'd probably require changing the control flow slightly, but it should be fine as long as we preserve the program logic. As a starting point, it seems to me that the reason why it's not trivial to convert this while-loop to a for-loop is because of this if-statement changing the value of `zahlen` inside the loop:\r\n    ```py\r\n    if count == zahlen:\r\n        zahlen += 1\r\n    ```\r\n    I think this condition is only there to ensure that `zahlen != count`. If my understanding is correct, then this would simply translate to a `continue` statement in a for-loop:\r\n    ```py\r\n    for zahlen in range(x):\r\n        if zahlen == count:\r\n            continue\r\n        ...\r\n    ```\r\n    From here, it should be pretty easy to convert this inner while-loop to a for-loop.\r\n2) Let's change the variable names to be in English. Nothing against German, but these names would be confusing when the rest of the repo is entirely in English.\r\n`zahlen` -> `number` or `num`\r\n`bruch` -> `fraction` or `frac`\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "linear_algebra/src/polynom_for_points.py",
    "pr_number": 8605,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1278846171,
    "comment_created_at": "2023-07-31T06:35:54Z"
  },
  {
    "code": "@@ -0,0 +1,232 @@\n+# Python program for A* Search Algorithm\n+import math\n+import heapq\n+from typing import List, Tuple\n+\n+# Define the Cell class\n+\n+\n+class Cell:\n+    def __init__(self)->None:\n+      # Parent cell's row index\n+        self.parent_i = 0\n+        # Parent cell's column index\n+        self.parent_j = 0\n+        # Total cost of the cell (g + h)\n+        self.f = float(\"inf\")\n+        # Cost from start to this cell\n+        self.g = float(\"inf\")\n+        # Heuristic cost from this cell to destination\n+        self.h = 0\n+\n+\n+# Define the size of the grid\n+ROW = 9\n+COL = 10\n+\n+# Check if a cell is valid (within the grid)\n+\n+\n+def is_valid(row: int, col: int) -> bool:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/a_star_search.py, please provide doctest for the function is_valid",
    "line_number": 30,
    "enriched": "File: searches/A_star_search.py\nCode: @@ -0,0 +1,232 @@\n+# Python program for A* Search Algorithm\n+import math\n+import heapq\n+from typing import List, Tuple\n+\n+# Define the Cell class\n+\n+\n+class Cell:\n+    def __init__(self)->None:\n+      # Parent cell's row index\n+        self.parent_i = 0\n+        # Parent cell's column index\n+        self.parent_j = 0\n+        # Total cost of the cell (g + h)\n+        self.f = float(\"inf\")\n+        # Cost from start to this cell\n+        self.g = float(\"inf\")\n+        # Heuristic cost from this cell to destination\n+        self.h = 0\n+\n+\n+# Define the size of the grid\n+ROW = 9\n+COL = 10\n+\n+# Check if a cell is valid (within the grid)\n+\n+\n+def is_valid(row: int, col: int) -> bool:\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/A_star_search.py`, please provide doctest for the function `is_valid`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "searches/A_star_search.py",
    "pr_number": 11737,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1787337750,
    "comment_created_at": "2024-10-04T08:12:37Z"
  },
  {
    "code": "@@ -0,0 +1,34 @@\n+def rotated_binary_search(arr: list[int], key: int) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/rotated_binary_search.py, please provide doctest for the function rotated_binary_search",
    "line_number": 1,
    "enriched": "File: searches/rotated_binary_search.py\nCode: @@ -0,0 +1,34 @@\n+def rotated_binary_search(arr: list[int], key: int) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/rotated_binary_search.py`, please provide doctest for the function `rotated_binary_search`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "searches/rotated_binary_search.py",
    "pr_number": 11984,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1797125201,
    "comment_created_at": "2024-10-11T15:33:58Z"
  },
  {
    "code": "@@ -0,0 +1,226 @@\n+import random\n+from collections.abc import Callable, Sequence\n+from concurrent.futures import ThreadPoolExecutor\n+\n+import numpy as np\n+\n+# Parameters\n+N_POPULATION = 100  # Population size\n+N_GENERATIONS = 500  # Maximum number of generations\n+N_SELECTED = 50  # Number of parents selected for the next generation\n+MUTATION_PROBABILITY = 0.1  # Mutation probability\n+CROSSOVER_RATE = 0.8  # Probability of crossover\n+SEARCH_SPACE = (-10, 10)  # Search space for the variables\n+\n+# Random number generator\n+rng = np.random.default_rng()\n+\n+\n+class GeneticAlgorithm:\n+    def __init__(\n+        self,\n+        function: Callable[[float, float], float],\n+        bounds: Sequence[tuple[int | float, int | float]],\n+        population_size: int,\n+        generations: int,\n+        mutation_prob: float,\n+        crossover_rate: float,\n+        maximize: bool = True,\n+    ) -> None:\n+        self.function = function  # Target function to optimize\n+        self.bounds = bounds  # Search space bounds (for each variable)\n+        self.population_size = population_size\n+        self.generations = generations\n+        self.mutation_prob = mutation_prob\n+        self.crossover_rate = crossover_rate\n+        self.maximize = maximize\n+        self.dim = len(bounds)  # Dimensionality of the function (number of variables)\n+\n+        # Initialize population\n+        self.population = self.initialize_population()\n+\n+    def initialize_population(self) -> list[np.ndarray]:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file genetic_algorithm/genetic_algorithm_optimization.py, please provide doctest for the function initialize_population",
    "line_number": 42,
    "enriched": "File: genetic_algorithm/genetic_algorithm_optimization.py\nCode: @@ -0,0 +1,226 @@\n+import random\n+from collections.abc import Callable, Sequence\n+from concurrent.futures import ThreadPoolExecutor\n+\n+import numpy as np\n+\n+# Parameters\n+N_POPULATION = 100  # Population size\n+N_GENERATIONS = 500  # Maximum number of generations\n+N_SELECTED = 50  # Number of parents selected for the next generation\n+MUTATION_PROBABILITY = 0.1  # Mutation probability\n+CROSSOVER_RATE = 0.8  # Probability of crossover\n+SEARCH_SPACE = (-10, 10)  # Search space for the variables\n+\n+# Random number generator\n+rng = np.random.default_rng()\n+\n+\n+class GeneticAlgorithm:\n+    def __init__(\n+        self,\n+        function: Callable[[float, float], float],\n+        bounds: Sequence[tuple[int | float, int | float]],\n+        population_size: int,\n+        generations: int,\n+        mutation_prob: float,\n+        crossover_rate: float,\n+        maximize: bool = True,\n+    ) -> None:\n+        self.function = function  # Target function to optimize\n+        self.bounds = bounds  # Search space bounds (for each variable)\n+        self.population_size = population_size\n+        self.generations = generations\n+        self.mutation_prob = mutation_prob\n+        self.crossover_rate = crossover_rate\n+        self.maximize = maximize\n+        self.dim = len(bounds)  # Dimensionality of the function (number of variables)\n+\n+        # Initialize population\n+        self.population = self.initialize_population()\n+\n+    def initialize_population(self) -> list[np.ndarray]:\nComment: As there is no test file in this pull request nor any test function or class in the file `genetic_algorithm/genetic_algorithm_optimization.py`, please provide doctest for the function `initialize_population`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "genetic_algorithm/genetic_algorithm_optimization.py",
    "pr_number": 12059,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1799179737,
    "comment_created_at": "2024-10-14T10:28:22Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+def camel_to_snake(camel_str : str) -> str :",
    "comment": "as there is no test file in this pull request nor any test function or class in the file strings/camel_case_to_snake_case.py, please provide doctest for the function camel_to_snake",
    "line_number": 1,
    "enriched": "File: strings/camel_case_to_snake_case.py\nCode: @@ -0,0 +1,42 @@\n+def camel_to_snake(camel_str : str) -> str :\nComment: As there is no test file in this pull request nor any test function or class in the file `strings/camel_case_to_snake_case.py`, please provide doctest for the function `camel_to_snake`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "strings/camel_case_to_snake_case.py",
    "pr_number": 9828,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347539746,
    "comment_created_at": "2023-10-05T14:42:11Z"
  },
  {
    "code": "@@ -0,0 +1,71 @@\n+\"\"\"\n+Project Euler Problem 79: https://projecteuler.net/problem=79\n+\n+Passcode derivation\n+\n+A common security method used for online banking is to ask the user for three\n+random characters from a passcode. For example, if the passcode was 531278,\n+they may ask for the 2nd, 3rd, and 5th characters; the expected reply would\n+be: 317.\n+\n+The text file, keylog.txt, contains fifty successful login attempts.\n+\n+Given that the three characters are always asked for in order, analyze the file\n+to determine the shortest possible secret passcode of unknown length.\n+\"\"\"\n+\n+from collections import Counter\n+\n+\n+\n+def find_secret_passcode(logins: list[str]) -> int:\n+    \"\"\"\n+    Find the shortest possible secret passcode of unknown length.\n+\n+    :param logins: A list of successful login attempts.\n+    :type logins: list[str]\n+    :return: The shortest possible secret passcode.\n+    :rtype: int\n+\n+    >>> find_secret_passcode([\"135\", \"259\", \"235\", \"189\", \"690\", \"168\", \"120\", \"136\", \"289\", \"589\", \"160\", \"165\", \"580\", \"369\", \"250\", \"280\"])\n+    12356890\n+\n+    >>> find_secret_passcode([\"426\", \"281\", \"061\", \"819\", \"268\", \"406\", \"420\", \"428\", \"209\", \"689\", \"019\", \"421\", \"469\", \"261\", \"681\", \"201\"])\n+    4206819\n+    \"\"\"\n+    s = Counter()\n+    c = Counter()\n+    r = []\n+\n+    for login in logins:\n+        for idx, char in enumerate(login):\n+            if char not in s:\n+                r.append(char)\n+            s[char] += idx\n+            c[char] += 1\n+\n+    r.sort(key=lambda a: s[a] / c[a])",
    "comment": "please provide descriptive name for the parameter: a",
    "line_number": 47,
    "enriched": "File: project_euler/problem_079/sol2.py\nCode: @@ -0,0 +1,71 @@\n+\"\"\"\n+Project Euler Problem 79: https://projecteuler.net/problem=79\n+\n+Passcode derivation\n+\n+A common security method used for online banking is to ask the user for three\n+random characters from a passcode. For example, if the passcode was 531278,\n+they may ask for the 2nd, 3rd, and 5th characters; the expected reply would\n+be: 317.\n+\n+The text file, keylog.txt, contains fifty successful login attempts.\n+\n+Given that the three characters are always asked for in order, analyze the file\n+to determine the shortest possible secret passcode of unknown length.\n+\"\"\"\n+\n+from collections import Counter\n+\n+\n+\n+def find_secret_passcode(logins: list[str]) -> int:\n+    \"\"\"\n+    Find the shortest possible secret passcode of unknown length.\n+\n+    :param logins: A list of successful login attempts.\n+    :type logins: list[str]\n+    :return: The shortest possible secret passcode.\n+    :rtype: int\n+\n+    >>> find_secret_passcode([\"135\", \"259\", \"235\", \"189\", \"690\", \"168\", \"120\", \"136\", \"289\", \"589\", \"160\", \"165\", \"580\", \"369\", \"250\", \"280\"])\n+    12356890\n+\n+    >>> find_secret_passcode([\"426\", \"281\", \"061\", \"819\", \"268\", \"406\", \"420\", \"428\", \"209\", \"689\", \"019\", \"421\", \"469\", \"261\", \"681\", \"201\"])\n+    4206819\n+    \"\"\"\n+    s = Counter()\n+    c = Counter()\n+    r = []\n+\n+    for login in logins:\n+        for idx, char in enumerate(login):\n+            if char not in s:\n+                r.append(char)\n+            s[char] += idx\n+            c[char] += 1\n+\n+    r.sort(key=lambda a: s[a] / c[a])\nComment: Please provide descriptive name for the parameter: `a`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "project_euler/problem_079/sol2.py",
    "pr_number": 10447,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359476719,
    "comment_created_at": "2023-10-14T16:18:59Z"
  },
  {
    "code": "@@ -0,0 +1,59 @@\n+\"\"\"\n+Adaptive Merge Sort implementation.\n+https://en.wikipedia.org/wiki/Merge_sort\n+\"\"\"\n+\n+def adaptive_merge_sort(sequence: list) -> list:\n+    \"\"\"\n+    >>> adaptive_merge_sort([12, 11, 13, 5, 6, 7])\n+    [5, 6, 7, 11, 12, 13]\n+\n+    >>> adaptive_merge_sort([4, 3, 2, 1])\n+    [1, 2, 3, 4]\n+\n+    >>> adaptive_merge_sort([\"apple\", \"zebra\", \"mango\", \"banana\"])\n+    ['apple', 'banana', 'mango', 'zebra']\n+    \"\"\"\n+    if len(sequence) < 2:\n+        return sequence\n+\n+    aux = sequence[:]\n+    adaptive_merge_sort_recursive(sequence, aux, 0, len(sequence) - 1)\n+    return sequence\n+\n+def adaptive_merge_sort_recursive(arr: list, aux: list, low: int, high: int) -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file sorts/adaptive_merge_sort.py, please provide doctest for the function adaptive_merge_sort_recursive",
    "line_number": 24,
    "enriched": "File: sorts/adaptive_merge_sort.py\nCode: @@ -0,0 +1,59 @@\n+\"\"\"\n+Adaptive Merge Sort implementation.\n+https://en.wikipedia.org/wiki/Merge_sort\n+\"\"\"\n+\n+def adaptive_merge_sort(sequence: list) -> list:\n+    \"\"\"\n+    >>> adaptive_merge_sort([12, 11, 13, 5, 6, 7])\n+    [5, 6, 7, 11, 12, 13]\n+\n+    >>> adaptive_merge_sort([4, 3, 2, 1])\n+    [1, 2, 3, 4]\n+\n+    >>> adaptive_merge_sort([\"apple\", \"zebra\", \"mango\", \"banana\"])\n+    ['apple', 'banana', 'mango', 'zebra']\n+    \"\"\"\n+    if len(sequence) < 2:\n+        return sequence\n+\n+    aux = sequence[:]\n+    adaptive_merge_sort_recursive(sequence, aux, 0, len(sequence) - 1)\n+    return sequence\n+\n+def adaptive_merge_sort_recursive(arr: list, aux: list, low: int, high: int) -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `sorts/adaptive_merge_sort.py`, please provide doctest for the function `adaptive_merge_sort_recursive`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "sorts/adaptive_merge_sort.py",
    "pr_number": 11991,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1797295381,
    "comment_created_at": "2024-10-11T18:23:45Z"
  },
  {
    "code": "@@ -0,0 +1,111 @@\n+#  Copyright (c) 2023 Diego Gasco (diego.gasco99@gmail.com), Diegomangasco on GitHub\n+\n+import logging\n+import numpy as np\n+import scipy\n+\n+logging.basicConfig(level=logging.INFO, format='%(message)s')\n+\n+\n+def column_reshape(input_array: np.ndarray) -> np.ndarray:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/dimensionality_reduction.py, please provide doctest for the function column_reshape",
    "line_number": 10,
    "enriched": "File: machine_learning/dimensionality_reduction.py\nCode: @@ -0,0 +1,111 @@\n+#  Copyright (c) 2023 Diego Gasco (diego.gasco99@gmail.com), Diegomangasco on GitHub\n+\n+import logging\n+import numpy as np\n+import scipy\n+\n+logging.basicConfig(level=logging.INFO, format='%(message)s')\n+\n+\n+def column_reshape(input_array: np.ndarray) -> np.ndarray:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/dimensionality_reduction.py`, please provide doctest for the function `column_reshape`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/dimensionality_reduction.py",
    "pr_number": 8590,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1154329928,
    "comment_created_at": "2023-03-31T10:53:03Z"
  },
  {
    "code": "@@ -0,0 +1,52 @@\n+def block_sort(lst: list) -> list:\n+    \"\"\"\n+    Sorts a list using the Block Sort algorithm.\n+\n+    The Block Sort algorithm works by dividing the input list into blocks of size\n+    sqrt(n), sorting each block, and then merging the sorted blocks into a single\n+    sorted list.\n+\n+    Args:\n+        lst (List[int]): The unsorted list to be sorted.\n+\n+    Returns:\n+        List[int]: The sorted list.\n+\n+    Examples:\n+        >>> block_sort([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5])\n+        [1, 1, 2, 3, 3, 4, 5, 5, 5, 6, 9]\n+        >>> block_sort([5, 4, 3, 2, 1])\n+        [1, 2, 3, 4, 5]\n+        >>> block_sort([])\n+        []\n+    \"\"\"",
    "comment": "can you add edge cases and some larger input lists like this:",
    "line_number": 22,
    "enriched": "File: sorts/block_sort.py\nCode: @@ -0,0 +1,52 @@\n+def block_sort(lst: list) -> list:\n+    \"\"\"\n+    Sorts a list using the Block Sort algorithm.\n+\n+    The Block Sort algorithm works by dividing the input list into blocks of size\n+    sqrt(n), sorting each block, and then merging the sorted blocks into a single\n+    sorted list.\n+\n+    Args:\n+        lst (List[int]): The unsorted list to be sorted.\n+\n+    Returns:\n+        List[int]: The sorted list.\n+\n+    Examples:\n+        >>> block_sort([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5])\n+        [1, 1, 2, 3, 3, 4, 5, 5, 5, 6, 9]\n+        >>> block_sort([5, 4, 3, 2, 1])\n+        [1, 2, 3, 4, 5]\n+        >>> block_sort([])\n+        []\n+    \"\"\"\nComment: Can you add edge cases and some larger input lists like this:\r\n\r\n```suggestion\r\n    \"\"\"\r\n    >>> block_sort([1, 1, 1, 1, 1])\r\n        [1, 1, 1, 1, 1]\r\n        >>> block_sort([7, 6, 5, 4, 3, 2, 1])\r\n        [1, 2, 3, 4, 5, 6, 7]\r\n        >>> block_sort([10, 9, 8, 7, 6, 5, 4, 3, 2, 1])\r\n        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n        >>> block_sort([1])\r\n        [1]\r\n        >>> block_sort([2, 1])\r\n        [1, 2]\r\n        >>> block_sort([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\r\n        [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\r\n        >>> block_sort([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\r\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n        >>> block_sort([1000, 100, 10, 1, 10000, 100000, 1000000])\r\n        [1, 10, 100, 1000, 10000, 100000, 1000000]\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "sorts/block_sort.py",
    "pr_number": 10734,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1369615107,
    "comment_created_at": "2023-10-24T04:20:12Z"
  },
  {
    "code": "@@ -0,0 +1,30 @@\n+\"\"\"\n+Mass-energy equivalence, described by Albert Einstein's E=mc^2 formula, reveals that mass and energy are interchangeable in a system's rest frame. Even when the system is moving, its relativistic energy and mass obey this formula. This principle asserts that a small amount of rest mass corresponds to a significant amount of energy, regardless of matter composition. Invariant mass is a fundamental property unaffected by momentum and is consistent across reference frames. When energy is lost in reactions, an equivalent mass is also lost, releasing energy in various forms. Einstein's groundbreaking idea, originating from special relativity, was published in 1905, and it has since played a crucial role in nuclear and particle physics.\n+\n+Reference:https://en.wikipedia.org/wiki/Mass%E2%80%93energy_equivalence\n+\"\"\"\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+import math\n+\n+def mass_energy_equivalence(mass:float)->float:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file physics/mass_energy_equivalence.py, please provide doctest for the function mass_energy_equivalence",
    "line_number": 18,
    "enriched": "File: physics/mass_energy_equivalence.py\nCode: @@ -0,0 +1,30 @@\n+\"\"\"\n+Mass-energy equivalence, described by Albert Einstein's E=mc^2 formula, reveals that mass and energy are interchangeable in a system's rest frame. Even when the system is moving, its relativistic energy and mass obey this formula. This principle asserts that a small amount of rest mass corresponds to a significant amount of energy, regardless of matter composition. Invariant mass is a fundamental property unaffected by momentum and is consistent across reference frames. When energy is lost in reactions, an equivalent mass is also lost, releasing energy in various forms. Einstein's groundbreaking idea, originating from special relativity, was published in 1905, and it has since played a crucial role in nuclear and particle physics.\n+\n+Reference:https://en.wikipedia.org/wiki/Mass%E2%80%93energy_equivalence\n+\"\"\"\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+import math\n+\n+def mass_energy_equivalence(mass:float)->float:\nComment: As there is no test file in this pull request nor any test function or class in the file `physics/mass_energy_equivalence.py`, please provide doctest for the function `mass_energy_equivalence`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "physics/mass_energy_equivalence.py",
    "pr_number": 9254,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342110338,
    "comment_created_at": "2023-10-01T10:00:58Z"
  },
  {
    "code": "@@ -0,0 +1,44 @@\n+\"\"\"\n+Implements the Scaled Exponential Linear Unit or SELU function.\n+The function takes a vector of K real numbers and two real numbers\n+alpha(default = 1.6732) & lambda (default = 1.0507) as input and\n+then applies the SELU function to each element of the vector.\n+SELU is a self-normalizing activation function. It is a variant\n+of the ELU. The main advantage of SELU is that we can be sure\n+that the output will always be standardized due to its\n+self-normalizing behavior. That means there is no need to\n+include Batch-Normalization layers.\n+References :\n+https://iq.opengenus.org/scaled-exponential-linear-unit/\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def scaled_exponential_linear_unit(\n+    vector: np.ndarray, alpha: float = 1.6732, _lambda: float = 1.0507",
    "comment": "is there a reason why _lambda has an underscore? is the user not meant to change this coefficient?",
    "line_number": 19,
    "enriched": "File: neural_network/activation_functions/scaled_exponential_linear_unit.py\nCode: @@ -0,0 +1,44 @@\n+\"\"\"\n+Implements the Scaled Exponential Linear Unit or SELU function.\n+The function takes a vector of K real numbers and two real numbers\n+alpha(default = 1.6732) & lambda (default = 1.0507) as input and\n+then applies the SELU function to each element of the vector.\n+SELU is a self-normalizing activation function. It is a variant\n+of the ELU. The main advantage of SELU is that we can be sure\n+that the output will always be standardized due to its\n+self-normalizing behavior. That means there is no need to\n+include Batch-Normalization layers.\n+References :\n+https://iq.opengenus.org/scaled-exponential-linear-unit/\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def scaled_exponential_linear_unit(\n+    vector: np.ndarray, alpha: float = 1.6732, _lambda: float = 1.0507\nComment: Is there a reason why `_lambda` has an underscore? Is the user not meant to change this coefficient?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "neural_network/activation_functions/scaled_exponential_linear_unit.py",
    "pr_number": 9027,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1316723239,
    "comment_created_at": "2023-09-06T04:52:48Z"
  },
  {
    "code": "@@ -0,0 +1,102 @@\n+import glob\n+import os\n+import random\n+from string import ascii_lowercase, digits\n+\n+import cv2\n+import numpy as np\n+\n+\n+# Params\n+IMAGE_DIR = \"1\"\n+OUTPUT_DIR = \"out\"\n+NOISE_TYPE = \"salt-and-pepper\" # (\"salt-and-pepper\" or \"gaussian\")\n+ALPHA = 0.1 # (\"salt-and-pepper\" amount or \"gaussian\" variance)\n+\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file computer_vision/noise_augmentation.py, please provide doctest for the function main",
    "line_number": 17,
    "enriched": "File: computer_vision/noise_augmentation.py\nCode: @@ -0,0 +1,102 @@\n+import glob\n+import os\n+import random\n+from string import ascii_lowercase, digits\n+\n+import cv2\n+import numpy as np\n+\n+\n+# Params\n+IMAGE_DIR = \"1\"\n+OUTPUT_DIR = \"out\"\n+NOISE_TYPE = \"salt-and-pepper\" # (\"salt-and-pepper\" or \"gaussian\")\n+ALPHA = 0.1 # (\"salt-and-pepper\" amount or \"gaussian\" variance)\n+\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `computer_vision/noise_augmentation.py`, please provide doctest for the function `main`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "computer_vision/noise_augmentation.py",
    "pr_number": 8049,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1057515367,
    "comment_created_at": "2022-12-27T08:14:52Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+\"\"\"\n+Project Euler Problem 79: https://projecteuler.net/problem=79\n+\n+Passcode derivation\n+\n+A common security method used for online banking is to ask the user for three\n+random characters from a passcode. For example, if the passcode was 531278,\n+they may ask for the 2nd, 3rd, and 5th characters; the expected reply would\n+be: 317.\n+\n+The text file, keylog.txt, contains fifty successful login attempts.\n+\n+Given that the three characters are always asked for in order, analyse the file\n+so as to determine the shortest possible secret passcode of unknown length.\n+...\n+\n+\"\"\"\n+import itertools\n+import os\n+\n+\n+def solution() -> int:\n+    \"\"\"\n+    Returns the shortest possible secret passcode of unknown length.\n+\n+    >>> solution()\n+    73162890\n+    \"\"\"\n+    with open(os.path.dirname(__file__) + \"/p079_keylog.txt\") as file:\n+        logins = [tuple(line.strip()) for line in file]\n+\n+    unique_chars = {char for login in logins for char in login}\n+\n+    for permutation in itertools.permutations(unique_chars):\n+        satisfied = True\n+        for login in logins:\n+            if not (\n+                permutation.index(login[0])\n+                < permutation.index(login[1])\n+                < permutation.index(login[2])\n+            ):\n+                satisfied = False\n+                break\n+\n+        if satisfied:\n+            return int(\"\".join(permutation))\n+            break\n+\n+    return 0",
    "comment": "it's better to raise an exception. what do you think?",
    "line_number": 49,
    "enriched": "File: project_euler/problem_079/sol1.py\nCode: @@ -0,0 +1,53 @@\n+\"\"\"\n+Project Euler Problem 79: https://projecteuler.net/problem=79\n+\n+Passcode derivation\n+\n+A common security method used for online banking is to ask the user for three\n+random characters from a passcode. For example, if the passcode was 531278,\n+they may ask for the 2nd, 3rd, and 5th characters; the expected reply would\n+be: 317.\n+\n+The text file, keylog.txt, contains fifty successful login attempts.\n+\n+Given that the three characters are always asked for in order, analyse the file\n+so as to determine the shortest possible secret passcode of unknown length.\n+...\n+\n+\"\"\"\n+import itertools\n+import os\n+\n+\n+def solution() -> int:\n+    \"\"\"\n+    Returns the shortest possible secret passcode of unknown length.\n+\n+    >>> solution()\n+    73162890\n+    \"\"\"\n+    with open(os.path.dirname(__file__) + \"/p079_keylog.txt\") as file:\n+        logins = [tuple(line.strip()) for line in file]\n+\n+    unique_chars = {char for login in logins for char in login}\n+\n+    for permutation in itertools.permutations(unique_chars):\n+        satisfied = True\n+        for login in logins:\n+            if not (\n+                permutation.index(login[0])\n+                < permutation.index(login[1])\n+                < permutation.index(login[2])\n+            ):\n+                satisfied = False\n+                break\n+\n+        if satisfied:\n+            return int(\"\".join(permutation))\n+            break\n+\n+    return 0\nComment: It's better to raise an exception. What do you think?\r\n\r\n```suggestion\r\n    raise Exception(\"Unable to find the secret passcode\")\r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "project_euler/problem_079/sol1.py",
    "pr_number": 8607,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1155239295,
    "comment_created_at": "2023-04-02T04:53:12Z"
  },
  {
    "code": "@@ -0,0 +1,79 @@\n+\"\"\"\n+TT-ENTAILS Algorithm (Propositional Logic)\n+Reference: [Russell & Norvig, Artificial Intelligence: A Modern Approach, Ch. 7](https://aima.cs.berkeley.edu/)\n+Wikipedia: [Entailment](https://en.wikipedia.org/wiki/Entailment)\n+\n+This algorithm checks if a knowledge base (KB) entails a query sentence (a)\n+using truth tables. Returns True if KB entails a, False otherwise.\n+\"\"\"\n+\n+import itertools\n+import ast\n+import operator\n+\n+OPS = {\n+    ast.And: operator.and_,\n+    ast.Or: operator.or_,\n+    ast.Not: operator.not_\n+}\n+\n+def safe_eval(expr: str, model: dict[str, bool]) -> bool:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/ttentails.py, please provide doctest for the function safe_eval",
    "line_number": 20,
    "enriched": "File: machine_learning/ttentails.py\nCode: @@ -0,0 +1,79 @@\n+\"\"\"\n+TT-ENTAILS Algorithm (Propositional Logic)\n+Reference: [Russell & Norvig, Artificial Intelligence: A Modern Approach, Ch. 7](https://aima.cs.berkeley.edu/)\n+Wikipedia: [Entailment](https://en.wikipedia.org/wiki/Entailment)\n+\n+This algorithm checks if a knowledge base (KB) entails a query sentence (a)\n+using truth tables. Returns True if KB entails a, False otherwise.\n+\"\"\"\n+\n+import itertools\n+import ast\n+import operator\n+\n+OPS = {\n+    ast.And: operator.and_,\n+    ast.Or: operator.or_,\n+    ast.Not: operator.not_\n+}\n+\n+def safe_eval(expr: str, model: dict[str, bool]) -> bool:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/ttentails.py`, please provide doctest for the function `safe_eval`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/ttentails.py",
    "pr_number": 12903,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2283205674,
    "comment_created_at": "2025-08-18T19:02:54Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+# Basic implementation of the collision of two circles.\n+def circle_collision(fpos: tuple[float, float, float], spos: tuple[float, float, float]) -> bool:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file physics/collisions.py, please provide doctest for the function circle_collision",
    "line_number": 2,
    "enriched": "File: physics/collisions.py\nCode: @@ -0,0 +1,21 @@\n+# Basic implementation of the collision of two circles.\n+def circle_collision(fpos: tuple[float, float, float], spos: tuple[float, float, float]) -> bool:\nComment: As there is no test file in this pull request nor any test function or class in the file `physics/collisions.py`, please provide doctest for the function `circle_collision`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "physics/collisions.py",
    "pr_number": 12574,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1948027735,
    "comment_created_at": "2025-02-09T07:36:16Z"
  },
  {
    "code": "@@ -83,6 +83,29 @@ def fib_iterative(n: int) -> list[int]:\n     return fib\n \n \n+def fib_recursive_term(i: int) -> int:\n+    \"\"\"\n+    Calculates the i-th (0-indexed) Fibonacci number using recursion\n+    >>> fib_recursive_term(0)\n+    0\n+    >>> fib_recursive_term(1)\n+    1\n+    >>> fib_recursive_term(5)\n+    5\n+    >>> fib_recursive_term(10)\n+    55\n+    >>> fib_recursive_term(-1)\n+    Traceback (most recent call last):\n+        ...\n+    Exception: n is negative\n+    \"\"\"",
    "comment": "can we:\r\n1. removed the large block of empty lines in the middle of the fib_recursive function.\r\n2. ensured the docstring for the fib_recursive function is correctly formatted and includes the expected outputs for the doctests.",
    "line_number": 101,
    "enriched": "File: maths/fibonacci.py\nCode: @@ -83,6 +83,29 @@ def fib_iterative(n: int) -> list[int]:\n     return fib\n \n \n+def fib_recursive_term(i: int) -> int:\n+    \"\"\"\n+    Calculates the i-th (0-indexed) Fibonacci number using recursion\n+    >>> fib_recursive_term(0)\n+    0\n+    >>> fib_recursive_term(1)\n+    1\n+    >>> fib_recursive_term(5)\n+    5\n+    >>> fib_recursive_term(10)\n+    55\n+    >>> fib_recursive_term(-1)\n+    Traceback (most recent call last):\n+        ...\n+    Exception: n is negative\n+    \"\"\"\nComment: Can we:\r\n1. Removed the large block of empty lines in the middle of the fib_recursive function.\r\n2. Ensured the docstring for the fib_recursive function is correctly formatted and includes the expected outputs for the doctests.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "maths/fibonacci.py",
    "pr_number": 11301,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1600843236,
    "comment_created_at": "2024-05-15T01:46:44Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+from typing import List\n+\n+class Solution:\n+    def change(self, amount: int, coins: List[int]) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/coin_change_ii.py, please provide doctest for the function change",
    "line_number": 4,
    "enriched": "File: dynamic_programming/coin_change_II.py\nCode: @@ -0,0 +1,21 @@\n+from typing import List\n+\n+class Solution:\n+    def change(self, amount: int, coins: List[int]) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/coin_change_II.py`, please provide doctest for the function `change`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/coin_change_II.py",
    "pr_number": 13291,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2408076292,
    "comment_created_at": "2025-10-06T19:16:04Z"
  },
  {
    "code": "@@ -0,0 +1,80 @@\n+# Author:- Arnab Nath\n+# Python program to find an element x\n+# in a sorted array using Exponential Search\n+\n+# A recursive binary search function returns\n+# location of x in given array arr[l..r] is\n+# present, otherwise -1\n+arr = [4, 7, 10, 34, 44, 70]\n+n = len(arr)\n+x = 44\n+if arr[0] == x:\n+    print(\"0\")\n+i = 1\n+# Finding range for binarySearch\n+while i < n and arr[i] <= x:\n+    i = i * 2\n+min1 = min(i, len(arr))\n+\n+\n+def binarysearch(arr: list, le: int, r: int, x: int) -> int:",
    "comment": "please provide descriptive name for the parameter: r\n\nplease provide descriptive name for the parameter: x",
    "line_number": 20,
    "enriched": "File: searches/exponentialsearch.py\nCode: @@ -0,0 +1,80 @@\n+# Author:- Arnab Nath\n+# Python program to find an element x\n+# in a sorted array using Exponential Search\n+\n+# A recursive binary search function returns\n+# location of x in given array arr[l..r] is\n+# present, otherwise -1\n+arr = [4, 7, 10, 34, 44, 70]\n+n = len(arr)\n+x = 44\n+if arr[0] == x:\n+    print(\"0\")\n+i = 1\n+# Finding range for binarySearch\n+while i < n and arr[i] <= x:\n+    i = i * 2\n+min1 = min(i, len(arr))\n+\n+\n+def binarysearch(arr: list, le: int, r: int, x: int) -> int:\nComment: Please provide descriptive name for the parameter: `r`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "searches/exponentialsearch.py",
    "pr_number": 7315,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996434089,
    "comment_created_at": "2022-10-16T12:06:26Z"
  },
  {
    "code": "@@ -0,0 +1,37 @@\n+\"\"\"\n+Author Suvan Banerjee (@suvanbanerjee)\n+Date: 2023-02-10\n+Description: This program calculates the future value of money based on the present\n+value, inflation rate, and number of years.\n+\n+--------------------------------------------\n+In economics, inflation is a general increase of the prices. This is usually measured \n+using the consumer price index (CPI).When the general price level rises, each unit of\n+ currency buys fewer goods and services; consequently, inflation corresponds\n+to a reduction in the purchasing power of money.\n+\n+Source: https://en.wikipedia.org/wiki/Inflation\n+\n+\"\"\"\n+\n+import doctest\n+\n+def calculate_future_value(present_value : float, inflation_rate : float , years : int) -> float:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file financial/inflation.py, please provide doctest for the function calculate_future_value",
    "line_number": 19,
    "enriched": "File: financial/inflation.py\nCode: @@ -0,0 +1,37 @@\n+\"\"\"\n+Author Suvan Banerjee (@suvanbanerjee)\n+Date: 2023-02-10\n+Description: This program calculates the future value of money based on the present\n+value, inflation rate, and number of years.\n+\n+--------------------------------------------\n+In economics, inflation is a general increase of the prices. This is usually measured \n+using the consumer price index (CPI).When the general price level rises, each unit of\n+ currency buys fewer goods and services; consequently, inflation corresponds\n+to a reduction in the purchasing power of money.\n+\n+Source: https://en.wikipedia.org/wiki/Inflation\n+\n+\"\"\"\n+\n+import doctest\n+\n+def calculate_future_value(present_value : float, inflation_rate : float , years : int) -> float:\nComment: As there is no test file in this pull request nor any test function or class in the file `financial/inflation.py`, please provide doctest for the function `calculate_future_value`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "financial/inflation.py",
    "pr_number": 9391,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342325593,
    "comment_created_at": "2023-10-02T06:43:30Z"
  },
  {
    "code": "@@ -0,0 +1,43 @@\n+\"\"\"\n+In the Combination Sum problem, we are given a list consisting of distinct integers.\n+We need to find all the combinations whose sum equals to target given.\n+We can use an element more than one.\n+Time complexity(Average Case): O(n!),\n+\"\"\"\n+\n+\n+def backtrack(",
    "comment": "as there is no test file in this pull request nor any test function or class in the file backtracking/combination_sum.py, please provide doctest for the function backtrack",
    "line_number": 9,
    "enriched": "File: backtracking/combination_sum.py\nCode: @@ -0,0 +1,43 @@\n+\"\"\"\n+In the Combination Sum problem, we are given a list consisting of distinct integers.\n+We need to find all the combinations whose sum equals to target given.\n+We can use an element more than one.\n+Time complexity(Average Case): O(n!),\n+\"\"\"\n+\n+\n+def backtrack(\nComment: As there is no test file in this pull request nor any test function or class in the file `backtracking/combination_sum.py`, please provide doctest for the function `backtrack`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "backtracking/combination_sum.py",
    "pr_number": 7346,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996533364,
    "comment_created_at": "2022-10-17T00:24:17Z"
  },
  {
    "code": "@@ -0,0 +1,45 @@\n+\"\"\"\n+The maximum submatrix sum problem is the task of finding the maximum sum that can be\n+obtained from a contiguous submatrix within a given 3D matrix of numbers. For example, given\n+the matrix \n+[[-2, 1, -3], \n+ [4, -1, 2], \n+ [1, -5, 4]], the contiguous submatrix with the maximum sum\n+is [[4, -1, 2], \n+    [1, -5, 4]], so the maximum submatrix sum is 6.\n+\n+Kadane's algorithm is a simple dynamic programming algorithm that solves the maximum\n+subarray sum problem in O(n) time and O(1) space. We modify this algorithm to solve the maximum\n+submatrix sum problem in O(nm^2) time and O(n) space.\n+\n+Reference: https://www.geeksforgeeks.org/maximum-sum-submatrix/\n+\"\"\"\n+\n+from collections.abc import Sequence\n+\n+def max_submatrix_sum(matrix: list[list[str]]) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/max_sum_submatrix.py, please provide doctest for the function max_submatrix_sum",
    "line_number": 20,
    "enriched": "File: dynamic_programming/max_sum_submatrix.py\nCode: @@ -0,0 +1,45 @@\n+\"\"\"\n+The maximum submatrix sum problem is the task of finding the maximum sum that can be\n+obtained from a contiguous submatrix within a given 3D matrix of numbers. For example, given\n+the matrix \n+[[-2, 1, -3], \n+ [4, -1, 2], \n+ [1, -5, 4]], the contiguous submatrix with the maximum sum\n+is [[4, -1, 2], \n+    [1, -5, 4]], so the maximum submatrix sum is 6.\n+\n+Kadane's algorithm is a simple dynamic programming algorithm that solves the maximum\n+subarray sum problem in O(n) time and O(1) space. We modify this algorithm to solve the maximum\n+submatrix sum problem in O(nm^2) time and O(n) space.\n+\n+Reference: https://www.geeksforgeeks.org/maximum-sum-submatrix/\n+\"\"\"\n+\n+from collections.abc import Sequence\n+\n+def max_submatrix_sum(matrix: list[list[str]]) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/max_sum_submatrix.py`, please provide doctest for the function `max_submatrix_sum`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/max_sum_submatrix.py",
    "pr_number": 9633,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1344543475,
    "comment_created_at": "2023-10-03T18:25:21Z"
  },
  {
    "code": "@@ -134,25 +134,27 @@ def max_parts(self, num_cuts: float) -> float:\n \n         >>> circle = Circle(5)\n         >>> circle.max_parts(0)\n-        1.0\n+        1",
    "comment": "why do you wanna add a \".0\" is the rest of test cases ???",
    "line_number": 137,
    "enriched": "File: geometry/geometry.py\nCode: @@ -134,25 +134,27 @@ def max_parts(self, num_cuts: float) -> float:\n \n         >>> circle = Circle(5)\n         >>> circle.max_parts(0)\n-        1.0\n+        1\nComment: why do you wanna add a \".0\" is the rest of test cases ???",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "geometry/geometry.py",
    "pr_number": 11271,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1518459173,
    "comment_created_at": "2024-03-09T02:22:19Z"
  },
  {
    "code": "@@ -0,0 +1,18 @@\n+def sum_of_hp(first_term: int, common_diff: int, num_of_terms: int) -> float:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/sum_of_harmonic_series.py, please provide doctest for the function sum_of_hp",
    "line_number": 1,
    "enriched": "File: maths/sum_of_harmonic_series.py\nCode: @@ -0,0 +1,18 @@\n+def sum_of_hp(first_term: int, common_diff: int, num_of_terms: int) -> float:\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/sum_of_harmonic_series.py`, please provide doctest for the function `sum_of_hp`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/sum_of_harmonic_series.py",
    "pr_number": 7471,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000979090,
    "comment_created_at": "2022-10-20T18:22:11Z"
  },
  {
    "code": "@@ -0,0 +1,150 @@\n+import numpy as np\r\n+\r\n+class ART1:\r\n+    \"\"\"\r\n+    Adaptive Resonance Theory 1 (ART1) model for binary data clustering.\r\n+\r\n+    The ART1 algorithm is a type of neural network used for unsupervised \r\n+    learning and clustering of binary input data. It continuously learns \r\n+    to categorize inputs based on similarity while preserving previously \r\n+    learned categories. The vigilance parameter controls the degree of \r\n+    similarity required to assign an input to an existing category,\r\n+    allowing for flexible and adaptive clustering.\r\n+\r\n+    Attributes:\r\n+        num_features (int): Number of features in the input data.\r\n+        vigilance (float): Threshold for similarity that determines whether \r\n+        an input matches an existing cluster.\r\n+        weights (list): List of cluster weights representing the learned categories.\r\n+    \"\"\"\r\n+    \r\n+    def __init__(self, num_features: int, vigilance: float = 0.7) -> None:\r\n+        \"\"\"\r\n+        Initialize the ART1 model with the given number of features and vigilance parameter.\r\n+\r\n+        Args:\r\n+            num_features (int): Number of features in the input data.\r\n+            vigilance (float): Threshold for similarity (default is 0.7).\r\n+            \r\n+        Examples:\r\n+            >>> model = ART1(num_features=4, vigilance=0.5)\r\n+            >>> model.num_features\r\n+            4\r\n+            >>> model.vigilance\r\n+            0.5\r\n+        \"\"\"\r\n+        self.vigilance = vigilance  # Controls cluster strictness\r\n+        self.num_features = num_features\r\n+        self.weights = []           # List of cluster weights\r\n+        \r\n+    def train(self, data: np.ndarray) -> None:\r\n+        \"\"\"\r\n+        Train the ART1 model on the provided data.\r\n+\r\n+        Args:\r\n+            data (np.ndarray): A 2D array of binary input data (num_samples x num_features).\r\n+\r\n+        Examples:\r\n+            >>> model = ART1(num_features=4, vigilance=0.5)\r\n+            >>> data = np.array([[1, 1, 0, 0], [1, 1, 1, 0]])\r\n+            >>> model.train(data)\r\n+            >>> len(model.weights)\r\n+            2\r\n+        \"\"\"\r\n+        for x in data:\r\n+            match = False\r\n+            for i, w in enumerate(self.weights):\r\n+                if self._similarity(w, x) >= self.vigilance:\r\n+                    self.weights[i] = self._learn(w, x)\r\n+                    match = True\r\n+                    break\r\n+            if not match:\r\n+                self.weights.append(x.copy())  # Add a new cluster\r\n+\r\n+    def _similarity(self, w: np.ndarray, x: np.ndarray) -> float:\r",
    "comment": "please provide descriptive name for the parameter: w\n\nplease provide descriptive name for the parameter: x",
    "line_number": 64,
    "enriched": "File: neural_network/adaptive_resonance_theory.py\nCode: @@ -0,0 +1,150 @@\n+import numpy as np\r\n+\r\n+class ART1:\r\n+    \"\"\"\r\n+    Adaptive Resonance Theory 1 (ART1) model for binary data clustering.\r\n+\r\n+    The ART1 algorithm is a type of neural network used for unsupervised \r\n+    learning and clustering of binary input data. It continuously learns \r\n+    to categorize inputs based on similarity while preserving previously \r\n+    learned categories. The vigilance parameter controls the degree of \r\n+    similarity required to assign an input to an existing category,\r\n+    allowing for flexible and adaptive clustering.\r\n+\r\n+    Attributes:\r\n+        num_features (int): Number of features in the input data.\r\n+        vigilance (float): Threshold for similarity that determines whether \r\n+        an input matches an existing cluster.\r\n+        weights (list): List of cluster weights representing the learned categories.\r\n+    \"\"\"\r\n+    \r\n+    def __init__(self, num_features: int, vigilance: float = 0.7) -> None:\r\n+        \"\"\"\r\n+        Initialize the ART1 model with the given number of features and vigilance parameter.\r\n+\r\n+        Args:\r\n+            num_features (int): Number of features in the input data.\r\n+            vigilance (float): Threshold for similarity (default is 0.7).\r\n+            \r\n+        Examples:\r\n+            >>> model = ART1(num_features=4, vigilance=0.5)\r\n+            >>> model.num_features\r\n+            4\r\n+            >>> model.vigilance\r\n+            0.5\r\n+        \"\"\"\r\n+        self.vigilance = vigilance  # Controls cluster strictness\r\n+        self.num_features = num_features\r\n+        self.weights = []           # List of cluster weights\r\n+        \r\n+    def train(self, data: np.ndarray) -> None:\r\n+        \"\"\"\r\n+        Train the ART1 model on the provided data.\r\n+\r\n+        Args:\r\n+            data (np.ndarray): A 2D array of binary input data (num_samples x num_features).\r\n+\r\n+        Examples:\r\n+            >>> model = ART1(num_features=4, vigilance=0.5)\r\n+            >>> data = np.array([[1, 1, 0, 0], [1, 1, 1, 0]])\r\n+            >>> model.train(data)\r\n+            >>> len(model.weights)\r\n+            2\r\n+        \"\"\"\r\n+        for x in data:\r\n+            match = False\r\n+            for i, w in enumerate(self.weights):\r\n+                if self._similarity(w, x) >= self.vigilance:\r\n+                    self.weights[i] = self._learn(w, x)\r\n+                    match = True\r\n+                    break\r\n+            if not match:\r\n+                self.weights.append(x.copy())  # Add a new cluster\r\n+\r\n+    def _similarity(self, w: np.ndarray, x: np.ndarray) -> float:\r\nComment: Please provide descriptive name for the parameter: `w`\n\nPlease provide descriptive name for the parameter: `x`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "neural_network/adaptive_resonance_theory.py",
    "pr_number": 12324,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1821851117,
    "comment_created_at": "2024-10-30T04:25:06Z"
  },
  {
    "code": "@@ -1,17 +1,28 @@\n #!/usr/bin/env python3\n \n+from __future__ import annotations",
    "comment": "this repo runs code on the current python (3.11 this month, 3.12 next month) so this import is not required.",
    "line_number": 3,
    "enriched": "File: scripts/build_directory_md.py\nCode: @@ -1,17 +1,28 @@\n #!/usr/bin/env python3\n \n+from __future__ import annotations\nComment: This repo runs code on the current Python (3.11 this month, 3.12 next month) so this import is not required.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "scripts/build_directory_md.py",
    "pr_number": 7592,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1316746627,
    "comment_created_at": "2023-09-06T05:27:52Z"
  },
  {
    "code": "@@ -4,34 +4,41 @@\n # quickly find a good approximation for the root of a real-valued function\n from __future__ import annotations\n \n-from decimal import Decimal\n-from math import *  # noqa: F403\n+from sympy import diff, symbols, sympify\n \n-from sympy import diff\n \n-\n-def newton_raphson(\n-    func: str, a: float | Decimal, precision: float = 10**-10\n-) -> float:\n+def newton_raphson(func: str, a: float, precision: float = 10**-10) -> float:",
    "comment": "can we come up with a more self-documenting name than a?",
    "line_number": 10,
    "enriched": "File: arithmetic_analysis/newton_raphson.py\nCode: @@ -4,34 +4,41 @@\n # quickly find a good approximation for the root of a real-valued function\n from __future__ import annotations\n \n-from decimal import Decimal\n-from math import *  # noqa: F403\n+from sympy import diff, symbols, sympify\n \n-from sympy import diff\n \n-\n-def newton_raphson(\n-    func: str, a: float | Decimal, precision: float = 10**-10\n-) -> float:\n+def newton_raphson(func: str, a: float, precision: float = 10**-10) -> float:\nComment: Can we come up with a more self-documenting name than `a`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "arithmetic_analysis/newton_raphson.py",
    "pr_number": 8869,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1264623592,
    "comment_created_at": "2023-07-16T07:11:14Z"
  },
  {
    "code": "@@ -68,6 +70,32 @@ def calculate_prime_numbers(max_number: int) -> list[int]:\n     return [2] + [2 * i + 1 for i in range(1, max_number // 2) if is_prime[i]]\n \n \n+def np_calculate_prime_numbers(max_number: int) -> list[int]:",
    "comment": "is it possible to make this as a helper function in a common module as i see it being used in your other pr as well or maybe add it in maths/ and re-use it here or (assuming that there's already similar function in maths/) re-use an existing implementation?",
    "line_number": 73,
    "enriched": "File: project_euler/problem_187/sol1.py\nCode: @@ -68,6 +70,32 @@ def calculate_prime_numbers(max_number: int) -> list[int]:\n     return [2] + [2 * i + 1 for i in range(1, max_number // 2) if is_prime[i]]\n \n \n+def np_calculate_prime_numbers(max_number: int) -> list[int]:\nComment: Is it possible to make this as a helper function in a common module as I see it being used in your other PR as well or maybe add it in `maths/` and re-use it here or (assuming that there's already similar function in `maths/`) re-use an existing implementation?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "project_euler/problem_187/sol1.py",
    "pr_number": 10580,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375369671,
    "comment_created_at": "2023-10-29T05:06:52Z"
  },
  {
    "code": "@@ -0,0 +1,111 @@\n+\"\"\"\n+Python implementation of the Primal-Dual Interior-Point Method for solving linear programs with\n+- `>=`, `<=`, and `=` constraints and\n+- each variable `x1, x2, ... >= 0`.\n+\n+Resources:\n+https://en.wikipedia.org/wiki/Interior-point_method\n+\"\"\"\n+\n+import numpy as np\n+\n+class InteriorPointMethod:\n+    \"\"\"\n+    Operate on linear programming problems using the Primal-Dual Interior-Point Method.\n+\n+    Attributes:\n+        c (np.ndarray): Coefficient matrix for the objective function.\n+        a (np.ndarray): Constraint matrix.\n+        b (np.ndarray): Constraint bounds.\n+        tol (float): Tolerance for stopping criterion.\n+        max_iter (int): Maximum number of iterations.\n+    \"\"\"\n+\n+    def __init__(self, c: np.ndarray, a: np.ndarray, b: np.ndarray, tol: float = 1e-8, max_iter: int = 100) -> None:",
    "comment": "please provide descriptive name for the parameter: c\n\nplease provide descriptive name for the parameter: a\n\nplease provide descriptive name for the parameter: b",
    "line_number": 24,
    "enriched": "File: linear_programming/interior_point_method.py\nCode: @@ -0,0 +1,111 @@\n+\"\"\"\n+Python implementation of the Primal-Dual Interior-Point Method for solving linear programs with\n+- `>=`, `<=`, and `=` constraints and\n+- each variable `x1, x2, ... >= 0`.\n+\n+Resources:\n+https://en.wikipedia.org/wiki/Interior-point_method\n+\"\"\"\n+\n+import numpy as np\n+\n+class InteriorPointMethod:\n+    \"\"\"\n+    Operate on linear programming problems using the Primal-Dual Interior-Point Method.\n+\n+    Attributes:\n+        c (np.ndarray): Coefficient matrix for the objective function.\n+        a (np.ndarray): Constraint matrix.\n+        b (np.ndarray): Constraint bounds.\n+        tol (float): Tolerance for stopping criterion.\n+        max_iter (int): Maximum number of iterations.\n+    \"\"\"\n+\n+    def __init__(self, c: np.ndarray, a: np.ndarray, b: np.ndarray, tol: float = 1e-8, max_iter: int = 100) -> None:\nComment: Please provide descriptive name for the parameter: `c`\n\nPlease provide descriptive name for the parameter: `a`\n\nPlease provide descriptive name for the parameter: `b`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "linear_programming/interior_point_method.py",
    "pr_number": 11497,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1694037338,
    "comment_created_at": "2024-07-28T00:38:49Z"
  },
  {
    "code": "@@ -0,0 +1,81 @@\n+from math import fabs\n+\n+\n+def split_list(timings: list) -> tuple:\n+\n+    \"\"\"\n+\n+    this is a case of the partition problem.\n+    it accepts a multiset ( list ) of positive integers,\n+    distributes them, and returns a tuple, containing two lists,\n+    with minimal difference between their sums\n+\n+    >>> split_list([27, 21, 92, 87, 1, 32])\n+    ([27, 21, 87], [92, 1, 32], 10)\n+    >>> split_list([52, 385, 9956, 25, 2367, 1111, 17, 925])\n+    ([9956], [52, 385, 25, 2367, 1111, 17, 925], 5074)\n+    >>> split_list([12, 10, 11, 9])\n+    ([10, 11], [12, 9], 0)\n+    >>> split_list([-1551, 2712, 2325, 2623])\n+    ([1551, 2712], [2325, 2623], 685)\n+    >>> split_list([\"12.5\", \"10\", \"11\", \"9\"])\n+    ([10, 11], [12.5, 9], 0.5)\n+    >>> split_list([\"twelve\", \"ten\", \"eleven\", \"nine\"])\n+    ([0], [0, 0, 0], 0)\n+\n+    \"\"\"\n+\n+    result = None\n+\n+    def split_workload(arr: list) -> tuple:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file scheduling/split_workload.py, please provide doctest for the function split_workload",
    "line_number": 30,
    "enriched": "File: scheduling/split_workload.py\nCode: @@ -0,0 +1,81 @@\n+from math import fabs\n+\n+\n+def split_list(timings: list) -> tuple:\n+\n+    \"\"\"\n+\n+    this is a case of the partition problem.\n+    it accepts a multiset ( list ) of positive integers,\n+    distributes them, and returns a tuple, containing two lists,\n+    with minimal difference between their sums\n+\n+    >>> split_list([27, 21, 92, 87, 1, 32])\n+    ([27, 21, 87], [92, 1, 32], 10)\n+    >>> split_list([52, 385, 9956, 25, 2367, 1111, 17, 925])\n+    ([9956], [52, 385, 25, 2367, 1111, 17, 925], 5074)\n+    >>> split_list([12, 10, 11, 9])\n+    ([10, 11], [12, 9], 0)\n+    >>> split_list([-1551, 2712, 2325, 2623])\n+    ([1551, 2712], [2325, 2623], 685)\n+    >>> split_list([\"12.5\", \"10\", \"11\", \"9\"])\n+    ([10, 11], [12.5, 9], 0.5)\n+    >>> split_list([\"twelve\", \"ten\", \"eleven\", \"nine\"])\n+    ([0], [0, 0, 0], 0)\n+\n+    \"\"\"\n+\n+    result = None\n+\n+    def split_workload(arr: list) -> tuple:\nComment: As there is no test file in this pull request nor any test function or class in the file `scheduling/split_workload.py`, please provide doctest for the function `split_workload`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "scheduling/split_workload.py",
    "pr_number": 8868,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1264399702,
    "comment_created_at": "2023-07-15T12:22:19Z"
  },
  {
    "code": "@@ -0,0 +1,36 @@\n+# Auteur : Bosolindo Edhiengene Roger\r\n+# email : rogerbosolinndo34@gmail.com\r\n+\r\n+\r\n+def calc(operations: dict) -> float:\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file other/time_algo_exec.py, please provide doctest for the function calc",
    "line_number": 5,
    "enriched": "File: other/time_algo_exec.py\nCode: @@ -0,0 +1,36 @@\n+# Auteur : Bosolindo Edhiengene Roger\r\n+# email : rogerbosolinndo34@gmail.com\r\n+\r\n+\r\n+def calc(operations: dict) -> float:\r\nComment: As there is no test file in this pull request nor any test function or class in the file `other/time_algo_exec.py`, please provide doctest for the function `calc`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "other/time_algo_exec.py",
    "pr_number": 12761,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2100372493,
    "comment_created_at": "2025-05-21T13:58:04Z"
  },
  {
    "code": "@@ -0,0 +1,76 @@\n+\"\"\"\n+Given an set of numbers in a stack, \n+find the minimum value from the stack at O(1)\n+\n+Problem: https://leetcode.com/problems/min-stack/description/\n+\"\"\"\n+\n+stack: list[int] = []\n+min_stack: list[int] = []\n+\n+\n+def push(value: int) -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/stacks/min_ele_const_time.py, please provide doctest for the function push",
    "line_number": 12,
    "enriched": "File: data_structures/stacks/min_ele_const_time.py\nCode: @@ -0,0 +1,76 @@\n+\"\"\"\n+Given an set of numbers in a stack, \n+find the minimum value from the stack at O(1)\n+\n+Problem: https://leetcode.com/problems/min-stack/description/\n+\"\"\"\n+\n+stack: list[int] = []\n+min_stack: list[int] = []\n+\n+\n+def push(value: int) -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/stacks/min_ele_const_time.py`, please provide doctest for the function `push`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/stacks/min_ele_const_time.py",
    "pr_number": 11909,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1792818760,
    "comment_created_at": "2024-10-09T04:20:44Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+def is_power(n: int, k: int) -> bool:",
    "comment": "please provide descriptive name for the parameter: n\n\nplease provide descriptive name for the parameter: k",
    "line_number": 1,
    "enriched": "File: bit_manipulation/is_K_power_of_n.py\nCode: @@ -0,0 +1,58 @@\n+def is_power(n: int, k: int) -> bool:\nComment: Please provide descriptive name for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `k`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "bit_manipulation/is_K_power_of_n.py",
    "pr_number": 12873,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2248095256,
    "comment_created_at": "2025-08-01T14:13:20Z"
  },
  {
    "code": "@@ -0,0 +1,46 @@\n+\"\"\"\n+Description : Centripetal force is the force acting on an object in \n+curvilinear motion directed towards the axis of rotation \n+or centre of curvature.\n+\n+The unit of centripetal force is newton.\n+\n+The centripetal force is always directed perpendicular to the \n+direction of the object\u2019s displacement. Using Newton\u2019s second \n+law of motion, it is found that the centripetal force of an object \n+moving in a circular path always acts towards the centre of the circle.\n+The Centripetal Force Formula is given as the product of mass (in kg) \n+and tangential velocity (in meters per second) squared, divided by the \n+radius (in meters) that implies that on doubling the tangential velocity, \n+the centripetal force will be quadrupled. Mathematically it is written as:\n+F = mv\u00b2/r\n+Where, F is the Centripetal force, m is the mass of the object, v is the \n+speed or velocity of the object and r is the radius.\n+\n+Reference: https://byjus.com/physics/centripetal-and-centrifugal-force/\n+\"\"\"\n+\n+def centripetal(m:float , v:float , r:float) -> float:",
    "comment": "please provide descriptive name for the parameter: m\n\nplease provide descriptive name for the parameter: v\n\nplease provide descriptive name for the parameter: r",
    "line_number": 23,
    "enriched": "File: physics/centripetal_force.py\nCode: @@ -0,0 +1,46 @@\n+\"\"\"\n+Description : Centripetal force is the force acting on an object in \n+curvilinear motion directed towards the axis of rotation \n+or centre of curvature.\n+\n+The unit of centripetal force is newton.\n+\n+The centripetal force is always directed perpendicular to the \n+direction of the object\u2019s displacement. Using Newton\u2019s second \n+law of motion, it is found that the centripetal force of an object \n+moving in a circular path always acts towards the centre of the circle.\n+The Centripetal Force Formula is given as the product of mass (in kg) \n+and tangential velocity (in meters per second) squared, divided by the \n+radius (in meters) that implies that on doubling the tangential velocity, \n+the centripetal force will be quadrupled. Mathematically it is written as:\n+F = mv\u00b2/r\n+Where, F is the Centripetal force, m is the mass of the object, v is the \n+speed or velocity of the object and r is the radius.\n+\n+Reference: https://byjus.com/physics/centripetal-and-centrifugal-force/\n+\"\"\"\n+\n+def centripetal(m:float , v:float , r:float) -> float:\nComment: Please provide descriptive name for the parameter: `m`\n\nPlease provide descriptive name for the parameter: `v`\n\nPlease provide descriptive name for the parameter: `r`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "physics/centripetal_force.py",
    "pr_number": 7778,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1007918736,
    "comment_created_at": "2022-10-28T10:35:30Z"
  },
  {
    "code": "@@ -0,0 +1,144 @@\n+# Implementation of a Fibonacci Heap based on the concepts described in \"Introduction to Algorithms\" by Cormen, Leiserson, Rivest, and Stein.\n+# Reference: https://en.wikipedia.org/wiki/Fibonacci_heap\n+\n+from __future__ import annotations\n+from collections.abc import Iterable, Iterator\n+from typing import Any, Generic, TypeVar\n+\n+T = TypeVar(\"T\", bound=int)\n+\n+\n+class FibonacciNode(Generic[T]):\n+    def __init__(self, key: T) -> None:\n+        \"\"\"\n+        Create a new FibonacciNode with the given key.\n+\n+        Args:\n+            key (T): The key value associated with the node.\n+        \"\"\"\n+        self.key: T = key\n+        self.degree: int = 0\n+        self.parent: FibonacciNode[T] | None = None\n+        self.child: FibonacciNode[T] | None = None\n+        self.is_marked: bool = False\n+        self.next: FibonacciNode[T] = self\n+        self.prev: FibonacciNode[T] = self\n+\n+\n+class FibonacciHeap(Generic[T]):\n+    def __init__(self) -> None:\n+        \"\"\"\n+        Create a new Fibonacci Heap.\n+\n+        The Fibonacci Heap is initialized as an empty heap.\n+        \"\"\"\n+        self.min_node: FibonacciNode[T] | None = None\n+        self.num_nodes: int = 0\n+\n+    def insert(self, key: T) -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/heap/fibonacci_heap.py, please provide doctest for the function insert",
    "line_number": 38,
    "enriched": "File: data_structures/heap/fibonacci_heap.py\nCode: @@ -0,0 +1,144 @@\n+# Implementation of a Fibonacci Heap based on the concepts described in \"Introduction to Algorithms\" by Cormen, Leiserson, Rivest, and Stein.\n+# Reference: https://en.wikipedia.org/wiki/Fibonacci_heap\n+\n+from __future__ import annotations\n+from collections.abc import Iterable, Iterator\n+from typing import Any, Generic, TypeVar\n+\n+T = TypeVar(\"T\", bound=int)\n+\n+\n+class FibonacciNode(Generic[T]):\n+    def __init__(self, key: T) -> None:\n+        \"\"\"\n+        Create a new FibonacciNode with the given key.\n+\n+        Args:\n+            key (T): The key value associated with the node.\n+        \"\"\"\n+        self.key: T = key\n+        self.degree: int = 0\n+        self.parent: FibonacciNode[T] | None = None\n+        self.child: FibonacciNode[T] | None = None\n+        self.is_marked: bool = False\n+        self.next: FibonacciNode[T] = self\n+        self.prev: FibonacciNode[T] = self\n+\n+\n+class FibonacciHeap(Generic[T]):\n+    def __init__(self) -> None:\n+        \"\"\"\n+        Create a new Fibonacci Heap.\n+\n+        The Fibonacci Heap is initialized as an empty heap.\n+        \"\"\"\n+        self.min_node: FibonacciNode[T] | None = None\n+        self.num_nodes: int = 0\n+\n+    def insert(self, key: T) -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/heap/fibonacci_heap.py`, please provide doctest for the function `insert`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/heap/fibonacci_heap.py",
    "pr_number": 9239,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342104982,
    "comment_created_at": "2023-10-01T09:15:59Z"
  },
  {
    "code": "@@ -0,0 +1,93 @@\n+import re\n+\n+\"\"\"\n+general info:\n+https://en.wikipedia.org/wiki/Naming_convention_(programming)#Python_and_Ruby\n+\n+pascal case [ an upper Camel Case ]: https://en.wikipedia.org/wiki/Camel_case\n+\n+camel case: https://en.wikipedia.org/wiki/Camel_case\n+\n+kebab case [ can be found in general info ]:\n+https://en.wikipedia.org/wiki/Naming_convention_(programming)#Python_and_Ruby\n+\n+snake case: https://en.wikipedia.org/wiki/Snake_case\n+\"\"\"\n+\n+# assistant functions\n+def split_input(str_: str) -> str:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file strings/string_switch_case.py, please provide doctest for the function split_input",
    "line_number": 18,
    "enriched": "File: strings/string_switch_case.py\nCode: @@ -0,0 +1,93 @@\n+import re\n+\n+\"\"\"\n+general info:\n+https://en.wikipedia.org/wiki/Naming_convention_(programming)#Python_and_Ruby\n+\n+pascal case [ an upper Camel Case ]: https://en.wikipedia.org/wiki/Camel_case\n+\n+camel case: https://en.wikipedia.org/wiki/Camel_case\n+\n+kebab case [ can be found in general info ]:\n+https://en.wikipedia.org/wiki/Naming_convention_(programming)#Python_and_Ruby\n+\n+snake case: https://en.wikipedia.org/wiki/Snake_case\n+\"\"\"\n+\n+# assistant functions\n+def split_input(str_: str) -> str:\nComment: As there is no test file in this pull request nor any test function or class in the file `strings/string_switch_case.py`, please provide doctest for the function `split_input`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "strings/string_switch_case.py",
    "pr_number": 7995,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1027091036,
    "comment_created_at": "2022-11-19T13:37:05Z"
  },
  {
    "code": "@@ -0,0 +1,242 @@\n+# \"\"\"\n+#   Connected-Component Labeling\n+#\n+#   Source:\n+#   https://en.wikipedia.org/wiki/Connected-component_labeling\n+#\n+#   Basically, there's two versions of CCL two-pass algorithm:\n+#   4-connectivity and 8-connectivity.\n+#   4-connectivity means that only horizontal and vertical neighbours\n+#   are considered, while 8-connectivity means that diagonal neighbours\n+#   are also considered.\n+# \"\"\"\n+\n+\n+def connected_component_labeling_8(matrix: list) -> list:\n+    \"\"\"\n+    Connected Component Labeling algorithm (8-connectivity version)\n+    Takes a matrix of binary values and returns a matrix of labels,\n+    where each connected component is assigned a unique label.\n+\n+    args:\n+        matrix: 2D list of 0s and 1s\n+\n+    returns:\n+        2D list of labels\n+\n+    >>> connected_component_labeling_8([[1, 0, 0, 1],\n+    ...                                 [1, 0, 0, 1],\n+    ...                                 [0, 0, 1, 0],\n+    ...                                 [0, 1, 1, 0]])\n+    [[1, 0, 0, 2], [1, 0, 0, 2], [0, 0, 2, 0], [0, 2, 2, 0]]\n+    >>> connected_component_labeling_8([[0, 1, 0, 1],\n+    ...                                 [1, 1, 0, 0],\n+    ...                                 [0, 0, 1, 1],\n+    ...                                 [1, 0, 1, 0]])\n+    [[0, 1, 0, 2], [1, 1, 0, 0], [0, 0, 1, 1], [3, 0, 1, 0]]\n+    >>> connected_component_labeling_4([])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be non-empty\n+    >>> connected_component_labeling_4([[1, 1, 1], [1, 0], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be rectangular\n+    >>> connected_component_labeling_4([[1, 0, 0], [1, 0, 2], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must contain only 0s and 1s\n+    \"\"\"\n+\n+    if len(matrix) == 0 or len(matrix[0]) == 0:\n+        raise ValueError(\"Matrix must be non-empty\")\n+    if not all(len(row) == len(matrix[0]) for row in matrix):\n+        raise ValueError(\"Matrix must be rectangular\")\n+    if not all(all(x in (0, 1) for x in row) for row in matrix):\n+        raise ValueError(\"Matrix must contain only 0s and 1s\")\n+\n+    row_len = len(matrix[0])\n+\n+    # add padding\n+    padded_matrix = (\n+        [[0] * (row_len + 2)]\n+        + [[0] + row + [0] for row in matrix]\n+        + [[0] * (row_len + 2)]\n+    )\n+\n+    # after the first traverse, we might label some elements incorrectly.\n+    # when we spot incorrect labeling, we add neighbours label as a key\n+    # and the element label as a value\n+    mapping: dict = {}\n+\n+    next_label = 1\n+\n+    # first traverse\n+    for i in range(1, len(padded_matrix) - 1):\n+        for j in range(1, row_len + 1):\n+\n+            if padded_matrix[i][j] == 1:\n+\n+                neighbours = [\n+                    padded_matrix[i - 1][j - 1],\n+                    padded_matrix[i - 1][j],\n+                    padded_matrix[i - 1][j + 1],\n+                    padded_matrix[i][j - 1],\n+                ]\n+\n+                if any(neighbours):\n+\n+                    min_label = min([v for v in neighbours if v > 0])\n+                    padded_matrix[i][j] = min_label\n+\n+                    for label in neighbours:\n+                        # if any of the neighbors appears to have label \\\n+                        # greater than current element - add it's \\\n+                        # label to mapping\n+                        if label > min_label:\n+                            if label not in mapping:\n+                                mapping[label] = set()\n+                            mapping[label].add(min_label)\n+\n+                else:\n+                    padded_matrix[i][j] = next_label\n+                    next_label += 1\n+\n+    # \"unpad\" the matrix\n+    matrix = [row[1:-1] for row in padded_matrix[1:-1]]\n+\n+    # restructure the mapping, so that:\n+    # 3: (2, 1)         3: (2, 1)\n+    # 6: (3)        ->  6: (3, 1)\n+    #                   2: (1,)\n+    for key in sorted(mapping.keys()):\n+        adj_labels = mapping[key]\n+        min_label = min(adj_labels)\n+        for adj_label in adj_labels:\n+            if adj_label not in mapping:\n+                mapping[adj_label] = set()\n+            mapping[adj_label].add(min_label)\n+\n+    # apply the mapping\n+    for i in range(len(matrix)):\n+        for j in range(row_len):\n+            if matrix[i][j] in mapping:\n+                matrix[i][j] = min(mapping[matrix[i][j]])\n+\n+    return matrix\n+\n+\n+def connected_component_labeling_4(matrix: list) -> list:\n+    \"\"\"\n+    Connected Component Labeling algorithm (4-connectivity version)\n+    Takes a matrix of binary values and returns a matrix of labels,\n+    where each connected component is assigned a unique label.\n+\n+\n+    args:\n+        matrix: 2D list of 0s and 1s\n+\n+    returns:\n+        2D list of labels\n+\n+    >>> connected_component_labeling_4([[1, 0, 0, 1],\n+    ...                                 [1, 0, 0, 1],\n+    ...                                 [0, 0, 1, 0],\n+    ...                                 [0, 1, 1, 0]])\n+    [[1, 0, 0, 2], [1, 0, 0, 2], [0, 0, 3, 0], [0, 3, 3, 0]]\n+    >>> connected_component_labeling_4([[0, 1, 0, 1],\n+    ...                                 [1, 1, 0, 0],\n+    ...                                 [0, 0, 1, 1],\n+    ...                                 [1, 0, 1, 0]])\n+    [[0, 1, 0, 2], [1, 1, 0, 0], [0, 0, 4, 4], [5, 0, 4, 0]]\n+    >>> connected_component_labeling_4([])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be non-empty\n+    >>> connected_component_labeling_4([[1, 1, 1], [1, 0], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be rectangular\n+    >>> connected_component_labeling_4([[1, 0, 0], [1, 0, 2], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must contain only 0s and 1s\n+    \"\"\"\n+\n+    if len(matrix) == 0 or len(matrix[0]) == 0:\n+        raise ValueError(\"Matrix must be non-empty\")\n+    if not all(len(row) == len(matrix[0]) for row in matrix):\n+        raise ValueError(\"Matrix must be rectangular\")\n+    if not all(all(x in (0, 1) for x in row) for row in matrix):\n+        raise ValueError(\"Matrix must contain only 0s and 1s\")\n+\n+    row_len = len(matrix[0])\n+\n+    # add padding\n+    padded_matrix = (\n+        [[0] * (row_len + 2)]\n+        + [[0] + row + [0] for row in matrix]\n+        + [[0] * (row_len + 2)]\n+    )\n+\n+    # after the first traverse, we might label some elements incorrectly.\n+    # when we spot incorrect labeling, we add neighbours label as a key\n+    # and the element label as a value\n+    mapping: dict = {}\n+\n+    next_label = 1\n+\n+    # first traverse\n+    for i in range(1, len(padded_matrix) - 1):\n+        for j in range(1, row_len + 1):\n+\n+            if padded_matrix[i][j] == 1:\n+\n+                neighbours = [padded_matrix[i - 1][j], padded_matrix[i][j - 1]]\n+\n+                if any(neighbours):\n+\n+                    min_label = min([v for v in neighbours if v > 0])\n+                    padded_matrix[i][j] = min_label\n+\n+                    for label in neighbours:\n+                        # if any of the neighbors appears to have label \\\n+                        # greater than current element - add it's \\\n+                        # label to mapping\n+                        if label > min_label:\n+                            if label not in mapping:\n+                                mapping[label] = set()\n+                            mapping[label].add(min_label)\n+\n+                else:\n+                    padded_matrix[i][j] = next_label\n+                    next_label += 1\n+\n+    # \"unpad\" the matrix\n+    matrix = [row[1:-1] for row in padded_matrix[1:-1]]\n+\n+    # restructure the mapping, so that:\n+    # 3: (2, 1)         3: (2, 1)\n+    # 6: (3)        ->  6: (3, 1)\n+    #                   2: (1,)\n+    for key in sorted(mapping.keys()):\n+        adj_labels = mapping[key]\n+        min_label = min(adj_labels)\n+        for adj_label in adj_labels:\n+            if adj_label not in mapping:\n+                mapping[adj_label] = set()\n+            mapping[adj_label].add(min_label)\n+\n+    # apply the mapping\n+    for i in range(len(matrix)):\n+        for j in range(row_len):\n+            if matrix[i][j] in mapping:\n+                matrix[i][j] = min(mapping[matrix[i][j]])\n+\n+    return matrix\n+\n+\n+if __name__ == \"__main__\":\n+    import doctest\n+\n+    doctest.testmod()",
    "comment": "as far as i'm concerned, github automatically removes one space in the browser from the end of the file.\r\n\r\nif the space was not actually at the end of the file, github would have been marked it as shown below:\r\n![image](https://user-images.githubusercontent.com/32400447/195177146-df76edf2-d22e-4834-b74e-58fe9e6a7946.png)\r\n\r\nthank you :)",
    "line_number": 242,
    "enriched": "File: computer_vision/connected_component_labeling.py\nCode: @@ -0,0 +1,242 @@\n+# \"\"\"\n+#   Connected-Component Labeling\n+#\n+#   Source:\n+#   https://en.wikipedia.org/wiki/Connected-component_labeling\n+#\n+#   Basically, there's two versions of CCL two-pass algorithm:\n+#   4-connectivity and 8-connectivity.\n+#   4-connectivity means that only horizontal and vertical neighbours\n+#   are considered, while 8-connectivity means that diagonal neighbours\n+#   are also considered.\n+# \"\"\"\n+\n+\n+def connected_component_labeling_8(matrix: list) -> list:\n+    \"\"\"\n+    Connected Component Labeling algorithm (8-connectivity version)\n+    Takes a matrix of binary values and returns a matrix of labels,\n+    where each connected component is assigned a unique label.\n+\n+    args:\n+        matrix: 2D list of 0s and 1s\n+\n+    returns:\n+        2D list of labels\n+\n+    >>> connected_component_labeling_8([[1, 0, 0, 1],\n+    ...                                 [1, 0, 0, 1],\n+    ...                                 [0, 0, 1, 0],\n+    ...                                 [0, 1, 1, 0]])\n+    [[1, 0, 0, 2], [1, 0, 0, 2], [0, 0, 2, 0], [0, 2, 2, 0]]\n+    >>> connected_component_labeling_8([[0, 1, 0, 1],\n+    ...                                 [1, 1, 0, 0],\n+    ...                                 [0, 0, 1, 1],\n+    ...                                 [1, 0, 1, 0]])\n+    [[0, 1, 0, 2], [1, 1, 0, 0], [0, 0, 1, 1], [3, 0, 1, 0]]\n+    >>> connected_component_labeling_4([])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be non-empty\n+    >>> connected_component_labeling_4([[1, 1, 1], [1, 0], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be rectangular\n+    >>> connected_component_labeling_4([[1, 0, 0], [1, 0, 2], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must contain only 0s and 1s\n+    \"\"\"\n+\n+    if len(matrix) == 0 or len(matrix[0]) == 0:\n+        raise ValueError(\"Matrix must be non-empty\")\n+    if not all(len(row) == len(matrix[0]) for row in matrix):\n+        raise ValueError(\"Matrix must be rectangular\")\n+    if not all(all(x in (0, 1) for x in row) for row in matrix):\n+        raise ValueError(\"Matrix must contain only 0s and 1s\")\n+\n+    row_len = len(matrix[0])\n+\n+    # add padding\n+    padded_matrix = (\n+        [[0] * (row_len + 2)]\n+        + [[0] + row + [0] for row in matrix]\n+        + [[0] * (row_len + 2)]\n+    )\n+\n+    # after the first traverse, we might label some elements incorrectly.\n+    # when we spot incorrect labeling, we add neighbours label as a key\n+    # and the element label as a value\n+    mapping: dict = {}\n+\n+    next_label = 1\n+\n+    # first traverse\n+    for i in range(1, len(padded_matrix) - 1):\n+        for j in range(1, row_len + 1):\n+\n+            if padded_matrix[i][j] == 1:\n+\n+                neighbours = [\n+                    padded_matrix[i - 1][j - 1],\n+                    padded_matrix[i - 1][j],\n+                    padded_matrix[i - 1][j + 1],\n+                    padded_matrix[i][j - 1],\n+                ]\n+\n+                if any(neighbours):\n+\n+                    min_label = min([v for v in neighbours if v > 0])\n+                    padded_matrix[i][j] = min_label\n+\n+                    for label in neighbours:\n+                        # if any of the neighbors appears to have label \\\n+                        # greater than current element - add it's \\\n+                        # label to mapping\n+                        if label > min_label:\n+                            if label not in mapping:\n+                                mapping[label] = set()\n+                            mapping[label].add(min_label)\n+\n+                else:\n+                    padded_matrix[i][j] = next_label\n+                    next_label += 1\n+\n+    # \"unpad\" the matrix\n+    matrix = [row[1:-1] for row in padded_matrix[1:-1]]\n+\n+    # restructure the mapping, so that:\n+    # 3: (2, 1)         3: (2, 1)\n+    # 6: (3)        ->  6: (3, 1)\n+    #                   2: (1,)\n+    for key in sorted(mapping.keys()):\n+        adj_labels = mapping[key]\n+        min_label = min(adj_labels)\n+        for adj_label in adj_labels:\n+            if adj_label not in mapping:\n+                mapping[adj_label] = set()\n+            mapping[adj_label].add(min_label)\n+\n+    # apply the mapping\n+    for i in range(len(matrix)):\n+        for j in range(row_len):\n+            if matrix[i][j] in mapping:\n+                matrix[i][j] = min(mapping[matrix[i][j]])\n+\n+    return matrix\n+\n+\n+def connected_component_labeling_4(matrix: list) -> list:\n+    \"\"\"\n+    Connected Component Labeling algorithm (4-connectivity version)\n+    Takes a matrix of binary values and returns a matrix of labels,\n+    where each connected component is assigned a unique label.\n+\n+\n+    args:\n+        matrix: 2D list of 0s and 1s\n+\n+    returns:\n+        2D list of labels\n+\n+    >>> connected_component_labeling_4([[1, 0, 0, 1],\n+    ...                                 [1, 0, 0, 1],\n+    ...                                 [0, 0, 1, 0],\n+    ...                                 [0, 1, 1, 0]])\n+    [[1, 0, 0, 2], [1, 0, 0, 2], [0, 0, 3, 0], [0, 3, 3, 0]]\n+    >>> connected_component_labeling_4([[0, 1, 0, 1],\n+    ...                                 [1, 1, 0, 0],\n+    ...                                 [0, 0, 1, 1],\n+    ...                                 [1, 0, 1, 0]])\n+    [[0, 1, 0, 2], [1, 1, 0, 0], [0, 0, 4, 4], [5, 0, 4, 0]]\n+    >>> connected_component_labeling_4([])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be non-empty\n+    >>> connected_component_labeling_4([[1, 1, 1], [1, 0], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must be rectangular\n+    >>> connected_component_labeling_4([[1, 0, 0], [1, 0, 2], [1, 0, 1]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix must contain only 0s and 1s\n+    \"\"\"\n+\n+    if len(matrix) == 0 or len(matrix[0]) == 0:\n+        raise ValueError(\"Matrix must be non-empty\")\n+    if not all(len(row) == len(matrix[0]) for row in matrix):\n+        raise ValueError(\"Matrix must be rectangular\")\n+    if not all(all(x in (0, 1) for x in row) for row in matrix):\n+        raise ValueError(\"Matrix must contain only 0s and 1s\")\n+\n+    row_len = len(matrix[0])\n+\n+    # add padding\n+    padded_matrix = (\n+        [[0] * (row_len + 2)]\n+        + [[0] + row + [0] for row in matrix]\n+        + [[0] * (row_len + 2)]\n+    )\n+\n+    # after the first traverse, we might label some elements incorrectly.\n+    # when we spot incorrect labeling, we add neighbours label as a key\n+    # and the element label as a value\n+    mapping: dict = {}\n+\n+    next_label = 1\n+\n+    # first traverse\n+    for i in range(1, len(padded_matrix) - 1):\n+        for j in range(1, row_len + 1):\n+\n+            if padded_matrix[i][j] == 1:\n+\n+                neighbours = [padded_matrix[i - 1][j], padded_matrix[i][j - 1]]\n+\n+                if any(neighbours):\n+\n+                    min_label = min([v for v in neighbours if v > 0])\n+                    padded_matrix[i][j] = min_label\n+\n+                    for label in neighbours:\n+                        # if any of the neighbors appears to have label \\\n+                        # greater than current element - add it's \\\n+                        # label to mapping\n+                        if label > min_label:\n+                            if label not in mapping:\n+                                mapping[label] = set()\n+                            mapping[label].add(min_label)\n+\n+                else:\n+                    padded_matrix[i][j] = next_label\n+                    next_label += 1\n+\n+    # \"unpad\" the matrix\n+    matrix = [row[1:-1] for row in padded_matrix[1:-1]]\n+\n+    # restructure the mapping, so that:\n+    # 3: (2, 1)         3: (2, 1)\n+    # 6: (3)        ->  6: (3, 1)\n+    #                   2: (1,)\n+    for key in sorted(mapping.keys()):\n+        adj_labels = mapping[key]\n+        min_label = min(adj_labels)\n+        for adj_label in adj_labels:\n+            if adj_label not in mapping:\n+                mapping[adj_label] = set()\n+            mapping[adj_label].add(min_label)\n+\n+    # apply the mapping\n+    for i in range(len(matrix)):\n+        for j in range(row_len):\n+            if matrix[i][j] in mapping:\n+                matrix[i][j] = min(mapping[matrix[i][j]])\n+\n+    return matrix\n+\n+\n+if __name__ == \"__main__\":\n+    import doctest\n+\n+    doctest.testmod()\nComment: As far as I'm concerned, github automatically removes one space in the browser from the end of the file.\r\n\r\nIf the space was not actually at the end of the file, github would have been marked it as shown below:\r\n![image](https://user-images.githubusercontent.com/32400447/195177146-df76edf2-d22e-4834-b74e-58fe9e6a7946.png)\r\n\r\nThank you :)\r\n",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "computer_vision/connected_component_labeling.py",
    "pr_number": 6960,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 992686904,
    "comment_created_at": "2022-10-11T19:06:11Z"
  },
  {
    "code": "@@ -0,0 +1,99 @@\n+import glob\n+import os\n+import random\n+from string import ascii_lowercase, digits\n+\n+import cv2\n+import numpy as np\n+\n+\n+# Params\n+IMAGE_DIR = \"1\"\n+OUTPUT_DIR = \"out\"\n+MAX_RECTANGLE_WIDTH_PERCENT = 20  # (between 0 and 100)\n+MAX_RECTANGLE_HEIGHT_PERCENT = 20  # (between 0 and 100)\n+RECTANGLE_COLOR = \"WHITE\" # (\"BLACK\" or \"WHITE\")\n+\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file computer_vision/random_erasing_augmentation.py, please provide doctest for the function main",
    "line_number": 18,
    "enriched": "File: computer_vision/random_erasing_augmentation.py\nCode: @@ -0,0 +1,99 @@\n+import glob\n+import os\n+import random\n+from string import ascii_lowercase, digits\n+\n+import cv2\n+import numpy as np\n+\n+\n+# Params\n+IMAGE_DIR = \"1\"\n+OUTPUT_DIR = \"out\"\n+MAX_RECTANGLE_WIDTH_PERCENT = 20  # (between 0 and 100)\n+MAX_RECTANGLE_HEIGHT_PERCENT = 20  # (between 0 and 100)\n+RECTANGLE_COLOR = \"WHITE\" # (\"BLACK\" or \"WHITE\")\n+\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `computer_vision/random_erasing_augmentation.py`, please provide doctest for the function `main`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "computer_vision/random_erasing_augmentation.py",
    "pr_number": 8048,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1057515198,
    "comment_created_at": "2022-12-27T08:14:31Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+from Crypto.Cipher import AES\n+from Crypto.Util.Padding import pad, unpad\n+import base64\n+\n+def aes_encrypt(plaintext: str, key: str) -> str:\n+    \"\"\"\n+    AES-128 Encryption using CBC mode and PKCS7 padding.\n+\n+    :param plaintext: The plaintext message to be encrypted.\n+    :param key: The encryption key (16 characters = 128 bits).\n+    :return: Encrypted message (Base64 encoded).\n+\n+    >>> msg = \"This is a secret message.\"\n+    >>> key = \"thisisaverysecret\"\n+    >>> enc = aes_encrypt(msg, key)\n+    >>> dec = aes_decrypt(enc, key)\n+    >>> aes_decrypt(enc, key) == msg\n+    True\n+    \"\"\"\n+    cipher = AES.new(key.encode('utf-8'), AES.MODE_CBC)\n+    ciphertext = cipher.encrypt(pad(plaintext.encode('utf-8'), AES.block_size))\n+    return base64.b64encode(cipher.iv + ciphertext).decode('utf-8')\n+\n+def aes_decrypt(ciphertext: str, key: str) -> str:\n+    \"\"\"\n+    AES-128 Decryption using CBC mode and PKCS7 padding.\n+\n+    :param ciphertext: The Base64 encoded encrypted message.\n+    :param key: The decryption key (16 characters = 128 bits).\n+    :return: Decrypted plaintext message.\n+\n+    >>> msg = \"This is a secret message.\"\n+    >>> key = \"thisisaverysecret\"\n+    >>> enc = aes_encrypt(msg, key)\n+    >>> dec = aes_decrypt(enc, key)\n+    >>> dec == msg\n+    True\n+    \"\"\"\n+    raw = base64.b64decode(ciphertext)\n+    cipher = AES.new(key.encode('utf-8'), AES.MODE_CBC, iv=raw[:AES.block_size])\n+    return unpad(cipher.decrypt(raw[AES.block_size:]), AES.block_size).decode('utf-8')\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file ciphers/aes_128.py, please provide doctest for the function main",
    "line_number": 43,
    "enriched": "File: ciphers/aes_128.py\nCode: @@ -0,0 +1,61 @@\n+from Crypto.Cipher import AES\n+from Crypto.Util.Padding import pad, unpad\n+import base64\n+\n+def aes_encrypt(plaintext: str, key: str) -> str:\n+    \"\"\"\n+    AES-128 Encryption using CBC mode and PKCS7 padding.\n+\n+    :param plaintext: The plaintext message to be encrypted.\n+    :param key: The encryption key (16 characters = 128 bits).\n+    :return: Encrypted message (Base64 encoded).\n+\n+    >>> msg = \"This is a secret message.\"\n+    >>> key = \"thisisaverysecret\"\n+    >>> enc = aes_encrypt(msg, key)\n+    >>> dec = aes_decrypt(enc, key)\n+    >>> aes_decrypt(enc, key) == msg\n+    True\n+    \"\"\"\n+    cipher = AES.new(key.encode('utf-8'), AES.MODE_CBC)\n+    ciphertext = cipher.encrypt(pad(plaintext.encode('utf-8'), AES.block_size))\n+    return base64.b64encode(cipher.iv + ciphertext).decode('utf-8')\n+\n+def aes_decrypt(ciphertext: str, key: str) -> str:\n+    \"\"\"\n+    AES-128 Decryption using CBC mode and PKCS7 padding.\n+\n+    :param ciphertext: The Base64 encoded encrypted message.\n+    :param key: The decryption key (16 characters = 128 bits).\n+    :return: Decrypted plaintext message.\n+\n+    >>> msg = \"This is a secret message.\"\n+    >>> key = \"thisisaverysecret\"\n+    >>> enc = aes_encrypt(msg, key)\n+    >>> dec = aes_decrypt(enc, key)\n+    >>> dec == msg\n+    True\n+    \"\"\"\n+    raw = base64.b64decode(ciphertext)\n+    cipher = AES.new(key.encode('utf-8'), AES.MODE_CBC, iv=raw[:AES.block_size])\n+    return unpad(cipher.decrypt(raw[AES.block_size:]), AES.block_size).decode('utf-8')\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `ciphers/aes_128.py`, please provide doctest for the function `main`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "ciphers/aes_128.py",
    "pr_number": 11716,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1786656321,
    "comment_created_at": "2024-10-03T18:07:52Z"
  },
  {
    "code": "@@ -8,7 +8,7 @@\n \n \n class FilterType(Protocol):\n-    def process(self, sample: float) -> float:\n+    def process(self, sample: float) -> float:  # noqa: ARG002",
    "comment": "what would happen if we delete the unused variables?",
    "line_number": 11,
    "enriched": "File: audio_filters/show_response.py\nCode: @@ -8,7 +8,7 @@\n \n \n class FilterType(Protocol):\n-    def process(self, sample: float) -> float:\n+    def process(self, sample: float) -> float:  # noqa: ARG002\nComment: What would happen if we delete the unused variables?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "audio_filters/show_response.py",
    "pr_number": 11382,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1586233967,
    "comment_created_at": "2024-05-01T12:06:10Z"
  },
  {
    "code": "@@ -0,0 +1,63 @@\n+\"\"\"\n+Intuition\n+1. Edge Calculation: The code starts by calculating all possible edges between the given points. It iterates through the points and computes the Manhattan distance (L1 distance) between each pair of points. These distances represent the cost of connecting each pair of points directly. The resulting list of edges contains information about which points are connected and the cost associated with each connection.\n+\n+2. Edge Sorting: After calculating all edges,the code sorts them in ascending order based on their costs. This step is essential for Kruskal's algorithm,which aims to build a minimum spanning tree by adding edges in increasing order of cost.\n+\n+3. Kruskal's Algorithm: The core of the code is Kruskal's algorithm,a greedy algorithm for finding the minimum spanning tree of a graph. The algorithm processes edges in ascending order of cost while ensuring that adding an edge does not create a cycle in the tree. The minimum spanning tree is a subset of the edges that connect all the points with the minimum possible total cost.\n+\n+4. Union\u2212FindDataStructure:Union-Find Data Structure:Union\u2212FindDataStructure: To efficiently detect and prevent cycles when adding edges,the code uses a union-find (disjoint-set) data structure. The parent array keeps track of the parent node for each point,and the rank array is used to maintain the depth of each node in the tree. The find operation finds the representative (root) of the set to which a point belongs,and the union operation merges two sets by updating their representatives and ranks.\n+\n+5. AddingEdgestotheMinimumSpanningTreeAdding Edges to the Minimum Spanning TreeAddingEdgestotheMinimumSpanningTree: The code iterates through the sorted edges and adds them to the minimum spanning tree if adding the edge does not create a cycle (i.e.,if the endpoints of the edge are not already in the same set). This process continues until the minimum spanning tree contains exactly n - 1 edges,where n is the number of points. At this point,the algorithm has connected all points with the minimum total cost.\n+\n+6. Minimum Cost Calculation: The code keeps track of the sum of edge costs as edges are added to the minimum spanning tree. This sum represents the minimum cost to connect all the given points.\n+\n+Complexity\n+Time complexity: O(n^2 * log(n))\n+Space complexity: O(n^2)\n+\n+\"\"\"\n+from typing import List\n+class Solution:\n+    def mincost(self,points: List[List[int]]) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/binary_tree/kd_tree.py, please provide doctest for the function mincost",
    "line_number": 22,
    "enriched": "File: data_structures/binary_tree/kd_tree.py\nCode: @@ -0,0 +1,63 @@\n+\"\"\"\n+Intuition\n+1. Edge Calculation: The code starts by calculating all possible edges between the given points. It iterates through the points and computes the Manhattan distance (L1 distance) between each pair of points. These distances represent the cost of connecting each pair of points directly. The resulting list of edges contains information about which points are connected and the cost associated with each connection.\n+\n+2. Edge Sorting: After calculating all edges,the code sorts them in ascending order based on their costs. This step is essential for Kruskal's algorithm,which aims to build a minimum spanning tree by adding edges in increasing order of cost.\n+\n+3. Kruskal's Algorithm: The core of the code is Kruskal's algorithm,a greedy algorithm for finding the minimum spanning tree of a graph. The algorithm processes edges in ascending order of cost while ensuring that adding an edge does not create a cycle in the tree. The minimum spanning tree is a subset of the edges that connect all the points with the minimum possible total cost.\n+\n+4. Union\u2212FindDataStructure:Union-Find Data Structure:Union\u2212FindDataStructure: To efficiently detect and prevent cycles when adding edges,the code uses a union-find (disjoint-set) data structure. The parent array keeps track of the parent node for each point,and the rank array is used to maintain the depth of each node in the tree. The find operation finds the representative (root) of the set to which a point belongs,and the union operation merges two sets by updating their representatives and ranks.\n+\n+5. AddingEdgestotheMinimumSpanningTreeAdding Edges to the Minimum Spanning TreeAddingEdgestotheMinimumSpanningTree: The code iterates through the sorted edges and adds them to the minimum spanning tree if adding the edge does not create a cycle (i.e.,if the endpoints of the edge are not already in the same set). This process continues until the minimum spanning tree contains exactly n - 1 edges,where n is the number of points. At this point,the algorithm has connected all points with the minimum total cost.\n+\n+6. Minimum Cost Calculation: The code keeps track of the sum of edge costs as edges are added to the minimum spanning tree. This sum represents the minimum cost to connect all the given points.\n+\n+Complexity\n+Time complexity: O(n^2 * log(n))\n+Space complexity: O(n^2)\n+\n+\"\"\"\n+from typing import List\n+class Solution:\n+    def mincost(self,points: List[List[int]]) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/binary_tree/kd_tree.py`, please provide doctest for the function `mincost`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/binary_tree/kd_tree.py",
    "pr_number": 10769,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367742254,
    "comment_created_at": "2023-10-21T14:57:44Z"
  },
  {
    "code": "@@ -27,7 +27,7 @@ def mf_knapsack(i, wt, val, j):\n \n \n def knapsack(w, wt, val, n):\n-    dp = [[0 for i in range(w + 1)] for j in range(n + 1)]\n+    dp = [[0 for _ in range(w + 1)] for _ in range(n + 1)]",
    "comment": "is some tool recommending that we do this?!?  if not then let's not do it as this **_reuse_** is unsettling.\r\n\r\nin general, i would rather that we do not pre-create a matrix unless it is vital.  it seems un-pythonic to do so.",
    "line_number": 30,
    "enriched": "File: dynamic_programming/knapsack.py\nCode: @@ -27,7 +27,7 @@ def mf_knapsack(i, wt, val, j):\n \n \n def knapsack(w, wt, val, n):\n-    dp = [[0 for i in range(w + 1)] for j in range(n + 1)]\n+    dp = [[0 for _ in range(w + 1)] for _ in range(n + 1)]\nComment: Is some tool recommending that we do this?!?  If not then let's not do it as this **_reuse_** is unsettling.\r\n```suggestion\r\n    dp = [[0] * (w + 1) for _ in range(n + 1)]\r\n```\r\nIn general, I would rather that we do not pre-create a matrix unless it is vital.  It seems un-Pythonic to do so.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "dynamic_programming/knapsack.py",
    "pr_number": 7271,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 999531579,
    "comment_created_at": "2022-10-19T14:23:59Z"
  },
  {
    "code": "@@ -659,7 +659,10 @@ def kullback_leibler_divergence(y_true: np.ndarray, y_pred: np.ndarray) -> float\n     if len(y_true) != len(y_pred):\n         raise ValueError(\"Input arrays must have the same length.\")\n \n-    kl_loss = y_true * np.log(y_true / y_pred)\n+    kl_loss = 0\n+    if y_true != 0:",
    "comment": "shouldn't you check the length of the y_true array greater than 0?",
    "line_number": 663,
    "enriched": "File: machine_learning/loss_functions.py\nCode: @@ -659,7 +659,10 @@ def kullback_leibler_divergence(y_true: np.ndarray, y_pred: np.ndarray) -> float\n     if len(y_true) != len(y_pred):\n         raise ValueError(\"Input arrays must have the same length.\")\n \n-    kl_loss = y_true * np.log(y_true / y_pred)\n+    kl_loss = 0\n+    if y_true != 0:\nComment: Shouldn't you check the length of the `y_true` array greater than 0?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "machine_learning/loss_functions.py",
    "pr_number": 12249,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1818136518,
    "comment_created_at": "2024-10-27T16:31:50Z"
  },
  {
    "code": "@@ -0,0 +1,59 @@\n+\"\"\"\n+wissamfawaz12@gmail.com | github.com/wissamfawaz\n+This implementation demonstrates how to generate the\n+elements of a Pascal's triangle.\n+What is Pascal's triangle?\n+- Refer to (https://en.wikipedia.org/wiki/Pascal%27s_triangle)\n+for more info about this triangle.\n+\"\"\"\n+\n+\n+def generate_pascal_triangle(num_rows: int) -> None:\n+    \"\"\"\n+    Print Pascal's triangle for different number of rows\n+    >>> generate_triangle(1)\n+    [[1]]\n+    >>> generate_triangle(2)\n+    [[1], [1, 1]]\n+    >>> generate_triangle(3)\n+    [[1], [1, 1], [1, 2, 1]]\n+    >>> generate_triangle(4)\n+    [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1]]\n+    >>> generate_triangle(5)\n+    [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1], [1, 4, 6, 4, 1]]\n+    \"\"\"\n+    triangle: list[list[int]] = []\n+    for current_row_idx in range(num_rows):\n+        populate_current_row(triangle, current_row_idx)\n+    print(triangle)\n+\n+\n+def populate_current_row(triangle: list[list[int]], current_row_idx: int) -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file other/pascal_triangle.py, please provide doctest for the function populate_current_row",
    "line_number": 31,
    "enriched": "File: other/pascal_triangle.py\nCode: @@ -0,0 +1,59 @@\n+\"\"\"\n+wissamfawaz12@gmail.com | github.com/wissamfawaz\n+This implementation demonstrates how to generate the\n+elements of a Pascal's triangle.\n+What is Pascal's triangle?\n+- Refer to (https://en.wikipedia.org/wiki/Pascal%27s_triangle)\n+for more info about this triangle.\n+\"\"\"\n+\n+\n+def generate_pascal_triangle(num_rows: int) -> None:\n+    \"\"\"\n+    Print Pascal's triangle for different number of rows\n+    >>> generate_triangle(1)\n+    [[1]]\n+    >>> generate_triangle(2)\n+    [[1], [1, 1]]\n+    >>> generate_triangle(3)\n+    [[1], [1, 1], [1, 2, 1]]\n+    >>> generate_triangle(4)\n+    [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1]]\n+    >>> generate_triangle(5)\n+    [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1], [1, 4, 6, 4, 1]]\n+    \"\"\"\n+    triangle: list[list[int]] = []\n+    for current_row_idx in range(num_rows):\n+        populate_current_row(triangle, current_row_idx)\n+    print(triangle)\n+\n+\n+def populate_current_row(triangle: list[list[int]], current_row_idx: int) -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `other/pascal_triangle.py`, please provide doctest for the function `populate_current_row`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "other/pascal_triangle.py",
    "pr_number": 7288,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996401579,
    "comment_created_at": "2022-10-16T07:20:04Z"
  },
  {
    "code": "@@ -0,0 +1,88 @@\n+def flash_sort(array: list[int]) -> list[int]:\n+    \"\"\"\n+    Perform Flashsort on the given array.\n+\n+    Flashsort is a distribution-based sorting algorithm. It divides the array\n+    into buckets based on value ranges and sorts within each bucket.\n+\n+    Arguments:\n+    array -- list of integers to sort\n+\n+    Returns:\n+    Sorted list of integers.\n+\n+    Example:\n+    >>> flash_sort([15, 13, 24, 7, 18, 3, 22, 9])\n+    [3, 7, 9, 13, 15, 18, 22, 24]\n+    >>> flash_sort([5, 1, 4, 2, 3])\n+    [1, 2, 3, 4, 5]\n+    \"\"\"\n+    n = len(array)\n+    \n+    # Step 1: Find minimum and maximum values\n+    min_value, max_value = array[0], array[0]\n+    for i in range(1, n):\n+        if array[i] > max_value:\n+            max_value = array[i]\n+        if array[i] < min_value:\n+            min_value = array[i]\n+    if min_value == max_value:\n+        return array  # All values are the same\n+    \n+    # Step 2: Divide array into m buckets\n+    m = max(int(0.45 * n), 1)\n+\n+    # Step 3: Count the number of elements in each class\n+    def get_bucket_id(value: int) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file sorts/flash_sort.py, please provide doctest for the function get_bucket_id",
    "line_number": 36,
    "enriched": "File: sorts/flash_sort.py\nCode: @@ -0,0 +1,88 @@\n+def flash_sort(array: list[int]) -> list[int]:\n+    \"\"\"\n+    Perform Flashsort on the given array.\n+\n+    Flashsort is a distribution-based sorting algorithm. It divides the array\n+    into buckets based on value ranges and sorts within each bucket.\n+\n+    Arguments:\n+    array -- list of integers to sort\n+\n+    Returns:\n+    Sorted list of integers.\n+\n+    Example:\n+    >>> flash_sort([15, 13, 24, 7, 18, 3, 22, 9])\n+    [3, 7, 9, 13, 15, 18, 22, 24]\n+    >>> flash_sort([5, 1, 4, 2, 3])\n+    [1, 2, 3, 4, 5]\n+    \"\"\"\n+    n = len(array)\n+    \n+    # Step 1: Find minimum and maximum values\n+    min_value, max_value = array[0], array[0]\n+    for i in range(1, n):\n+        if array[i] > max_value:\n+            max_value = array[i]\n+        if array[i] < min_value:\n+            min_value = array[i]\n+    if min_value == max_value:\n+        return array  # All values are the same\n+    \n+    # Step 2: Divide array into m buckets\n+    m = max(int(0.45 * n), 1)\n+\n+    # Step 3: Count the number of elements in each class\n+    def get_bucket_id(value: int) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `sorts/flash_sort.py`, please provide doctest for the function `get_bucket_id`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "sorts/flash_sort.py",
    "pr_number": 11970,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1796594089,
    "comment_created_at": "2024-10-11T08:12:01Z"
  },
  {
    "code": "@@ -1,8 +1,8 @@\n alphabets = [chr(i) for i in range(32, 126)]\n-gear_one = [i for i in range(len(alphabets))]\n-gear_two = [i for i in range(len(alphabets))]\n-gear_three = [i for i in range(len(alphabets))]\n-reflector = [i for i in reversed(range(len(alphabets)))]\n+gear_one = range(len(alphabets))\n+gear_two = range(len(alphabets))\n+gear_three = range(len(alphabets))\n+reflector = reversed(range(len(alphabets)))",
    "comment": "the old code is constructing a list, but the updated code is giving us a range object. is this ok?",
    "line_number": 5,
    "enriched": "File: hashes/enigma_machine.py\nCode: @@ -1,8 +1,8 @@\n alphabets = [chr(i) for i in range(32, 126)]\n-gear_one = [i for i in range(len(alphabets))]\n-gear_two = [i for i in range(len(alphabets))]\n-gear_three = [i for i in range(len(alphabets))]\n-reflector = [i for i in reversed(range(len(alphabets)))]\n+gear_one = range(len(alphabets))\n+gear_two = range(len(alphabets))\n+gear_three = range(len(alphabets))\n+reflector = reversed(range(len(alphabets)))\nComment: The old code is constructing a list, but the updated code is giving us a `range` object. Is this ok?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "hashes/enigma_machine.py",
    "pr_number": 7235,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996319996,
    "comment_created_at": "2022-10-15T15:26:28Z"
  },
  {
    "code": "@@ -0,0 +1,327 @@\n+\"\"\"\n+Python implementation of the simplex algorithm for solving linear programs in\n+tabular form with\n+- `>=`, `<=`, and `=` constraints and\n+- each variable `x1, x2, ...>= 0`.\n+\n+See https://gist.github.com/imengus/f9619a568f7da5bc74eaf20169a24d98 for how\n+to convert linear programs to simplex tableaus, and the steps taken in the\n+simplex algorithm.\n+\n+Resources:\n+https://en.wikipedia.org/wiki/Linear_programming\n+https://en.wikipedia.org/wiki/Simplex_algorithm\n+\n+https://towardsdatascience.com/ \\\n+    a-beginners-guide-to-linear-programming-and-the-simplex-algorithm \\\n+    -87db017e92b4",
    "comment": "i can't click this link, either use a shortener or put it all on one line",
    "line_number": 17,
    "enriched": "File: linear_programming/simplex.py\nCode: @@ -0,0 +1,327 @@\n+\"\"\"\n+Python implementation of the simplex algorithm for solving linear programs in\n+tabular form with\n+- `>=`, `<=`, and `=` constraints and\n+- each variable `x1, x2, ...>= 0`.\n+\n+See https://gist.github.com/imengus/f9619a568f7da5bc74eaf20169a24d98 for how\n+to convert linear programs to simplex tableaus, and the steps taken in the\n+simplex algorithm.\n+\n+Resources:\n+https://en.wikipedia.org/wiki/Linear_programming\n+https://en.wikipedia.org/wiki/Simplex_algorithm\n+\n+https://towardsdatascience.com/ \\\n+    a-beginners-guide-to-linear-programming-and-the-simplex-algorithm \\\n+    -87db017e92b4\nComment: I can't click this link, either use a shortener or put it all on one line",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "linear_programming/simplex.py",
    "pr_number": 8825,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1232802680,
    "comment_created_at": "2023-06-16T21:33:53Z"
  },
  {
    "code": "@@ -0,0 +1,73 @@\n+# LGBM Classifier Example\n+import numpy as np\n+from matplotlib import pyplot as plt\n+from sklearn.datasets import load_iris\n+from sklearn.metrics import ConfusionMatrixDisplay\n+from sklearn.model_selection import train_test_split\n+from lightgbm import LGBMClassifier\n+\n+\n+def data_handling(data: dict) -> tuple:\n+    \"\"\"\n+    Splits dataset into features and target labels.\n+\n+    >>> data_handling({'data': '[5.1, 3.5, 1.4, 0.2]', 'target': [0]})\n+    ('[5.1, 3.5, 1.4, 0.2]', [0])\n+    >>> data_handling({'data': '[4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2]', 'target': [0, 0]})\n+    ('[4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2]', [0, 0])\n+    \"\"\"\n+    return data[\"data\"], data[\"target\"]\n+\n+\n+def lgbm_classifier(features: np.ndarray, target: np.ndarray) -> LGBMClassifier:\n+    \"\"\"\n+    Trains an LGBM Classifier on the given features and target labels.\n+\n+    >>> lgbm_classifier(np.array([[5.1, 3.6, 1.4, 0.2]]), np.array([0]))\n+    LGBMClassifier()\n+    \"\"\"\n+    classifier = LGBMClassifier()\n+    classifier.fit(features, target)\n+    return classifier\n+\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/lgbm_classifier.py, please provide doctest for the function main",
    "line_number": 34,
    "enriched": "File: machine_learning/lgbm_classifier.py\nCode: @@ -0,0 +1,73 @@\n+# LGBM Classifier Example\n+import numpy as np\n+from matplotlib import pyplot as plt\n+from sklearn.datasets import load_iris\n+from sklearn.metrics import ConfusionMatrixDisplay\n+from sklearn.model_selection import train_test_split\n+from lightgbm import LGBMClassifier\n+\n+\n+def data_handling(data: dict) -> tuple:\n+    \"\"\"\n+    Splits dataset into features and target labels.\n+\n+    >>> data_handling({'data': '[5.1, 3.5, 1.4, 0.2]', 'target': [0]})\n+    ('[5.1, 3.5, 1.4, 0.2]', [0])\n+    >>> data_handling({'data': '[4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2]', 'target': [0, 0]})\n+    ('[4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2]', [0, 0])\n+    \"\"\"\n+    return data[\"data\"], data[\"target\"]\n+\n+\n+def lgbm_classifier(features: np.ndarray, target: np.ndarray) -> LGBMClassifier:\n+    \"\"\"\n+    Trains an LGBM Classifier on the given features and target labels.\n+\n+    >>> lgbm_classifier(np.array([[5.1, 3.6, 1.4, 0.2]]), np.array([0]))\n+    LGBMClassifier()\n+    \"\"\"\n+    classifier = LGBMClassifier()\n+    classifier.fit(features, target)\n+    return classifier\n+\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/lgbm_classifier.py`, please provide doctest for the function `main`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "machine_learning/lgbm_classifier.py",
    "pr_number": 11759,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1788107138,
    "comment_created_at": "2024-10-04T18:04:30Z"
  },
  {
    "code": "@@ -0,0 +1,59 @@\n+\"\"\"\n+    https://en.wikipedia.org/wiki/Merge_algorithm   \n+\"\"\"\n+\n+def merge_two_sorted_arrays(nums1: list[int], nums2: list[int]) -> list[int]:\n+\n+    \"\"\"\n+    Merge two sorted arrays.\n+\n+    Args:\n+        nums1: The first array.\n+        nums2: The second array.\n+\n+    Returns:\n+        The merged sorted array.\n+\n+    Examples:\n+        >>> merge_two_sorted_arrays([1,5,7], [0,4,9])\n+        [0,1,4,5,7,9]\n+\n+        >>> merge_two_sorted_arrays([-11,5,45], [0,5,9])\n+        [-11,0,5,5,9,45]\n+\"\"\"\n+\n+\n+def merge_two_sorted_arrays(nums1: list[int], nums2: list[int]) -> list[int]:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/arrays/merge_two_sorted_arrays.py, please provide doctest for the function merge_two_sorted_arrays",
    "line_number": 26,
    "enriched": "File: data_structures/arrays/merge_two_sorted_arrays.py\nCode: @@ -0,0 +1,59 @@\n+\"\"\"\n+    https://en.wikipedia.org/wiki/Merge_algorithm   \n+\"\"\"\n+\n+def merge_two_sorted_arrays(nums1: list[int], nums2: list[int]) -> list[int]:\n+\n+    \"\"\"\n+    Merge two sorted arrays.\n+\n+    Args:\n+        nums1: The first array.\n+        nums2: The second array.\n+\n+    Returns:\n+        The merged sorted array.\n+\n+    Examples:\n+        >>> merge_two_sorted_arrays([1,5,7], [0,4,9])\n+        [0,1,4,5,7,9]\n+\n+        >>> merge_two_sorted_arrays([-11,5,45], [0,5,9])\n+        [-11,0,5,5,9,45]\n+\"\"\"\n+\n+\n+def merge_two_sorted_arrays(nums1: list[int], nums2: list[int]) -> list[int]:\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/arrays/merge_two_sorted_arrays.py`, please provide doctest for the function `merge_two_sorted_arrays`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/arrays/merge_two_sorted_arrays.py",
    "pr_number": 10940,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1371296323,
    "comment_created_at": "2023-10-25T07:45:18Z"
  },
  {
    "code": "@@ -0,0 +1,55 @@\n+\"\"\"\n+A XOR Gate is a logic gate in boolean algebra which results to True(1)\n+if only one of the two inputs is 1, and False(1) if even number\n+of inputs are 1.\n+Following is the truth table of a XOR Gate:\n+    ------------------------------\n+    | Input 1 | Input 2 | Output |\n+    ------------------------------\n+    |    0    |    0    |    0   |\n+    |    0    |    1    |    1   |\n+    |    1    |    0    |    1   |\n+    |    1    |    1    |    0   |\n+    ------------------------------\n+\n+Refer - https://www.geeksforgeeks.org/logic-gates-in-python/\n+\n+\"\"\"\n+\n+\n+def xor_gate(input_1: int, input_2: int) -> int:\n+    \"\"\"\n+    >>> xor_gate(0, 0)\n+    0\n+    >>> xor_gate(0, 1)\n+    1\n+    >>> xor_gate(1, 0)\n+    1\n+    >>> xor_gate(1, 1)\n+    0\n+    \"\"\"\n+    num_ones = 0\n+\n+    if input_1 != 0:\n+        num_ones += 1\n+    \n+    if input_2 != 0:\n+        num_ones += 1\n+    \n+    return int(num_ones % 2 != 0)\n+\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file boolean_algebra/xor_gate.py, please provide doctest for the function main",
    "line_number": 42,
    "enriched": "File: boolean_algebra/xor_gate.py\nCode: @@ -0,0 +1,55 @@\n+\"\"\"\n+A XOR Gate is a logic gate in boolean algebra which results to True(1)\n+if only one of the two inputs is 1, and False(1) if even number\n+of inputs are 1.\n+Following is the truth table of a XOR Gate:\n+    ------------------------------\n+    | Input 1 | Input 2 | Output |\n+    ------------------------------\n+    |    0    |    0    |    0   |\n+    |    0    |    1    |    1   |\n+    |    1    |    0    |    1   |\n+    |    1    |    1    |    0   |\n+    ------------------------------\n+\n+Refer - https://www.geeksforgeeks.org/logic-gates-in-python/\n+\n+\"\"\"\n+\n+\n+def xor_gate(input_1: int, input_2: int) -> int:\n+    \"\"\"\n+    >>> xor_gate(0, 0)\n+    0\n+    >>> xor_gate(0, 1)\n+    1\n+    >>> xor_gate(1, 0)\n+    1\n+    >>> xor_gate(1, 1)\n+    0\n+    \"\"\"\n+    num_ones = 0\n+\n+    if input_1 != 0:\n+        num_ones += 1\n+    \n+    if input_2 != 0:\n+        num_ones += 1\n+    \n+    return int(num_ones % 2 != 0)\n+\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `boolean_algebra/xor_gate.py`, please provide doctest for the function `main`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "boolean_algebra/xor_gate.py",
    "pr_number": 7588,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002913002,
    "comment_created_at": "2022-10-24T06:17:59Z"
  },
  {
    "code": "@@ -0,0 +1,152 @@\n+#!/usr/bin/env python3\n+import re\n+\n+\n+def split_words(input_text: str) -> list[str]:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file strings/word_counter.py, please provide doctest for the function split_words",
    "line_number": 5,
    "enriched": "File: strings/word_counter.py\nCode: @@ -0,0 +1,152 @@\n+#!/usr/bin/env python3\n+import re\n+\n+\n+def split_words(input_text: str) -> list[str]:\nComment: As there is no test file in this pull request nor any test function or class in the file `strings/word_counter.py`, please provide doctest for the function `split_words`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "strings/word_counter.py",
    "pr_number": 9838,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347669850,
    "comment_created_at": "2023-10-05T16:09:42Z"
  },
  {
    "code": "@@ -0,0 +1,18 @@\n+# 5 Fibonacci series\n+n = int(input(\"Number of terms in Fibonacci series : \"))\n+a, b = 0, 1\n+count = 0\n+# check if the number of terms is valid\n+if n <= 0:\n+    print(\"Please enter a positive integer\")\n+elif n == 1:\n+    print(\"Fibonacci series upto\", n, \"terms is :\", a)\n+# generate fibonacci sequence\n+else:\n+    print(\"Fibonacci series upto \", n, \" terms is :\")\n+    while count < n:\n+        print(a, end=\" \")\n+        c = a + b\n+        a = b\n+        b = c\n+        count += 1",
    "comment": "did you do pre-commit run --all-files",
    "line_number": 18,
    "enriched": "File: fib.py\nCode: @@ -0,0 +1,18 @@\n+# 5 Fibonacci series\n+n = int(input(\"Number of terms in Fibonacci series : \"))\n+a, b = 0, 1\n+count = 0\n+# check if the number of terms is valid\n+if n <= 0:\n+    print(\"Please enter a positive integer\")\n+elif n == 1:\n+    print(\"Fibonacci series upto\", n, \"terms is :\", a)\n+# generate fibonacci sequence\n+else:\n+    print(\"Fibonacci series upto \", n, \" terms is :\")\n+    while count < n:\n+        print(a, end=\" \")\n+        c = a + b\n+        a = b\n+        b = c\n+        count += 1\nComment: did you do `pre-commit run --all-files`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "fib.py",
    "pr_number": 10979,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1372585394,
    "comment_created_at": "2023-10-26T04:59:50Z"
  },
  {
    "code": "@@ -0,0 +1,108 @@\n+def generate_square(keyword: str) -> list:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file ciphers/two_square_cipher.py, please provide doctest for the function generate_square",
    "line_number": 1,
    "enriched": "File: ciphers/two_square_cipher.py\nCode: @@ -0,0 +1,108 @@\n+def generate_square(keyword: str) -> list:\nComment: As there is no test file in this pull request nor any test function or class in the file `ciphers/two_square_cipher.py`, please provide doctest for the function `generate_square`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "ciphers/two_square_cipher.py",
    "pr_number": 13308,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2409514692,
    "comment_created_at": "2025-10-07T06:22:06Z"
  },
  {
    "code": "@@ -0,0 +1,192 @@\n+#!/usr/bin/env python3\n+import itertools\n+import math\n+\n+\n+class InvalidGraphError(ValueError):\n+    \"\"\"Custom error for invalid graph inputs.\"\"\"\n+\n+\n+def euclidean_distance(point1: list[float], point2: list[float]) -> float:\n+    \"\"\"\n+    Calculate the Euclidean distance between two points in 2D space.\n+\n+    :param point1: Coordinates of the first point [x, y]\n+    :param point2: Coordinates of the second point [x, y]\n+    :return: The Euclidean distance between the two points\n+\n+    >>> euclidean_distance([0, 0], [3, 4])\n+    5.0\n+    >>> euclidean_distance([1, 1], [1, 1])\n+    0.0\n+    >>> euclidean_distance([1, 1], ['a', 1])\n+    Traceback (most recent call last):\n+    ...\n+    ValueError: Invalid input: Points must be numerical coordinates\n+    \"\"\"\n+    try:\n+        return math.sqrt((point2[0] - point1[0]) ** 2 + (point2[1] - point1[1]) ** 2)\n+    except TypeError:\n+        raise ValueError(\"Invalid input: Points must be numerical coordinates\")\n+\n+\n+def validate_graph(graph_points: dict[str, list[float]]) -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file graphs/travlelling_salesman_problem.py, please provide doctest for the function validate_graph",
    "line_number": 33,
    "enriched": "File: graphs/travlelling_salesman_problem.py\nCode: @@ -0,0 +1,192 @@\n+#!/usr/bin/env python3\n+import itertools\n+import math\n+\n+\n+class InvalidGraphError(ValueError):\n+    \"\"\"Custom error for invalid graph inputs.\"\"\"\n+\n+\n+def euclidean_distance(point1: list[float], point2: list[float]) -> float:\n+    \"\"\"\n+    Calculate the Euclidean distance between two points in 2D space.\n+\n+    :param point1: Coordinates of the first point [x, y]\n+    :param point2: Coordinates of the second point [x, y]\n+    :return: The Euclidean distance between the two points\n+\n+    >>> euclidean_distance([0, 0], [3, 4])\n+    5.0\n+    >>> euclidean_distance([1, 1], [1, 1])\n+    0.0\n+    >>> euclidean_distance([1, 1], ['a', 1])\n+    Traceback (most recent call last):\n+    ...\n+    ValueError: Invalid input: Points must be numerical coordinates\n+    \"\"\"\n+    try:\n+        return math.sqrt((point2[0] - point1[0]) ** 2 + (point2[1] - point1[1]) ** 2)\n+    except TypeError:\n+        raise ValueError(\"Invalid input: Points must be numerical coordinates\")\n+\n+\n+def validate_graph(graph_points: dict[str, list[float]]) -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `graphs/travlelling_salesman_problem.py`, please provide doctest for the function `validate_graph`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "graphs/travlelling_salesman_problem.py",
    "pr_number": 12144,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1806940220,
    "comment_created_at": "2024-10-18T19:31:51Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+\"\"\"\r\n+Program to find max length of valid paranthesis\r\n+author : aakash_2001\r\n+\"\"\"\r\n+def find_max_len(string) -> int:\r",
    "comment": "please provide type hint for the parameter: string",
    "line_number": 5,
    "enriched": "File: strings/valid_paranthesis.py\nCode: @@ -0,0 +1,56 @@\n+\"\"\"\r\n+Program to find max length of valid paranthesis\r\n+author : aakash_2001\r\n+\"\"\"\r\n+def find_max_len(string) -> int:\r\nComment: Please provide type hint for the parameter: `string`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "strings/valid_paranthesis.py",
    "pr_number": 7716,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1006611867,
    "comment_created_at": "2022-10-27T09:13:51Z"
  },
  {
    "code": "@@ -0,0 +1,46 @@\n+import random\n+from typing import List\n+\n+def bogo_sort(collection: List[int]) -> List[int]:\n+    \"\"\"Pure implementation of the bogosort algorithm in Python.\n+    Bogosort generates random permutations until it guesses the correct one.\n+    \n+    More info on: https://en.wikipedia.org/wiki/Bogosort\n+    Args:\n+        collection (List[int]): A mutable ordered collection with comparable items.\n+    Returns:\n+        List[int]: The same collection ordered by ascending.\n+    Examples:\n+    >>> bogo_sort([0, 5, 3, 2, 2])\n+    [0, 2, 2, 3, 5]\n+    >>> bogo_sort([])\n+    []\n+    >>> bogo_sort([-2, -5, -45])\n+    [-45, -5, -2]\n+    Raises:\n+        ValueError: If the input is not a list of integers.\n+    Note:\n+        This is not an efficient sorting algorithm and is mainly used for educational purposes.\n+    For doctests, run the following command:\n+    python -m doctest -v bogo_sort.py\n+    or\n+    python3 -m doctest -v bogo_sort.py\n+    \n+    For manual testing, run:\n+    python bogo_sort.py\n+    \"\"\"\n+\n+    def is_sorted(collection: List[int]) -> bool:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file sorts/flashsort.py, please provide doctest for the function is_sorted",
    "line_number": 33,
    "enriched": "File: sorts/flashsort.py\nCode: @@ -0,0 +1,46 @@\n+import random\n+from typing import List\n+\n+def bogo_sort(collection: List[int]) -> List[int]:\n+    \"\"\"Pure implementation of the bogosort algorithm in Python.\n+    Bogosort generates random permutations until it guesses the correct one.\n+    \n+    More info on: https://en.wikipedia.org/wiki/Bogosort\n+    Args:\n+        collection (List[int]): A mutable ordered collection with comparable items.\n+    Returns:\n+        List[int]: The same collection ordered by ascending.\n+    Examples:\n+    >>> bogo_sort([0, 5, 3, 2, 2])\n+    [0, 2, 2, 3, 5]\n+    >>> bogo_sort([])\n+    []\n+    >>> bogo_sort([-2, -5, -45])\n+    [-45, -5, -2]\n+    Raises:\n+        ValueError: If the input is not a list of integers.\n+    Note:\n+        This is not an efficient sorting algorithm and is mainly used for educational purposes.\n+    For doctests, run the following command:\n+    python -m doctest -v bogo_sort.py\n+    or\n+    python3 -m doctest -v bogo_sort.py\n+    \n+    For manual testing, run:\n+    python bogo_sort.py\n+    \"\"\"\n+\n+    def is_sorted(collection: List[int]) -> bool:\nComment: As there is no test file in this pull request nor any test function or class in the file `sorts/flashsort.py`, please provide doctest for the function `is_sorted`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "sorts/flashsort.py",
    "pr_number": 11952,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1796082070,
    "comment_created_at": "2024-10-10T20:54:21Z"
  },
  {
    "code": "@@ -6,19 +6,61 @@\n https://en.wikipedia.org/wiki/Morse_code\n \"\"\"\n \n-# fmt: off\n MORSE_CODE_DICT = {\n-    \"A\": \".-\", \"B\": \"-...\", \"C\": \"-.-.\", \"D\": \"-..\", \"E\": \".\", \"F\": \"..-.\", \"G\": \"--.\",\n-    \"H\": \"....\", \"I\": \"..\", \"J\": \".---\", \"K\": \"-.-\", \"L\": \".-..\", \"M\": \"--\", \"N\": \"-.\",\n-    \"O\": \"---\", \"P\": \".--.\", \"Q\": \"--.-\", \"R\": \".-.\", \"S\": \"...\", \"T\": \"-\", \"U\": \"..-\",\n-    \"V\": \"...-\", \"W\": \".--\", \"X\": \"-..-\", \"Y\": \"-.--\", \"Z\": \"--..\", \"1\": \".----\",\n-    \"2\": \"..---\", \"3\": \"...--\", \"4\": \"....-\", \"5\": \".....\", \"6\": \"-....\", \"7\": \"--...\",\n-    \"8\": \"---..\", \"9\": \"----.\", \"0\": \"-----\", \"&\": \".-...\", \"@\": \".--.-.\",\n-    \":\": \"---...\", \",\": \"--..--\", \".\": \".-.-.-\", \"'\": \".----.\", '\"': \".-..-.\",\n-    \"?\": \"..--..\", \"/\": \"-..-.\", \"=\": \"-...-\", \"+\": \".-.-.\", \"-\": \"-....-\",\n-    \"(\": \"-.--.\", \")\": \"-.--.-\", \"!\": \"-.-.--\", \" \": \"/\"\n-}  # Exclamation mark is not in ITU-R recommendation\n-# fmt: on",
    "comment": "the current code uses # fmt: off and #fmt: on properly and i see no reason to mess with that.",
    "line_number": 21,
    "enriched": "File: ciphers/morse_code.py\nCode: @@ -6,19 +6,61 @@\n https://en.wikipedia.org/wiki/Morse_code\n \"\"\"\n \n-# fmt: off\n MORSE_CODE_DICT = {\n-    \"A\": \".-\", \"B\": \"-...\", \"C\": \"-.-.\", \"D\": \"-..\", \"E\": \".\", \"F\": \"..-.\", \"G\": \"--.\",\n-    \"H\": \"....\", \"I\": \"..\", \"J\": \".---\", \"K\": \"-.-\", \"L\": \".-..\", \"M\": \"--\", \"N\": \"-.\",\n-    \"O\": \"---\", \"P\": \".--.\", \"Q\": \"--.-\", \"R\": \".-.\", \"S\": \"...\", \"T\": \"-\", \"U\": \"..-\",\n-    \"V\": \"...-\", \"W\": \".--\", \"X\": \"-..-\", \"Y\": \"-.--\", \"Z\": \"--..\", \"1\": \".----\",\n-    \"2\": \"..---\", \"3\": \"...--\", \"4\": \"....-\", \"5\": \".....\", \"6\": \"-....\", \"7\": \"--...\",\n-    \"8\": \"---..\", \"9\": \"----.\", \"0\": \"-----\", \"&\": \".-...\", \"@\": \".--.-.\",\n-    \":\": \"---...\", \",\": \"--..--\", \".\": \".-.-.-\", \"'\": \".----.\", '\"': \".-..-.\",\n-    \"?\": \"..--..\", \"/\": \"-..-.\", \"=\": \"-...-\", \"+\": \".-.-.\", \"-\": \"-....-\",\n-    \"(\": \"-.--.\", \")\": \"-.--.-\", \"!\": \"-.-.--\", \" \": \"/\"\n-}  # Exclamation mark is not in ITU-R recommendation\n-# fmt: on\nComment: The current code uses `# fmt: off` and `#fmt: on` properly and I see no reason to mess with that.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "ciphers/morse_code.py",
    "pr_number": 8649,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1169789774,
    "comment_created_at": "2023-04-18T10:01:03Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+# Einstein's Mass-Energy Equivalence\n+# This code calculates the energy equivalent of a given mass using Einstein's mass-energy equivalence equation, E=mc\u00b2.\n+\n+# Source: https://en.wikipedia.org/wiki/Mass%E2%80%93energy_equivalence\n+\n+def mass_energy_equivalence(mass: float, c: float = 299792458) -> float:",
    "comment": "please provide descriptive name for the parameter: c",
    "line_number": 6,
    "enriched": "File: physics/einstein_mass_energy_equivalence.py\nCode: @@ -0,0 +1,35 @@\n+# Einstein's Mass-Energy Equivalence\n+# This code calculates the energy equivalent of a given mass using Einstein's mass-energy equivalence equation, E=mc\u00b2.\n+\n+# Source: https://en.wikipedia.org/wiki/Mass%E2%80%93energy_equivalence\n+\n+def mass_energy_equivalence(mass: float, c: float = 299792458) -> float:\nComment: Please provide descriptive name for the parameter: `c`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "physics/einstein_mass_energy_equivalence.py",
    "pr_number": 9224,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342097353,
    "comment_created_at": "2023-10-01T08:17:20Z"
  },
  {
    "code": "@@ -1,44 +1,25 @@\n-# Print all subset combinations of n element in given set of r element.\n-\n-\n-def combination_util(arr, n, r, index, data, i):\n-    \"\"\"\n-    Current combination is ready to be printed, print it\n-    arr[]  ---> Input Array\n-    data[] ---> Temporary array to store current combination\n-    start & end ---> Staring and Ending indexes in arr[]\n-    index  ---> Current index in data[]\n-    r ---> Size of a combination to be printed\n-    \"\"\"\n-    if index == r:\n-        for j in range(r):\n-            print(data[j], end=\" \")\n-        print(\" \")\n-        return\n-    #  When no more elements are there to put in data[]\n-    if i >= n:\n-        return\n-    # current is included, put next at next location\n-    data[index] = arr[i]\n-    combination_util(arr, n, r, index + 1, data, i + 1)\n-    # current is excluded, replace it with\n-    # next (Note that i+1 is passed, but\n-    # index is not changed)\n-    combination_util(arr, n, r, index, data, i + 1)\n-    # The main function that prints all combinations\n-    # of size r in arr[] of size n. This function\n-    # mainly uses combinationUtil()\n-\n-\n-def print_combination(arr, n, r):\n-    # A temporary array to store all combination one by one\n-    data = [0] * r\n-    # Print all combination using temporary array 'data[]'\n-    combination_util(arr, n, r, 0, data, 0)\n+# return all subset combinations of n element in given set of r element.\n+\n+\n+def subset_combinations_dp(elements, n):",
    "comment": "no type hints, no docstring, no doctests, n is not a self-documenting variable name.\r\n\r\nwhy ask me to review this pull request when we have 200+ open pull requests that are more in alignment with contributing.md?",
    "line_number": 4,
    "enriched": "File: dynamic_programming/subset_generation.py\nCode: @@ -1,44 +1,25 @@\n-# Print all subset combinations of n element in given set of r element.\n-\n-\n-def combination_util(arr, n, r, index, data, i):\n-    \"\"\"\n-    Current combination is ready to be printed, print it\n-    arr[]  ---> Input Array\n-    data[] ---> Temporary array to store current combination\n-    start & end ---> Staring and Ending indexes in arr[]\n-    index  ---> Current index in data[]\n-    r ---> Size of a combination to be printed\n-    \"\"\"\n-    if index == r:\n-        for j in range(r):\n-            print(data[j], end=\" \")\n-        print(\" \")\n-        return\n-    #  When no more elements are there to put in data[]\n-    if i >= n:\n-        return\n-    # current is included, put next at next location\n-    data[index] = arr[i]\n-    combination_util(arr, n, r, index + 1, data, i + 1)\n-    # current is excluded, replace it with\n-    # next (Note that i+1 is passed, but\n-    # index is not changed)\n-    combination_util(arr, n, r, index, data, i + 1)\n-    # The main function that prints all combinations\n-    # of size r in arr[] of size n. This function\n-    # mainly uses combinationUtil()\n-\n-\n-def print_combination(arr, n, r):\n-    # A temporary array to store all combination one by one\n-    data = [0] * r\n-    # Print all combination using temporary array 'data[]'\n-    combination_util(arr, n, r, 0, data, 0)\n+# return all subset combinations of n element in given set of r element.\n+\n+\n+def subset_combinations_dp(elements, n):\nComment: No type hints, No docstring, no doctests, `n` is not a self-documenting variable name.\r\n\r\nWhy ask me to review this pull request when we have 200+ open pull requests that are more in alignment with CONTRIBUTING.md?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "dynamic_programming/subset_generation.py",
    "pr_number": 10191,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359900443,
    "comment_created_at": "2023-10-15T15:40:01Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+from __future__ import annotations\n+\n+ELECTRON_CHARGE = 1.6021e-19  # units = C\n+5 * 2",
    "comment": "?",
    "line_number": 4,
    "enriched": "File: electronics/electric_conductivity.py\nCode: @@ -0,0 +1,56 @@\n+from __future__ import annotations\n+\n+ELECTRON_CHARGE = 1.6021e-19  # units = C\n+5 * 2\nComment: ```suggestion\r\n```\r\n?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "electronics/electric_conductivity.py",
    "pr_number": 7449,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000455367,
    "comment_created_at": "2022-10-20T10:34:44Z"
  },
  {
    "code": "@@ -35,17 +33,17 @@ def join(separator: str, separated: list[str]) -> str:\n     Additional test case with a different separator:\n     >>> join(\"-\", [\"apple\", \"banana\", \"cherry\"])\n     'apple-banana-cherry'\n+    >>> join(\",\", [\"\", \"\", \"\"])\n+    ',,,'  # This test will now pass correctly\n+\n     \"\"\"\n \n-    joined = \"\"\n-    for word_or_phrase in separated:\n-        if not isinstance(word_or_phrase, str):\n-            raise Exception(\"join() accepts only strings\")\n-        joined += word_or_phrase + separator\n+    if not all(isinstance(word_or_phrase, str) for word_or_phrase in separated):\n+        raise Exception(\"join() accepts only strings\")\n+\n+    joined = separator.join(separated)",
    "comment": "question about this join though. isn't the point of this repository to show possible implementations of python functions? because if that's the case, i was wondering why the join function now internally uses str.join?",
    "line_number": 44,
    "enriched": "File: strings/join.py\nCode: @@ -35,17 +33,17 @@ def join(separator: str, separated: list[str]) -> str:\n     Additional test case with a different separator:\n     >>> join(\"-\", [\"apple\", \"banana\", \"cherry\"])\n     'apple-banana-cherry'\n+    >>> join(\",\", [\"\", \"\", \"\"])\n+    ',,,'  # This test will now pass correctly\n+\n     \"\"\"\n \n-    joined = \"\"\n-    for word_or_phrase in separated:\n-        if not isinstance(word_or_phrase, str):\n-            raise Exception(\"join() accepts only strings\")\n-        joined += word_or_phrase + separator\n+    if not all(isinstance(word_or_phrase, str) for word_or_phrase in separated):\n+        raise Exception(\"join() accepts only strings\")\n+\n+    joined = separator.join(separated)\nComment: Question about this join though. Isn't the point of this repository to show possible implementations of python functions? Because if that's the case, I was wondering why the join function now internally uses str.join?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "strings/join.py",
    "pr_number": 12438,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1887684538,
    "comment_created_at": "2024-12-16T22:58:37Z"
  },
  {
    "code": "@@ -0,0 +1,324 @@\n+from collections.abc import Callable\n+from typing import Any, Dict, List, Tuple\n+\n+\n+def viterbi(\n+    observations_space: List[str],\n+    states_space: List[str],\n+    initial_probabilities: Dict[str, float],\n+    transition_probabilities: Dict[str, Dict[str, float]],\n+    emission_probabilities: Dict[str, Dict[str, float]],\n+) -> List[str]:\n+    \"\"\"\n+    Viterbi Algorithm, to find the most likely path of\n+    states from the start and the expected output.\n+    https://en.wikipedia.org/wiki/Viterbi_algorithm\n+\n+    Wikipedia example\n+    >>> observations = [\"normal\", \"cold\", \"dizzy\"]\n+    >>> states = [\"Healthy\", \"Fever\"]\n+    >>> start_p = {\"Healthy\": 0.6, \"Fever\": 0.4}\n+    >>> trans_p = {\n+    ...     \"Healthy\": {\"Healthy\": 0.7, \"Fever\": 0.3},\n+    ...     \"Fever\": {\"Healthy\": 0.4, \"Fever\": 0.6},\n+    ... }\n+    >>> emit_p = {\n+    ...     \"Healthy\": {\"normal\": 0.5, \"cold\": 0.4, \"dizzy\": 0.1},\n+    ...     \"Fever\": {\"normal\": 0.1, \"cold\": 0.3, \"dizzy\": 0.6},\n+    ... }\n+    >>> viterbi(observations, states, start_p, trans_p, emit_p)\n+    ['Healthy', 'Healthy', 'Fever']\n+\n+    # >>> viterbi((), states, start_p, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: There's an empty parameter\n+    #\n+    # >>> viterbi(observations, (), start_p, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: There's an empty parameter\n+    #\n+    # >>> viterbi(observations, states, {}, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: There's an empty parameter\n+    #\n+    # >>> viterbi(observations, states, start_p, {}, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: There's an empty parameter\n+    #\n+    # >>> viterbi(observations, states, start_p, trans_p, {})\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: There's an empty parameter\n+    #\n+    # >>> viterbi(\"invalid\", states, start_p, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: observations_space must be a list\n+    #\n+    # >>> viterbi((\"valid\", 123), states, start_p, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: observations_space must be a list of strings\n+    #\n+    # >>> viterbi(observations, \"invalid\", start_p, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: states_space must be a list\n+    #\n+    # >>> viterbi(observations, (\"valid\", 123), start_p, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: states_space must be a list of strings\n+    #\n+    # >>> viterbi(observations, states, \"invalid\", trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: initial_probabilities must be a dict\n+    #\n+    # >>> viterbi(observations, states, {2:2}, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: initial_probabilities all keys must be strings\n+    #\n+    # >>> viterbi(observations, states, {\"a\":2}, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: initial_probabilities all values must be float\n+    #\n+    # >>> viterbi(observations, states, start_p, \"invalid\", emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: transition_probabilities must be a dict\n+    #\n+    # >>> viterbi(observations, states, start_p, {\"a\":2}, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: transition_probabilities all values must be dict\n+    #\n+    # >>> viterbi(observations, states, start_p, {2:{2:2}}, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: transition_probabilities all keys must be strings\n+    #\n+    # >>> viterbi(observations, states, start_p, {\"a\":{2:2}}, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: transition_probabilities all keys must be strings\n+    #\n+    # >>> viterbi(observations, states, start_p, {\"a\":{\"b\":2}}, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: transition_probabilities nested dictionary all values must be float\n+    #\n+    # >>> viterbi(observations, states, start_p, trans_p, \"invalid\")\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: emission_probabilities must be a dict\n+    #\n+    # >>> viterbi(observations, states, start_p, trans_p, None)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: There's an empty parameter\n+\n+    \"\"\"\n+    _validation(\n+        observations_space,\n+        states_space,\n+        initial_probabilities,\n+        transition_probabilities,\n+        emission_probabilities,\n+    )\n+    # Creates data structures and fill initial step\n+    pointers, probabilities = _initialise_probabilities_and_pointers(\n+        observations_space,\n+        states_space,\n+        initial_probabilities,\n+        emission_probabilities,\n+    )\n+\n+    # Function for the process forward calculations\n+    def _prior_state(observation: str, prior_observation: str, state: str) -> Callable:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file dynamic_programming/viterbi.py, please provide doctest for the function _prior_state",
    "line_number": 144,
    "enriched": "File: dynamic_programming/viterbi.py\nCode: @@ -0,0 +1,324 @@\n+from collections.abc import Callable\n+from typing import Any, Dict, List, Tuple\n+\n+\n+def viterbi(\n+    observations_space: List[str],\n+    states_space: List[str],\n+    initial_probabilities: Dict[str, float],\n+    transition_probabilities: Dict[str, Dict[str, float]],\n+    emission_probabilities: Dict[str, Dict[str, float]],\n+) -> List[str]:\n+    \"\"\"\n+    Viterbi Algorithm, to find the most likely path of\n+    states from the start and the expected output.\n+    https://en.wikipedia.org/wiki/Viterbi_algorithm\n+\n+    Wikipedia example\n+    >>> observations = [\"normal\", \"cold\", \"dizzy\"]\n+    >>> states = [\"Healthy\", \"Fever\"]\n+    >>> start_p = {\"Healthy\": 0.6, \"Fever\": 0.4}\n+    >>> trans_p = {\n+    ...     \"Healthy\": {\"Healthy\": 0.7, \"Fever\": 0.3},\n+    ...     \"Fever\": {\"Healthy\": 0.4, \"Fever\": 0.6},\n+    ... }\n+    >>> emit_p = {\n+    ...     \"Healthy\": {\"normal\": 0.5, \"cold\": 0.4, \"dizzy\": 0.1},\n+    ...     \"Fever\": {\"normal\": 0.1, \"cold\": 0.3, \"dizzy\": 0.6},\n+    ... }\n+    >>> viterbi(observations, states, start_p, trans_p, emit_p)\n+    ['Healthy', 'Healthy', 'Fever']\n+\n+    # >>> viterbi((), states, start_p, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: There's an empty parameter\n+    #\n+    # >>> viterbi(observations, (), start_p, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: There's an empty parameter\n+    #\n+    # >>> viterbi(observations, states, {}, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: There's an empty parameter\n+    #\n+    # >>> viterbi(observations, states, start_p, {}, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: There's an empty parameter\n+    #\n+    # >>> viterbi(observations, states, start_p, trans_p, {})\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: There's an empty parameter\n+    #\n+    # >>> viterbi(\"invalid\", states, start_p, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: observations_space must be a list\n+    #\n+    # >>> viterbi((\"valid\", 123), states, start_p, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: observations_space must be a list of strings\n+    #\n+    # >>> viterbi(observations, \"invalid\", start_p, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: states_space must be a list\n+    #\n+    # >>> viterbi(observations, (\"valid\", 123), start_p, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: states_space must be a list of strings\n+    #\n+    # >>> viterbi(observations, states, \"invalid\", trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: initial_probabilities must be a dict\n+    #\n+    # >>> viterbi(observations, states, {2:2}, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: initial_probabilities all keys must be strings\n+    #\n+    # >>> viterbi(observations, states, {\"a\":2}, trans_p, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: initial_probabilities all values must be float\n+    #\n+    # >>> viterbi(observations, states, start_p, \"invalid\", emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: transition_probabilities must be a dict\n+    #\n+    # >>> viterbi(observations, states, start_p, {\"a\":2}, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: transition_probabilities all values must be dict\n+    #\n+    # >>> viterbi(observations, states, start_p, {2:{2:2}}, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: transition_probabilities all keys must be strings\n+    #\n+    # >>> viterbi(observations, states, start_p, {\"a\":{2:2}}, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: transition_probabilities all keys must be strings\n+    #\n+    # >>> viterbi(observations, states, start_p, {\"a\":{\"b\":2}}, emit_p)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: transition_probabilities nested dictionary all values must be float\n+    #\n+    # >>> viterbi(observations, states, start_p, trans_p, \"invalid\")\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: emission_probabilities must be a dict\n+    #\n+    # >>> viterbi(observations, states, start_p, trans_p, None)\n+    # Traceback (most recent call last):\n+    #     ...\n+    # ValueError: There's an empty parameter\n+\n+    \"\"\"\n+    _validation(\n+        observations_space,\n+        states_space,\n+        initial_probabilities,\n+        transition_probabilities,\n+        emission_probabilities,\n+    )\n+    # Creates data structures and fill initial step\n+    pointers, probabilities = _initialise_probabilities_and_pointers(\n+        observations_space,\n+        states_space,\n+        initial_probabilities,\n+        emission_probabilities,\n+    )\n+\n+    # Function for the process forward calculations\n+    def _prior_state(observation: str, prior_observation: str, state: str) -> Callable:\nComment: As there is no test file in this pull request nor any test function or class in the file `dynamic_programming/viterbi.py`, please provide doctest for the function `_prior_state`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/viterbi.py",
    "pr_number": 7509,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002440963,
    "comment_created_at": "2022-10-22T10:52:19Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+BASE58_ALPHABET = \"123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz\"\n+\n+\n+def base58_encode(data: bytes) -> str:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file ciphers/base58.py, please provide doctest for the function base58_encode",
    "line_number": 4,
    "enriched": "File: ciphers/base58.py\nCode: @@ -0,0 +1,60 @@\n+BASE58_ALPHABET = \"123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz\"\n+\n+\n+def base58_encode(data: bytes) -> str:\nComment: As there is no test file in this pull request nor any test function or class in the file `ciphers/base58.py`, please provide doctest for the function `base58_encode`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "ciphers/base58.py",
    "pr_number": 12121,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1803743116,
    "comment_created_at": "2024-10-16T20:16:30Z"
  },
  {
    "code": "@@ -35,8 +35,8 @@ def binary_and(a: int, b: int) -> str:\n     if a < 0 or b < 0:\n         raise ValueError(\"the value of both inputs must be positive\")\n \n-    a_binary = str(bin(a))[2:]  # remove the leading \"0b\"",
    "comment": "why we need to more the comments?? \r\nin the given line of code, the comment is explaining what the line does, which is to remove the leading \"0b\" from the binary representation of the integer a. comments are useful for making the code more understandable to other developers (or to your future self) who might read the code later.",
    "line_number": 38,
    "enriched": "File: bit_manipulation/binary_and_operator.py\nCode: @@ -35,8 +35,8 @@ def binary_and(a: int, b: int) -> str:\n     if a < 0 or b < 0:\n         raise ValueError(\"the value of both inputs must be positive\")\n \n-    a_binary = str(bin(a))[2:]  # remove the leading \"0b\"\nComment: Why we need to more the comments?? \r\nIn the given line of code, the comment is explaining what the line does, which is to remove the leading \"0b\" from the binary representation of the integer a. Comments are useful for making the code more understandable to other developers (or to your future self) who might read the code later.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "bit_manipulation/binary_and_operator.py",
    "pr_number": 11307,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1503561897,
    "comment_created_at": "2024-02-27T02:41:37Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+\"\"\"\n+Title : calculating the time period of a simple pendulum\n+\n+A simple pendulum can be defined as a device where its point mass\n+is attached to a light inextensible string and suspended from a fixed\n+support. Equilibrium position is subjected to a restoring force when a\n+pendulum is displaced sideways from its resting due to its gravity\n+that will accelerate it back toward the equilibrium position. A simple\n+pendulum consists of a small metal ball (called bob) suspended by a\n+long thread from a rigid support by a massless and inextensible\n+string, such that the bob is free to swing back and forth. When the\n+bob from its mean position is dragged to one side and then released,\n+the pendulum is set to motion and the bob moves oppositely on either\n+side of its mean position. And when the pendulum bob is displaced it\n+oscillates on a plane about the vertical line through the support.\n+\n+The Period (T) of a simple pendulum is the time taken for the bob to complete one\n+oscillation about the mean position.\n+\n+Period (T) of a simple pendulum is T=2\u03c0\u221aL/g.\n+\n+where :\n+T = time period of the pendulum (in seconds)\n+L = length of the massless and inextensible string (in meters)\n+g = acceleration due to gravity (value taken : 9.8 m/s^2)\n+\u03c0 = mathematical constant (value taken : 3.14159)\n+\n+Reference : https://unacademy.com/content/nda/study-material/physics/what-is-the-time-period/#:~:text=The%20formula%20for%20determining%20the,full%20back%20and%20forth%20swing.\n+\"\"\"\n+\n+\n+def simple_pendulum_time_period(l: float) -> float:",
    "comment": "please provide descriptive name for the parameter: l",
    "line_number": 32,
    "enriched": "File: physics/simple_pendulum_time_period.py\nCode: @@ -0,0 +1,58 @@\n+\"\"\"\n+Title : calculating the time period of a simple pendulum\n+\n+A simple pendulum can be defined as a device where its point mass\n+is attached to a light inextensible string and suspended from a fixed\n+support. Equilibrium position is subjected to a restoring force when a\n+pendulum is displaced sideways from its resting due to its gravity\n+that will accelerate it back toward the equilibrium position. A simple\n+pendulum consists of a small metal ball (called bob) suspended by a\n+long thread from a rigid support by a massless and inextensible\n+string, such that the bob is free to swing back and forth. When the\n+bob from its mean position is dragged to one side and then released,\n+the pendulum is set to motion and the bob moves oppositely on either\n+side of its mean position. And when the pendulum bob is displaced it\n+oscillates on a plane about the vertical line through the support.\n+\n+The Period (T) of a simple pendulum is the time taken for the bob to complete one\n+oscillation about the mean position.\n+\n+Period (T) of a simple pendulum is T=2\u03c0\u221aL/g.\n+\n+where :\n+T = time period of the pendulum (in seconds)\n+L = length of the massless and inextensible string (in meters)\n+g = acceleration due to gravity (value taken : 9.8 m/s^2)\n+\u03c0 = mathematical constant (value taken : 3.14159)\n+\n+Reference : https://unacademy.com/content/nda/study-material/physics/what-is-the-time-period/#:~:text=The%20formula%20for%20determining%20the,full%20back%20and%20forth%20swing.\n+\"\"\"\n+\n+\n+def simple_pendulum_time_period(l: float) -> float:\nComment: Please provide descriptive name for the parameter: `l`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "physics/simple_pendulum_time_period.py",
    "pr_number": 9734,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1346278609,
    "comment_created_at": "2023-10-04T18:13:03Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+from typing import List, Optional\n+\n+def binary_search(arr: List[int], target: int, low: int, high: int) -> Optional[int]:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/binary_search_recursive.py, please provide doctest for the function binary_search",
    "line_number": 3,
    "enriched": "File: searches/binary_search_recursive.py\nCode: @@ -0,0 +1,35 @@\n+from typing import List, Optional\n+\n+def binary_search(arr: List[int], target: int, low: int, high: int) -> Optional[int]:\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/binary_search_recursive.py`, please provide doctest for the function `binary_search`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "searches/binary_search_recursive.py",
    "pr_number": 10924,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1370819167,
    "comment_created_at": "2023-10-24T21:14:14Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+\"\"\"\n+Implements the Exponential Linear Unit or ELU function.\n+\n+The function takes a vector of K real numbers and a real number alpha as\n+input and then applies the ELU function to each element of the vector.\n+\n+Script inspired from its corresponding Wikipedia article\n+https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def elu_activation(vector: np.array, alpha: float) -> np.array:",
    "comment": "mypy is not happy with the type hint:\r\nnone\r\nfunction \"numpy.core.multiarray.array\" is not valid as a type  [valid-type]mypy(error)\r\n\r\ni guess you need to change to np.ndarray and give it a type parameter.",
    "line_number": 14,
    "enriched": "File: maths/elu_activation.py\nCode: @@ -0,0 +1,39 @@\n+\"\"\"\n+Implements the Exponential Linear Unit or ELU function.\n+\n+The function takes a vector of K real numbers and a real number alpha as\n+input and then applies the ELU function to each element of the vector.\n+\n+Script inspired from its corresponding Wikipedia article\n+https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def elu_activation(vector: np.array, alpha: float) -> np.array:\nComment: Mypy is not happy with the type hint:\r\n```none\r\nFunction \"numpy.core.multiarray.array\" is not valid as a type  [valid-type]mypy(error)\r\n```\r\nI guess you need to change to `np.ndarray` and give it a type parameter.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "maths/elu_activation.py",
    "pr_number": 8694,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1179788828,
    "comment_created_at": "2023-04-27T23:14:17Z"
  },
  {
    "code": "@@ -13,26 +22,19 @@\n \n     url = f\"https://www.google.com/search?q={query}&num=100\"\n \n-    res = requests.get(\n+    res = httpx.get(\n         url,\n         headers={\"User-Agent\": str(UserAgent().random)},\n         timeout=10,\n     )\n+    print(BeautifulSoup(res.text, \"html.parser\"))",
    "comment": "extra print. maybe this was added for debug purpose?",
    "line_number": 30,
    "enriched": "File: web_programming/open_google_results.py\nCode: @@ -13,26 +22,19 @@\n \n     url = f\"https://www.google.com/search?q={query}&num=100\"\n \n-    res = requests.get(\n+    res = httpx.get(\n         url,\n         headers={\"User-Agent\": str(UserAgent().random)},\n         timeout=10,\n     )\n+    print(BeautifulSoup(res.text, \"html.parser\"))\nComment: Extra `print`. Maybe this was added for debug purpose?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "web_programming/open_google_results.py",
    "pr_number": 12744,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2087663345,
    "comment_created_at": "2025-05-13T21:29:44Z"
  },
  {
    "code": "@@ -128,7 +128,11 @@ addopts = [\n ]\n \n [tool.coverage.report]\n-omit = [\".env/*\"]\n+omit = [\n+  \".env/*\",\n+  \"project_euler/*\",\n+  \"scripts/*\"",
    "comment": "let's leave scripts in.  i dislike not testing things.\r\n\r\n\r\ni think the coverage report is way too long and users need to scroll across hundreds of lines (that no one reads) to find their failing pytests.\r\n\r\nwould it be possible to put coverage in a separate github actions step so its output is not tagged directly on the end of pytest output?",
    "line_number": 134,
    "enriched": "File: pyproject.toml\nCode: @@ -128,7 +128,11 @@ addopts = [\n ]\n \n [tool.coverage.report]\n-omit = [\".env/*\"]\n+omit = [\n+  \".env/*\",\n+  \"project_euler/*\",\n+  \"scripts/*\"\nComment: Let's leave scripts in.  I dislike not testing things.\r\n```suggestion\r\n```\r\n\r\nI think the coverage report is way too long and users need to scroll across hundreds of lines (that no one reads) to find their failing pytests.\r\n\r\nWould it be possible to put coverage in a separate GitHub Actions step so its output is not tagged directly on the end of pytest output?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pyproject.toml",
    "pr_number": 10469,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359638328,
    "comment_created_at": "2023-10-14T20:30:13Z"
  },
  {
    "code": "@@ -0,0 +1,37 @@\n+\"\"\" A XNOR Gate is a logic gate in boolean algebra which results to false(0)\n+    if both the inputs are different and results to true(1) if both the inputs are same.\n+   Following is the truth table of an XNOR Gate:\n+   | Input 1 | Input 2 |  Output |\n+   |      0      |     0      |      1      |\n+   |      0      |     1      |      0      |\n+   |      1      |     0      |      0      |\n+   |      1      |     1      |      1      |\n+\"\"\"\n+\"\"\"Following is the code implementation of the XNOR Gate\"\"\"\n+\n+\n+def xnor_gate(input_1: int, input_2: int) -> int:\n+    \"\"\"\n+    >>> xnor_gate(0, 0)\n+    1\n+    >>> xnor_gate(0, 1)\n+    0\n+    >>> xnor_gate(1, 0)\n+    0\n+    >>> xnor_gate(1, 1)\n+    1\n+    \"\"\"\n+    return int((not(input_1 != input_2)))\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file boolean_algebra/xnorgate.py, please provide doctest for the function main",
    "line_number": 26,
    "enriched": "File: boolean_algebra/xnorgate.py\nCode: @@ -0,0 +1,37 @@\n+\"\"\" A XNOR Gate is a logic gate in boolean algebra which results to false(0)\n+    if both the inputs are different and results to true(1) if both the inputs are same.\n+   Following is the truth table of an XNOR Gate:\n+   | Input 1 | Input 2 |  Output |\n+   |      0      |     0      |      1      |\n+   |      0      |     1      |      0      |\n+   |      1      |     0      |      0      |\n+   |      1      |     1      |      1      |\n+\"\"\"\n+\"\"\"Following is the code implementation of the XNOR Gate\"\"\"\n+\n+\n+def xnor_gate(input_1: int, input_2: int) -> int:\n+    \"\"\"\n+    >>> xnor_gate(0, 0)\n+    1\n+    >>> xnor_gate(0, 1)\n+    0\n+    >>> xnor_gate(1, 0)\n+    0\n+    >>> xnor_gate(1, 1)\n+    1\n+    \"\"\"\n+    return int((not(input_1 != input_2)))\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `boolean_algebra/xnorgate.py`, please provide doctest for the function `main`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "boolean_algebra/xnorgate.py",
    "pr_number": 7221,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996287963,
    "comment_created_at": "2022-10-15T10:40:15Z"
  },
  {
    "code": "@@ -1,38 +1,69 @@\n \"\"\"\n-Reference: https://www.investopedia.com/terms/p/presentvalue.asp\n+Reference: https://www.auditexcel.co.za/blog/discounting-cash-flows-with-multiple-discount-rates/\n \n An algorithm that calculates the present value of a stream of yearly cash flows given...\n-1. The discount rate (as a decimal, not a percent)\n+1. An array containing tuples of discount rates and their associated duration.\n+\n+For example [(0.05, 2), (0.06, 3)] would mean that a discount rate of 5% is applied to\n+the first 2 cash flows, while a discount rate of 6% is applied to the next 3 cash flows.\n+\n+If the discount rate is fixed for the entire duration, then the user may provide just\n+the discount rate(as a float).\n+\n 2. An array of cash flows, with the index of the cash flow being the associated year\n \n Note: This algorithm assumes that cash flows are paid at the end of the specified year\n \"\"\"\n \n \n-def present_value(discount_rate: float, cash_flows: list[float]) -> float:\n+def present_value(discount_rates: list[tuple], cash_flows: list[float]) -> float:\n     \"\"\"\n     >>> present_value(0.13, [10, 20.70, -293, 297])",
    "comment": "i am confused as to why line 21 delivers 4.69 in the old code and 4.15 in the new.\r\nwill it do the same if 0.13 is changed to [0.13] on line 21?\r\ni would like us to keep the old tests (with modifications if required) to prove that the old and new are equivelent.",
    "line_number": 21,
    "enriched": "File: financial/present_value.py\nCode: @@ -1,38 +1,69 @@\n \"\"\"\n-Reference: https://www.investopedia.com/terms/p/presentvalue.asp\n+Reference: https://www.auditexcel.co.za/blog/discounting-cash-flows-with-multiple-discount-rates/\n \n An algorithm that calculates the present value of a stream of yearly cash flows given...\n-1. The discount rate (as a decimal, not a percent)\n+1. An array containing tuples of discount rates and their associated duration.\n+\n+For example [(0.05, 2), (0.06, 3)] would mean that a discount rate of 5% is applied to\n+the first 2 cash flows, while a discount rate of 6% is applied to the next 3 cash flows.\n+\n+If the discount rate is fixed for the entire duration, then the user may provide just\n+the discount rate(as a float).\n+\n 2. An array of cash flows, with the index of the cash flow being the associated year\n \n Note: This algorithm assumes that cash flows are paid at the end of the specified year\n \"\"\"\n \n \n-def present_value(discount_rate: float, cash_flows: list[float]) -> float:\n+def present_value(discount_rates: list[tuple], cash_flows: list[float]) -> float:\n     \"\"\"\n     >>> present_value(0.13, [10, 20.70, -293, 297])\nComment: I am confused as to why line 21 delivers `4.69` in the old code and `4.15` in the new.\r\nWill it do the same if `0.13` is changed to `[0.13]` on line 21?\r\nI would like us to keep the old tests (with modifications if required) to prove that the old and new are equivelent.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "financial/present_value.py",
    "pr_number": 8834,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1241931724,
    "comment_created_at": "2023-06-26T09:54:46Z"
  },
  {
    "code": "@@ -29,12 +29,20 @@ def is_prime(number: int) -> bool:\n     True\n     >>> is_prime(67483)\n     False\n+    >>> is_prime(16.1)\n+    Traceback (most recent call last):\n+        ...\n+    AssertionError: is_prime() only accepts positive integers\n+    >>> is_prime(-4)\n+    Traceback (most recent call last):\n+        ...\n+    AssertionError: is_prime() only accepts positive integers\n     \"\"\"\n \n     # precondition\n     assert isinstance(number, int) and (\n         number >= 0\n-    ), \"'number' must been an int and positive\"\n+    ), \"is_prime() only accepts positive integers\"",
    "comment": "could you change this assert to a valueerror instead?",
    "line_number": 45,
    "enriched": "File: maths/prime_check.py\nCode: @@ -29,12 +29,20 @@ def is_prime(number: int) -> bool:\n     True\n     >>> is_prime(67483)\n     False\n+    >>> is_prime(16.1)\n+    Traceback (most recent call last):\n+        ...\n+    AssertionError: is_prime() only accepts positive integers\n+    >>> is_prime(-4)\n+    Traceback (most recent call last):\n+        ...\n+    AssertionError: is_prime() only accepts positive integers\n     \"\"\"\n \n     # precondition\n     assert isinstance(number, int) and (\n         number >= 0\n-    ), \"'number' must been an int and positive\"\n+    ), \"is_prime() only accepts positive integers\"\nComment: Could you change this `assert` to a `ValueError` instead?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "maths/prime_check.py",
    "pr_number": 10930,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1372281983,
    "comment_created_at": "2023-10-25T20:18:37Z"
  },
  {
    "code": "@@ -0,0 +1,91 @@\n+class Node:\r\n+    def __init__(self, value: int = 0) -> None:\r\n+        self.value = value\r\n+        self.left = None\r\n+        self.right = None\r\n+\r\n+\r\n+class PersistentSegmentTree:\r\n+    def __init__(self, arr: list[int]) -> None:\r\n+        \"\"\"\r\n+        Initialize the Persistent Segment Tree with the given array.\r\n+\r\n+        >>> pst = PersistentSegmentTree([1, 2, 3])\r\n+        >>> pst.query(0, 0, 2)\r\n+        6\r\n+        \"\"\"\r\n+        self.n = len(arr)\r\n+        self.roots: list[Node] = []\r\n+        self.roots.append(self._build(arr, 0, self.n - 1))\r\n+\r\n+    def _build(self, arr: list[int], start: int, end: int) -> Node:\r",
    "comment": "as there is no test file in this pull request nor any test function or class in the file data_structures/persistent_segment_tree.py, please provide doctest for the function _build",
    "line_number": 21,
    "enriched": "File: data_structures/persistent_segment_tree.py\nCode: @@ -0,0 +1,91 @@\n+class Node:\r\n+    def __init__(self, value: int = 0) -> None:\r\n+        self.value = value\r\n+        self.left = None\r\n+        self.right = None\r\n+\r\n+\r\n+class PersistentSegmentTree:\r\n+    def __init__(self, arr: list[int]) -> None:\r\n+        \"\"\"\r\n+        Initialize the Persistent Segment Tree with the given array.\r\n+\r\n+        >>> pst = PersistentSegmentTree([1, 2, 3])\r\n+        >>> pst.query(0, 0, 2)\r\n+        6\r\n+        \"\"\"\r\n+        self.n = len(arr)\r\n+        self.roots: list[Node] = []\r\n+        self.roots.append(self._build(arr, 0, self.n - 1))\r\n+\r\n+    def _build(self, arr: list[int], start: int, end: int) -> Node:\r\nComment: As there is no test file in this pull request nor any test function or class in the file `data_structures/persistent_segment_tree.py`, please provide doctest for the function `_build`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/persistent_segment_tree.py",
    "pr_number": 12178,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1807286294,
    "comment_created_at": "2024-10-19T10:47:13Z"
  },
  {
    "code": "@@ -0,0 +1,34 @@\n+def get_highest_set_bit_position(number: int) -> int:\n+    \"\"\"\n+    Returns position of the highest set bit of a number.\n+    Ref - https://graphics.stanford.edu/~seander/bithacks.html#IntegerLogObvious\n+    >>> get_highest_set_bit_position(25)\n+    5\n+    >>> get_highest_set_bit_position(37)\n+    6\n+    >>> get_highest_set_bit_position(1)\n+    1\n+    >>> get_highest_set_bit_position(4)\n+    3\n+    >>> get_highest_set_bit_position(0.8)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value must be an 'int' type\n+    \"\"\"\n+    if not isinstance(number, int):\n+        raise TypeError(\"Input value must be an 'int' type\")\n+\n+    position = 0\n+    while number:\n+        position += 1\n+        number >>= 1\n+        if number == 0:\n+            break",
    "comment": "what happens if we remove these two lines?",
    "line_number": 26,
    "enriched": "File: bit_manipulation/highest_set_bit.py\nCode: @@ -0,0 +1,34 @@\n+def get_highest_set_bit_position(number: int) -> int:\n+    \"\"\"\n+    Returns position of the highest set bit of a number.\n+    Ref - https://graphics.stanford.edu/~seander/bithacks.html#IntegerLogObvious\n+    >>> get_highest_set_bit_position(25)\n+    5\n+    >>> get_highest_set_bit_position(37)\n+    6\n+    >>> get_highest_set_bit_position(1)\n+    1\n+    >>> get_highest_set_bit_position(4)\n+    3\n+    >>> get_highest_set_bit_position(0.8)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value must be an 'int' type\n+    \"\"\"\n+    if not isinstance(number, int):\n+        raise TypeError(\"Input value must be an 'int' type\")\n+\n+    position = 0\n+    while number:\n+        position += 1\n+        number >>= 1\n+        if number == 0:\n+            break\nComment: What happens if we remove these two lines?\r\n```suggestion\r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "bit_manipulation/highest_set_bit.py",
    "pr_number": 7586,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1004979052,
    "comment_created_at": "2022-10-25T21:26:27Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+def is_happy_number(number: int) -> bool:\r\n+    \"\"\"\r\n+    Check if a number is a happy number.\r\n+    https://en.wikipedia.org/wiki/Happy_number\r\n+     A happy number is defined by the following process:\r\n+    1. Starting with any positive integer, replace the number by\r\n+       the sum of the squares of its digits.\r\n+    2. Repeat the process until the number equals 1 (happy) or\r\n+       it loops endlessly in a cycle (not happy).\r\n+\r\n+    Args:\r\n+        n (int): The number to check for happiness.\r\n+\r\n+    Returns:\r\n+        bool: True if the number is a happy number, False otherwise.\r\n+\r\n+    Examples:\r\n+    >>> is_happy_number(19)\r\n+    True\r\n+    >>> is_happy_number(4)\r\n+    False\r\n+    >>> is_happy_number(23)\r\n+    True\r",
    "comment": "please add tests the break things.",
    "line_number": 23,
    "enriched": "File: maths/special_numbers/happy_number.py\nCode: @@ -0,0 +1,39 @@\n+def is_happy_number(number: int) -> bool:\r\n+    \"\"\"\r\n+    Check if a number is a happy number.\r\n+    https://en.wikipedia.org/wiki/Happy_number\r\n+     A happy number is defined by the following process:\r\n+    1. Starting with any positive integer, replace the number by\r\n+       the sum of the squares of its digits.\r\n+    2. Repeat the process until the number equals 1 (happy) or\r\n+       it loops endlessly in a cycle (not happy).\r\n+\r\n+    Args:\r\n+        n (int): The number to check for happiness.\r\n+\r\n+    Returns:\r\n+        bool: True if the number is a happy number, False otherwise.\r\n+\r\n+    Examples:\r\n+    >>> is_happy_number(19)\r\n+    True\r\n+    >>> is_happy_number(4)\r\n+    False\r\n+    >>> is_happy_number(23)\r\n+    True\r\nComment: Please add tests the break things.\r\n```suggestion\r\n    True\r\n    >>> is_happy_number(0)\r\n    ?\r\n    >>> is_happy_number(-19)\r\n    ?\r\n    >>> is_happy_number(19.1)\r\n    ?\r\n    >>> is_happy_number(\"Happy\")\r\n    ?\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/special_numbers/happy_number.py",
    "pr_number": 10864,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375818540,
    "comment_created_at": "2023-10-30T08:13:23Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+\"\"\"\n+References for Binary, Octal, and Hexadecimal Numbers\n+\n+https://en.wikipedia.org/wiki/Binary_number\n+https://en.wikipedia.org/wiki/Octal\n+https://en.wikipedia.org/wiki/Hexadecimal\n+\n+\"\"\"\n+ \n+\n+def decimal_conversions(dec: int) -> int:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file maths/decimal_conversions.py, please provide doctest for the function decimal_conversions",
    "line_number": 11,
    "enriched": "File: maths/decimal_conversions.py\nCode: @@ -0,0 +1,56 @@\n+\"\"\"\n+References for Binary, Octal, and Hexadecimal Numbers\n+\n+https://en.wikipedia.org/wiki/Binary_number\n+https://en.wikipedia.org/wiki/Octal\n+https://en.wikipedia.org/wiki/Hexadecimal\n+\n+\"\"\"\n+ \n+\n+def decimal_conversions(dec: int) -> int:\nComment: As there is no test file in this pull request nor any test function or class in the file `maths/decimal_conversions.py`, please provide doctest for the function `decimal_conversions`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/decimal_conversions.py",
    "pr_number": 7252,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996339887,
    "comment_created_at": "2022-10-15T18:30:40Z"
  },
  {
    "code": "@@ -29,12 +28,14 @@ def __init__(self, key: int = 0):\n         # private field\n         self.__key = key\n \n-    def encrypt(self, content: str, key: int) -> list[str]:\n+    def encrypt(self, content: str, key: int, type_: int) -> list[str] | str:",
    "comment": "why don't you overload and typehint type_ to either literal 0 or literal 1",
    "line_number": 31,
    "enriched": "File: ciphers/xor_cipher.py\nCode: @@ -29,12 +28,14 @@ def __init__(self, key: int = 0):\n         # private field\n         self.__key = key\n \n-    def encrypt(self, content: str, key: int) -> list[str]:\n+    def encrypt(self, content: str, key: int, type_: int) -> list[str] | str:\nComment: Why don't you overload and typehint `type_` to either literal `0` or literal `1`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "ciphers/xor_cipher.py",
    "pr_number": 7112,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 995861312,
    "comment_created_at": "2022-10-14T15:02:46Z"
  },
  {
    "code": "@@ -0,0 +1,151 @@\n+\"\"\"\n+________________________________________________________________________________________\n+The first law of thermodynamics states that, when energy passes into or out of a system\n+(as work, heat, or matter), the system's internal energy changes in accordance with the\n+law of conservation of energy. This also results in the observation that, in an\n+externally isolated system, even with internal changes, the sum of all forms of energy\n+must remain constant, as energy cannot be created or destroyed.\n+\n+Check out the formula used to calculate this flux:\n+ --------------\n+ | Q = \u0394U + W |\n+ --------------\n+\n+Q = heat added or removed from the system.\n+\u0394U = variation of internal energy of the system.\n+W = work done by the system on its surroundings.\n+\n+OBS: All units must be equal to each other.\n+(Description adapted from https://en.wikipedia.org/wiki/Laws_of_thermodynamics )\n+\"\"\"\n+\n+\n+def check_args(argument: float) -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file physics/first_law_of_thermodynamics.py, please provide doctest for the function check_args",
    "line_number": 23,
    "enriched": "File: physics/first_law_of_thermodynamics.py\nCode: @@ -0,0 +1,151 @@\n+\"\"\"\n+________________________________________________________________________________________\n+The first law of thermodynamics states that, when energy passes into or out of a system\n+(as work, heat, or matter), the system's internal energy changes in accordance with the\n+law of conservation of energy. This also results in the observation that, in an\n+externally isolated system, even with internal changes, the sum of all forms of energy\n+must remain constant, as energy cannot be created or destroyed.\n+\n+Check out the formula used to calculate this flux:\n+ --------------\n+ | Q = \u0394U + W |\n+ --------------\n+\n+Q = heat added or removed from the system.\n+\u0394U = variation of internal energy of the system.\n+W = work done by the system on its surroundings.\n+\n+OBS: All units must be equal to each other.\n+(Description adapted from https://en.wikipedia.org/wiki/Laws_of_thermodynamics )\n+\"\"\"\n+\n+\n+def check_args(argument: float) -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `physics/first_law_of_thermodynamics.py`, please provide doctest for the function `check_args`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "physics/first_law_of_thermodynamics.py",
    "pr_number": 13266,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2406675894,
    "comment_created_at": "2025-10-06T14:30:32Z"
  },
  {
    "code": "@@ -112,17 +112,19 @@ def strassen(matrix1: list, matrix2: list) -> list:\n     [[139, 163], [121, 134], [100, 121]]\n     \"\"\"\n     if matrix_dimensions(matrix1)[1] != matrix_dimensions(matrix2)[0]:\n-        raise Exception(\n-            \"Unable to multiply these matrices, please check the dimensions. \\n\"\n-            f\"Matrix A:{matrix1} \\nMatrix B:{matrix2}\"\n+        msg = (\n+            \"Unable to multiply these matrices, please check the dimensions.\\n\"\n+            f\"Matrix A: {matrix1}\\n\"\n+            f\"Matrix B: {matrix2}\"\n         )\n+        raise Exception(msg)",
    "comment": "what is the point of this change? i preferred it without this change",
    "line_number": 120,
    "enriched": "File: divide_and_conquer/strassen_matrix_multiplication.py\nCode: @@ -112,17 +112,19 @@ def strassen(matrix1: list, matrix2: list) -> list:\n     [[139, 163], [121, 134], [100, 121]]\n     \"\"\"\n     if matrix_dimensions(matrix1)[1] != matrix_dimensions(matrix2)[0]:\n-        raise Exception(\n-            \"Unable to multiply these matrices, please check the dimensions. \\n\"\n-            f\"Matrix A:{matrix1} \\nMatrix B:{matrix2}\"\n+        msg = (\n+            \"Unable to multiply these matrices, please check the dimensions.\\n\"\n+            f\"Matrix A: {matrix1}\\n\"\n+            f\"Matrix B: {matrix2}\"\n         )\n+        raise Exception(msg)\nComment: What is the point of this change? I preferred it without this change ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "divide_and_conquer/strassen_matrix_multiplication.py",
    "pr_number": 8784,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1209382911,
    "comment_created_at": "2023-05-29T14:48:47Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+\"\"\"\n+# Git Push to Slack Notifier\n+\n+This Python script allows you to send a message to a Slack channel when a Git push event occurs in your repository. You can use this script to receive notifications about code changes directly in your Slack workspace.\n+\n+## Prerequisites\n+\n+Before using this script, make sure you have the following:\n+\n+- Python 3.x installed on your system.\n+- The `requests` library installed. You can install it using `pip`:\n+\n+   ```bash\n+   pip install requests\n+\n+\"\"\"\n+\n+import requests\n+import json\n+\n+# Your Slack webhook URL\n+slack_webhook_url = \"YOUR_SLACK_WEBHOOK_URL\"\n+\n+# Define the message to send to Slack\n+push_message = {\n+    \"text\": \"A Git push event has occurred!\",\n+    \"attachments\": [\n+        {\n+            \"color\": \"#36a64f\",  # Slack attachment color\n+            \"title\": \"Git Push Event\",\n+            \"fields\": [\n+                {\"title\": \"Repository\", \"value\": \"Your Repository Name\", \"short\": True},\n+                {\"title\": \"Branch\", \"value\": \"Branch Name\", \"short\": True},\n+                {\"title\": \"Committer\", \"value\": \"Committer Name\", \"short\": True},\n+            ],\n+        }\n+    ],\n+}\n+\n+# Send the message to Slack\n+response = requests.post(\n+    slack_webhook_url,\n+    data=json.dumps(push_message),",
    "comment": "i thought that requests.post() would automatically do the json.dumps() piece for you.\r\nhttps://requests.readthedocs.io/en/latest/api/#requests.post",
    "line_number": 43,
    "enriched": "File: web_programming/git_to_slack.py\nCode: @@ -0,0 +1,53 @@\n+\"\"\"\n+# Git Push to Slack Notifier\n+\n+This Python script allows you to send a message to a Slack channel when a Git push event occurs in your repository. You can use this script to receive notifications about code changes directly in your Slack workspace.\n+\n+## Prerequisites\n+\n+Before using this script, make sure you have the following:\n+\n+- Python 3.x installed on your system.\n+- The `requests` library installed. You can install it using `pip`:\n+\n+   ```bash\n+   pip install requests\n+\n+\"\"\"\n+\n+import requests\n+import json\n+\n+# Your Slack webhook URL\n+slack_webhook_url = \"YOUR_SLACK_WEBHOOK_URL\"\n+\n+# Define the message to send to Slack\n+push_message = {\n+    \"text\": \"A Git push event has occurred!\",\n+    \"attachments\": [\n+        {\n+            \"color\": \"#36a64f\",  # Slack attachment color\n+            \"title\": \"Git Push Event\",\n+            \"fields\": [\n+                {\"title\": \"Repository\", \"value\": \"Your Repository Name\", \"short\": True},\n+                {\"title\": \"Branch\", \"value\": \"Branch Name\", \"short\": True},\n+                {\"title\": \"Committer\", \"value\": \"Committer Name\", \"short\": True},\n+            ],\n+        }\n+    ],\n+}\n+\n+# Send the message to Slack\n+response = requests.post(\n+    slack_webhook_url,\n+    data=json.dumps(push_message),\nComment: I thought that `requests.post()` would automatically do the `json.dumps()` piece for you.\r\nhttps://requests.readthedocs.io/en/latest/api/#requests.post",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "web_programming/git_to_slack.py",
    "pr_number": 11087,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375709135,
    "comment_created_at": "2023-10-30T06:07:10Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+# https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm\n+\n+from typing import List\n+\n+def knuth_morris_pratt(text: str, pattern: str) -> List[int]:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file searches/knuth_morris_pratt_algorithm.py, please provide doctest for the function knuth_morris_pratt",
    "line_number": 5,
    "enriched": "File: searches/knuth_morris_pratt_algorithm.py\nCode: @@ -0,0 +1,61 @@\n+# https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm\n+\n+from typing import List\n+\n+def knuth_morris_pratt(text: str, pattern: str) -> List[int]:\nComment: As there is no test file in this pull request nor any test function or class in the file `searches/knuth_morris_pratt_algorithm.py`, please provide doctest for the function `knuth_morris_pratt`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "searches/knuth_morris_pratt_algorithm.py",
    "pr_number": 9361,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342285511,
    "comment_created_at": "2023-10-02T05:07:00Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+\"\"\"\r\n+In geometry, Stewart's theorem yields a relation between the\r\n+lengths of the sides and the length of a cevian in a triangle.\r\n+Its name is in honour of the Scottish mathematician Matthew\r\n+Stewart, who published the theorem in 1746.[1]\r\n+\r\n+Source: https://en.wikipedia.org/wiki/Stewart%27s_theorem\r\n+\"\"\"\r\n+\r\n+\r\n+def stewart(a: float, b: float, c: float, n: float, m: float) -> float:\r",
    "comment": "please provide descriptive name for the parameter: a\n\nplease provide descriptive name for the parameter: b\n\nplease provide descriptive name for the parameter: c\n\nplease provide descriptive name for the parameter: n\n\nplease provide descriptive name for the parameter: m",
    "line_number": 11,
    "enriched": "File: maths/stewart.py\nCode: @@ -0,0 +1,42 @@\n+\"\"\"\r\n+In geometry, Stewart's theorem yields a relation between the\r\n+lengths of the sides and the length of a cevian in a triangle.\r\n+Its name is in honour of the Scottish mathematician Matthew\r\n+Stewart, who published the theorem in 1746.[1]\r\n+\r\n+Source: https://en.wikipedia.org/wiki/Stewart%27s_theorem\r\n+\"\"\"\r\n+\r\n+\r\n+def stewart(a: float, b: float, c: float, n: float, m: float) -> float:\r\nComment: Please provide descriptive name for the parameter: `a`\n\nPlease provide descriptive name for the parameter: `b`\n\nPlease provide descriptive name for the parameter: `c`\n\nPlease provide descriptive name for the parameter: `n`\n\nPlease provide descriptive name for the parameter: `m`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/stewart.py",
    "pr_number": 9887,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1348162734,
    "comment_created_at": "2023-10-06T01:49:29Z"
  },
  {
    "code": "@@ -11,75 +11,54 @@\n from collections import defaultdict\n \n \n-class AssignmentUsingBitmask:\n-    def __init__(self, task_performed, total):\n-        self.total_tasks = total  # total no of tasks (N)\n-\n-        # DP table will have a dimension of (2^M)*N\n-        # initially all values are set to -1\n+class TaskAssignment:\n+    def __init__(self, task_performed, total_tasks):",
    "comment": "no type hint, no doctests?",
    "line_number": 15,
    "enriched": "File: dynamic_programming/bitmask.py\nCode: @@ -11,75 +11,54 @@\n from collections import defaultdict\n \n \n-class AssignmentUsingBitmask:\n-    def __init__(self, task_performed, total):\n-        self.total_tasks = total  # total no of tasks (N)\n-\n-        # DP table will have a dimension of (2^M)*N\n-        # initially all values are set to -1\n+class TaskAssignment:\n+    def __init__(self, task_performed, total_tasks):\nComment: No type hint, no doctests?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "dynamic_programming/bitmask.py",
    "pr_number": 9699,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1345963404,
    "comment_created_at": "2023-10-04T14:58:18Z"
  },
  {
    "code": "@@ -0,0 +1,100 @@\n+\"\"\"\n+Brent's Method for root finding.\n+\n+This function implements Brent's Method, an efficient algorithm for finding the\n+root of a function. It combines the bisection method, the secant method, and\n+inverse quadratic interpolation.\n+\n+Reference:\n+- https://en.wikipedia.org/wiki/Brent%27s_method\n+- https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brentq.html\n+\n+\n+>>> def f(x): return x**3 - x - 2\n+>>> round(brent_method(f, 1, 2), 6)\n+1.52138\n+>>> brent_method(f, 1, 1.5)  # No sign change, should raise an error\n+Traceback (most recent call last):\n+    ...\n+ValueError: f(a) and f(b) must have different signs\n+\"\"\"\n+\n+from collections.abc import Callable\n+\n+\n+def brent_method(\n+    f: Callable[[float], float],",
    "comment": "please provide descriptive name for the parameter: f",
    "line_number": 26,
    "enriched": "File: maths/numerical_analysis/brent_method.py\nCode: @@ -0,0 +1,100 @@\n+\"\"\"\n+Brent's Method for root finding.\n+\n+This function implements Brent's Method, an efficient algorithm for finding the\n+root of a function. It combines the bisection method, the secant method, and\n+inverse quadratic interpolation.\n+\n+Reference:\n+- https://en.wikipedia.org/wiki/Brent%27s_method\n+- https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brentq.html\n+\n+\n+>>> def f(x): return x**3 - x - 2\n+>>> round(brent_method(f, 1, 2), 6)\n+1.52138\n+>>> brent_method(f, 1, 1.5)  # No sign change, should raise an error\n+Traceback (most recent call last):\n+    ...\n+ValueError: f(a) and f(b) must have different signs\n+\"\"\"\n+\n+from collections.abc import Callable\n+\n+\n+def brent_method(\n+    f: Callable[[float], float],\nComment: Please provide descriptive name for the parameter: `f`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "maths/numerical_analysis/brent_method.py",
    "pr_number": 13089,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2397200227,
    "comment_created_at": "2025-10-02T06:25:49Z"
  },
  {
    "code": "@@ -0,0 +1,80 @@\n+import doctest\n+\n+import numpy as np\n+from matplotlib import pyplot as plt\n+from sklearn.datasets import load_iris\n+from sklearn.ensemble import AdaBoostClassifier\n+from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n+from sklearn.model_selection import train_test_split\n+\n+\"\"\"\n+AdaBoost or Adaptive Boosting, is an ensemble learning technique in machine learning.\n+It operates by combining multiple weak learners,in an iterative manner.\n+AdaBoost assigns varying weights to data points,\n+prioritizing misclassified samples with each iteration.\n+\"\"\"\n+\n+def data_handling(data: dict) -> tuple:\n+    # Split dataset into features and target\n+    # data is features\n+    \"\"\"\n+    Split dataset into features and target.\n+    >>> data_handling({'data':'[5.1, 3.5, 1.4, 0.2]','target':([0])})\n+    ('[5.1, 3.5, 1.4, 0.2]', [0])\n+    >>> data_handling({'data': '[4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2]',\n+    'target': ([0, 0])})\n+    ('[4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2]', [0, 0])\n+    \"\"\"\n+    return (data[\"data\"], data[\"target\"])\n+\n+def adaboost(features: np.ndarray, target: np.ndarray) -> AdaBoostClassifier:\n+    \"\"\"\n+    Initialize and train an AdaBoost classifier.\n+\n+    >>> adaboost(np.array([[5.1, 3.6, 1.4, 0.2]]), np.array([0]))\n+    AdaBoostClassifier(...)\n+    \"\"\"\n+    classifier = AdaBoostClassifier()\n+    classifier.fit(features, target)\n+    return classifier\n+\n+def main() -> None:",
    "comment": "as there is no test file in this pull request nor any test function or class in the file machine_learning/adaboost_classifier.py, please provide doctest for the function main",
    "line_number": 41,
    "enriched": "File: machine_learning/adaboost_classifier.py\nCode: @@ -0,0 +1,80 @@\n+import doctest\n+\n+import numpy as np\n+from matplotlib import pyplot as plt\n+from sklearn.datasets import load_iris\n+from sklearn.ensemble import AdaBoostClassifier\n+from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n+from sklearn.model_selection import train_test_split\n+\n+\"\"\"\n+AdaBoost or Adaptive Boosting, is an ensemble learning technique in machine learning.\n+It operates by combining multiple weak learners,in an iterative manner.\n+AdaBoost assigns varying weights to data points,\n+prioritizing misclassified samples with each iteration.\n+\"\"\"\n+\n+def data_handling(data: dict) -> tuple:\n+    # Split dataset into features and target\n+    # data is features\n+    \"\"\"\n+    Split dataset into features and target.\n+    >>> data_handling({'data':'[5.1, 3.5, 1.4, 0.2]','target':([0])})\n+    ('[5.1, 3.5, 1.4, 0.2]', [0])\n+    >>> data_handling({'data': '[4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2]',\n+    'target': ([0, 0])})\n+    ('[4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2]', [0, 0])\n+    \"\"\"\n+    return (data[\"data\"], data[\"target\"])\n+\n+def adaboost(features: np.ndarray, target: np.ndarray) -> AdaBoostClassifier:\n+    \"\"\"\n+    Initialize and train an AdaBoost classifier.\n+\n+    >>> adaboost(np.array([[5.1, 3.6, 1.4, 0.2]]), np.array([0]))\n+    AdaBoostClassifier(...)\n+    \"\"\"\n+    classifier = AdaBoostClassifier()\n+    classifier.fit(features, target)\n+    return classifier\n+\n+def main() -> None:\nComment: As there is no test file in this pull request nor any test function or class in the file `machine_learning/adaboost_classifier.py`, please provide doctest for the function `main`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "machine_learning/adaboost_classifier.py",
    "pr_number": 10511,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359845931,
    "comment_created_at": "2023-10-15T09:59:59Z"
  },
  {
    "code": "@@ -0,0 +1,33 @@\n+\"\"\"\n+Example to show how this method works\n+\n+>>> swap_case (\"Hello\")\n+hELLO\n+\n+>>> swap_case (\"water\")\n+WATER\n+\"\"\"\n+\n+\n+def swap_case(input_string) -> str:",
    "comment": "please provide type hint for the parameter: input_string",
    "line_number": 12,
    "enriched": "File: strings/swapcase.py\nCode: @@ -0,0 +1,33 @@\n+\"\"\"\n+Example to show how this method works\n+\n+>>> swap_case (\"Hello\")\n+hELLO\n+\n+>>> swap_case (\"water\")\n+WATER\n+\"\"\"\n+\n+\n+def swap_case(input_string) -> str:\nComment: Please provide type hint for the parameter: `input_string`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "strings/swapcase.py",
    "pr_number": 11745,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1787663750,
    "comment_created_at": "2024-10-04T12:45:46Z"
  },
  {
    "code": "@@ -0,0 +1,100 @@\n+\"\"\"\n+Range Maximum Query Problem:\n+\n+    Given an array of integers and q queries,\n+    For each query find the maximum value in the range [l, r).\n+\n+    The array is 0-indexed and the queries are 0-indexed.\n+\n+    More details:\n+        https://www.geeksforgeeks.org/range-maximum-query-using-sparse-table/\n+\"\"\"\n+\n+\n+class RangeMaximumQuery:\n+    def __init__(self, array: list) -> None:\n+        \"\"\"\n+        Initialize RangeMaximumQuery with given array.\n+\n+        Parameters:\n+            array: list[int]\n+\n+        Example:\n+        >>> rmq = RangeMaximumQuery([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n+        \"\"\"\n+        self.array = array\n+        self.size = len(array)",
    "comment": "please define a [__len__() function](https://docs.python.org/3/reference/datamodel.html#object.__len__) instead of using a size variable.",
    "line_number": 26,
    "enriched": "File: dynamic_programming/range_maximum_query.py\nCode: @@ -0,0 +1,100 @@\n+\"\"\"\n+Range Maximum Query Problem:\n+\n+    Given an array of integers and q queries,\n+    For each query find the maximum value in the range [l, r).\n+\n+    The array is 0-indexed and the queries are 0-indexed.\n+\n+    More details:\n+        https://www.geeksforgeeks.org/range-maximum-query-using-sparse-table/\n+\"\"\"\n+\n+\n+class RangeMaximumQuery:\n+    def __init__(self, array: list) -> None:\n+        \"\"\"\n+        Initialize RangeMaximumQuery with given array.\n+\n+        Parameters:\n+            array: list[int]\n+\n+        Example:\n+        >>> rmq = RangeMaximumQuery([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n+        \"\"\"\n+        self.array = array\n+        self.size = len(array)\nComment: Please define a [`__len__()` function](https://docs.python.org/3/reference/datamodel.html#object.__len__) instead of using a `size` variable.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "dynamic_programming/range_maximum_query.py",
    "pr_number": 8056,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1057915910,
    "comment_created_at": "2022-12-27T21:07:57Z"
  },
  {
    "code": "@@ -0,0 +1,63 @@\n+\"\"\"\n+Title : computing the Reynolds number to find\n+        out the type of flow (laminar or turbulent)\n+\n+Reynolds number is a dimensionless quantity that is used to determine\n+the type of flow pattern as laminar or turbulent while flowing through a\n+pipe. Reynolds number is defined by the ratio of inertial forces to that of\n+viscous forces.\n+\n+R = Inertial Forces / Viscous Forces\n+R = (\u03c1 * V * D)/\u03bc\n+\n+where :\n+\u03c1 = Density of fluid (in Kg/m^3)\n+D = Diameter of pipe through which fluid flows (in m)\n+V = Velocity of flow of the fluid (in m/s)\n+\u03bc = Viscosity of the fluid (in Ns/m^2)\n+\n+If the Reynolds number calculated is high (greater than 2000), then the\n+flow through the pipe is said to be turbulent. If Reynolds number is low\n+(less than 2000), the flow is said to be laminar. Numerically, these are\n+acceptable values, although in general the laminar and turbulent flows\n+are classified according to a range. Laminar flow falls below Reynolds\n+number of 1100 and turbulent falls in a range greater than 2200.\n+Laminar flow is the type of flow in which the fluid travels smoothly in\n+regular paths. Conversely, turbulent flow isn't smooth and follows an\n+irregular math with lots of mixing.",
    "comment": "typo",
    "line_number": 27,
    "enriched": "File: physics/reynolds_number.py\nCode: @@ -0,0 +1,63 @@\n+\"\"\"\n+Title : computing the Reynolds number to find\n+        out the type of flow (laminar or turbulent)\n+\n+Reynolds number is a dimensionless quantity that is used to determine\n+the type of flow pattern as laminar or turbulent while flowing through a\n+pipe. Reynolds number is defined by the ratio of inertial forces to that of\n+viscous forces.\n+\n+R = Inertial Forces / Viscous Forces\n+R = (\u03c1 * V * D)/\u03bc\n+\n+where :\n+\u03c1 = Density of fluid (in Kg/m^3)\n+D = Diameter of pipe through which fluid flows (in m)\n+V = Velocity of flow of the fluid (in m/s)\n+\u03bc = Viscosity of the fluid (in Ns/m^2)\n+\n+If the Reynolds number calculated is high (greater than 2000), then the\n+flow through the pipe is said to be turbulent. If Reynolds number is low\n+(less than 2000), the flow is said to be laminar. Numerically, these are\n+acceptable values, although in general the laminar and turbulent flows\n+are classified according to a range. Laminar flow falls below Reynolds\n+number of 1100 and turbulent falls in a range greater than 2200.\n+Laminar flow is the type of flow in which the fluid travels smoothly in\n+regular paths. Conversely, turbulent flow isn't smooth and follows an\n+irregular math with lots of mixing.\nComment: ```suggestion\r\nirregular path with lots of mixing.\r\n```\r\nTypo",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "physics/reynolds_number.py",
    "pr_number": 9913,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1351503035,
    "comment_created_at": "2023-10-10T05:15:55Z"
  },
  {
    "code": "@@ -0,0 +1,73 @@\n+# https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm\n+\n+\n+def knuth_morris_pratt(text: str, pattern: str) -> list[int]:",
    "comment": "there are a couple edgecases you could add support for:\r\n- an empty pattern\r\n- a pattern longer than the text",
    "line_number": 4,
    "enriched": "File: searches/knuth_morris_pratt_algorithm.py\nCode: @@ -0,0 +1,73 @@\n+# https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm\n+\n+\n+def knuth_morris_pratt(text: str, pattern: str) -> list[int]:\nComment: There are a couple edgecases you could add support for:\r\n- an empty pattern\r\n- a pattern longer than the text",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "searches/knuth_morris_pratt_algorithm.py",
    "pr_number": 9366,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1792539943,
    "comment_created_at": "2024-10-08T21:35:24Z"
  },
  {
    "code": "@@ -0,0 +1,38 @@\n+\"\"\"\n+Squareplus Activation Function\n+\n+Use Case: Squareplus designed to enhance positive values and suppress negative values.\n+For more detailed information, you can refer to the following link:\n+https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Squareplus\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def squareplus(vector: np.ndarray, beta: float) -> np.ndarray:\n+    \"\"\"\n+    Implements the SquarePlus activation function.\n+\n+    Parameters:\n+        vector (np.ndarray): The input array for the SquarePlus activation.\n+        beta (float): size of the curved region\n+\n+    Returns:\n+        np.ndarray: The input array after applying the SquarePlus activation.\n+\n+    Formula: f(x) = ( x + (x^2 + b)**0.5 ) / 2",
    "comment": "small fix in the formula to make it more readable",
    "line_number": 23,
    "enriched": "File: neural_network/activation_functions/squareplus.py\nCode: @@ -0,0 +1,38 @@\n+\"\"\"\n+Squareplus Activation Function\n+\n+Use Case: Squareplus designed to enhance positive values and suppress negative values.\n+For more detailed information, you can refer to the following link:\n+https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Squareplus\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def squareplus(vector: np.ndarray, beta: float) -> np.ndarray:\n+    \"\"\"\n+    Implements the SquarePlus activation function.\n+\n+    Parameters:\n+        vector (np.ndarray): The input array for the SquarePlus activation.\n+        beta (float): size of the curved region\n+\n+    Returns:\n+        np.ndarray: The input array after applying the SquarePlus activation.\n+\n+    Formula: f(x) = ( x + (x^2 + b)**0.5 ) / 2\nComment: ```suggestion\r\n    Formula: f(x) = ( x + sqrt(x^2 + b) ) / 2\r\n```\r\nSmall fix in the formula to make it more readable",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "neural_network/activation_functions/squareplus.py",
    "pr_number": 9977,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349739937,
    "comment_created_at": "2023-10-08T16:05:54Z"
  },
  {
    "code": "@@ -28,6 +28,59 @@ def find_min(nums: list[int | float]) -> int | float:\n     return min_num\n \n \n+# Divide and Conquer algorithm\n+def find_min_recursive(nums: list[int | float], left: int, right: int) -> int | float:\n+    \"\"\"\n+    find min value in list\n+    :param nums: contains elements\n+    :param left: index of first element\n+    :param right: index of last element\n+    :return: min in nums\n+\n+    >>> for nums in ([3, 2, 1], [-3, -2, -1], [3, -3, 0], [3.0, 3.1, 2.9]):\n+    ...     find_min_recursive(nums, 0, len(nums) - 1) == min(nums)\n+    True\n+    True\n+    True\n+    True\n+    >>> nums = [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\n+    >>> find_min_recursive(nums, 0, len(nums) - 1) == min(nums)\n+    True\n+    >>> find_min_recursive([], 0, 0)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: find_min_recursive() arg is an empty sequence\n+    >>> find_min_recursive(nums, 0, len(nums)) == min(nums)\n+    Traceback (most recent call last):\n+        ...\n+    IndexError: list index out of range\n+    >>> find_min_recursive(nums, -len(nums), -1) == min(nums)\n+    True\n+    >>> find_min_recursive(nums, -len(nums) - 1, -1) == min(nums)\n+    Traceback (most recent call last):\n+        ...\n+    IndexError: list index out of range\n+    \"\"\"\n+    if len(nums) == 0:\n+        raise ValueError(\"find_min_recursive() arg is an empty sequence\")\n+    if (\n+        left >= len(nums)\n+        or left < -len(nums)\n+        or right >= len(nums)\n+        or right < -len(nums)\n+    ):\n+        raise IndexError(\"list index out of range\")\n+    if left == right:\n+        return nums[left]\n+    mid = (left + right) >> 1  # the middle",
    "comment": "if the goal of this line is to take the int average of left and right, then i think this is clearer",
    "line_number": 75,
    "enriched": "File: maths/find_min.py\nCode: @@ -28,6 +28,59 @@ def find_min(nums: list[int | float]) -> int | float:\n     return min_num\n \n \n+# Divide and Conquer algorithm\n+def find_min_recursive(nums: list[int | float], left: int, right: int) -> int | float:\n+    \"\"\"\n+    find min value in list\n+    :param nums: contains elements\n+    :param left: index of first element\n+    :param right: index of last element\n+    :return: min in nums\n+\n+    >>> for nums in ([3, 2, 1], [-3, -2, -1], [3, -3, 0], [3.0, 3.1, 2.9]):\n+    ...     find_min_recursive(nums, 0, len(nums) - 1) == min(nums)\n+    True\n+    True\n+    True\n+    True\n+    >>> nums = [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\n+    >>> find_min_recursive(nums, 0, len(nums) - 1) == min(nums)\n+    True\n+    >>> find_min_recursive([], 0, 0)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: find_min_recursive() arg is an empty sequence\n+    >>> find_min_recursive(nums, 0, len(nums)) == min(nums)\n+    Traceback (most recent call last):\n+        ...\n+    IndexError: list index out of range\n+    >>> find_min_recursive(nums, -len(nums), -1) == min(nums)\n+    True\n+    >>> find_min_recursive(nums, -len(nums) - 1, -1) == min(nums)\n+    Traceback (most recent call last):\n+        ...\n+    IndexError: list index out of range\n+    \"\"\"\n+    if len(nums) == 0:\n+        raise ValueError(\"find_min_recursive() arg is an empty sequence\")\n+    if (\n+        left >= len(nums)\n+        or left < -len(nums)\n+        or right >= len(nums)\n+        or right < -len(nums)\n+    ):\n+        raise IndexError(\"list index out of range\")\n+    if left == right:\n+        return nums[left]\n+    mid = (left + right) >> 1  # the middle\nComment: ```suggestion\r\n    mid = (left + right) // 2  # the middle\r\n```\r\nIf the goal of this line is to take the int average of `left` and `right`, then I think this is clearer",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/find_min.py",
    "pr_number": 8103,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1097736658,
    "comment_created_at": "2023-02-06T17:56:41Z"
  },
  {
    "code": "@@ -6,7 +6,7 @@\n from __future__ import annotations\n \n import numpy as np\n-import numpy.typing as NDArray\n+import numpy.typing as NDArray  # noqa: N812",
    "comment": "and then on lines 14 and 15 change ndarray --> arraylike",
    "line_number": 9,
    "enriched": "File: arithmetic_analysis/lu_decomposition.py\nCode: @@ -6,7 +6,7 @@\n from __future__ import annotations\n \n import numpy as np\n-import numpy.typing as NDArray\n+import numpy.typing as NDArray  # noqa: N812\nComment: ```suggestion\r\nfrom numpy.typing import ArrayLike\r\n```\r\nAnd then on lines 14 and 15 change `NDArray` --> `ArrayLike`",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "arithmetic_analysis/lu_decomposition.py",
    "pr_number": 7062,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 993779382,
    "comment_created_at": "2022-10-12T18:29:47Z"
  },
  {
    "code": "@@ -0,0 +1,25 @@\n+# First, make sure you have Matplotlib installed. You can install it using pip if you haven't already",
    "comment": "make sure pass through ruff",
    "line_number": 1,
    "enriched": "File: physics/bernouli_principle.py\nCode: @@ -0,0 +1,25 @@\n+# First, make sure you have Matplotlib installed. You can install it using pip if you haven't already\nComment: make sure pass through ruff",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "physics/bernouli_principle.py",
    "pr_number": 10309,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1356543451,
    "comment_created_at": "2023-10-12T09:23:03Z"
  },
  {
    "code": "@@ -0,0 +1,47 @@\n+\"\"\"\r\n+A NAND Gate is a logic gate in boolean algebra which results to 0 (False) if both the\r\n+inputs are 1, and 1 (True) otherwise. It's similar to adding\r\n+a NOT gate along with an AND gate.\r\n+Following is the truth table of a NAND Gate:\r\n+    ------------------------------\r\n+    | Input 1 | Input 2 | Output |\r\n+    ------------------------------\r\n+    |    0    |    0    |    1   |\r\n+    |    0    |    1    |    1   |\r\n+    |    1    |    0    |    1   |\r\n+    |    1    |    1    |    0   |\r\n+    ------------------------------\r\n+Refer - https://www.geeksforgeeks.org/logic-gates-in-python/\r\n+\"\"\"\r\n+\r\n+\r\n+def nand_gate(input_1: int, input_2: int) -> int:\r\n+    \"\"\"\r\n+    Calculate AND of the input values\r",
    "comment": "and -> nand",
    "line_number": 20,
    "enriched": "File: boolean_algebra/nand_gate.py\nCode: @@ -0,0 +1,47 @@\n+\"\"\"\r\n+A NAND Gate is a logic gate in boolean algebra which results to 0 (False) if both the\r\n+inputs are 1, and 1 (True) otherwise. It's similar to adding\r\n+a NOT gate along with an AND gate.\r\n+Following is the truth table of a NAND Gate:\r\n+    ------------------------------\r\n+    | Input 1 | Input 2 | Output |\r\n+    ------------------------------\r\n+    |    0    |    0    |    1   |\r\n+    |    0    |    1    |    1   |\r\n+    |    1    |    0    |    1   |\r\n+    |    1    |    1    |    0   |\r\n+    ------------------------------\r\n+Refer - https://www.geeksforgeeks.org/logic-gates-in-python/\r\n+\"\"\"\r\n+\r\n+\r\n+def nand_gate(input_1: int, input_2: int) -> int:\r\n+    \"\"\"\r\n+    Calculate AND of the input values\r\nComment: AND -> NAND",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "boolean_algebra/nand_gate.py",
    "pr_number": 7596,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1003372515,
    "comment_created_at": "2022-10-24T14:17:56Z"
  },
  {
    "code": "@@ -61,23 +61,35 @@ def get_failure_array(pattern: str) -> list[int]:\n     pattern = \"abc1abc12\"\n     text1 = \"alskfjaldsabc1abc1abc12k23adsfabcabc\"\n     text2 = \"alskfjaldsk23adsfabcabc\"\n-    assert kmp(pattern, text1) and not kmp(pattern, text2)\n+    assert knuth_morris_pratt(text1, pattern) and knuth_morris_pratt(text2, pattern)\n \n     # Test 2)\n     pattern = \"ABABX\"\n     text = \"ABABZABABYABABX\"\n-    assert kmp(pattern, text)\n+    assert knuth_morris_pratt(text, pattern)\n \n     # Test 3)\n     pattern = \"AAAB\"\n     text = \"ABAAAAAB\"\n-    assert kmp(pattern, text)\n+    assert knuth_morris_pratt(text, pattern)\n \n     # Test 4)\n     pattern = \"abcdabcy\"\n     text = \"abcxabcdabxabcdabcdabcy\"\n-    assert kmp(pattern, text)\n-\n-    # Test 5)\n+    assert knuth_morris_pratt(text, pattern)\n+\n+    # Test 5) -> Doctests\n+    kmp = \"knuth_morris_pratt\"\n+    assert knuth_morris_pratt(kmp, \"kn\") == kmp.find(\"kn\")\n+    assert knuth_morris_pratt(kmp, \"h_m\") == kmp.find(\"h_m\")\n+    assert knuth_morris_pratt(kmp, \"rr\") == kmp.find(\"rr\")\n+    assert knuth_morris_pratt(kmp, \"tt\") == kmp.find(\"tt\")\n+    assert knuth_morris_pratt(kmp, \"not there\") == kmp.find(\"not there\")",
    "comment": "remove the duplicates.",
    "line_number": 87,
    "enriched": "File: strings/knuth_morris_pratt.py\nCode: @@ -61,23 +61,35 @@ def get_failure_array(pattern: str) -> list[int]:\n     pattern = \"abc1abc12\"\n     text1 = \"alskfjaldsabc1abc1abc12k23adsfabcabc\"\n     text2 = \"alskfjaldsk23adsfabcabc\"\n-    assert kmp(pattern, text1) and not kmp(pattern, text2)\n+    assert knuth_morris_pratt(text1, pattern) and knuth_morris_pratt(text2, pattern)\n \n     # Test 2)\n     pattern = \"ABABX\"\n     text = \"ABABZABABYABABX\"\n-    assert kmp(pattern, text)\n+    assert knuth_morris_pratt(text, pattern)\n \n     # Test 3)\n     pattern = \"AAAB\"\n     text = \"ABAAAAAB\"\n-    assert kmp(pattern, text)\n+    assert knuth_morris_pratt(text, pattern)\n \n     # Test 4)\n     pattern = \"abcdabcy\"\n     text = \"abcxabcdabxabcdabcdabcy\"\n-    assert kmp(pattern, text)\n-\n-    # Test 5)\n+    assert knuth_morris_pratt(text, pattern)\n+\n+    # Test 5) -> Doctests\n+    kmp = \"knuth_morris_pratt\"\n+    assert knuth_morris_pratt(kmp, \"kn\") == kmp.find(\"kn\")\n+    assert knuth_morris_pratt(kmp, \"h_m\") == kmp.find(\"h_m\")\n+    assert knuth_morris_pratt(kmp, \"rr\") == kmp.find(\"rr\")\n+    assert knuth_morris_pratt(kmp, \"tt\") == kmp.find(\"tt\")\n+    assert knuth_morris_pratt(kmp, \"not there\") == kmp.find(\"not there\")\nComment: Remove the duplicates.\r\n```suggestion\r\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "strings/knuth_morris_pratt.py",
    "pr_number": 9083,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1340512077,
    "comment_created_at": "2023-09-28T18:10:26Z"
  },
  {
    "code": "@@ -0,0 +1,131 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval: int | str | float = None) -> None:\n+        \"\"\"\n+        An empty left and right branches will be created for every value inserted,\n+        to perform better in recursive methods\n+        \"\"\"\n+        self.value = initval\n+        if self.value:\n+            self.left = Tree()\n+            self.right = Tree()\n+        else:\n+            self.left = None\n+            self.right = None\n+        return\n+\n+    # Empty nodes are None valued\n+    def isempty(self) -> bool:\n+        \"\"\"\n+        Returns true if tree is empty else False\n+        >>> isempty([])\n+        True\n+        >>> isempty([1,2,3])\n+        False\n+        \"\"\"\n+        return self.value == None\n+\n+    def isleaf(self) -> bool:\n+        \"\"\"\n+        Returns true if leaf is a node\n+        Suppose in [1,2,3,4,5,6,7,8,9,10]\n+        4 had empty left and right branches.\n+        >>> isleaf(4)\n+        True\n+        \"\"\"\n+        return self.value != None and self.left.isempty() and self.right.isempty()\n+\n+    # Inorder Traversal\n+    def inorder(self) -> list:\n+        \"\"\"\n+        Returns a list sorted in increasing order from the Binary Search Tree.\n+        >>> tree_sort([23235,82107,35775,91961,4323,40556,76603,64302,27316,74372])\n+        [4323, 23235, 27316, 35775, 40556, 64302, 74372, 76603, 82107, 91961]\n+        >>> tree_sort([50,52,54,74,93,100,114,124,130,143])\n+        [50, 52, 54, 74, 93, 100, 114, 124, 130, 143]\n+        \"\"\"\n+        # Corner Case\n+        if self.isempty():\n+            return []\n+        else:\n+            return self.left.inorder() + [self.value] + self.right.inorder()\n+\n+    # Display Tree\n+    def __str__(self) -> str:\n+        \"\"\"\n+        Prints the sorted tree\n+        Suppose tree is t.inorder() = [1,2,3]\n+        >>> print(t)\n+        [1,2,3]\n+        \"\"\"\n+        return str(self.inorder())\n+\n+    # Insert new element\n+    def insert(self, data: int | str | float) -> None:\n+        \"\"\"\n+        Inserts the value data to the Binary Search Tree\n+        Let\n+        t = Tree()\n+        >>> t.insert(1)\n+        [1]\n+        >>> t.insert(2)\n+        [1,2]\n+        >>> t.insert(3)\n+        [1,2,3]\n+        \"\"\"\n+        # Create new tree\n+        if self.isempty():\n+            self.value = data\n+            self.left = Tree()\n+            self.right = Tree()\n+\n+        # Value already exists\n+        if self.value == data:\n+            return\n+\n+        if data < self.value:\n+            self.left.insert(data)\n+            return\n+        if data > self.value:\n+            self.right.insert(data)\n+            return\n+\n+\n+def tree_sort(list_data: list) -> list:\n+    \"\"\"\n+    Returns a list array sorted in increasing order from the Binary Search Tree.\n+    >>> tree_sort([23235,82107,35775,91961,4323,40556,76603,64302,27316,74372])\n+    [4323, 23235, 27316, 35775, 40556, 64302, 74372, 76603, 82107, 91961]\n+    >>> tree_sort([50,52,54,74,93,100,114,124,130,143])\n+    [50, 52, 54, 74, 93, 100, 114, 124, 130, 143]\n+    \"\"\"\n+    if len(list_data) == 0 or len(list_data) == 1:\n+        return list_data\n+    tree = Tree()\n+    for i in list_data:\n+        tree.insert(i)\n+    return tree.inorder()\n+\n+\n+if __name__ == \"__main__\":",
    "comment": "can you run the doctest's in here please",
    "line_number": 128,
    "enriched": "File: sorts/tree_sort_2.py\nCode: @@ -0,0 +1,131 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval: int | str | float = None) -> None:\n+        \"\"\"\n+        An empty left and right branches will be created for every value inserted,\n+        to perform better in recursive methods\n+        \"\"\"\n+        self.value = initval\n+        if self.value:\n+            self.left = Tree()\n+            self.right = Tree()\n+        else:\n+            self.left = None\n+            self.right = None\n+        return\n+\n+    # Empty nodes are None valued\n+    def isempty(self) -> bool:\n+        \"\"\"\n+        Returns true if tree is empty else False\n+        >>> isempty([])\n+        True\n+        >>> isempty([1,2,3])\n+        False\n+        \"\"\"\n+        return self.value == None\n+\n+    def isleaf(self) -> bool:\n+        \"\"\"\n+        Returns true if leaf is a node\n+        Suppose in [1,2,3,4,5,6,7,8,9,10]\n+        4 had empty left and right branches.\n+        >>> isleaf(4)\n+        True\n+        \"\"\"\n+        return self.value != None and self.left.isempty() and self.right.isempty()\n+\n+    # Inorder Traversal\n+    def inorder(self) -> list:\n+        \"\"\"\n+        Returns a list sorted in increasing order from the Binary Search Tree.\n+        >>> tree_sort([23235,82107,35775,91961,4323,40556,76603,64302,27316,74372])\n+        [4323, 23235, 27316, 35775, 40556, 64302, 74372, 76603, 82107, 91961]\n+        >>> tree_sort([50,52,54,74,93,100,114,124,130,143])\n+        [50, 52, 54, 74, 93, 100, 114, 124, 130, 143]\n+        \"\"\"\n+        # Corner Case\n+        if self.isempty():\n+            return []\n+        else:\n+            return self.left.inorder() + [self.value] + self.right.inorder()\n+\n+    # Display Tree\n+    def __str__(self) -> str:\n+        \"\"\"\n+        Prints the sorted tree\n+        Suppose tree is t.inorder() = [1,2,3]\n+        >>> print(t)\n+        [1,2,3]\n+        \"\"\"\n+        return str(self.inorder())\n+\n+    # Insert new element\n+    def insert(self, data: int | str | float) -> None:\n+        \"\"\"\n+        Inserts the value data to the Binary Search Tree\n+        Let\n+        t = Tree()\n+        >>> t.insert(1)\n+        [1]\n+        >>> t.insert(2)\n+        [1,2]\n+        >>> t.insert(3)\n+        [1,2,3]\n+        \"\"\"\n+        # Create new tree\n+        if self.isempty():\n+            self.value = data\n+            self.left = Tree()\n+            self.right = Tree()\n+\n+        # Value already exists\n+        if self.value == data:\n+            return\n+\n+        if data < self.value:\n+            self.left.insert(data)\n+            return\n+        if data > self.value:\n+            self.right.insert(data)\n+            return\n+\n+\n+def tree_sort(list_data: list) -> list:\n+    \"\"\"\n+    Returns a list array sorted in increasing order from the Binary Search Tree.\n+    >>> tree_sort([23235,82107,35775,91961,4323,40556,76603,64302,27316,74372])\n+    [4323, 23235, 27316, 35775, 40556, 64302, 74372, 76603, 82107, 91961]\n+    >>> tree_sort([50,52,54,74,93,100,114,124,130,143])\n+    [50, 52, 54, 74, 93, 100, 114, 124, 130, 143]\n+    \"\"\"\n+    if len(list_data) == 0 or len(list_data) == 1:\n+        return list_data\n+    tree = Tree()\n+    for i in list_data:\n+        tree.insert(i)\n+    return tree.inorder()\n+\n+\n+if __name__ == \"__main__\":\nComment: Can you run the doctest's in here please\r\n```suggestion\r\nif __name__ == \"__main__\":\r\n    import doctest\r\n    \r\n    doctest.testmod()\r\n    \r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "sorts/tree_sort_2.py",
    "pr_number": 7462,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000831332,
    "comment_created_at": "2022-10-20T15:59:37Z"
  },
  {
    "code": "@@ -10,13 +10,12 @@\n \n import math\n \n-import qiskit\n-from qiskit import Aer, ClassicalRegister, QuantumCircuit, QuantumRegister, execute\n+import qiskit as q",
    "comment": "using just q reduces readability for new users of qiskit and does not really save much space.",
    "line_number": 13,
    "enriched": "File: quantum/q_full_adder.py\nCode: @@ -10,13 +10,12 @@\n \n import math\n \n-import qiskit\n-from qiskit import Aer, ClassicalRegister, QuantumCircuit, QuantumRegister, execute\n+import qiskit as q\nComment: Using just `q` reduces readability for new users of qiskit and does not really save much space.\r\n```suggestion\r\nimport qiskit\r\n```",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "quantum/q_full_adder.py",
    "pr_number": 7417,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 998936286,
    "comment_created_at": "2022-10-19T04:44:28Z"
  },
  {
    "code": "@@ -0,0 +1,49 @@\n+A greedy algorithm is an algorithmic paradigm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. In other words, it makes the best choice at each step without considering the long-term consequences. Greedy algorithms are often used for optimization problems, where the goal is to find the best solution among a set of possible solutions.",
    "comment": "an error occurred while parsing the file: greedy_methods/greedy.py\npython\ntraceback (most recent call last):\n  file \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 146, in parse\n    reports = lint_file(\n              ^^^^^^^^^^\nlibcst._exceptions.parsersyntaxerror: syntax error @ 1:1.\ntokenizer error: unterminated string literal\n\na greedy algorithm is an algorithmic paradigm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. in other words, it makes the best choice at each step without considering the long-term consequences. greedy algorithms are often used for optimization problems, where the goal is to find the best solution among a set of possible solutions.\n^",
    "line_number": 1,
    "enriched": "File: greedy_methods/greedy.py\nCode: @@ -0,0 +1,49 @@\n+A greedy algorithm is an algorithmic paradigm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. In other words, it makes the best choice at each step without considering the long-term consequences. Greedy algorithms are often used for optimization problems, where the goal is to find the best solution among a set of possible solutions.\nComment: An error occurred while parsing the file: `greedy_methods/greedy.py`\n```python\nTraceback (most recent call last):\n  File \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 146, in parse\n    reports = lint_file(\n              ^^^^^^^^^^\nlibcst._exceptions.ParserSyntaxError: Syntax Error @ 1:1.\ntokenizer error: unterminated string literal\n\nA greedy algorithm is an algorithmic paradigm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. In other words, it makes the best choice at each step without considering the long-term consequences. Greedy algorithms are often used for optimization problems, where the goal is to find the best solution among a set of possible solutions.\n^\n\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "greedy_methods/greedy.py",
    "pr_number": 11103,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1376564783,
    "comment_created_at": "2023-10-30T17:16:15Z"
  },
  {
    "code": "@@ -33,10 +33,10 @@ def retroactive_resolution(\n \n     x: NDArray[float64] = np.zeros((rows, 1), dtype=float)\n     for row in reversed(range(rows)):\n-        total = 0\n-        for col in range(row + 1, columns):\n-            total += coefficients[row, col] * x[col]\n-\n+        total = np.dot(\n+            [coefficients[row, col] for col in range(row + 1, columns)],\n+            [x[col] for col in range(row + 1, columns)],\n+        )",
    "comment": "i believe you can simplify this even further using numpy array slicing:",
    "line_number": 39,
    "enriched": "File: arithmetic_analysis/gaussian_elimination.py\nCode: @@ -33,10 +33,10 @@ def retroactive_resolution(\n \n     x: NDArray[float64] = np.zeros((rows, 1), dtype=float)\n     for row in reversed(range(rows)):\n-        total = 0\n-        for col in range(row + 1, columns):\n-            total += coefficients[row, col] * x[col]\n-\n+        total = np.dot(\n+            [coefficients[row, col] for col in range(row + 1, columns)],\n+            [x[col] for col in range(row + 1, columns)],\n+        )\nComment: I believe you can simplify this even further using numpy array slicing:\r\n```suggestion\r\n        total = np.dot(coefficients[row, row + 1 :], x[row + 1 :])\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "arithmetic_analysis/gaussian_elimination.py",
    "pr_number": 8987,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1298398126,
    "comment_created_at": "2023-08-18T12:32:28Z"
  },
  {
    "code": "@@ -1,3 +1,6 @@\n+from math import sqrt\n+",
    "comment": "import doctest and call the testmod()",
    "line_number": 2,
    "enriched": "File: maths/factors.py\nCode: @@ -1,3 +1,6 @@\n+from math import sqrt\n+\nComment: import doctest and call the `testmod()`",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/factors.py",
    "pr_number": 7429,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 999567845,
    "comment_created_at": "2022-10-19T14:45:44Z"
  },
  {
    "code": "@@ -3,36 +3,40 @@\n \n def climb_stairs(number_of_steps: int) -> int:\n     \"\"\"\n-    LeetCdoe No.70: Climbing Stairs\n-    Distinct ways to climb a number_of_steps staircase where each time you can either\n-    climb 1 or 2 steps.\n+    Calculate the number of distinct ways to climb a staircase with a given number of steps.\n+\n+    This problem is from LeetCode No. 70: Climbing Stairs.\n \n     Args:\n-        number_of_steps: number of steps on the staircase\n+        number_of_steps (int): The number of steps on the staircase.\n \n     Returns:\n-        Distinct ways to climb a number_of_steps staircase\n+        int: The number of distinct ways to climb the staircase.\n \n     Raises:\n-        AssertionError: number_of_steps not positive integer\n-\n-    >>> climb_stairs(3)\n-    3\n-    >>> climb_stairs(1)\n-    1\n-    >>> climb_stairs(-7)  # doctest: +ELLIPSIS\n-    Traceback (most recent call last):\n-        ...\n-    AssertionError: number_of_steps needs to be positive integer, your input -7\n+        AssertionError: If number_of_steps is not a positive integer.\n+\n+    Example:\n+        >>> climb_stairs(3)\n+        3\n+        >>> climb_stairs(1)\n+        1\n+        >>> climb_stairs(-7)  # doctest: +ELLIPSIS\n+        Traceback (most recent call last):\n+            ...\n+        AssertionError: number_of_steps needs to be a positive integer, your input: -7\n     \"\"\"\n     assert (\n         isinstance(number_of_steps, int) and number_of_steps > 0\n-    ), f\"number_of_steps needs to be positive integer, your input {number_of_steps}\"\n+    ), f\"number_of_steps needs to be a positive integer, your input: {number_of_steps}\"",
    "comment": "please change this assert to a valueerror",
    "line_number": 31,
    "enriched": "File: dynamic_programming/climbing_stairs.py\nCode: @@ -3,36 +3,40 @@\n \n def climb_stairs(number_of_steps: int) -> int:\n     \"\"\"\n-    LeetCdoe No.70: Climbing Stairs\n-    Distinct ways to climb a number_of_steps staircase where each time you can either\n-    climb 1 or 2 steps.\n+    Calculate the number of distinct ways to climb a staircase with a given number of steps.\n+\n+    This problem is from LeetCode No. 70: Climbing Stairs.\n \n     Args:\n-        number_of_steps: number of steps on the staircase\n+        number_of_steps (int): The number of steps on the staircase.\n \n     Returns:\n-        Distinct ways to climb a number_of_steps staircase\n+        int: The number of distinct ways to climb the staircase.\n \n     Raises:\n-        AssertionError: number_of_steps not positive integer\n-\n-    >>> climb_stairs(3)\n-    3\n-    >>> climb_stairs(1)\n-    1\n-    >>> climb_stairs(-7)  # doctest: +ELLIPSIS\n-    Traceback (most recent call last):\n-        ...\n-    AssertionError: number_of_steps needs to be positive integer, your input -7\n+        AssertionError: If number_of_steps is not a positive integer.\n+\n+    Example:\n+        >>> climb_stairs(3)\n+        3\n+        >>> climb_stairs(1)\n+        1\n+        >>> climb_stairs(-7)  # doctest: +ELLIPSIS\n+        Traceback (most recent call last):\n+            ...\n+        AssertionError: number_of_steps needs to be a positive integer, your input: -7\n     \"\"\"\n     assert (\n         isinstance(number_of_steps, int) and number_of_steps > 0\n-    ), f\"number_of_steps needs to be positive integer, your input {number_of_steps}\"\n+    ), f\"number_of_steps needs to be a positive integer, your input: {number_of_steps}\"\nComment: Please change this assert to a ValueError",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "dynamic_programming/climbing_stairs.py",
    "pr_number": 10685,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1365580805,
    "comment_created_at": "2023-10-19T13:50:36Z"
  },
  {
    "code": "@@ -97,9 +97,9 @@ def benchmark():\n     from timeit import timeit\n \n     setup = \"from __main__ import slow_primes, primes, fast_primes\"\n-    print(timeit(\"slow_primes(1_000_000_000_000)\", setup=setup, number=1_000_000))\n-    print(timeit(\"primes(1_000_000_000_000)\", setup=setup, number=1_000_000))\n-    print(timeit(\"fast_primes(1_000_000_000_000)\", setup=setup, number=1_000_000))\n+    print(timeit(\"slow_primes(1_000)\", setup=setup, number=1_000))\n+    print(timeit(\"primes(1_000)\", setup=setup, number=1_000))\n+    print(timeit(\"fast_primes(1_000)\", setup=setup, number=1_000))",
    "comment": "please revert these three lines.  pytest does not run the code in __main__() so it does not run the benchmarks.",
    "line_number": 102,
    "enriched": "File: maths/prime_numbers.py\nCode: @@ -97,9 +97,9 @@ def benchmark():\n     from timeit import timeit\n \n     setup = \"from __main__ import slow_primes, primes, fast_primes\"\n-    print(timeit(\"slow_primes(1_000_000_000_000)\", setup=setup, number=1_000_000))\n-    print(timeit(\"primes(1_000_000_000_000)\", setup=setup, number=1_000_000))\n-    print(timeit(\"fast_primes(1_000_000_000_000)\", setup=setup, number=1_000_000))\n+    print(timeit(\"slow_primes(1_000)\", setup=setup, number=1_000))\n+    print(timeit(\"primes(1_000)\", setup=setup, number=1_000))\n+    print(timeit(\"fast_primes(1_000)\", setup=setup, number=1_000))\nComment: Please revert these three lines.  pytest does not run the code in `__main__()` so it does not run the benchmarks.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/prime_numbers.py",
    "pr_number": 9851,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347830134,
    "comment_created_at": "2023-10-05T18:37:28Z"
  },
  {
    "code": "@@ -1,8 +1,28 @@\n-# https://en.wikipedia.org/wiki/Trifid_cipher\r\n+\"\"\"\r\n+The trifid cipher uses a table to fractionate each plaintext letter into a\r\n+trigram,mixes the constituents of the trigrams, and then applies the table\r\n+in reverse to turn these mixed trigrams into ciphertext letters.\r\n+https://en.wikipedia.org/wiki/Trifid_cipher\r\n+\"\"\"\r\n+\r\n from __future__ import annotations\r\n \r\n \r",
    "comment": "let's define character_to_number just once outside the functions.\r\n\r\nnumber_to_character can be created from character_to_number in one line of code.\r\n\r\nhttps://github.com/thealgorithms/python/blob/579937613a6dc7e099b710e3d57767a2fab115ad/ciphers/morse_code.py#l9-l22",
    "line_number": 10,
    "enriched": "File: ciphers/trifid_cipher.py\nCode: @@ -1,8 +1,28 @@\n-# https://en.wikipedia.org/wiki/Trifid_cipher\r\n+\"\"\"\r\n+The trifid cipher uses a table to fractionate each plaintext letter into a\r\n+trigram,mixes the constituents of the trigrams, and then applies the table\r\n+in reverse to turn these mixed trigrams into ciphertext letters.\r\n+https://en.wikipedia.org/wiki/Trifid_cipher\r\n+\"\"\"\r\n+\r\n from __future__ import annotations\r\n \r\n \r\nComment: Let's define `character_to_number` just once outside the functions.\r\n\r\n`number_to_character` can be created from `character_to_number` in one line of code.\r\n\r\nhttps://github.com/TheAlgorithms/Python/blob/579937613a6dc7e099b710e3d57767a2fab115ad/ciphers/morse_code.py#L9-L22",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "ciphers/trifid_cipher.py",
    "pr_number": 10716,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1366863821,
    "comment_created_at": "2023-10-20T11:43:32Z"
  },
  {
    "code": "@@ -0,0 +1,520 @@\n+\"\"\"",
    "comment": "please rename the file path to machine_learning/random_forest/random_forest.py",
    "line_number": 1,
    "enriched": "File: machine_learning/Random Forest/RandomForest.py\nCode: @@ -0,0 +1,520 @@\n+\"\"\"\nComment: Please rename the file path to `machine_learning/random_forest/random_forest.py`",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "machine_learning/Random Forest/RandomForest.py",
    "pr_number": 11018,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375030099,
    "comment_created_at": "2023-10-27T20:47:40Z"
  },
  {
    "code": "@@ -208,27 +207,33 @@ def make_highshelf(\n \n     >>> filter = make_highshelf(1000, 48000, 6)\n     >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE\n-    [2.2229172136088806, -3.9587208137297303, 1.7841414181566304, 4.295432981120543,\n-     -7.922740859457287, 3.6756456963725253]\n+    [2.2229172136088806, -3.616080374557819, 1.4122939941901636, 0.5690651073937642,\n+     -1.3335115824061685, 0.7666141956641752]\n     \"\"\"\n     w0 = tau * frequency / samplerate\n     _sin = sin(w0)\n     _cos = cos(w0)\n     alpha = _sin / (2 * q_factor)\n     big_a = 10 ** (gain_db / 40)\n-    pmc = (big_a + 1) - (big_a - 1) * _cos\n-    ppmc = (big_a + 1) + (big_a - 1) * _cos\n-    mpc = (big_a - 1) - (big_a + 1) * _cos\n-    pmpc = (big_a - 1) + (big_a + 1) * _cos\n+    pmc = (big_a + 1) + (big_a - 1) * _cos\n+    ppmc = (big_a + 1) - (big_a - 1) * _cos\n+    mpc = (big_a - 1) + (big_a + 1) * _cos\n+    pmpc = (big_a - 1) - (big_a + 1) * _cos\n     aa2 = 2 * sqrt(big_a) * alpha\n \n-    b0 = big_a * (ppmc + aa2)\n-    b1 = -2 * big_a * pmpc\n-    b2 = big_a * (ppmc - aa2)\n-    a0 = pmc + aa2\n-    a1 = 2 * mpc\n-    a2 = pmc - aa2\n+    b0 = big_a * (pmc - aa2)\n+    b1 = -2 * big_a * mpc\n+    b2 = big_a * (pmc + aa2)\n+    a0 = ppmc - aa2\n+    a1 = 2 * pmpc\n+    a2 = ppmc + aa2\n \n     filt = IIRFilter(2)\n     filt.set_coefficients([a0, a1, a2], [b0, b1, b2])\n     return filt\n+\n+\n+if __name__ == \"__main__\":\n+    import doctest\n+\n+    doctest.testmod()",
    "comment": "from math import cos, sin, sqrt, tau\r\nfrom audio_filters.iir_filter import iirfilter\r\n\r\n\"\"\"\r\ncreate 2nd-order iir filters with butterworth design.\r\ncode based on https://webaudio.github.io/audio-eq-cookbook/audio-eq-cookbook.html\r\nalternatively, you can use scipy.signal.butter, which should yield the same results.\r\n\"\"\"\r\n\r\ndef make_highshelf(frequency, samplerate, gain_db, q_factor=0.707):\r\n    \"\"\"\r\n    creates a high-shelf filter based on the specified parameters.\r\n\r\n    args:\r\n        frequency (float): center frequency of the filter.\r\n        samplerate (float): sampling rate.\r\n        gain_db (float): gain in decibels.\r\n        q_factor (float): quality factor.\r\n\r\n    returns:\r\n        iirfilter: configured high-shelf filter.\r\n\r\n    examples:\r\n        >>> filter = make_highshelf(1000, 48000, 6)\r\n        >>> filter.a_coeffs  # doctest: +normalize_whitespace\r\n        [2.2229172136088806, -3.9587208137297303, 1.7841414181566304]\r\n        >>> filter.b_coeffs  # doctest: +normalize_whitespace\r\n        [4.295432981120543, -7.922740859457287, 3.6756456963725253]\r\n    \"\"\"\r\n    w0 = tau * frequency / samplerate\r\n    _sin = sin(w0)\r\n    _cos = cos(w0)\r\n    alpha = _sin / (2 * q_factor)\r\n    big_a = 10 ** (gain_db / 40)\r\n\r\n    pmc = (big_a + 1) + (big_a - 1) * _cos\r\n    ppmc = (big_a + 1) - (big_a - 1) * _cos\r\n    mpc = (big_a - 1) + (big_a + 1) * _cos\r\n    pmpc = (big_a - 1) - (big_a + 1) * _cos\r\n    aa2 = 2 * sqrt(big_a) * alpha\r\n\r\n    b0 = big_a * (ppmc + aa2)\r\n    b1 = -2 * big_a * pmpc\r\n    b2 = big_a * (ppmc - aa2)\r\n    a0 = pmc + aa2\r\n    a1 = -2 * mpc\r\n    a2 = pmc - aa2\r\n\r\n    filt = iirfilter(2)\r\n    filt.set_coefficients([a0, a1, a2], [b0, b1, b2])\r\n    return filt\r\n\r\nif __name__ == \"__main__\":\r\n    import doctest\r\n    doctest.testmod()",
    "line_number": 239,
    "enriched": "File: audio_filters/butterworth_filter.py\nCode: @@ -208,27 +207,33 @@ def make_highshelf(\n \n     >>> filter = make_highshelf(1000, 48000, 6)\n     >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE\n-    [2.2229172136088806, -3.9587208137297303, 1.7841414181566304, 4.295432981120543,\n-     -7.922740859457287, 3.6756456963725253]\n+    [2.2229172136088806, -3.616080374557819, 1.4122939941901636, 0.5690651073937642,\n+     -1.3335115824061685, 0.7666141956641752]\n     \"\"\"\n     w0 = tau * frequency / samplerate\n     _sin = sin(w0)\n     _cos = cos(w0)\n     alpha = _sin / (2 * q_factor)\n     big_a = 10 ** (gain_db / 40)\n-    pmc = (big_a + 1) - (big_a - 1) * _cos\n-    ppmc = (big_a + 1) + (big_a - 1) * _cos\n-    mpc = (big_a - 1) - (big_a + 1) * _cos\n-    pmpc = (big_a - 1) + (big_a + 1) * _cos\n+    pmc = (big_a + 1) + (big_a - 1) * _cos\n+    ppmc = (big_a + 1) - (big_a - 1) * _cos\n+    mpc = (big_a - 1) + (big_a + 1) * _cos\n+    pmpc = (big_a - 1) - (big_a + 1) * _cos\n     aa2 = 2 * sqrt(big_a) * alpha\n \n-    b0 = big_a * (ppmc + aa2)\n-    b1 = -2 * big_a * pmpc\n-    b2 = big_a * (ppmc - aa2)\n-    a0 = pmc + aa2\n-    a1 = 2 * mpc\n-    a2 = pmc - aa2\n+    b0 = big_a * (pmc - aa2)\n+    b1 = -2 * big_a * mpc\n+    b2 = big_a * (pmc + aa2)\n+    a0 = ppmc - aa2\n+    a1 = 2 * pmpc\n+    a2 = ppmc + aa2\n \n     filt = IIRFilter(2)\n     filt.set_coefficients([a0, a1, a2], [b0, b1, b2])\n     return filt\n+\n+\n+if __name__ == \"__main__\":\n+    import doctest\n+\n+    doctest.testmod()\nComment: ```\r\nfrom math import cos, sin, sqrt, tau\r\nfrom audio_filters.iir_filter import IIRFilter\r\n\r\n\"\"\"\r\nCreate 2nd-order IIR filters with Butterworth design.\r\nCode based on https://webaudio.github.io/Audio-EQ-Cookbook/audio-eq-cookbook.html\r\nAlternatively, you can use scipy.signal.butter, which should yield the same results.\r\n\"\"\"\r\n\r\ndef make_highshelf(frequency, samplerate, gain_db, q_factor=0.707):\r\n    \"\"\"\r\n    Creates a high-shelf filter based on the specified parameters.\r\n\r\n    Args:\r\n        frequency (float): Center frequency of the filter.\r\n        samplerate (float): Sampling rate.\r\n        gain_db (float): Gain in decibels.\r\n        q_factor (float): Quality factor.\r\n\r\n    Returns:\r\n        IIRFilter: Configured high-shelf filter.\r\n\r\n    Examples:\r\n        >>> filter = make_highshelf(1000, 48000, 6)\r\n        >>> filter.a_coeffs  # doctest: +NORMALIZE_WHITESPACE\r\n        [2.2229172136088806, -3.9587208137297303, 1.7841414181566304]\r\n        >>> filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE\r\n        [4.295432981120543, -7.922740859457287, 3.6756456963725253]\r\n    \"\"\"\r\n    w0 = tau * frequency / samplerate\r\n    _sin = sin(w0)\r\n    _cos = cos(w0)\r\n    alpha = _sin / (2 * q_factor)\r\n    big_a = 10 ** (gain_db / 40)\r\n\r\n    pmc = (big_a + 1) + (big_a - 1) * _cos\r\n    ppmc = (big_a + 1) - (big_a - 1) * _cos\r\n    mpc = (big_a - 1) + (big_a + 1) * _cos\r\n    pmpc = (big_a - 1) - (big_a + 1) * _cos\r\n    aa2 = 2 * sqrt(big_a) * alpha\r\n\r\n    b0 = big_a * (ppmc + aa2)\r\n    b1 = -2 * big_a * pmpc\r\n    b2 = big_a * (ppmc - aa2)\r\n    a0 = pmc + aa2\r\n    a1 = -2 * mpc\r\n    a2 = pmc - aa2\r\n\r\n    filt = IIRFilter(2)\r\n    filt.set_coefficients([a0, a1, a2], [b0, b1, b2])\r\n    return filt\r\n\r\nif __name__ == \"__main__\":\r\n    import doctest\r\n    doctest.testmod()\r\n```",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "audio_filters/butterworth_filter.py",
    "pr_number": 12348,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1830371455,
    "comment_created_at": "2024-11-06T04:23:55Z"
  },
  {
    "code": "@@ -136,22 +153,21 @@ def get_successors(self, parent: Node) -> list[Node]:\n             pos_x = parent.pos_x + action[1]",
    "comment": "you could get the successors using a list comprehension:\r\npy\r\nsuccessors = [\r\n    node(\r\n        (pos_x := parent.pos_x + action[1]),\r\n        (pos_y := parent.pos_y + action[0]),\r\n        self.target.pos_x,\r\n        self.target.pos_y,\r\n        parent.g_cost + 1,\r\n        parent,\r\n    )\r\n    for action in delta\r\n    if (\r\n        0 <= pos_x < len(self.grid[0])\r\n        and 0 <= pos_y < len(self.grid)\r\n        and self.grid[pos_y][pos_x] == 0\r\n    )\r\n]\r\n\r\n\r\nbut ultimately i could go either way since this isn't necessarily more readable than what you already have.",
    "line_number": 153,
    "enriched": "File: graphs/greedy_best_first.py\nCode: @@ -136,22 +153,21 @@ def get_successors(self, parent: Node) -> list[Node]:\n             pos_x = parent.pos_x + action[1]\nComment: You could get the successors using a list comprehension:\r\n```py\r\nsuccessors = [\r\n    Node(\r\n        (pos_x := parent.pos_x + action[1]),\r\n        (pos_y := parent.pos_y + action[0]),\r\n        self.target.pos_x,\r\n        self.target.pos_y,\r\n        parent.g_cost + 1,\r\n        parent,\r\n    )\r\n    for action in delta\r\n    if (\r\n        0 <= pos_x < len(self.grid[0])\r\n        and 0 <= pos_y < len(self.grid)\r\n        and self.grid[pos_y][pos_x] == 0\r\n    )\r\n]\r\n```\r\n\r\nbut ultimately I could go either way since this isn't necessarily more readable than what you already have.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "graphs/greedy_best_first.py",
    "pr_number": 8775,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1278600899,
    "comment_created_at": "2023-07-30T18:25:12Z"
  },
  {
    "code": "@@ -96,7 +96,7 @@ def test_nearest_neighbour(\n \n \n def test_local_binary_pattern():\n-    file_path = \"digital_image_processing/image_data/lena.jpg\"\n+    file_path = \"digital_image_processing/image_data/lena_small.jpg\"",
    "comment": "let's use https://docs.github.com/en/actions/learn-github-actions/variables#default-environment-variables",
    "line_number": 99,
    "enriched": "File: digital_image_processing/test_digital_image_processing.py\nCode: @@ -96,7 +96,7 @@ def test_nearest_neighbour(\n \n \n def test_local_binary_pattern():\n-    file_path = \"digital_image_processing/image_data/lena.jpg\"\n+    file_path = \"digital_image_processing/image_data/lena_small.jpg\"\nComment: Let's use https://docs.github.com/en/actions/learn-github-actions/variables#default-environment-variables\r\n```suggestion\r\n    from os import getenv  # Speed up our Continuous Integration tests\r\n    file_name = \"lena_small.jpg\" if getenv(\"CI\") else \"lena.jpg\"\r\n    file_path = f\"digital_image_processing/image_data/{file_name}.jpg\"\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "digital_image_processing/test_digital_image_processing.py",
    "pr_number": 10161,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1350214823,
    "comment_created_at": "2023-10-09T12:02:42Z"
  },
  {
    "code": "@@ -12,6 +12,19 @@\n def method_2(boundary, steps):\n     # \"Simpson Rule\"\n     # int(f) = delta_x/2 * (b-a)/3*(f1 + 4f2 + 2f_3 + ... + fn)\n+    \"\"\"\n+    Calculate the definite integral of a function using Simpson's Rule.\n+\n+     :param boundary: A list containing the lower and upper bounds of integration.\n+     :param steps: The number of steps or resolution for the integration.\n+     :return: The approximate integral value.\n+\n+     >>> round(method_2([0, 1], 10), 10)\n+     0.3333333333\n+\n+     >>> round(method_2([0, 2], 10), 10)\n+     2.6666666667\n+    \"\"\"",
    "comment": "please add a few more tests for unexpected inputs...",
    "line_number": 27,
    "enriched": "File: maths/simpson_rule.py\nCode: @@ -12,6 +12,19 @@\n def method_2(boundary, steps):\n     # \"Simpson Rule\"\n     # int(f) = delta_x/2 * (b-a)/3*(f1 + 4f2 + 2f_3 + ... + fn)\n+    \"\"\"\n+    Calculate the definite integral of a function using Simpson's Rule.\n+\n+     :param boundary: A list containing the lower and upper bounds of integration.\n+     :param steps: The number of steps or resolution for the integration.\n+     :return: The approximate integral value.\n+\n+     >>> round(method_2([0, 1], 10), 10)\n+     0.3333333333\n+\n+     >>> round(method_2([0, 2], 10), 10)\n+     2.6666666667\n+    \"\"\"\nComment: Please add a few more tests for unexpected inputs...\r\n```suggestion\r\n    >>> round(method_2([0, 2], 0), 10)\r\n    >>> round(method_2((0, 2), -10), 10)\r\n    >>> round(method_2([0, 2, 4], 10), 10)\r\n    >>> round(method_2([2, 0], 10), 10)\r\n    >>> round(method_2([-2, -1], 10), 10)\r\n    \"\"\"\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/simpson_rule.py",
    "pr_number": 10269,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1354420273,
    "comment_created_at": "2023-10-11T08:37:43Z"
  },
  {
    "code": "@@ -0,0 +1,38 @@\n+import re\n+\n+\n+def srilanka_phone_validator(phone: str) -> bool:\n+    \"\"\"\n+    Determine whether the string is a valid sri lankan phone number or not\n+    :param phone:\n+    :return: Boolean",
    "comment": "this tells us nothing that we don't already know",
    "line_number": 8,
    "enriched": "File: strings/srilankan_phone_validator.py\nCode: @@ -0,0 +1,38 @@\n+import re\n+\n+\n+def srilanka_phone_validator(phone: str) -> bool:\n+    \"\"\"\n+    Determine whether the string is a valid sri lankan phone number or not\n+    :param phone:\n+    :return: Boolean\nComment: ```suggestion\n\n```\nThis tells us nothing that we don't already know",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "strings/srilankan_phone_validator.py",
    "pr_number": 7658,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1005413254,
    "comment_created_at": "2022-10-26T09:01:32Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+\"\"\"\n+Author  : Naman Sharma\n+Date    : October 2, 2023\n+\n+Task:\n+Find largest power of 2 less than or equal to a given number\n+\n+Implementation notes: Use bit manipulation.\n+We start from 1 & left shift the set bit to check if (res<<1)<=number.\n+Each left bit shift represents a pow of 2.\n+\n+For example:\n+number: 15\n+res:    1   0b1\n+        2   0b10\n+        4   0b100\n+        8   0b1000\n+        16  0b10000 (Exit)\n+\"\"\"\n+\n+",
    "comment": "i dont think we need so long one instead we can use \r\ndef find(a):\r\n    if a<=0:\r\n        return 0;\r\n    for i in range(a+1):\r\n        if (2**i <= a):\r\n            c=i;\r\n        else :\r\n            return c;\r\nb=int(input())\r\nprint(find(b))",
    "line_number": 21,
    "enriched": "File: bit_manipulation/largest_pow_of_two_less_than_or_equal_to_a_number.py\nCode: @@ -0,0 +1,60 @@\n+\"\"\"\n+Author  : Naman Sharma\n+Date    : October 2, 2023\n+\n+Task:\n+Find largest power of 2 less than or equal to a given number\n+\n+Implementation notes: Use bit manipulation.\n+We start from 1 & left shift the set bit to check if (res<<1)<=number.\n+Each left bit shift represents a pow of 2.\n+\n+For example:\n+number: 15\n+res:    1   0b1\n+        2   0b10\n+        4   0b100\n+        8   0b1000\n+        16  0b10000 (Exit)\n+\"\"\"\n+\n+\nComment: i dont think we need so long one instead we can use \r\ndef find(a):\r\n    if a<=0:\r\n        return 0;\r\n    for i in range(a+1):\r\n        if (2**i <= a):\r\n            c=i;\r\n        else :\r\n            return c;\r\nb=int(input())\r\nprint(find(b))\r\n",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "bit_manipulation/largest_pow_of_two_less_than_or_equal_to_a_number.py",
    "pr_number": 9374,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1345305670,
    "comment_created_at": "2023-10-04T07:13:28Z"
  },
  {
    "code": "@@ -19,116 +17,94 @@\n from digital_image_processing.filters import sobel_filter as sob\n from digital_image_processing.resize import resize as rs\n \n-img = imread(r\"digital_image_processing/image_data/lena_small.jpg\")\n-gray = cvtColor(img, COLOR_BGR2GRAY)\n+# Sample image path for testing\n+IMG_PATH = r\"C:/Users/USER/Python/digital_image_processing/image_data/lena_small.jpg\"",
    "comment": "our github actions-based tests run on ubuntu linux so they are not going to find files on a hard-coded windows path.  faster but failing the tests is not going to be accepted.",
    "line_number": 21,
    "enriched": "File: digital_image_processing/test_digital_image_processing.py\nCode: @@ -19,116 +17,94 @@\n from digital_image_processing.filters import sobel_filter as sob\n from digital_image_processing.resize import resize as rs\n \n-img = imread(r\"digital_image_processing/image_data/lena_small.jpg\")\n-gray = cvtColor(img, COLOR_BGR2GRAY)\n+# Sample image path for testing\n+IMG_PATH = r\"C:/Users/USER/Python/digital_image_processing/image_data/lena_small.jpg\"\nComment: Our GitHub Actions-based tests run on Ubuntu Linux so they are not going to find files on a hard-coded Windows path.  Faster but failing the tests is not going to be accepted.",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "digital_image_processing/test_digital_image_processing.py",
    "pr_number": 11482,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1693982566,
    "comment_created_at": "2024-07-27T16:16:27Z"
  },
  {
    "code": "@@ -1,35 +1,38 @@\n+from collections.abc import Callable\n+\n+\n class Heap:\n     \"\"\"\n     A generic Heap class, can be used as min or max by passing the key function\n     accordingly.\n     \"\"\"\n \n-    def __init__(self, key=None):\n+    def __init__(self, key: Callable | None = None) -> None:\n         # Stores actual heap items.\n-        self.arr = list()\n+        self.arr: list = list()\n         # Stores indexes of each item for supporting updates and deletion.\n-        self.pos_map = {}\n+        self.pos_map: dict = {}",
    "comment": "these are really not needed.  both python and mypy are able to figure this out without hinting.",
    "line_number": 14,
    "enriched": "File: data_structures/heap/heap_generic.py\nCode: @@ -1,35 +1,38 @@\n+from collections.abc import Callable\n+\n+\n class Heap:\n     \"\"\"\n     A generic Heap class, can be used as min or max by passing the key function\n     accordingly.\n     \"\"\"\n \n-    def __init__(self, key=None):\n+    def __init__(self, key: Callable | None = None) -> None:\n         # Stores actual heap items.\n-        self.arr = list()\n+        self.arr: list = list()\n         # Stores indexes of each item for supporting updates and deletion.\n-        self.pos_map = {}\n+        self.pos_map: dict = {}\nComment: These are really not needed.  Both Python and mypy are able to figure this out without hinting.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "data_structures/heap/heap_generic.py",
    "pr_number": 7044,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 993402128,
    "comment_created_at": "2022-10-12T12:37:00Z"
  },
  {
    "code": "@@ -96,7 +96,7 @@ We want your work to be readable by others; therefore, we encourage you to note\n \n   ```bash\n   python3 -m pip install ruff  # only required the first time\n-  ruff .\n+  ruff check .",
    "comment": "looks like we don't need to specify the current directory anymore, since ruff will run in the current directory by default: https://docs.astral.sh/ruff/tutorial/#getting-started",
    "line_number": 99,
    "enriched": "File: CONTRIBUTING.md\nCode: @@ -96,7 +96,7 @@ We want your work to be readable by others; therefore, we encourage you to note\n \n   ```bash\n   python3 -m pip install ruff  # only required the first time\n-  ruff .\n+  ruff check .\nComment: ```suggestion\r\n  ruff check\r\n```\r\nLooks like we don't need to specify the current directory anymore, since ruff will run in the current directory by default: https://docs.astral.sh/ruff/tutorial/#getting-started",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "CONTRIBUTING.md",
    "pr_number": 11772,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1788648593,
    "comment_created_at": "2024-10-05T17:17:36Z"
  },
  {
    "code": "@@ -0,0 +1,63 @@\n+\"\"\"\n+Description : The law states that the magnitude of the electrostatic force of attraction or repulsion between two point charges is directly proportional",
    "comment": "use ruff . then ruff . --fix to fix ruff and pre-commit issues",
    "line_number": 2,
    "enriched": "File: physics/coulomb_law.py\nCode: @@ -0,0 +1,63 @@\n+\"\"\"\n+Description : The law states that the magnitude of the electrostatic force of attraction or repulsion between two point charges is directly proportional\nComment: use `ruff .` then `ruff . --fix` to fix ruff and pre-commit issues",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "physics/coulomb_law.py",
    "pr_number": 8714,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1186863643,
    "comment_created_at": "2023-05-07T14:30:18Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+from __future__ import annotations",
    "comment": "the code in this repository is targeted towards python 3.10+",
    "line_number": 1,
    "enriched": "File: dynamic_programming/Minimum_cost_path.py\nCode: @@ -0,0 +1,35 @@\n+from __future__ import annotations\nComment: ```suggestion\r\n```\r\nThe code in this repository is targeted towards python 3.10+",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "dynamic_programming/Minimum_cost_path.py",
    "pr_number": 7410,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 998609830,
    "comment_created_at": "2022-10-18T19:06:25Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+\"\"\"\n+== Juggler Sequence ==\n+Juggler sequence start with any positive integer n. The next term is\n+obtained as follows:\n+    If n term is even, the next term is floor value of square root of n .\n+    If n is odd, the next term is floor value of 3 time the square root of n.\n+\n+https://en.wikipedia.org/wiki/Juggler_sequence\n+\"\"\"\n+\n+# Author : Akshay Dubey (https://github.com/itsAkshayDubey)\n+import math\n+\n+\n+def juggler_sequence(number: int) -> list[int]:\n+    \"\"\"\n+    # doctest: +NORMALIZE_WHITESPACE\n+    >>> juggler_sequence(0)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value of [number=0] must be greater than 0\n+    >>> juggler_sequence(1)\n+    [1]\n+    >>> juggler_sequence(2)\n+    [2, 1]\n+    >>> juggler_sequence(3)\n+    [3, 5, 11, 36, 6, 2, 1]\n+    >>> juggler_sequence(5)\n+    [5, 11, 36, 6, 2, 1]\n+    >>> juggler_sequence(6.0)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value of [number=6.0] must be an integer\n+    >>> juggler_sequence(-1)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value of [number=-1] must be positive\n+    \"\"\"\n+    if not isinstance(number, int):\n+        raise TypeError(f\"Input value of [number={number}] must be an integer\")\n+    if number < 0:\n+        raise TypeError(f\"Input value of [number={number}] must be positive\")\n+    if number == 0:\n+        raise TypeError(f\"Input value of [number={number}] must be greater than 0\")\n+    sequence = [number]\n+    while number != 1:\n+        if number % 2 == 0:\n+            temp = int(math.floor(math.sqrt(number)))\n+        else:\n+            temp = int(\n+                math.floor(math.sqrt(number) * math.sqrt(number) * math.sqrt(number))\n+            )\n+        number = temp",
    "comment": "temp is not needed...",
    "line_number": 53,
    "enriched": "File: maths/juggler_sequence.py\nCode: @@ -0,0 +1,61 @@\n+\"\"\"\n+== Juggler Sequence ==\n+Juggler sequence start with any positive integer n. The next term is\n+obtained as follows:\n+    If n term is even, the next term is floor value of square root of n .\n+    If n is odd, the next term is floor value of 3 time the square root of n.\n+\n+https://en.wikipedia.org/wiki/Juggler_sequence\n+\"\"\"\n+\n+# Author : Akshay Dubey (https://github.com/itsAkshayDubey)\n+import math\n+\n+\n+def juggler_sequence(number: int) -> list[int]:\n+    \"\"\"\n+    # doctest: +NORMALIZE_WHITESPACE\n+    >>> juggler_sequence(0)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value of [number=0] must be greater than 0\n+    >>> juggler_sequence(1)\n+    [1]\n+    >>> juggler_sequence(2)\n+    [2, 1]\n+    >>> juggler_sequence(3)\n+    [3, 5, 11, 36, 6, 2, 1]\n+    >>> juggler_sequence(5)\n+    [5, 11, 36, 6, 2, 1]\n+    >>> juggler_sequence(6.0)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value of [number=6.0] must be an integer\n+    >>> juggler_sequence(-1)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Input value of [number=-1] must be positive\n+    \"\"\"\n+    if not isinstance(number, int):\n+        raise TypeError(f\"Input value of [number={number}] must be an integer\")\n+    if number < 0:\n+        raise TypeError(f\"Input value of [number={number}] must be positive\")\n+    if number == 0:\n+        raise TypeError(f\"Input value of [number={number}] must be greater than 0\")\n+    sequence = [number]\n+    while number != 1:\n+        if number % 2 == 0:\n+            temp = int(math.floor(math.sqrt(number)))\n+        else:\n+            temp = int(\n+                math.floor(math.sqrt(number) * math.sqrt(number) * math.sqrt(number))\n+            )\n+        number = temp\nComment: `temp` is not needed...\r\n```suggestion\r\n            number = int(math.floor(math.sqrt(number)))\r\n        else:\r\n            number = int(\r\n                math.floor(math.sqrt(number) * math.sqrt(number) * math.sqrt(number))\r\n            )\r\n```",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "maths/juggler_sequence.py",
    "pr_number": 7985,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1022809797,
    "comment_created_at": "2022-11-15T13:49:44Z"
  },
  {
    "code": "@@ -82,3 +82,4 @@ def triangle(\n \n     vertices = [(-175, -125), (0, 175), (175, -125)]  # vertices of triangle\n     triangle(vertices[0], vertices[1], vertices[2], int(sys.argv[1]))\n+    turtle.Screen().exitonclick()",
    "comment": "make sure the file ends in an empty line",
    "line_number": 85,
    "enriched": "File: fractals/sierpinski_triangle.py\nCode: @@ -82,3 +82,4 @@ def triangle(\n \n     vertices = [(-175, -125), (0, 175), (175, -125)]  # vertices of triangle\n     triangle(vertices[0], vertices[1], vertices[2], int(sys.argv[1]))\n+    turtle.Screen().exitonclick()\nComment: ```suggestion\r\n    turtle.Screen().exitonclick()\r\n\r\n```\r\nMake sure the file ends in an empty line",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "fractals/sierpinski_triangle.py",
    "pr_number": 8625,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1280060200,
    "comment_created_at": "2023-08-01T02:55:03Z"
  },
  {
    "code": "@@ -25,6 +25,41 @@ def add(self, item: Any) -> None:\n         self.head = Node(item, self.head)\n         self.size += 1\n \n+    def add_at_position(self, item: Any, position: int) -> bool:\n+        \"\"\"\n+        Adds a new node with the given item at the specified position in the linked list\n+\n+        Args:\n+            item (Any): The item to be added to the linked list.\n+            position (int): The position at which the item should be inserted.\n+\n+        Returns:\n+            bool: True if the insertion was successful, False otherwise.\n+\n+        >>> linked_list = LinkedList()\n+        >>> linked_list.add(1)\n+        >>> linked_list.add(2)\n+        >>> linked_list.add(3)\n+        >>> linked_list.add_at_position(10, 1)\n+        True\n+",
    "comment": "here there should be a test displaying linked_list",
    "line_number": 45,
    "enriched": "File: data_structures/linked_list/__init__.py\nCode: @@ -25,6 +25,41 @@ def add(self, item: Any) -> None:\n         self.head = Node(item, self.head)\n         self.size += 1\n \n+    def add_at_position(self, item: Any, position: int) -> bool:\n+        \"\"\"\n+        Adds a new node with the given item at the specified position in the linked list\n+\n+        Args:\n+            item (Any): The item to be added to the linked list.\n+            position (int): The position at which the item should be inserted.\n+\n+        Returns:\n+            bool: True if the insertion was successful, False otherwise.\n+\n+        >>> linked_list = LinkedList()\n+        >>> linked_list.add(1)\n+        >>> linked_list.add(2)\n+        >>> linked_list.add(3)\n+        >>> linked_list.add_at_position(10, 1)\n+        True\n+\nComment: Here there should be a test displaying `linked_list`",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "data_structures/linked_list/__init__.py",
    "pr_number": 9020,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1310825027,
    "comment_created_at": "2023-08-30T21:06:22Z"
  },
  {
    "code": "@@ -0,0 +1,62 @@\n+from __future__ import annotations\n+\n+\n+def largest_divisible_subset(array: list[int]) -> list[int]:\n+    \"\"\"\n+    Algorithm to find the biggest subset\n+    in the given array such that for any\n+    2 elements x and y in the subset,\n+    either x divides y or y divides x\n+    >>> largest_divisible_subset([1,16,7,8,4])\n+    [16, 8, 4, 1]\n+    >>> largest_divisible_subset([1,2,3])\n+    [2, 1]\n+    >>> largest_divisible_subset([1, 2, 4, 8])\n+    [8, 4, 2, 1]",
    "comment": "let's add some test cases.",
    "line_number": 15,
    "enriched": "File: dynamic_programming/largest_divisible_subset.py\nCode: @@ -0,0 +1,62 @@\n+from __future__ import annotations\n+\n+\n+def largest_divisible_subset(array: list[int]) -> list[int]:\n+    \"\"\"\n+    Algorithm to find the biggest subset\n+    in the given array such that for any\n+    2 elements x and y in the subset,\n+    either x divides y or y divides x\n+    >>> largest_divisible_subset([1,16,7,8,4])\n+    [16, 8, 4, 1]\n+    >>> largest_divisible_subset([1,2,3])\n+    [2, 1]\n+    >>> largest_divisible_subset([1, 2, 4, 8])\n+    [8, 4, 2, 1]\nComment: Let's add some test cases.\r\n```suggestion\r\n    >>> largest_divisible_subset([1, 16, 7, 8, 4])\r\n    [16, 8, 4, 1]\r\n    >>> largest_divisible_subset([1, 2, 3])\r\n    [2, 1]\r\n    >>> largest_divisible_subset([-1, -2, -3])\r\n    [-3]\r\n    >>> largest_divisible_subset([1, 2, 4, 8])\r\n    [8, 4, 2, 1]\r\n    >>> largest_divisible_subset((1, 2, 4, 8))\r\n    [8, 4, 2, 1]\r\n    >>> largest_divisible_subset([1, 1, 1])\r\n    [1, 1, 1]\r\n    >>> largest_divisible_subset([0, 0, 0])\r\n    [0, 0, 0]\r\n    >>> largest_divisible_subset([-1, -1, -1])\r\n    [-1, -1, -1]\r\n    >>> largest_divisible_subset([])\r\n    []\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/largest_divisible_subset.py",
    "pr_number": 9825,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1347549941,
    "comment_created_at": "2023-10-05T14:48:48Z"
  },
  {
    "code": "@@ -0,0 +1,98 @@\n+import random\n+def GenerateMineSweeperMap(n, k):\n+    arr = [[0 for row in range(n)] for column in range(n)]\n+    for num in range(k):\n+        x = random.randint(0,n-1)\n+        y = random.randint(0,n-1)\n+        arr[y][x] = 'X'\n+        if (x >=0 and x <= n-2) and (y >= 0 and y <= n-1):\n+            if arr[y][x+1] != 'X':\n+                arr[y][x+1] += 1 # center right\n+        if (x >=1 and x <= n-1) and (y >= 0 and y <= n-1):\n+            if arr[y][x-1] != 'X':\n+                arr[y][x-1] += 1 # center left\n+        if (x >= 1 and x <= n-1) and (y >= 1 and y <= n-1):\n+            if arr[y-1][x-1] != 'X':\n+                arr[y-1][x-1] += 1 # top left\n+ \n+        if (x >= 0 and x <= n-2) and (y >= 1 and y <= n-1):\n+            if arr[y-1][x+1] != 'X':\n+                arr[y-1][x+1] += 1 # top right\n+        if (x >= 0 and x <= n-1) and (y >= 1 and y <= n-1):\n+            if arr[y-1][x] != 'X':\n+                arr[y-1][x] += 1 # top center\n+ \n+        if (x >=0 and x <= n-2) and (y >= 0 and y <= n-2):\n+            if arr[y+1][x+1] != 'X':\n+                arr[y+1][x+1] += 1 # bottom right\n+        if (x >= 1 and x <= n-1) and (y >= 0 and y <= n-2):\n+            if arr[y+1][x-1] != 'X':\n+                arr[y+1][x-1] += 1 # bottom left\n+        if (x >= 0 and x <= n-1) and (y >= 0 and y <= n-2):\n+            if arr[y+1][x] != 'X':\n+                arr[y+1][x] += 1 # bottom center\n+    return arr\n+def GeneratePlayerMap(n):\n+    arr = [['-' for row in range(n)] for column in range(n)]\n+    return arr\n+def DisplayMap(map):\n+    for row in map:\n+        print(\" \".join(str(cell) for cell in row))\n+        print(\"\")\n+def CheckWon(map):\n+    for row in map:\n+        for cell in row:\n+            if cell == '-':\n+                return False\n+    return True\n+def CheckContinueGame(score):\n+    print(\"Your score: \", score)\n+    isContinue = input(\"Do you want to try again? (y/n) :\")\n+    if isContinue == 'n':\n+        return False\n+    return True\n+def Game():\n+    GameStatus = True\n+    while GameStatus:\n+        difficulty = input(\"Select your difficulty (b, i, h):\")\n+        if difficulty.lower() == 'b':\n+            n = 5\n+            k = 3\n+        elif difficulty.lower() == 'i':\n+            n = 6\n+            k = 8\n+        else:\n+            n = 8\n+            k = 20\n+ \n+        minesweeper_map = GenerateMineSweeperMap(n, k)\n+        player_map = GeneratePlayerMap(n)\n+        score = 0\n+        while True:\n+            if CheckWon(player_map) == False:\n+                print(\"Enter your cell you want to open :\")\n+                x = input(\"X (1 to 5) :\")\n+                y = input(\"Y (1 to 5) :\")\n+                x = int(x) \u2014 1 # 0 based indexing",
    "comment": "an error occured while parsing the file: backtracking/minesweeper_manil-keo.py\npython\ntraceback (most recent call last):\n  file \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 145, in parse\n    reports = lint_file(\nlibcst._exceptions.parsersyntaxerror: syntax error @ 76:28.\n'\u2014' is not a valid token.\n\n                x = int(x) \u2014 1 # 0 based indexing\n                           ^",
    "line_number": 76,
    "enriched": "File: backtracking/minesweeper_ManiL-Keo.py\nCode: @@ -0,0 +1,98 @@\n+import random\n+def GenerateMineSweeperMap(n, k):\n+    arr = [[0 for row in range(n)] for column in range(n)]\n+    for num in range(k):\n+        x = random.randint(0,n-1)\n+        y = random.randint(0,n-1)\n+        arr[y][x] = 'X'\n+        if (x >=0 and x <= n-2) and (y >= 0 and y <= n-1):\n+            if arr[y][x+1] != 'X':\n+                arr[y][x+1] += 1 # center right\n+        if (x >=1 and x <= n-1) and (y >= 0 and y <= n-1):\n+            if arr[y][x-1] != 'X':\n+                arr[y][x-1] += 1 # center left\n+        if (x >= 1 and x <= n-1) and (y >= 1 and y <= n-1):\n+            if arr[y-1][x-1] != 'X':\n+                arr[y-1][x-1] += 1 # top left\n+ \n+        if (x >= 0 and x <= n-2) and (y >= 1 and y <= n-1):\n+            if arr[y-1][x+1] != 'X':\n+                arr[y-1][x+1] += 1 # top right\n+        if (x >= 0 and x <= n-1) and (y >= 1 and y <= n-1):\n+            if arr[y-1][x] != 'X':\n+                arr[y-1][x] += 1 # top center\n+ \n+        if (x >=0 and x <= n-2) and (y >= 0 and y <= n-2):\n+            if arr[y+1][x+1] != 'X':\n+                arr[y+1][x+1] += 1 # bottom right\n+        if (x >= 1 and x <= n-1) and (y >= 0 and y <= n-2):\n+            if arr[y+1][x-1] != 'X':\n+                arr[y+1][x-1] += 1 # bottom left\n+        if (x >= 0 and x <= n-1) and (y >= 0 and y <= n-2):\n+            if arr[y+1][x] != 'X':\n+                arr[y+1][x] += 1 # bottom center\n+    return arr\n+def GeneratePlayerMap(n):\n+    arr = [['-' for row in range(n)] for column in range(n)]\n+    return arr\n+def DisplayMap(map):\n+    for row in map:\n+        print(\" \".join(str(cell) for cell in row))\n+        print(\"\")\n+def CheckWon(map):\n+    for row in map:\n+        for cell in row:\n+            if cell == '-':\n+                return False\n+    return True\n+def CheckContinueGame(score):\n+    print(\"Your score: \", score)\n+    isContinue = input(\"Do you want to try again? (y/n) :\")\n+    if isContinue == 'n':\n+        return False\n+    return True\n+def Game():\n+    GameStatus = True\n+    while GameStatus:\n+        difficulty = input(\"Select your difficulty (b, i, h):\")\n+        if difficulty.lower() == 'b':\n+            n = 5\n+            k = 3\n+        elif difficulty.lower() == 'i':\n+            n = 6\n+            k = 8\n+        else:\n+            n = 8\n+            k = 20\n+ \n+        minesweeper_map = GenerateMineSweeperMap(n, k)\n+        player_map = GeneratePlayerMap(n)\n+        score = 0\n+        while True:\n+            if CheckWon(player_map) == False:\n+                print(\"Enter your cell you want to open :\")\n+                x = input(\"X (1 to 5) :\")\n+                y = input(\"Y (1 to 5) :\")\n+                x = int(x) \u2014 1 # 0 based indexing\nComment: An error occured while parsing the file: `backtracking/minesweeper_ManiL-Keo.py`\n```python\nTraceback (most recent call last):\n  File \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 145, in parse\n    reports = lint_file(\nlibcst._exceptions.ParserSyntaxError: Syntax Error @ 76:28.\n'\u2014' is not a valid token.\n\n                x = int(x) \u2014 1 # 0 based indexing\n                           ^\n\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "backtracking/minesweeper_ManiL-Keo.py",
    "pr_number": 7120,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 994647725,
    "comment_created_at": "2022-10-13T13:29:35Z"
  },
  {
    "code": "@@ -62,6 +63,45 @@ def random_characters(chars_incl, i):\n     pass  # Put your code here...\n \n \n+# This Will Check Whether A Given Password Is Strong Or Not\n+# It Follows The Rule that Length Of Password Should Be At Least 8 Characters\n+# And At Least 1 Lower, 1 Upper, 1 Number And 1 Special Character\n+def strong_password_detector(password: str, min_length: int = 8) -> str:\n+    \"\"\"\n+    >>> strong_password_detector('Hwea7$2!')\n+    'This is a strong Password'\n+\n+    >>> strong_password_detector('Sh0r1')\n+    'Your Password must be at least 8 characters long'\n+\n+    >>> strong_password_detector('Hello123')\n+    'Password should contain UPPERCASE, lowercase, numbers, special characters'\n+\n+    >>> strong_password_detector('Hello1238udfhiaf038fajdvjjf!jaiuFhkqi1')\n+    'This is a strong Password'\n+\n+    >>> strong_password_detector(0)\n+    'Your Password must be at least 8 characters long'\n+    \"\"\"\n+\n+    if len(str(password)) < 8:",
    "comment": "also, short passwords and weak passwords should raise valueerrors.",
    "line_number": 87,
    "enriched": "File: other/password.py\nCode: @@ -62,6 +63,45 @@ def random_characters(chars_incl, i):\n     pass  # Put your code here...\n \n \n+# This Will Check Whether A Given Password Is Strong Or Not\n+# It Follows The Rule that Length Of Password Should Be At Least 8 Characters\n+# And At Least 1 Lower, 1 Upper, 1 Number And 1 Special Character\n+def strong_password_detector(password: str, min_length: int = 8) -> str:\n+    \"\"\"\n+    >>> strong_password_detector('Hwea7$2!')\n+    'This is a strong Password'\n+\n+    >>> strong_password_detector('Sh0r1')\n+    'Your Password must be at least 8 characters long'\n+\n+    >>> strong_password_detector('Hello123')\n+    'Password should contain UPPERCASE, lowercase, numbers, special characters'\n+\n+    >>> strong_password_detector('Hello1238udfhiaf038fajdvjjf!jaiuFhkqi1')\n+    'This is a strong Password'\n+\n+    >>> strong_password_detector(0)\n+    'Your Password must be at least 8 characters long'\n+    \"\"\"\n+\n+    if len(str(password)) < 8:\nComment: ```suggestion\r\n    if len(password) < min_length:\r\n```\r\n Also, short passwords and weak passwords should raise ValueErrors.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "other/password.py",
    "pr_number": 7939,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1011688023,
    "comment_created_at": "2022-11-02T12:31:17Z"
  },
  {
    "code": "@@ -11,6 +11,39 @@ def perfect_cube(n: int) -> bool:\n     return (val * val * val) == n\n \n \n+def perfect_cube_binary_search(n: int) -> bool:\n+    \"\"\"\n+    Check if a number is a perfect cube or not using binary search.\n+    Time complexity : O(Log(n))\n+    Space complexity: O(1)\n+\n+    >>> perfect_cube_binary_search(27)\n+    True\n+    >>> perfect_cube_binary_search(64)\n+    True\n+    >>> perfect_cube_binary_search(4)\n+    False\n+    >>> perfect_cube_binary_search(\"a\")\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: '<=' not supported between instances of 'int' and 'str'",
    "comment": "please add your own typeerror check for invalid inputs like this rather than relying on the built-in error message for <=",
    "line_number": 29,
    "enriched": "File: maths/perfect_cube.py\nCode: @@ -11,6 +11,39 @@ def perfect_cube(n: int) -> bool:\n     return (val * val * val) == n\n \n \n+def perfect_cube_binary_search(n: int) -> bool:\n+    \"\"\"\n+    Check if a number is a perfect cube or not using binary search.\n+    Time complexity : O(Log(n))\n+    Space complexity: O(1)\n+\n+    >>> perfect_cube_binary_search(27)\n+    True\n+    >>> perfect_cube_binary_search(64)\n+    True\n+    >>> perfect_cube_binary_search(4)\n+    False\n+    >>> perfect_cube_binary_search(\"a\")\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: '<=' not supported between instances of 'int' and 'str'\nComment: Please add your own TypeError check for invalid inputs like this rather than relying on the built-in error message for `<=`",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "maths/perfect_cube.py",
    "pr_number": 10477,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1361630517,
    "comment_created_at": "2023-10-17T07:30:24Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+def strong_password_detection(password):\n+    AllowedSymbols = [\"#\", \"@\", \"$\", \"_\", \"*\", \"-\"]",
    "comment": "we require type hints and doctests...  please add tests to cover all the cases below.",
    "line_number": 2,
    "enriched": "File: strong_password_detection.py\nCode: @@ -0,0 +1,42 @@\n+def strong_password_detection(password):\n+    AllowedSymbols = [\"#\", \"@\", \"$\", \"_\", \"*\", \"-\"]\nComment: We require type hints and doctests...  Please add tests to cover all the cases below.\r\n```suggestion\r\ndef strong_password_detection(password: str) -> bool:\r\n    \"\"\"\r\n    >>> strong_password_detection(\"str0ng@Password\")\r\n    True\r\n    >>> strong_password_detection(\"str0ngPassword\")\r\n    False\r\n    \"\"\"\r\n    allowed_symbols = {c for c in \"#@$_*-\"}\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "strong_password_detection.py",
    "pr_number": 10885,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1369908079,
    "comment_created_at": "2023-10-24T09:56:38Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+def first_missing_positive(nums: list[int]) -> int:\n+    \"\"\"\n+    Given an unsorted integer array nums, returns the smallest missing positive integer.\n+\n+    The algorithm must run in O(n) time and use O(1) auxiliary space.\n+\n+    Hints:\n+    - Try to place each number in its correct position, so that nums[i] equals i + 1.\n+    - Iterate through the array to find the first position where nums[i] != i + 1,\n+      indicating the missing positive integer.\n+\n+    :param nums: The input unsorted integer array.\n+    :return: The smallest missing positive integer.\n+\n+    Examples:\n+    >>> first_missing_positive([1, 2, 0])",
    "comment": "please add doctests for\r\n\r\n>>> first_missing_positive([])\r\n>>> first_missing_positive([0])\r\n>>> first_missing_positive([1])\r\n>>> first_missing_positive([1.1, 2.2, 3.3])\r\n>>> first_missing_positive([-1, -2, -3])\r\n>>> first_missing_positive(\"abc\")",
    "line_number": 16,
    "enriched": "File: data_structures/arrays/first_missing_positive.py\nCode: @@ -0,0 +1,60 @@\n+def first_missing_positive(nums: list[int]) -> int:\n+    \"\"\"\n+    Given an unsorted integer array nums, returns the smallest missing positive integer.\n+\n+    The algorithm must run in O(n) time and use O(1) auxiliary space.\n+\n+    Hints:\n+    - Try to place each number in its correct position, so that nums[i] equals i + 1.\n+    - Iterate through the array to find the first position where nums[i] != i + 1,\n+      indicating the missing positive integer.\n+\n+    :param nums: The input unsorted integer array.\n+    :return: The smallest missing positive integer.\n+\n+    Examples:\n+    >>> first_missing_positive([1, 2, 0])\nComment: Please add doctests for\r\n```\r\n>>> first_missing_positive([])\r\n>>> first_missing_positive([0])\r\n>>> first_missing_positive([1])\r\n>>> first_missing_positive([1.1, 2.2, 3.3])\r\n>>> first_missing_positive([-1, -2, -3])\r\n>>> first_missing_positive(\"ABC\")\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/arrays/first_missing_positive.py",
    "pr_number": 11186,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1409791999,
    "comment_created_at": "2023-11-29T19:48:31Z"
  },
  {
    "code": "@@ -0,0 +1,85 @@\n+from collections.abc import Callable\n+\n+import numpy as np\n+\n+\n+def weierstrass_method(\n+    polynomial: Callable[[np.ndarray], np.ndarray],\n+    degree: int,\n+    roots: np.ndarray | None = None,\n+    max_iter: int = 100,\n+) -> np.ndarray:\n+    \"\"\"\n+    Approximates all complex roots of a polynomial using the\n+    Weierstrass (Durand-Kerner) method.\n+    Args:\n+        polynomial: A function that takes a NumPy array of complex numbers and returns\n+                    the polynomial values at those points.\n+        degree: Degree of the polynomial (number of roots to find). Must be \u2265 1.\n+        roots:  Optional initial guess as a NumPy array of complex numbers.\n+                Must have length equal to 'degree'.\n+                If None, perturbed complex roots of unity are used.\n+        max_iter: Number of iterations to perform (default: 100).\n+\n+    Returns:\n+        np.ndarray: Array of approximated complex roots.\n+\n+    Raises:\n+        ValueError: If degree < 1, or if initial roots length doesn't match the degree.\n+\n+    Note:\n+        - Root updates are clipped to prevent numerical overflow.\n+\n+    Example:\n+        >>> import numpy as np\n+        >>> def f(x): return x**2 - 1\n+        >>> roots = weierstrass_method(f, 2)\n+        >>> np.allclose(np.sort(roots), np.sort(np.array([1, -1])))\n+        True\n+",
    "comment": "lets add some more tests:",
    "line_number": 51,
    "enriched": "File: maths/numerical_analysis/weierstrass_method.py\nCode: @@ -0,0 +1,85 @@\n+from collections.abc import Callable\n+\n+import numpy as np\n+\n+\n+def weierstrass_method(\n+    polynomial: Callable[[np.ndarray], np.ndarray],\n+    degree: int,\n+    roots: np.ndarray | None = None,\n+    max_iter: int = 100,\n+) -> np.ndarray:\n+    \"\"\"\n+    Approximates all complex roots of a polynomial using the\n+    Weierstrass (Durand-Kerner) method.\n+    Args:\n+        polynomial: A function that takes a NumPy array of complex numbers and returns\n+                    the polynomial values at those points.\n+        degree: Degree of the polynomial (number of roots to find). Must be \u2265 1.\n+        roots:  Optional initial guess as a NumPy array of complex numbers.\n+                Must have length equal to 'degree'.\n+                If None, perturbed complex roots of unity are used.\n+        max_iter: Number of iterations to perform (default: 100).\n+\n+    Returns:\n+        np.ndarray: Array of approximated complex roots.\n+\n+    Raises:\n+        ValueError: If degree < 1, or if initial roots length doesn't match the degree.\n+\n+    Note:\n+        - Root updates are clipped to prevent numerical overflow.\n+\n+    Example:\n+        >>> import numpy as np\n+        >>> def f(x): return x**2 - 1\n+        >>> roots = weierstrass_method(f, 2)\n+        >>> np.allclose(np.sort(roots), np.sort(np.array([1, -1])))\n+        True\n+\nComment: Lets add some more tests:\r\n```suggestion\r\n        >>> def f(x): return x**3 - 6*x**2 + 11*x - 6\r\n        >>> roots = weierstrass_method(f, 3)\r\n        >>> np.allclose(np.sort(roots), np.array([1, 2, 3]))\r\n        True\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/numerical_analysis/weierstrass_method.py",
    "pr_number": 12877,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2307894524,
    "comment_created_at": "2025-08-28T16:10:29Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+import numpy as np",
    "comment": "an error occurred while parsing the file: audio_filters/kalman_filter.py\npython\ntraceback (most recent call last):\n  file \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 146, in parse\n    reports = lint_file(\n              ^^^^^^^^^^\nlibcst._exceptions.parsersyntaxerror: syntax error @ 1:1.\ntokenizer error: \"\\u{a0}\\u{a0}kf\" is not a valid identifier\n\nimport numpy as np\n^",
    "line_number": 1,
    "enriched": "File: audio_filters/kalman_filter.py\nCode: @@ -0,0 +1,35 @@\n+import numpy as np\nComment: An error occurred while parsing the file: `audio_filters/kalman_filter.py`\n```python\nTraceback (most recent call last):\n  File \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 146, in parse\n    reports = lint_file(\n              ^^^^^^^^^^\nlibcst._exceptions.ParserSyntaxError: Syntax Error @ 1:1.\ntokenizer error: \"\\u{a0}\\u{a0}kf\" is not a valid identifier\n\nimport numpy as np\n^\n\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "audio_filters/kalman_filter.py",
    "pr_number": 10402,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359141645,
    "comment_created_at": "2023-10-14T04:24:24Z"
  },
  {
    "code": "@@ -0,0 +1,41 @@\n+def minkowski_distance(\n+    point_a: list[float],\n+    point_b: list[float],\n+    order: int,\n+) -> float:\n+    \"\"\"\n+    This function calculates the Minkowski distance for a given order between\n+    two n-dimensional points represented as lists. For the case of order = 1,\n+    the Minkowski distance degenerates to the Manhattan distance. For\n+    order = 2, the usual Euclidean distance is obtained.\n+\n+    https://en.wikipedia.org/wiki/Minkowski_distance\n+\n+    >>> minkowski_distance([1.0, 1.0], [2.0, 2.0], 1)\n+    2.0\n+    >>> minkowski_distance([1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], 2)\n+    8.0\n+    >>> minkowski_distance([1.0], [2.0], -1)\n+    Traceback (most recent call last):\n+        ...\n+    Exception: The order must be greater than or equal to 1.\n+    >>> minkowski_distance([1.0], [1.0, 2.0], 1)\n+    Traceback (most recent call last):\n+        ...\n+    Exception: Both points must have the same dimension.\n+    \"\"\"\n+    if order < 1:\n+        raise Exception(\"The order must be greater than or equal to 1.\")",
    "comment": "please make the error of a more specific type",
    "line_number": 28,
    "enriched": "File: maths/minkowski_distance.py\nCode: @@ -0,0 +1,41 @@\n+def minkowski_distance(\n+    point_a: list[float],\n+    point_b: list[float],\n+    order: int,\n+) -> float:\n+    \"\"\"\n+    This function calculates the Minkowski distance for a given order between\n+    two n-dimensional points represented as lists. For the case of order = 1,\n+    the Minkowski distance degenerates to the Manhattan distance. For\n+    order = 2, the usual Euclidean distance is obtained.\n+\n+    https://en.wikipedia.org/wiki/Minkowski_distance\n+\n+    >>> minkowski_distance([1.0, 1.0], [2.0, 2.0], 1)\n+    2.0\n+    >>> minkowski_distance([1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], 2)\n+    8.0\n+    >>> minkowski_distance([1.0], [2.0], -1)\n+    Traceback (most recent call last):\n+        ...\n+    Exception: The order must be greater than or equal to 1.\n+    >>> minkowski_distance([1.0], [1.0, 2.0], 1)\n+    Traceback (most recent call last):\n+        ...\n+    Exception: Both points must have the same dimension.\n+    \"\"\"\n+    if order < 1:\n+        raise Exception(\"The order must be greater than or equal to 1.\")\nComment: ```suggestion\r\n        raise ValueError(\"The order must be greater than or equal to 1.\")\r\n```\r\nPlease make the error of a more specific type",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/minkowski_distance.py",
    "pr_number": 10143,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349789820,
    "comment_created_at": "2023-10-08T22:48:59Z"
  },
  {
    "code": "@@ -0,0 +1,43 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 1, 2022\n+\n+Task:\n+Given a positive int number. Return True if this number is power of 2\n+and False otherwise.\n+\n+Implementation notes: Use bit manipulation.\n+For example if the number is the power of two it's bits representation:\n+n     = 0..100..00\n+n - 1 = 0..011..11\n+\n+n & (n - 1) - no intersections = 0\n+\n+\"\"\"\n+\n+\n+def is_power_of_two(number: int) -> bool:\n+    \"\"\"\n+    >>> is_power_of_two(2)",
    "comment": "please add tests for 0, -2, -8, 8.0, 8.1",
    "line_number": 21,
    "enriched": "File: bit_manipulation/is_power_of_two.py\nCode: @@ -0,0 +1,43 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 1, 2022\n+\n+Task:\n+Given a positive int number. Return True if this number is power of 2\n+and False otherwise.\n+\n+Implementation notes: Use bit manipulation.\n+For example if the number is the power of two it's bits representation:\n+n     = 0..100..00\n+n - 1 = 0..011..11\n+\n+n & (n - 1) - no intersections = 0\n+\n+\"\"\"\n+\n+\n+def is_power_of_two(number: int) -> bool:\n+    \"\"\"\n+    >>> is_power_of_two(2)\nComment: Please add tests for 0, -2, -8, 8.0, 8.1",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "bit_manipulation/is_power_of_two.py",
    "pr_number": 7936,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1010555451,
    "comment_created_at": "2022-11-01T15:18:15Z"
  },
  {
    "code": "@@ -0,0 +1,164 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 24, 2022\n+\n+Task:\n+Given an m x n grid of characters board and a string word,\n+return true if word exists in the grid.\n+\n+The word can be constructed from letters of sequentially adjacent cells,\n+where adjacent cells are horizontally or vertically neighboring.\n+The same letter cell may not be used more than once.\n+\n+Example:\n+\n+Matrix:\n+---------\n+|A|B|C|E|\n+|S|F|C|S|\n+|A|D|E|E|\n+---------\n+\n+Word:\n+\"ABCCED\"\n+\n+Result:\n+True\n+\n+Implementation notes: Use backtracking approach.\n+For each point need to check neigh neighbours\n+and try to find left suffix of the word.\n+\n+leetcode: https://leetcode.com/problems/word-search/\n+\n+\"\"\"\n+\n+\n+def word_exists(board: list[list[str]], word: str) -> bool:\n+    \"\"\"\n+    >>> word_exists([[\"A\",\"B\",\"C\",\"E\"],[\"S\",\"F\",\"C\",\"S\"],[\"A\",\"D\",\"E\",\"E\"]], \"ABCCED\")\n+    True\n+    >>> word_exists([[\"A\",\"B\",\"C\",\"E\"],[\"S\",\"F\",\"C\",\"S\"],[\"A\",\"D\",\"E\",\"E\"]], \"SEE\")\n+    True\n+    >>> word_exists([[\"A\",\"B\",\"C\",\"E\"],[\"S\",\"F\",\"C\",\"S\"],[\"A\",\"D\",\"E\",\"E\"]], \"ABCB\")\n+    False\n+    >>> word_exists([[\"A\"]], \"A\")\n+    True\n+    >>> word_exists([[\"A\",\"A\",\"A\",\"A\",\"A\",\"A\"],\\\n+                     [\"A\",\"A\",\"A\",\"A\",\"A\",\"A\"],\\\n+                     [\"A\",\"A\",\"A\",\"A\",\"A\",\"A\"],\\\n+                     [\"A\",\"A\",\"A\",\"A\",\"A\",\"A\"],\\\n+                     [\"A\",\"A\",\"A\",\"A\",\"A\",\"B\"],\\\n+                     [\"A\",\"A\",\"A\",\"A\",\"B\",\"A\"]],\\\n+                    \"AAAAAAAAAAAAABB\")\n+    False\n+    >>> word_exists([[\"A\"]], 123)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The word parameter should be a string of length greater than 0.\n+    >>> word_exists([[\"A\"]], \"\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The word parameter should be a string of length greater than 0.\n+    >>> word_exists([[]], \"AB\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The board should be a non empty matrix of single chars strings.\n+    >>> word_exists([], \"AB\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The board should be a non empty matrix of single chars strings.\n+    >>> word_exists([[\"A\"], [21]], \"AB\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The board should be a non empty matrix of single chars strings.\n+    \"\"\"\n+\n+    def validate_board(board: list[list[str]]):",
    "comment": "please provide return type hint for the function: validate_board. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 77,
    "enriched": "File: backtracking/word_search.py\nCode: @@ -0,0 +1,164 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 24, 2022\n+\n+Task:\n+Given an m x n grid of characters board and a string word,\n+return true if word exists in the grid.\n+\n+The word can be constructed from letters of sequentially adjacent cells,\n+where adjacent cells are horizontally or vertically neighboring.\n+The same letter cell may not be used more than once.\n+\n+Example:\n+\n+Matrix:\n+---------\n+|A|B|C|E|\n+|S|F|C|S|\n+|A|D|E|E|\n+---------\n+\n+Word:\n+\"ABCCED\"\n+\n+Result:\n+True\n+\n+Implementation notes: Use backtracking approach.\n+For each point need to check neigh neighbours\n+and try to find left suffix of the word.\n+\n+leetcode: https://leetcode.com/problems/word-search/\n+\n+\"\"\"\n+\n+\n+def word_exists(board: list[list[str]], word: str) -> bool:\n+    \"\"\"\n+    >>> word_exists([[\"A\",\"B\",\"C\",\"E\"],[\"S\",\"F\",\"C\",\"S\"],[\"A\",\"D\",\"E\",\"E\"]], \"ABCCED\")\n+    True\n+    >>> word_exists([[\"A\",\"B\",\"C\",\"E\"],[\"S\",\"F\",\"C\",\"S\"],[\"A\",\"D\",\"E\",\"E\"]], \"SEE\")\n+    True\n+    >>> word_exists([[\"A\",\"B\",\"C\",\"E\"],[\"S\",\"F\",\"C\",\"S\"],[\"A\",\"D\",\"E\",\"E\"]], \"ABCB\")\n+    False\n+    >>> word_exists([[\"A\"]], \"A\")\n+    True\n+    >>> word_exists([[\"A\",\"A\",\"A\",\"A\",\"A\",\"A\"],\\\n+                     [\"A\",\"A\",\"A\",\"A\",\"A\",\"A\"],\\\n+                     [\"A\",\"A\",\"A\",\"A\",\"A\",\"A\"],\\\n+                     [\"A\",\"A\",\"A\",\"A\",\"A\",\"A\"],\\\n+                     [\"A\",\"A\",\"A\",\"A\",\"A\",\"B\"],\\\n+                     [\"A\",\"A\",\"A\",\"A\",\"B\",\"A\"]],\\\n+                    \"AAAAAAAAAAAAABB\")\n+    False\n+    >>> word_exists([[\"A\"]], 123)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The word parameter should be a string of length greater than 0.\n+    >>> word_exists([[\"A\"]], \"\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The word parameter should be a string of length greater than 0.\n+    >>> word_exists([[]], \"AB\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The board should be a non empty matrix of single chars strings.\n+    >>> word_exists([], \"AB\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The board should be a non empty matrix of single chars strings.\n+    >>> word_exists([[\"A\"], [21]], \"AB\")\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: The board should be a non empty matrix of single chars strings.\n+    \"\"\"\n+\n+    def validate_board(board: list[list[str]]):\nComment: Please provide return type hint for the function: `validate_board`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "backtracking/word_search.py",
    "pr_number": 8005,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1033190550,
    "comment_created_at": "2022-11-28T07:12:42Z"
  },
  {
    "code": "@@ -0,0 +1,155 @@\n+import warnings\n+\n+import numpy as np\n+from sklearn.datasets import load_breast_cancer\n+from sklearn.metrics import accuracy_score\n+from sklearn.preprocessing import MinMaxScaler\n+\n+warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n+\n+\n+def train_network(",
    "comment": "as there is no test file in this pull request nor any test function or class in the file neural_network/backpropagation_weight_decay.py, please provide doctest for the function train_network",
    "line_number": 11,
    "enriched": "File: neural_network/backpropagation_weight_decay.py\nCode: @@ -0,0 +1,155 @@\n+import warnings\n+\n+import numpy as np\n+from sklearn.datasets import load_breast_cancer\n+from sklearn.metrics import accuracy_score\n+from sklearn.preprocessing import MinMaxScaler\n+\n+warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n+\n+\n+def train_network(\nComment: As there is no test file in this pull request nor any test function or class in the file `neural_network/backpropagation_weight_decay.py`, please provide doctest for the function `train_network`",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "neural_network/backpropagation_weight_decay.py",
    "pr_number": 12420,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1870394144,
    "comment_created_at": "2024-12-04T23:03:02Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+def nth_sgonal_num(n: int, s: int) -> int:\n+    \"\"\"\n+    Returns the `n`th `s`-gonal number. It is assumed that `n` >= 0 and `s` >= 3\n+    (see for reference https://en.wikipedia.org/wiki/Polygonal_number).\n+\n+    >>> nth_sgonal_num(0, 3)\n+    0\n+    >>> nth_sgonal_num(3, 3)\n+    6\n+    >>> nth_sgonal_num(5, 4)\n+    25\n+    >>> nth_sgonal_num(2, 5)\n+    5\n+    \"\"\"\n+    return ((s - 2) * (n**2) - (s - 4) * n) // 2",
    "comment": "handle special cases: the code currently returns 0 for n = 0, but it doesn't handle the case when n > 0 and s < 3.\r\n\r\n         if n < 0 or sides < 3:\r\n                raise valueerror(\"invalid input: n must be >= 0 and sides must be >= 3.\")",
    "line_number": 15,
    "enriched": "File: maths/nth_sgonal_num.py\nCode: @@ -0,0 +1,21 @@\n+def nth_sgonal_num(n: int, s: int) -> int:\n+    \"\"\"\n+    Returns the `n`th `s`-gonal number. It is assumed that `n` >= 0 and `s` >= 3\n+    (see for reference https://en.wikipedia.org/wiki/Polygonal_number).\n+\n+    >>> nth_sgonal_num(0, 3)\n+    0\n+    >>> nth_sgonal_num(3, 3)\n+    6\n+    >>> nth_sgonal_num(5, 4)\n+    25\n+    >>> nth_sgonal_num(2, 5)\n+    5\n+    \"\"\"\n+    return ((s - 2) * (n**2) - (s - 4) * n) // 2\nComment: Handle special cases: The code currently returns 0 for n = 0, but it doesn't handle the case when n > 0 and s < 3.\r\n\r\n         if n < 0 or sides < 3:\r\n                raise ValueError(\"Invalid input: n must be >= 0 and sides must be >= 3.\")\r\n",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/nth_sgonal_num.py",
    "pr_number": 8750,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1199548146,
    "comment_created_at": "2023-05-20T03:37:14Z"
  },
  {
    "code": "@@ -39,23 +40,30 @@ def binary_insertion_sort(collection: list) -> list:\n \n     n = len(collection)\n     for i in range(1, n):\n-        val = collection[i]\n+        value_to_insert = collection[i]\n         low = 0\n         high = i - 1\n \n         while low <= high:\n             mid = (low + high) // 2\n-            if val < collection[mid]:\n+            if value_to_insert < collection[mid]:\n                 high = mid - 1\n             else:\n                 low = mid + 1\n+\n         for j in range(i, low, -1):\n             collection[j] = collection[j - 1]\n-        collection[low] = val\n+\n+        collection[low] = value_to_insert\n+\n     return collection\n \n \n-if __name__ == \"__main__\":\n-    user_input = input(\"Enter numbers separated by a comma:\\n\").strip()\n-    unsorted = [int(item) for item in user_input.split(\",\")]\n-    print(binary_insertion_sort(unsorted))\n+if __name__ == \"__main\":\n+    try:\n+        user_input = input(\"Enter numbers separated by a comma:\\n\").strip()\n+        unsorted = [int(item) for item in user_input.split(\",\")]\n+        sorted_list = binary_insertion_sort(unsorted)\n+        print(sorted_list)\n+    except ValueError:\n+        print(\"Invalid input. Please enter valid integers separated by commas.\")",
    "comment": "https://peps.python.org/pep-0008/#programming-recommendations\r\n> limit the try clause to the absolute minimum amount of code necessary.",
    "line_number": 69,
    "enriched": "File: sorts/binary_insertion_sort.py\nCode: @@ -39,23 +40,30 @@ def binary_insertion_sort(collection: list) -> list:\n \n     n = len(collection)\n     for i in range(1, n):\n-        val = collection[i]\n+        value_to_insert = collection[i]\n         low = 0\n         high = i - 1\n \n         while low <= high:\n             mid = (low + high) // 2\n-            if val < collection[mid]:\n+            if value_to_insert < collection[mid]:\n                 high = mid - 1\n             else:\n                 low = mid + 1\n+\n         for j in range(i, low, -1):\n             collection[j] = collection[j - 1]\n-        collection[low] = val\n+\n+        collection[low] = value_to_insert\n+\n     return collection\n \n \n-if __name__ == \"__main__\":\n-    user_input = input(\"Enter numbers separated by a comma:\\n\").strip()\n-    unsorted = [int(item) for item in user_input.split(\",\")]\n-    print(binary_insertion_sort(unsorted))\n+if __name__ == \"__main\":\n+    try:\n+        user_input = input(\"Enter numbers separated by a comma:\\n\").strip()\n+        unsorted = [int(item) for item in user_input.split(\",\")]\n+        sorted_list = binary_insertion_sort(unsorted)\n+        print(sorted_list)\n+    except ValueError:\n+        print(\"Invalid input. Please enter valid integers separated by commas.\")\nComment: https://peps.python.org/pep-0008/#programming-recommendations\r\n> Limit the try clause to the absolute minimum amount of code necessary.\r\n```suggestion\r\n    user_input = input(\"Enter numbers separated by a comma:\\n\").strip()\r\n    try:\r\n        unsorted = [int(item) for item in user_input.split(\",\")]\r\n    except ValueError:\r\n        print(\"Invalid input. Please enter valid integers separated by commas.\")\r\n        raise\r\n    print(f\"{binary_insertion_sort(unsorted) = }\")\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "sorts/binary_insertion_sort.py",
    "pr_number": 10918,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1370849357,
    "comment_created_at": "2023-10-24T21:45:29Z"
  },
  {
    "code": "@@ -0,0 +1,83 @@\n+\"\"\"\n+https://atharayil.medium.com/median-of-two-sorted-arrays-day-36-python-fcbd2dbbb668\n+\"\"\"\n+\n+\n+def find_median_sorted_arrays(nums1: list[int], nums2: list[int]) -> float:\n+    \"\"\"\n+    Finds the median of two sorted arrays.\n+\n+    :param nums1: The first sorted array.\n+    :param nums2: The second sorted array.\n+    :return: The median of the combined sorted arrays.\n+    :raises ValueError: If both input arrays are empty.\n+\n+    >>> find_median_sorted_arrays([1, 3], [2])",
    "comment": "test please for negative numbers, floating point numbers, letters, two empty arrays, arrays that are not sorted, etc?",
    "line_number": 15,
    "enriched": "File: data_structures/arrays/find_median_sorted_arrays.py\nCode: @@ -0,0 +1,83 @@\n+\"\"\"\n+https://atharayil.medium.com/median-of-two-sorted-arrays-day-36-python-fcbd2dbbb668\n+\"\"\"\n+\n+\n+def find_median_sorted_arrays(nums1: list[int], nums2: list[int]) -> float:\n+    \"\"\"\n+    Finds the median of two sorted arrays.\n+\n+    :param nums1: The first sorted array.\n+    :param nums2: The second sorted array.\n+    :return: The median of the combined sorted arrays.\n+    :raises ValueError: If both input arrays are empty.\n+\n+    >>> find_median_sorted_arrays([1, 3], [2])\nComment: Test please for negative numbers, floating point numbers, letters, two empty arrays, arrays that are not sorted, etc?",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "data_structures/arrays/find_median_sorted_arrays.py",
    "pr_number": 11182,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1409832063,
    "comment_created_at": "2023-11-29T20:30:44Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 1, 2022\n+\n+Task:\n+Given a list of days when you need to travel. Each day is int from 1 to 365.\n+You are able to use tickets for 1 day, 7 days and 30 days.\n+Each ticket has a cost.\n+\n+Find the minimum cost you need to travel every day in the given list of days.\n+\n+Implementation notes:\n+implementation Dynamic Programming up bottom approach.\n+\n+The implementation was tested on the\n+leetcode: https://leetcode.com/problems/minimum-cost-for-tickets/\n+\"\"\"\n+\n+from typing import List\n+\n+\"\"\"\n+Minimum Cost For Tickets\n+Dynamic Programming: up -> down.\n+\"\"\"\n+\n+\n+def mincost_tickets(days: list[int], costs: list[int]) -> int:\n+    \"\"\"\n+    >>> mincost_tickets([1,4,6,7,8,20], [2,7,15])\n+    11\n+    >>> mincost_tickets([1,2,3,4,5,6,7,8,9,10,30,31], [2,7,15])\n+    17\n+    >>> mincost_tickets([1,2,3,4,5,6,7,8,9,10,30,31], [2,90,150])\n+    24",
    "comment": "please add tests with zero, negative numbers, floating point numbers, and mixing empty lists and non-empty lists.",
    "line_number": 34,
    "enriched": "File: dynamic_programming/minimum_tickets_cost.py\nCode: @@ -0,0 +1,61 @@\n+\"\"\"\n+Author  : Alexander Pantyukhin\n+Date    : November 1, 2022\n+\n+Task:\n+Given a list of days when you need to travel. Each day is int from 1 to 365.\n+You are able to use tickets for 1 day, 7 days and 30 days.\n+Each ticket has a cost.\n+\n+Find the minimum cost you need to travel every day in the given list of days.\n+\n+Implementation notes:\n+implementation Dynamic Programming up bottom approach.\n+\n+The implementation was tested on the\n+leetcode: https://leetcode.com/problems/minimum-cost-for-tickets/\n+\"\"\"\n+\n+from typing import List\n+\n+\"\"\"\n+Minimum Cost For Tickets\n+Dynamic Programming: up -> down.\n+\"\"\"\n+\n+\n+def mincost_tickets(days: list[int], costs: list[int]) -> int:\n+    \"\"\"\n+    >>> mincost_tickets([1,4,6,7,8,20], [2,7,15])\n+    11\n+    >>> mincost_tickets([1,2,3,4,5,6,7,8,9,10,30,31], [2,7,15])\n+    17\n+    >>> mincost_tickets([1,2,3,4,5,6,7,8,9,10,30,31], [2,90,150])\n+    24\nComment: Please add tests with zero, negative numbers, floating point numbers, and mixing empty lists and non-empty lists.",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "dynamic_programming/minimum_tickets_cost.py",
    "pr_number": 7934,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1010821032,
    "comment_created_at": "2022-11-01T19:28:12Z"
  },
  {
    "code": "@@ -0,0 +1,72 @@\n+\"\"\"\n+wissamfawaz12@gmail.com | github.com/wissamfawaz\n+This implementation demonstrates how to generate the\n+elements of a Pascal's triangle.\n+What is Pascal's triangle?\n+- Refer to (https://en.wikipedia.org/wiki/Pascal%27s_triangle)\n+for more info about this triangle.\n+\"\"\"\n+\n+\n+def generate_pascal_triangle(num_rows: int) -> None:\n+    \"\"\"\n+    Print Pascal's triangle for different number of rows\n+    >>> generate_pascal_triangle(1)\n+    [[1]]\n+    >>> generate_pascal_triangle(2)\n+    [[1], [1, 1]]\n+    >>> generate_pascal_triangle(3)\n+    [[1], [1, 1], [1, 2, 1]]\n+    >>> generate_pascal_triangle(4)\n+    [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1]]\n+    >>> generate_pascal_triangle(5)\n+    [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1], [1, 4, 6, 4, 1]]\n+    \"\"\"\n+    triangle: list[list[int]] = []\n+    for current_row_idx in range(num_rows):\n+        populate_current_row(triangle, current_row_idx)\n+    print(triangle)",
    "comment": "you should return the triangle, not print it",
    "line_number": 28,
    "enriched": "File: other/pascal_triangle.py\nCode: @@ -0,0 +1,72 @@\n+\"\"\"\n+wissamfawaz12@gmail.com | github.com/wissamfawaz\n+This implementation demonstrates how to generate the\n+elements of a Pascal's triangle.\n+What is Pascal's triangle?\n+- Refer to (https://en.wikipedia.org/wiki/Pascal%27s_triangle)\n+for more info about this triangle.\n+\"\"\"\n+\n+\n+def generate_pascal_triangle(num_rows: int) -> None:\n+    \"\"\"\n+    Print Pascal's triangle for different number of rows\n+    >>> generate_pascal_triangle(1)\n+    [[1]]\n+    >>> generate_pascal_triangle(2)\n+    [[1], [1, 1]]\n+    >>> generate_pascal_triangle(3)\n+    [[1], [1, 1], [1, 2, 1]]\n+    >>> generate_pascal_triangle(4)\n+    [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1]]\n+    >>> generate_pascal_triangle(5)\n+    [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1], [1, 4, 6, 4, 1]]\n+    \"\"\"\n+    triangle: list[list[int]] = []\n+    for current_row_idx in range(num_rows):\n+        populate_current_row(triangle, current_row_idx)\n+    print(triangle)\nComment: You should return the triangle, not print it",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "other/pascal_triangle.py",
    "pr_number": 7310,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996428833,
    "comment_created_at": "2022-10-16T11:20:08Z"
  },
  {
    "code": "@@ -1,11 +1,27 @@\n def perfect_cube(n: int) -> bool:\n     \"\"\"\n-    Check if a number is a perfect cube or not.\n+      Check if a number is a perfect cube or not.\n \n     >>> perfect_cube(27)\n-    True\n-    >>> perfect_cube(4)\n-    False\n+      True\n+      >>> perfect_cube(4)\n+      False\n+      >>> perfect_cube(8)\n+      False\n+      >>> perfect_cube(125)\n+      True\n+      >>> perfect_cube(64)\n+      True\n+      >>> perfect_cube(0)\n+      True\n+      >>> perfect_cube(1)\n+      True\n+      >>> perfect_cube(3.375)",
    "comment": "there are three tests that should raise valueerror",
    "line_number": 19,
    "enriched": "File: maths/perfect_cube.py\nCode: @@ -1,11 +1,27 @@\n def perfect_cube(n: int) -> bool:\n     \"\"\"\n-    Check if a number is a perfect cube or not.\n+      Check if a number is a perfect cube or not.\n \n     >>> perfect_cube(27)\n-    True\n-    >>> perfect_cube(4)\n-    False\n+      True\n+      >>> perfect_cube(4)\n+      False\n+      >>> perfect_cube(8)\n+      False\n+      >>> perfect_cube(125)\n+      True\n+      >>> perfect_cube(64)\n+      True\n+      >>> perfect_cube(0)\n+      True\n+      >>> perfect_cube(1)\n+      True\n+      >>> perfect_cube(3.375)\nComment: There are three tests that should raise ValueError\r\n",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "maths/perfect_cube.py",
    "pr_number": 10801,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367881728,
    "comment_created_at": "2023-10-22T10:47:38Z"
  },
  {
    "code": "@@ -8,36 +8,54 @@\n import math\n import os\n \n-import cv2\n-import numpy as np\n-\n PIXEL_MAX = 255.0\n \n \n-def peak_signal_to_noise_ratio(original: float, contrast: float) -> float:\n-    mse = np.mean((original - contrast) ** 2)\n+def peak_signal_to_noise_ratio(original, contrast):",
    "comment": "please revert the changes to this line. removing the type hints is not allowed.  that would be going in the wrong direction since it would be blinding mypy which runs in our github actions.\r\n\r\nmodifying functions that have no tests is also not allowed.  please create a separate pull request that adds doctests.  once that pull request is merged then we can come back to this pull request.",
    "line_number": 14,
    "enriched": "File: compression/peak_signal_to_noise_ratio.py\nCode: @@ -8,36 +8,54 @@\n import math\n import os\n \n-import cv2\n-import numpy as np\n-\n PIXEL_MAX = 255.0\n \n \n-def peak_signal_to_noise_ratio(original: float, contrast: float) -> float:\n-    mse = np.mean((original - contrast) ** 2)\n+def peak_signal_to_noise_ratio(original, contrast):\nComment: Please revert the changes to this line. Removing the type hints is NOT allowed.  That would be going in the wrong direction since it would be blinding `mypy` which runs in our GitHub Actions.\r\n\r\nModifying functions that have no tests is also NOT allowed.  Please create a separate pull request that adds doctests.  Once that pull request is merged then we can come back to this pull request.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "compression/peak_signal_to_noise_ratio.py",
    "pr_number": 10744,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367662824,
    "comment_created_at": "2023-10-21T04:50:57Z"
  },
  {
    "code": "@@ -35,36 +50,45 @@ def minimax(\n     12\n     \"\"\"\n \n+    # Check for invalid inputs",
    "comment": "i feel that this comment isn't really necessary. the if-statement is very simple so it's already pretty clear what it does.",
    "line_number": 53,
    "enriched": "File: backtracking/minimax.py\nCode: @@ -35,36 +50,45 @@ def minimax(\n     12\n     \"\"\"\n \n+    # Check for invalid inputs\nComment: ```suggestion\r\n```\r\nI feel that this comment isn't really necessary. The if-statement is very simple so it's already pretty clear what it does.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "backtracking/minimax.py",
    "pr_number": 10838,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368675753,
    "comment_created_at": "2023-10-23T13:31:11Z"
  },
  {
    "code": "@@ -79,24 +79,26 @@ def jacobi_iteration_method(\n     rows2, cols2 = constant_matrix.shape\r\n \r\n     if rows1 != cols1:\r\n-        raise ValueError(\r\n-            f\"Coefficient matrix dimensions must be nxn but received {rows1}x{cols1}\"\r\n-        )\r\n+        msg = f\"Coefficient matrix dimensions must be nxn but received {rows1}x{cols1}\"\r\n+        raise ValueError(msg)\r\n \r\n     if cols2 != 1:\r\n-        raise ValueError(f\"Constant matrix must be nx1 but received {rows2}x{cols2}\")\r\n+        msg = f\"Constant matrix must be nx1 but received {rows2}x{cols2}\"\r\n+        raise ValueError(msg)\r\n \r\n     if rows1 != rows2:\r\n-        raise ValueError(\r\n-            f\"\"\"Coefficient and constant matrices dimensions must be nxn and nx1 but\r\n-            received {rows1}x{cols1} and {rows2}x{cols2}\"\"\"\r\n+        msg = (\r\n+            \"Coefficient and constant matrices dimensions must be nxn and nx1 but\\n\"\r\n+            f\"            received {rows1}x{cols1} and {rows2}x{cols2}\"\r",
    "comment": "ruff did format this correctly as earlier it was in triple quoted string, but this makes more sense.",
    "line_number": 92,
    "enriched": "File: arithmetic_analysis/jacobi_iteration_method.py\nCode: @@ -79,24 +79,26 @@ def jacobi_iteration_method(\n     rows2, cols2 = constant_matrix.shape\r\n \r\n     if rows1 != cols1:\r\n-        raise ValueError(\r\n-            f\"Coefficient matrix dimensions must be nxn but received {rows1}x{cols1}\"\r\n-        )\r\n+        msg = f\"Coefficient matrix dimensions must be nxn but received {rows1}x{cols1}\"\r\n+        raise ValueError(msg)\r\n \r\n     if cols2 != 1:\r\n-        raise ValueError(f\"Constant matrix must be nx1 but received {rows2}x{cols2}\")\r\n+        msg = f\"Constant matrix must be nx1 but received {rows2}x{cols2}\"\r\n+        raise ValueError(msg)\r\n \r\n     if rows1 != rows2:\r\n-        raise ValueError(\r\n-            f\"\"\"Coefficient and constant matrices dimensions must be nxn and nx1 but\r\n-            received {rows1}x{cols1} and {rows2}x{cols2}\"\"\"\r\n+        msg = (\r\n+            \"Coefficient and constant matrices dimensions must be nxn and nx1 but\\n\"\r\n+            f\"            received {rows1}x{cols1} and {rows2}x{cols2}\"\r\nComment: ```suggestion\r\n        msg = (\r\n            \"Coefficient and constant matrices dimensions must be nxn and nx1 but \"\r\n            f\"received {rows1}x{cols1} and {rows2}x{cols2}\"\r\n```\r\n\r\nRuff did format this correctly as earlier it was in triple quoted string, but this makes more sense.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "arithmetic_analysis/jacobi_iteration_method.py",
    "pr_number": 8767,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1206290976,
    "comment_created_at": "2023-05-26T06:27:31Z"
  },
  {
    "code": "@@ -65,18 +93,68 @@ def peek(self) -> T:\n         return self.stack[-1]\n \n     def is_empty(self) -> bool:\n-        \"\"\"Check if a stack is empty.\"\"\"\n+        \"\"\"\n+        Check if a stack is empty.\n+\n+        >>> S = Stack()\n+        >>> S.is_empty()\n+        True\n+\n+        >>> S = Stack()\n+        >>> S.push(10)\n+        >>> S.is_empty()\n+        False\n+        \"\"\"\n         return not bool(self.stack)\n \n     def is_full(self) -> bool:\n+        \"\"\"\n+        >>> S = Stack()\n+        >>> S.is_full()\n+        False\n+\n+        >>> S = Stack(1)\n+        >>> S.push(10)\n+        >>> S.is_full()\n+        True\n+        \"\"\"\n         return self.size() == self.limit\n \n     def size(self) -> int:\n-        \"\"\"Return the size of the stack.\"\"\"\n+        \"\"\"\n+        Return the size of the stack.\n+\n+        >>> S = Stack(3)\n+        >>> S.size()\n+        0\n+\n+        >>> S = Stack(3)\n+        >>> S.push(10)\n+        >>> S.size()\n+        1\n+\n+        >>> S = Stack(3)\n+        >>> S.push(10)\n+        >>> S.push(20)\n+        >>> S.size()\n+        2\n+        \"\"\"\n         return len(self.stack)\n \n     def __contains__(self, item: T) -> bool:\n-        \"\"\"Check if item is in stack\"\"\"\n+        \"\"\"\n+        Check if item is in stack\n+\n+        >>> S = Stack(3)\n+        >>> S.push(10)\n+        >>> S.__contains__(10)",
    "comment": "the point of __contains__ is to overload the built-in in operator",
    "line_number": 150,
    "enriched": "File: data_structures/stacks/stack.py\nCode: @@ -65,18 +93,68 @@ def peek(self) -> T:\n         return self.stack[-1]\n \n     def is_empty(self) -> bool:\n-        \"\"\"Check if a stack is empty.\"\"\"\n+        \"\"\"\n+        Check if a stack is empty.\n+\n+        >>> S = Stack()\n+        >>> S.is_empty()\n+        True\n+\n+        >>> S = Stack()\n+        >>> S.push(10)\n+        >>> S.is_empty()\n+        False\n+        \"\"\"\n         return not bool(self.stack)\n \n     def is_full(self) -> bool:\n+        \"\"\"\n+        >>> S = Stack()\n+        >>> S.is_full()\n+        False\n+\n+        >>> S = Stack(1)\n+        >>> S.push(10)\n+        >>> S.is_full()\n+        True\n+        \"\"\"\n         return self.size() == self.limit\n \n     def size(self) -> int:\n-        \"\"\"Return the size of the stack.\"\"\"\n+        \"\"\"\n+        Return the size of the stack.\n+\n+        >>> S = Stack(3)\n+        >>> S.size()\n+        0\n+\n+        >>> S = Stack(3)\n+        >>> S.push(10)\n+        >>> S.size()\n+        1\n+\n+        >>> S = Stack(3)\n+        >>> S.push(10)\n+        >>> S.push(20)\n+        >>> S.size()\n+        2\n+        \"\"\"\n         return len(self.stack)\n \n     def __contains__(self, item: T) -> bool:\n-        \"\"\"Check if item is in stack\"\"\"\n+        \"\"\"\n+        Check if item is in stack\n+\n+        >>> S = Stack(3)\n+        >>> S.push(10)\n+        >>> S.__contains__(10)\nComment: ```suggestion\r\n        >>> 10 in S\r\n```\r\nThe point of `__contains__` is to overload the built-in `in` operator",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "data_structures/stacks/stack.py",
    "pr_number": 11149,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1389865216,
    "comment_created_at": "2023-11-10T20:07:22Z"
  },
  {
    "code": "@@ -1,7 +1,7 @@\n \"\"\"\n YouTube Explanation: https://www.youtube.com/watch?v=f2xi3c1S95M\n \n-Given an integer n, return the minimum steps to 1\n+Integer n is given, return the minimum steps from n to 1",
    "comment": "the phrasing in the first part of the sentence was more natural",
    "line_number": 4,
    "enriched": "File: dynamic_programming/minimum_steps_to_one.py\nCode: @@ -1,7 +1,7 @@\n \"\"\"\n YouTube Explanation: https://www.youtube.com/watch?v=f2xi3c1S95M\n \n-Given an integer n, return the minimum steps to 1\n+Integer n is given, return the minimum steps from n to 1\nComment: ```suggestion\r\nGiven an integer n, return the minimum steps from n to 1\r\n```\r\nThe phrasing in the first part of the sentence was more natural",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "dynamic_programming/minimum_steps_to_one.py",
    "pr_number": 9841,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1351404736,
    "comment_created_at": "2023-10-10T04:29:51Z"
  },
  {
    "code": "@@ -0,0 +1,83 @@\n+\"\"\"\n+Convert speed units\n+\n+https://en.wikipedia.org/wiki/Kilometres_per_hour\n+https://en.wikipedia.org/wiki/Miles_per_hour\n+https://en.wikipedia.org/wiki/Knot_(unit)\n+https://en.wikipedia.org/wiki/Metre_per_second\n+\n+\"\"\"\n+\n+speed_chart: dict[str, float] = {\n+    \"km/h\": 1.0,\n+    \"m/s\": 3.6,\n+    \"mph\": 1.609344,\n+    \"knot\": 1.852,\n+}\n+\n+speed_chart_inverse: dict[str, float] = {\n+    \"km/h\": 1.0,\n+    \"m/s\": 0.277777778,\n+    \"mph\": 0.621371192,\n+    \"knot\": 0.539956803,\n+}\n+\n+\n+def convert_speed(speed: float, unit_from: str, unit_to: str) -> float:\n+    \"\"\"\n+    Convert speed from one unit to another unit using the speed_chart\n+\n+    \"km/h\": 1.0,\n+    \"m/s\": 3.6,\n+    \"mph\": 1.609344,\n+    \"knot\": 1.852,\n+\n+    >>> convert_speed(100, \"km/h\", \"m/s\")\n+    27.778\n+\n+    >>> convert_speed(100, \"km/h\", \"mph\")\n+    62.137\n+\n+    >>> convert_speed(100, \"km/h\", \"knot\")\n+    53.996\n+\n+    >>> convert_speed(100, \"m/s\", \"km/h\")\n+    360.0\n+\n+    >>> convert_speed(100, \"m/s\", \"mph\")\n+    223.694\n+\n+    >>> convert_speed(100, \"m/s\", \"knot\")\n+    194.384\n+\n+    >>> convert_speed(100, \"mph\", \"km/h\")\n+    160.934\n+\n+    >>> convert_speed(100, \"mph\", \"m/s\")\n+    44.704\n+\n+    >>> convert_speed(100, \"mph\", \"knot\")\n+    86.898\n+\n+    >>> convert_speed(100, \"knot\", \"km/h\")\n+    185.2\n+\n+    >>> convert_speed(100, \"knot\", \"m/s\")\n+    51.444\n+",
    "comment": "let's decrease the amount of scrolling required to read the implementation.  ;-)",
    "line_number": 67,
    "enriched": "File: conversions/speed_conversions.py\nCode: @@ -0,0 +1,83 @@\n+\"\"\"\n+Convert speed units\n+\n+https://en.wikipedia.org/wiki/Kilometres_per_hour\n+https://en.wikipedia.org/wiki/Miles_per_hour\n+https://en.wikipedia.org/wiki/Knot_(unit)\n+https://en.wikipedia.org/wiki/Metre_per_second\n+\n+\"\"\"\n+\n+speed_chart: dict[str, float] = {\n+    \"km/h\": 1.0,\n+    \"m/s\": 3.6,\n+    \"mph\": 1.609344,\n+    \"knot\": 1.852,\n+}\n+\n+speed_chart_inverse: dict[str, float] = {\n+    \"km/h\": 1.0,\n+    \"m/s\": 0.277777778,\n+    \"mph\": 0.621371192,\n+    \"knot\": 0.539956803,\n+}\n+\n+\n+def convert_speed(speed: float, unit_from: str, unit_to: str) -> float:\n+    \"\"\"\n+    Convert speed from one unit to another unit using the speed_chart\n+\n+    \"km/h\": 1.0,\n+    \"m/s\": 3.6,\n+    \"mph\": 1.609344,\n+    \"knot\": 1.852,\n+\n+    >>> convert_speed(100, \"km/h\", \"m/s\")\n+    27.778\n+\n+    >>> convert_speed(100, \"km/h\", \"mph\")\n+    62.137\n+\n+    >>> convert_speed(100, \"km/h\", \"knot\")\n+    53.996\n+\n+    >>> convert_speed(100, \"m/s\", \"km/h\")\n+    360.0\n+\n+    >>> convert_speed(100, \"m/s\", \"mph\")\n+    223.694\n+\n+    >>> convert_speed(100, \"m/s\", \"knot\")\n+    194.384\n+\n+    >>> convert_speed(100, \"mph\", \"km/h\")\n+    160.934\n+\n+    >>> convert_speed(100, \"mph\", \"m/s\")\n+    44.704\n+\n+    >>> convert_speed(100, \"mph\", \"knot\")\n+    86.898\n+\n+    >>> convert_speed(100, \"knot\", \"km/h\")\n+    185.2\n+\n+    >>> convert_speed(100, \"knot\", \"m/s\")\n+    51.444\n+\nComment: Let's decrease the amount of scrolling required to read the implementation.  ;-)\r\n```suggestion\r\n    >>> convert_speed(100, \"km/h\", \"mph\")\r\n    62.137\r\n    >>> convert_speed(100, \"km/h\", \"knot\")\r\n    53.996\r\n    >>> convert_speed(100, \"m/s\", \"km/h\")\r\n    360.0\r\n    >>> convert_speed(100, \"m/s\", \"mph\")\r\n    223.694\r\n    >>> convert_speed(100, \"m/s\", \"knot\")\r\n    194.384\r\n    >>> convert_speed(100, \"mph\", \"km/h\")\r\n    160.934\r\n    >>> convert_speed(100, \"mph\", \"m/s\")\r\n    44.704\r\n    >>> convert_speed(100, \"mph\", \"knot\")\r\n    86.898\r\n    >>> convert_speed(100, \"knot\", \"km/h\")\r\n    185.2\r\n    >>> convert_speed(100, \"knot\", \"m/s\")\r\n    51.444\r\n```",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "conversions/speed_conversions.py",
    "pr_number": 7128,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 995422428,
    "comment_created_at": "2022-10-14T07:14:37Z"
  },
  {
    "code": "@@ -36,7 +36,7 @@ def download_image(url: str) -> str:\n     if not image_data:\n         return f\"Failed to download the image from {image_url}.\"\n \n-    file_name = f\"{datetime.now():%Y-%m-%d_%H:%M:%S}.jpg\"\n+    file_name = f\"{datetime.now(tz=UTC):%Y-%m-%d_%H:%M:%S}.jpg\"",
    "comment": "i would think the user would expect us to use their local timezone here.",
    "line_number": 39,
    "enriched": "File: web_programming/instagram_pic.py\nCode: @@ -36,7 +36,7 @@ def download_image(url: str) -> str:\n     if not image_data:\n         return f\"Failed to download the image from {image_url}.\"\n \n-    file_name = f\"{datetime.now():%Y-%m-%d_%H:%M:%S}.jpg\"\n+    file_name = f\"{datetime.now(tz=UTC):%Y-%m-%d_%H:%M:%S}.jpg\"\nComment: I would think the user would expect us to use their local timezone here.\r\n```suggestion\r\n    file_name = f\"{datetime.now(tz=None):%Y-%m-%d_%H:%M:%S}.jpg\"\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "web_programming/instagram_pic.py",
    "pr_number": 11327,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1538179982,
    "comment_created_at": "2024-03-25T20:15:44Z"
  },
  {
    "code": "@@ -25,20 +27,26 @@ def main():\n \n     # Gaussian Naive Bayes\n     nb_model = GaussianNB()\n-    nb_model.fit(x_train, y_train)\n+    time.sleep(2.9)",
    "comment": "why are we waiting for 3 seconds?!?  comments needed to be added to explain such things.",
    "line_number": 30,
    "enriched": "File: machine_learning/gaussian_naive_bayes.py\nCode: @@ -25,20 +27,26 @@ def main():\n \n     # Gaussian Naive Bayes\n     nb_model = GaussianNB()\n-    nb_model.fit(x_train, y_train)\n+    time.sleep(2.9)\nComment: Why are we waiting for 3 seconds?!?  Comments needed to be added to explain such things.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "machine_learning/gaussian_naive_bayes.py",
    "pr_number": 7406,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 999327659,
    "comment_created_at": "2022-10-19T11:42:51Z"
  },
  {
    "code": "@@ -3,52 +3,34 @@\n import string\n \n \n-def atbash_slow(sequence: str) -> str:\n+def atbash(text: str) -> str:\n     \"\"\"\n-    >>> atbash_slow(\"ABCDEFG\")\n-    'ZYXWVUT'\n+    Encodes or decodes text using the Atbash cipher.\n \n-    >>> atbash_slow(\"aW;;123BX\")\n-    'zD;;123YC'\n-    \"\"\"\n-    output = \"\"\n-    for i in sequence:\n-        extract = ord(i)\n-        if 65 <= extract <= 90:\n-            output += chr(155 - extract)\n-        elif 97 <= extract <= 122:\n-            output += chr(219 - extract)\n-        else:\n-            output += i\n-    return output\n-\n-\n-def atbash(sequence: str) -> str:\n-    \"\"\"\n-    >>> atbash(\"ABCDEFG\")\n-    'ZYXWVUT'\n-\n-    >>> atbash(\"aW;;123BX\")\n-    'zD;;123YC'\n-    \"\"\"\n-    letters = string.ascii_letters\n-    letters_reversed = string.ascii_lowercase[::-1] + string.ascii_uppercase[::-1]\n-    return \"\".join(\n-        letters_reversed[letters.index(c)] if c in letters else c for c in sequence\n-    )\n+    The Atbash cipher substitutes each letter with its mirror in the alphabet:\n+    A -> Z, B -> Y, C -> X, ... Z -> A (case is preserved)\n+    Non-alphabetic characters are left unchanged.\n \n+    Args:\n+        text: The input string to encode/decode\n \n-def benchmark() -> None:\n-    \"\"\"Let's benchmark our functions side-by-side...\"\"\"\n-    from timeit import timeit\n+    Returns:\n+        The transformed string\n+    \"\"\"\n+    # Create translation tables for uppercase and lowercase\n+    lowercase_map = str.maketrans(string.ascii_lowercase, string.ascii_lowercase[::-1])\n+    uppercase_map = str.maketrans(string.ascii_uppercase, string.ascii_uppercase[::-1])\n \n-    print(\"Running performance benchmarks...\")\n-    setup = \"from string import printable ; from __main__ import atbash, atbash_slow\"\n-    print(f\"> atbash_slow(): {timeit('atbash_slow(printable)', setup=setup)} seconds\")\n-    print(f\">      atbash(): {timeit('atbash(printable)', setup=setup)} seconds\")\n+    # Apply both translation mappings\n+    return text.translate(lowercase_map).translate(uppercase_map)",
    "comment": "the goal of benchmark is to measure the speed of the algorithms.  replacing the algorithm defeats the purpose of this function.",
    "line_number": 25,
    "enriched": "File: ciphers/atbash.py\nCode: @@ -3,52 +3,34 @@\n import string\n \n \n-def atbash_slow(sequence: str) -> str:\n+def atbash(text: str) -> str:\n     \"\"\"\n-    >>> atbash_slow(\"ABCDEFG\")\n-    'ZYXWVUT'\n+    Encodes or decodes text using the Atbash cipher.\n \n-    >>> atbash_slow(\"aW;;123BX\")\n-    'zD;;123YC'\n-    \"\"\"\n-    output = \"\"\n-    for i in sequence:\n-        extract = ord(i)\n-        if 65 <= extract <= 90:\n-            output += chr(155 - extract)\n-        elif 97 <= extract <= 122:\n-            output += chr(219 - extract)\n-        else:\n-            output += i\n-    return output\n-\n-\n-def atbash(sequence: str) -> str:\n-    \"\"\"\n-    >>> atbash(\"ABCDEFG\")\n-    'ZYXWVUT'\n-\n-    >>> atbash(\"aW;;123BX\")\n-    'zD;;123YC'\n-    \"\"\"\n-    letters = string.ascii_letters\n-    letters_reversed = string.ascii_lowercase[::-1] + string.ascii_uppercase[::-1]\n-    return \"\".join(\n-        letters_reversed[letters.index(c)] if c in letters else c for c in sequence\n-    )\n+    The Atbash cipher substitutes each letter with its mirror in the alphabet:\n+    A -> Z, B -> Y, C -> X, ... Z -> A (case is preserved)\n+    Non-alphabetic characters are left unchanged.\n \n+    Args:\n+        text: The input string to encode/decode\n \n-def benchmark() -> None:\n-    \"\"\"Let's benchmark our functions side-by-side...\"\"\"\n-    from timeit import timeit\n+    Returns:\n+        The transformed string\n+    \"\"\"\n+    # Create translation tables for uppercase and lowercase\n+    lowercase_map = str.maketrans(string.ascii_lowercase, string.ascii_lowercase[::-1])\n+    uppercase_map = str.maketrans(string.ascii_uppercase, string.ascii_uppercase[::-1])\n \n-    print(\"Running performance benchmarks...\")\n-    setup = \"from string import printable ; from __main__ import atbash, atbash_slow\"\n-    print(f\"> atbash_slow(): {timeit('atbash_slow(printable)', setup=setup)} seconds\")\n-    print(f\">      atbash(): {timeit('atbash(printable)', setup=setup)} seconds\")\n+    # Apply both translation mappings\n+    return text.translate(lowercase_map).translate(uppercase_map)\nComment: The goal of benchmark is to measure the speed of the algorithms.  Replacing the algorithm defeats the purpose of this function.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "ciphers/atbash.py",
    "pr_number": 12811,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2186967507,
    "comment_created_at": "2025-07-05T08:37:22Z"
  },
  {
    "code": "@@ -58,9 +79,29 @@ def is_palindrome_slice(s: str) -> bool:\n     return s == s[::-1]\n \n \n+def benchmark_function(name: str) -> None:\n+    setup = f\"from __main__ import test_data, {name}\"\n+    number = 100000\n+    res = timeit(\n+        f\"all({name}(key) is value for key, value in test_data.items())\",\n+        setup=setup,\n+        number=number,\n+    )\n+    print(f\"{name:<35} finished {number} runs in {res:.5f} seconds\")\n+\n+\n if __name__ == \"__main__\":\n     for key, value in test_data.items():\n         assert is_palindrome(key) is is_palindrome_recursive(key)\n         assert is_palindrome(key) is is_palindrome_slice(key)\n         print(f\"{key:21} {value}\")\n     print(\"a man a plan a canal panama\")\n+\n+    benchmark_function(\"is_palindrome\")  # finished 100000 runs in 0.33785 seconds\n+    benchmark_function(\n+        \"is_palindrome_traversal\"\n+    )  # finished 100000 runs in 0.70002 seconds\n+    benchmark_function(\n+        \"is_palindrome_recursive\"\n+    )  # finished 100000 runs in 0.48514 seconds\n+    benchmark_function(\"is_palindrome_slice\")  # finished 100000 runs in 0.18703 seconds",
    "comment": "easier to read... sorted fastest to slowest.",
    "line_number": 107,
    "enriched": "File: strings/palindrome.py\nCode: @@ -58,9 +79,29 @@ def is_palindrome_slice(s: str) -> bool:\n     return s == s[::-1]\n \n \n+def benchmark_function(name: str) -> None:\n+    setup = f\"from __main__ import test_data, {name}\"\n+    number = 100000\n+    res = timeit(\n+        f\"all({name}(key) is value for key, value in test_data.items())\",\n+        setup=setup,\n+        number=number,\n+    )\n+    print(f\"{name:<35} finished {number} runs in {res:.5f} seconds\")\n+\n+\n if __name__ == \"__main__\":\n     for key, value in test_data.items():\n         assert is_palindrome(key) is is_palindrome_recursive(key)\n         assert is_palindrome(key) is is_palindrome_slice(key)\n         print(f\"{key:21} {value}\")\n     print(\"a man a plan a canal panama\")\n+\n+    benchmark_function(\"is_palindrome\")  # finished 100000 runs in 0.33785 seconds\n+    benchmark_function(\n+        \"is_palindrome_traversal\"\n+    )  # finished 100000 runs in 0.70002 seconds\n+    benchmark_function(\n+        \"is_palindrome_recursive\"\n+    )  # finished 100000 runs in 0.48514 seconds\n+    benchmark_function(\"is_palindrome_slice\")  # finished 100000 runs in 0.18703 seconds\nComment: Easier to read... Sorted fastest to slowest.\r\n```suggestion\r\n    # finished 100,000 runs in 0.18703 seconds\r\n    benchmark_function(\"is_palindrome_slice\")\r\n    # finished 100,000 runs in 0.33785 seconds\r\n    benchmark_function(\"is_palindrome\")\r\n    # finished 100,000 runs in 0.48514 seconds\r\n    benchmark_function(\"is_palindrome_recursive\")\r\n    # finished 100,000 runs in 0.70002 seconds\r\n    benchmark_function(\"is_palindrome_traversal\")\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "strings/palindrome.py",
    "pr_number": 8749,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1205194096,
    "comment_created_at": "2023-05-25T08:43:10Z"
  },
  {
    "code": "@@ -0,0 +1,57 @@\n+\"\"\"\n+A unit of time is any particular time interval, used as a standard\n+way of measuring or expressing duration.\n+The base unit of time in the International System of Units (SI),\n+and by extension most of the Western world, is the second,\n+defined as about 9 billion oscillations of the caesium atom.\n+\n+WIKI: https://en.wikipedia.org/wiki/Unit_of_time",
    "comment": "let's use our 88 characters per line...",
    "line_number": 8,
    "enriched": "File: conversions/time_conversions.py\nCode: @@ -0,0 +1,57 @@\n+\"\"\"\n+A unit of time is any particular time interval, used as a standard\n+way of measuring or expressing duration.\n+The base unit of time in the International System of Units (SI),\n+and by extension most of the Western world, is the second,\n+defined as about 9 billion oscillations of the caesium atom.\n+\n+WIKI: https://en.wikipedia.org/wiki/Unit_of_time\nComment: Let's use our 88 characters per line...\r\n```suggestion\r\nA unit of time is any particular time interval, used as a standard way of measuring or\r\nexpressing duration.  The base unit of time in the International System of Units (SI),\r\nand by extension most of the Western world, is the second, defined as about 9 billion\r\noscillations of the caesium atom.\r\n\r\nhttps://en.wikipedia.org/wiki/Unit_of_time\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "conversions/time_conversions.py",
    "pr_number": 10749,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367851198,
    "comment_created_at": "2023-10-22T07:13:06Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+\"\"\"\n+Knuth-Yao dynamic programming speedup,\n+also known as the Knuth-Yao optimization, is a technique that accelerates\n+the computation of certain dynamic programming algorithms.\n+It was introduced by Donald Knuth and Andrew Yao in their paper\n+\"Efficient Binary-Search Trees\" in 1976.\n+\n+This implementation the following recurrence relation,\n+\n+dp[i][j] = min(i < k < j){dp[i][k] + dp[k][j] + cost[i][j]},\n+\n+where opt[i][j-1] <= opt[i][j] <= opt[i+1][j] with opt[i][j] representing\n+the value of 'k' that minimizes the given expression.\n+\n+Equivalently, the cost function satisfies either of the following conditions.\n+\n+1) cost[b][c] <= cost[a][d]\n+2) cost[a][c]+cost[b][d] <= cost[a][d]+cost[b][c]\n+\n+Reference: https://cp-algorithms.com/dynamic_programming/knuth-optimization.html\n+\n+- time complexity: O(n^2)\n+- space complexity: O(n^2)\n+\n+>>> knuth_yao_speedup([[1,2,3,4],[3,4,5,1],[1,1,1,3],[2,2,2,2]])\n+15\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import sys\n+\n+\n+def knuth_yao_speedup(cost: list[list[int]]) -> int:\n+    n = len(cost)",
    "comment": "add below doctests or add yours if you have...\r\n\r\n        >>> knuth_yao_speedup([[1]])\r\n        0\r\n        >>> knuth_yao_speedup([[1, 2], [3, 4]])\r\n        5\r\n        >>> knuth_yao_speedup([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\r\n        15",
    "line_number": 35,
    "enriched": "File: dynamic_programming/knuth_yao_speedup.py\nCode: @@ -0,0 +1,61 @@\n+\"\"\"\n+Knuth-Yao dynamic programming speedup,\n+also known as the Knuth-Yao optimization, is a technique that accelerates\n+the computation of certain dynamic programming algorithms.\n+It was introduced by Donald Knuth and Andrew Yao in their paper\n+\"Efficient Binary-Search Trees\" in 1976.\n+\n+This implementation the following recurrence relation,\n+\n+dp[i][j] = min(i < k < j){dp[i][k] + dp[k][j] + cost[i][j]},\n+\n+where opt[i][j-1] <= opt[i][j] <= opt[i+1][j] with opt[i][j] representing\n+the value of 'k' that minimizes the given expression.\n+\n+Equivalently, the cost function satisfies either of the following conditions.\n+\n+1) cost[b][c] <= cost[a][d]\n+2) cost[a][c]+cost[b][d] <= cost[a][d]+cost[b][c]\n+\n+Reference: https://cp-algorithms.com/dynamic_programming/knuth-optimization.html\n+\n+- time complexity: O(n^2)\n+- space complexity: O(n^2)\n+\n+>>> knuth_yao_speedup([[1,2,3,4],[3,4,5,1],[1,1,1,3],[2,2,2,2]])\n+15\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import sys\n+\n+\n+def knuth_yao_speedup(cost: list[list[int]]) -> int:\n+    n = len(cost)\nComment: add below doctests or add yours if you have...\r\n\r\n        >>> knuth_yao_speedup([[1]])\r\n        0\r\n        >>> knuth_yao_speedup([[1, 2], [3, 4]])\r\n        5\r\n        >>> knuth_yao_speedup([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\r\n        15",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "dynamic_programming/knuth_yao_speedup.py",
    "pr_number": 9024,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1316860375,
    "comment_created_at": "2023-09-06T07:33:58Z"
  },
  {
    "code": "@@ -0,0 +1,120 @@\n+\"\"\"\n+https://en.wikipedia.org/wiki/Kruskal%27s_algorithm\n+\"\"\"\n+\n+import doctest",
    "comment": "please move this down into __main__ because we only need it there.",
    "line_number": 5,
    "enriched": "File: greedy_methods/kruskal.py\nCode: @@ -0,0 +1,120 @@\n+\"\"\"\n+https://en.wikipedia.org/wiki/Kruskal%27s_algorithm\n+\"\"\"\n+\n+import doctest\nComment: Please move this down into `__main__` because we only need it there.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "greedy_methods/kruskal.py",
    "pr_number": 11185,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1409816168,
    "comment_created_at": "2023-11-29T20:13:27Z"
  },
  {
    "code": "@@ -14,29 +17,32 @@ def double_sort(lst):\n     >>> double_sort([-3, 10, 16, -42, 29]) == sorted([-3, 10, 16, -42, 29])\r\n     True\r\n     \"\"\"\r\n-    no_of_elements = len(lst)\r\n+    no_of_elements = len(collection)\r\n     for _ in range(\r\n         int(((no_of_elements - 1) / 2) + 1)\r\n     ):  # we don't need to traverse to end of list as\r\n         for j in range(no_of_elements - 1):\r\n             if (\r\n-                lst[j + 1] < lst[j]\r\n+                collection[j + 1] < collection[j]\r\n             ):  # applying bubble sort algorithm from left to right (or forwards)\r\n-                temp = lst[j + 1]\r\n-                lst[j + 1] = lst[j]\r\n-                lst[j] = temp\r\n+                collection[j], collection[j + 1] = collection[j + 1], collection[j]\r\n             if (\r\n-                lst[no_of_elements - 1 - j] < lst[no_of_elements - 2 - j]\r\n+                collection[no_of_elements - 1 - j] < collection[no_of_elements - 2 - j]\r\n             ):  # applying bubble sort algorithm from right to left (or backwards)\r",
    "comment": "let's not wrap lines of code just to make room for a trailing comment.  please put the comment on a line of its own _before_ the if statement.  please repeat on any other comments in this file that force a line to be wrapped.",
    "line_number": 31,
    "enriched": "File: sorts/double_sort.py\nCode: @@ -14,29 +17,32 @@ def double_sort(lst):\n     >>> double_sort([-3, 10, 16, -42, 29]) == sorted([-3, 10, 16, -42, 29])\r\n     True\r\n     \"\"\"\r\n-    no_of_elements = len(lst)\r\n+    no_of_elements = len(collection)\r\n     for _ in range(\r\n         int(((no_of_elements - 1) / 2) + 1)\r\n     ):  # we don't need to traverse to end of list as\r\n         for j in range(no_of_elements - 1):\r\n             if (\r\n-                lst[j + 1] < lst[j]\r\n+                collection[j + 1] < collection[j]\r\n             ):  # applying bubble sort algorithm from left to right (or forwards)\r\n-                temp = lst[j + 1]\r\n-                lst[j + 1] = lst[j]\r\n-                lst[j] = temp\r\n+                collection[j], collection[j + 1] = collection[j + 1], collection[j]\r\n             if (\r\n-                lst[no_of_elements - 1 - j] < lst[no_of_elements - 2 - j]\r\n+                collection[no_of_elements - 1 - j] < collection[no_of_elements - 2 - j]\r\n             ):  # applying bubble sort algorithm from right to left (or backwards)\r\nComment: Let's not wrap lines of code just to make room for a trailing comment.  Please put the comment on a line of its own _before_ the `if` statement.  Please repeat on any other comments in this file that force a line to be wrapped.\r\n```suggestion\r\n            ):\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "sorts/double_sort.py",
    "pr_number": 10798,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1368238206,
    "comment_created_at": "2023-10-23T07:35:28Z"
  },
  {
    "code": "@@ -47,6 +47,7 @@ repos:\n           - flake8-broken-line\n           - flake8-comprehensions\n           - pep8-naming\n+          - flake8-docstrings",
    "comment": "please keep lists of dependencies in alphabetical order to make it easy to spot missing dependencies and difficult to add duplicates.",
    "line_number": 50,
    "enriched": "File: .pre-commit-config.yaml\nCode: @@ -47,6 +47,7 @@ repos:\n           - flake8-broken-line\n           - flake8-comprehensions\n           - pep8-naming\n+          - flake8-docstrings\nComment: Please keep lists of dependencies in alphabetical order to make it easy to spot missing dependencies and difficult to add duplicates.\r\n```suggestion\r\n          - flake8-comprehensions\r\n          - flake8-docstrings\r\n          - pep8-naming\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": ".pre-commit-config.yaml",
    "pr_number": 7962,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1014622798,
    "comment_created_at": "2022-11-05T11:25:25Z"
  },
  {
    "code": "@@ -3,7 +3,7 @@\n name: directory_writer\n on: [push]\n jobs:\n-  build:\n+  directory_writer:",
    "comment": "it is too confusing to have two github actions jobs named build, so rename this job to document what it really does.",
    "line_number": 6,
    "enriched": "File: .github/workflows/directory_writer.yml\nCode: @@ -3,7 +3,7 @@\n name: directory_writer\n on: [push]\n jobs:\n-  build:\n+  directory_writer:\nComment: It is too confusing to have two GitHub Actions jobs named `build`, so rename this job to document what it really does.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": ".github/workflows/directory_writer.yml",
    "pr_number": 12772,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2112460559,
    "comment_created_at": "2025-05-28T17:57:21Z"
  },
  {
    "code": "@@ -0,0 +1,121 @@\n+'''\n+Wiki Explanation: https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding\n+'''\n+class  node :\n+    def __init__(self) -> None:\n+        self.sym=''",
    "comment": "class names should follow the [camelcase](https://en.wikipedia.org/wiki/camel_case) naming convention. please update the following name accordingly: node",
    "line_number": 6,
    "enriched": "File: compression/shannon_fano.py\nCode: @@ -0,0 +1,121 @@\n+'''\n+Wiki Explanation: https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding\n+'''\n+class  node :\n+    def __init__(self) -> None:\n+        self.sym=''\nComment: Class names should follow the [`CamelCase`](https://en.wikipedia.org/wiki/Camel_case) naming convention. Please update the following name accordingly: `node`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "compression/shannon_fano.py",
    "pr_number": 7524,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1002538297,
    "comment_created_at": "2022-10-22T17:19:49Z"
  },
  {
    "code": "@@ -7,6 +7,9 @@\n \"\"\"\n \n \n+from typing import List",
    "comment": "you can just use list to keep consistency with the file\r\nthis repository also targets at python 3.10+",
    "line_number": 10,
    "enriched": "File: matrix/spiral_print.py\nCode: @@ -7,6 +7,9 @@\n \"\"\"\n \n \n+from typing import List\nComment: ```suggestion\r\n```\r\nYou can just use `list` to keep consistency with the file\r\nThis repository also targets at python 3.10+",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "matrix/spiral_print.py",
    "pr_number": 7674,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1005745870,
    "comment_created_at": "2022-10-26T14:17:34Z"
  },
  {
    "code": "@@ -12,6 +12,40 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n     def _collision_resolution(self, key, data=None):\n+        \"\"\"\n+        Quadratic probing is an open addressing scheme used for resolving\n+        collisions in hash table.\n+\n+        It works by taking the original hash index and adding successive\n+        values of an arbitrary quadratic polynomial until open slot is found.\n+\n+        Hash + 1\u00b2, Hash + 2\u00b2, Hash + 3\u00b2 .... Hash + n\u00b2\n+\n+        e.g:\n+        1. Create hash table with size 7\n+        >>> qp = QuadraticProbing(7)\n+        >>> qp.insert_data(90)\n+        >>> qp.insert_data(340)\n+        >>> qp.insert_data(24)\n+        >>> qp.insert_data(45)\n+        >>> qp.insert_data(99)\n+        >>> qp.insert_data(73)\n+        >>> qp.insert_data(7)\n+        >>> qp.keys()\n+        {11: 45, 14: 99, 7: 24, 0: 340, 5: 73, 6: 90, 8: 7}\n+\n+        2. Create hash table with size 8\n+        >>> qp = QuadraticProbing(8)\n+        >>> qp.insert_data(0)\n+        >>> qp.insert_data(999)\n+        >>> qp.insert_data(111)\n+        >>> qp.keys()\n+        {0: 0, 7: 999, 3: 111}\n+\n+        reference:\n+            - https://en.wikipedia.org/wiki/Quadratic_probing",
    "comment": "let's put the url above the tests so visitors can look at the tests and implementation without any visual clutter.",
    "line_number": 46,
    "enriched": "File: data_structures/hashing/quadratic_probing.py\nCode: @@ -12,6 +12,40 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n     def _collision_resolution(self, key, data=None):\n+        \"\"\"\n+        Quadratic probing is an open addressing scheme used for resolving\n+        collisions in hash table.\n+\n+        It works by taking the original hash index and adding successive\n+        values of an arbitrary quadratic polynomial until open slot is found.\n+\n+        Hash + 1\u00b2, Hash + 2\u00b2, Hash + 3\u00b2 .... Hash + n\u00b2\n+\n+        e.g:\n+        1. Create hash table with size 7\n+        >>> qp = QuadraticProbing(7)\n+        >>> qp.insert_data(90)\n+        >>> qp.insert_data(340)\n+        >>> qp.insert_data(24)\n+        >>> qp.insert_data(45)\n+        >>> qp.insert_data(99)\n+        >>> qp.insert_data(73)\n+        >>> qp.insert_data(7)\n+        >>> qp.keys()\n+        {11: 45, 14: 99, 7: 24, 0: 340, 5: 73, 6: 90, 8: 7}\n+\n+        2. Create hash table with size 8\n+        >>> qp = QuadraticProbing(8)\n+        >>> qp.insert_data(0)\n+        >>> qp.insert_data(999)\n+        >>> qp.insert_data(111)\n+        >>> qp.keys()\n+        {0: 0, 7: 999, 3: 111}\n+\n+        reference:\n+            - https://en.wikipedia.org/wiki/Quadratic_probing\nComment: Let's put the URL above the tests so visitors can look at the tests and implementation without any visual clutter.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "data_structures/hashing/quadratic_probing.py",
    "pr_number": 10996,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1373104628,
    "comment_created_at": "2023-10-26T12:40:10Z"
  },
  {
    "code": "@@ -115,6 +115,20 @@ def jacobi_iteration_method(\n \r\n     strictly_diagonally_dominant(table)\r\n \r\n+    \"\"\"\r\n+    denom - a list of values along the diagonal\r\n+    val - values of the last column of the table array\r\n+\r\n+    masks - boolean mask of all strings without diagonal\r\n+    elements array coefficient_matrix\r\n+\r\n+    ttt - coefficient_matrix array values without diagonal elements\r\n+    ind - column indexes for each row without diagonal elements\r\n+    arr - list obtained by column indexes from the list init_val\r",
    "comment": "there shouldn't be such a long comment in the middle of the algorithm. instead of writing a block comment explaining what each variable does, why not just make the variable names clearer? you could also just add comments to individual lines.",
    "line_number": 127,
    "enriched": "File: arithmetic_analysis/jacobi_iteration_method.py\nCode: @@ -115,6 +115,20 @@ def jacobi_iteration_method(\n \r\n     strictly_diagonally_dominant(table)\r\n \r\n+    \"\"\"\r\n+    denom - a list of values along the diagonal\r\n+    val - values of the last column of the table array\r\n+\r\n+    masks - boolean mask of all strings without diagonal\r\n+    elements array coefficient_matrix\r\n+\r\n+    ttt - coefficient_matrix array values without diagonal elements\r\n+    ind - column indexes for each row without diagonal elements\r\n+    arr - list obtained by column indexes from the list init_val\r\nComment: There shouldn't be such a long comment in the middle of the algorithm. Instead of writing a block comment explaining what each variable does, why not just make the variable names clearer? You could also just add comments to individual lines.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "arithmetic_analysis/jacobi_iteration_method.py",
    "pr_number": 8938,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1292730790,
    "comment_created_at": "2023-08-13T09:18:29Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+class Solution:\n+    def find_median_sorted_arrays(self, nums1: list[int], nums2: list[int]) -> float:",
    "comment": "the class has a name that is not self documenting and self is never used so make it a function, not a class.",
    "line_number": 2,
    "enriched": "File: data_structures/arrays/median_two_array.py\nCode: @@ -0,0 +1,48 @@\n+class Solution:\n+    def find_median_sorted_arrays(self, nums1: list[int], nums2: list[int]) -> float:\nComment: The class has a name that is not self documenting and `self` is never used so make it a function, not a class.\r\n\r\n```suggestion\r\ndef find_median_sorted_arrays(nums1: list[int], nums2: list[int]) -> float:\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "data_structures/arrays/median_two_array.py",
    "pr_number": 9386,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342331367,
    "comment_created_at": "2023-10-02T06:53:34Z"
  },
  {
    "code": "@@ -90,6 +90,9 @@ def quantum_fourier_transform(number_of_qubits: int = 3) -> qiskit.result.counts\n \n \n if __name__ == \"__main__\":\n+    import doctest",
    "comment": "we need to add some of the new tests here, rather than adding a import statement!!",
    "line_number": 93,
    "enriched": "File: quantum/q_fourier_transform.py\nCode: @@ -90,6 +90,9 @@ def quantum_fourier_transform(number_of_qubits: int = 3) -> qiskit.result.counts\n \n \n if __name__ == \"__main__\":\n+    import doctest\nComment: We need to add some of the new tests here, rather than adding a import statement!!",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "quantum/q_fourier_transform.py",
    "pr_number": 10931,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1371143718,
    "comment_created_at": "2023-10-25T04:36:50Z"
  },
  {
    "code": "@@ -0,0 +1,120 @@\n+import pandas as pd\n+import math\n+import matplotlib.pyplot as plt\n+from typing import Dict, List\n+class dbscan:",
    "comment": "class names should follow the [camelcase](https://en.wikipedia.org/wiki/camel_case) naming convention. please update the following name accordingly: dbscan",
    "line_number": 5,
    "enriched": "File: machine_learning/dbscan.py\nCode: @@ -0,0 +1,120 @@\n+import pandas as pd\n+import math\n+import matplotlib.pyplot as plt\n+from typing import Dict, List\n+class dbscan:\nComment: Class names should follow the [`CamelCase`](https://en.wikipedia.org/wiki/Camel_case) naming convention. Please update the following name accordingly: `dbscan`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "machine_learning/dbscan.py",
    "pr_number": 11632,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1783079031,
    "comment_created_at": "2024-10-01T15:26:48Z"
  },
  {
    "code": "@@ -65,7 +68,7 @@ def get_forbes_real_time_billionaires() -> list[dict[str, int | str]]:\n             \"Country\": person[\"countryOfCitizenship\"],\n             \"Gender\": person[\"gender\"],\n             \"Worth ($)\": f\"{person['finalWorth'] / 1000:.1f} Billion\",\n-            \"Age\": years_old(person[\"birthDate\"]),\n+            \"Age\": str(years_old(person[\"birthDate\"] / 1000)),",
    "comment": "if this / 1000 is required then please move it into the years_old() function and ensure that the tests pass.",
    "line_number": 71,
    "enriched": "File: web_programming/get_top_billionaires.py\nCode: @@ -65,7 +68,7 @@ def get_forbes_real_time_billionaires() -> list[dict[str, int | str]]:\n             \"Country\": person[\"countryOfCitizenship\"],\n             \"Gender\": person[\"gender\"],\n             \"Worth ($)\": f\"{person['finalWorth'] / 1000:.1f} Billion\",\n-            \"Age\": years_old(person[\"birthDate\"]),\n+            \"Age\": str(years_old(person[\"birthDate\"] / 1000)),\nComment: If this `/ 1000` is required then please move it into the `years_old()` function and ensure that the tests pass.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "web_programming/get_top_billionaires.py",
    "pr_number": 11466,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1641989870,
    "comment_created_at": "2024-06-16T21:53:26Z"
  },
  {
    "code": "@@ -0,0 +1,196 @@\n+class resizable_array:",
    "comment": "class names should follow the [camelcase](https://en.wikipedia.org/wiki/camel_case) naming convention. please update the following name accordingly: resizable_array",
    "line_number": 1,
    "enriched": "File: other/Resizable_Array.py\nCode: @@ -0,0 +1,196 @@\n+class resizable_array:\nComment: Class names should follow the [`CamelCase`](https://en.wikipedia.org/wiki/Camel_case) naming convention. Please update the following name accordingly: `resizable_array`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "other/Resizable_Array.py",
    "pr_number": 7016,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 992698688,
    "comment_created_at": "2022-10-11T19:21:29Z"
  },
  {
    "code": "@@ -183,6 +182,16 @@ def angle(self, other: Vector, deg: bool = False) -> float:\n         else:\n             return math.acos(num / den)\n \n+    def __eq__(self, vector: object) -> bool:",
    "comment": "could you move the method higher up in the file, before all the public methods? having this one dunder method within all the public methods, rather than having it alongside all the other dunder methods, makes it harder to find.",
    "line_number": 185,
    "enriched": "File: linear_algebra/src/lib.py\nCode: @@ -183,6 +182,16 @@ def angle(self, other: Vector, deg: bool = False) -> float:\n         else:\n             return math.acos(num / den)\n \n+    def __eq__(self, vector: object) -> bool:\nComment: Could you move the method higher up in the file, before all the public methods? Having this one dunder method within all the public methods, rather than having it alongside all the other dunder methods, makes it harder to find.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "linear_algebra/src/lib.py",
    "pr_number": 12448,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1898881020,
    "comment_created_at": "2024-12-28T09:35:22Z"
  },
  {
    "code": "@@ -1,47 +1,103 @@\n \"\"\"\n Numerical integration or quadrature for a smooth function f with known values at x_i\n \n-This method is the classical approach of suming 'Equally Spaced Abscissas'\n+This method is the classical approach of summing 'Equally Spaced Abscissas'\n \n-method 1:\n-\"extended trapezoidal rule\"\n+Method 1:\n+\"Extended Trapezoidal Rule\"\n \n \"\"\"\n \n \n def method_1(boundary, steps):",
    "comment": "let's rename the function to something more descriptive. i'm not sure why the code calls it \"method 1\" when there's only one method in the file.",
    "line_number": 12,
    "enriched": "File: maths/trapezoidal_rule.py\nCode: @@ -1,47 +1,103 @@\n \"\"\"\n Numerical integration or quadrature for a smooth function f with known values at x_i\n \n-This method is the classical approach of suming 'Equally Spaced Abscissas'\n+This method is the classical approach of summing 'Equally Spaced Abscissas'\n \n-method 1:\n-\"extended trapezoidal rule\"\n+Method 1:\n+\"Extended Trapezoidal Rule\"\n \n \"\"\"\n \n \n def method_1(boundary, steps):\nComment: ```suggestion\r\ndef trapezoidal_rule(boundary, steps):\r\n```\r\nLet's rename the function to something more descriptive. I'm not sure why the code calls it \"method 1\" when there's only one method in the file.",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "maths/trapezoidal_rule.py",
    "pr_number": 11640,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1787486089,
    "comment_created_at": "2024-10-04T10:05:30Z"
  },
  {
    "code": "@@ -19,50 +21,49 @@ def mixed_keyword(key: str = \"college\", pt: str = \"UNIVERSITY\") -> str:\n      'Y': 'T', 'Z': 'Y'}\n     'XKJGUFMJST'\n     \"\"\"\n-    key = key.upper()\n-    pt = pt.upper()\n-    temp = []\n-    for i in key:\n-        if i not in temp:\n-            temp.append(i)\n-    len_temp = len(temp)\n-    # print(temp)\n-    alpha = []\n-    modalpha = []\n-    for j in range(65, 91):\n-        t = chr(j)\n-        alpha.append(t)\n-        if t not in temp:\n-            temp.append(t)\n-    # print(temp)\n-    r = int(26 / 4)\n-    # print(r)\n+    keyword = keyword.upper()\n+    plaintext = plaintext.upper()\n+\n+    unique_chars = []\n+    for char in keyword:\n+        if char not in unique_chars:\n+            unique_chars.append(char)",
    "comment": "please make this a set.\r\n* https://docs.python.org/3/library/stdtypes.html#set",
    "line_number": 30,
    "enriched": "File: ciphers/mixed_keyword_cypher.py\nCode: @@ -19,50 +21,49 @@ def mixed_keyword(key: str = \"college\", pt: str = \"UNIVERSITY\") -> str:\n      'Y': 'T', 'Z': 'Y'}\n     'XKJGUFMJST'\n     \"\"\"\n-    key = key.upper()\n-    pt = pt.upper()\n-    temp = []\n-    for i in key:\n-        if i not in temp:\n-            temp.append(i)\n-    len_temp = len(temp)\n-    # print(temp)\n-    alpha = []\n-    modalpha = []\n-    for j in range(65, 91):\n-        t = chr(j)\n-        alpha.append(t)\n-        if t not in temp:\n-            temp.append(t)\n-    # print(temp)\n-    r = int(26 / 4)\n-    # print(r)\n+    keyword = keyword.upper()\n+    plaintext = plaintext.upper()\n+\n+    unique_chars = []\n+    for char in keyword:\n+        if char not in unique_chars:\n+            unique_chars.append(char)\nComment: Please make this a `set`.\r\n* https://docs.python.org/3/library/stdtypes.html#set",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "ciphers/mixed_keyword_cypher.py",
    "pr_number": 8626,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1161226961,
    "comment_created_at": "2023-04-09T06:38:38Z"
  },
  {
    "code": "@@ -0,0 +1,29 @@\n+rect1 = {\n+    \"x\": 10,\n+    \"y\": 10,\n+    \"height\": 30,\n+    \"width\": 50\n+}\n+\n+rect2 = {\n+    \"x\": 20,\n+    \"y\": 30,\n+    \"height\": 40,\n+    \"width\": 30\n+}\n+\n+def checkCollision(rect1, rect2) -> bool:\n+      \"\"\"\n+    Check if two rectangle are colliding/overlaping\n+    \n+    >>> checkCollision(rect1, rect2)\n+    True\n+    \"\"\"\n+    x_bound = rect1[\"x\"] < rect2[\"x\"] + rect1[\"width\"] and rect1[\"x\"] + rect2[\"width\"] > rect2[\"x\"]",
    "comment": "an error occured while parsing the file: collision_between_rectangles.py\npython\ntraceback (most recent call last):\n  file \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 145, in parse\n    reports = lint_file(\nlibcst._exceptions.parsersyntaxerror: syntax error @ 22:1.\ninconsistent indentation. expected a dedent.\n\n    x_bound = rect1[\"x\"] < rect2[\"x\"] + rect1[\"width\"] and rect1[\"x\"] + rect2[\"width\"] > rect2[\"x\"]\n^",
    "line_number": 22,
    "enriched": "File: collision_between_rectangles.py\nCode: @@ -0,0 +1,29 @@\n+rect1 = {\n+    \"x\": 10,\n+    \"y\": 10,\n+    \"height\": 30,\n+    \"width\": 50\n+}\n+\n+rect2 = {\n+    \"x\": 20,\n+    \"y\": 30,\n+    \"height\": 40,\n+    \"width\": 30\n+}\n+\n+def checkCollision(rect1, rect2) -> bool:\n+      \"\"\"\n+    Check if two rectangle are colliding/overlaping\n+    \n+    >>> checkCollision(rect1, rect2)\n+    True\n+    \"\"\"\n+    x_bound = rect1[\"x\"] < rect2[\"x\"] + rect1[\"width\"] and rect1[\"x\"] + rect2[\"width\"] > rect2[\"x\"]\nComment: An error occured while parsing the file: `collision_between_rectangles.py`\n```python\nTraceback (most recent call last):\n  File \"/opt/render/project/src/algorithms_keeper/parser/python_parser.py\", line 145, in parse\n    reports = lint_file(\nlibcst._exceptions.ParserSyntaxError: Syntax Error @ 22:1.\nInconsistent indentation. Expected a dedent.\n\n    x_bound = rect1[\"x\"] < rect2[\"x\"] + rect1[\"width\"] and rect1[\"x\"] + rect2[\"width\"] > rect2[\"x\"]\n^\n\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "collision_between_rectangles.py",
    "pr_number": 7829,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008652638,
    "comment_created_at": "2022-10-29T07:25:19Z"
  },
  {
    "code": "@@ -13,3 +13,7 @@ def arc_length(angle: int, radius: int) -> float:\n \n if __name__ == \"__main__\":\n     print(arc_length(90, 10))\n+    \"\"\"\n+    >>> arc_length(90, 10)\n+    15.7079632679",
    "comment": "this should be placed with the rest of the doctests, not separately in the __main__ block",
    "line_number": 18,
    "enriched": "File: maths/arc_length.py\nCode: @@ -13,3 +13,7 @@ def arc_length(angle: int, radius: int) -> float:\n \n if __name__ == \"__main__\":\n     print(arc_length(90, 10))\n+    \"\"\"\n+    >>> arc_length(90, 10)\n+    15.7079632679\nComment: This should be placed with the rest of the doctests, not separately in the `__main__` block",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/arc_length.py",
    "pr_number": 8964,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1297755329,
    "comment_created_at": "2023-08-17T21:10:09Z"
  },
  {
    "code": "@@ -0,0 +1,78 @@\n+# Python3 code to find minimum steps to reach\n+# to specific cell in minimum moves by Knight\n+\n+\n+class cell:",
    "comment": "class names should follow the [camelcase](https://en.wikipedia.org/wiki/camel_case) naming convention. please update the following name accordingly: cell",
    "line_number": 5,
    "enriched": "File: graphs/steps_by_knight.py\nCode: @@ -0,0 +1,78 @@\n+# Python3 code to find minimum steps to reach\n+# to specific cell in minimum moves by Knight\n+\n+\n+class cell:\nComment: Class names should follow the [`CamelCase`](https://en.wikipedia.org/wiki/Camel_case) naming convention. Please update the following name accordingly: `cell`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "graphs/steps_by_knight.py",
    "pr_number": 10340,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1357329780,
    "comment_created_at": "2023-10-12T19:49:23Z"
  },
  {
    "code": "@@ -0,0 +1,253 @@\n+\"\"\"\n+Title: Simplified Implementation of Artificial Neural Networks\n+\n+Description: This Python code offers a straightforward approach to utilizing Artificial Neural Networks for tasks involving both classification and regression. For more information on multiclass classification, you can refer to [this Wikipedia page](https://en.wikipedia.org/wiki/Multiclass_classification).\n+\n+\"\"\"\n+import tensorflow as tf\n+import pandas as pd\n+import numpy as np\n+import matplotlib.pyplot as plt\n+from tensorflow import keras\n+\n+class ANN_classifier:",
    "comment": "class names should follow the [camelcase](https://en.wikipedia.org/wiki/camel_case) naming convention. please update the following name accordingly: ann_classifier",
    "line_number": 13,
    "enriched": "File: neural_network/artificial_neural_networks.py\nCode: @@ -0,0 +1,253 @@\n+\"\"\"\n+Title: Simplified Implementation of Artificial Neural Networks\n+\n+Description: This Python code offers a straightforward approach to utilizing Artificial Neural Networks for tasks involving both classification and regression. For more information on multiclass classification, you can refer to [this Wikipedia page](https://en.wikipedia.org/wiki/Multiclass_classification).\n+\n+\"\"\"\n+import tensorflow as tf\n+import pandas as pd\n+import numpy as np\n+import matplotlib.pyplot as plt\n+from tensorflow import keras\n+\n+class ANN_classifier:\nComment: Class names should follow the [`CamelCase`](https://en.wikipedia.org/wiki/Camel_case) naming convention. Please update the following name accordingly: `ANN_classifier`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "neural_network/artificial_neural_networks.py",
    "pr_number": 9273,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342119890,
    "comment_created_at": "2023-10-01T11:18:47Z"
  },
  {
    "code": "@@ -60,3 +60,34 @@ def quick_select(items: list, index: int):\n     # must be in larger\n     else:\n         return quick_select(larger, index - (m + count))\n+\n+\n+def median(data: list):",
    "comment": "since this is an application of the quick_select algorithm, can you move it to its own file?",
    "line_number": 65,
    "enriched": "File: searches/quick_select.py\nCode: @@ -60,3 +60,34 @@ def quick_select(items: list, index: int):\n     # must be in larger\n     else:\n         return quick_select(larger, index - (m + count))\n+\n+\n+def median(data: list):\nComment: Since this is an application of the quick_select algorithm, can you move it to its own file?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "searches/quick_select.py",
    "pr_number": 12676,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 2083925716,
    "comment_created_at": "2025-05-12T06:38:43Z"
  },
  {
    "code": "@@ -0,0 +1,269 @@\n+\"\"\"\n+Generative Adversarial Network\n+\n+Objective : To train a GAN model to generate handwritten digits that can be transferred to other domains.\n+\n+Resources GAN Theory :\n+    https://en.wikipedia.org/wiki/Generative_adversarial_network\n+Resources PyTorch: https://pytorch.org/\n+\n+Download dataset from :\n+PyTorch internal function\n+\n+1. Fetch the Dataset with PyTorch function.\n+2. Create Dataloader.\n+3. Create Discriminator and Generator.\n+4. Set the hyperparameters and models.\n+5. Set the loss functions.\n+6. Create the training loop.\n+7. Visualize the losses.\n+8. Visualize the result from GAN.\n+\n+\"\"\"\n+\n+import numpy as np\n+import torch\n+import matplotlib.pyplot as plt\n+from torchvision import datasets\n+import torchvision.transforms as transforms\n+\n+# number of subprocesses to use for data loading\n+num_workers = 0\n+# how many samples per batch to load\n+batch_size = 64\n+\n+# convert data to torch.FloatTensor\n+transform = transforms.ToTensor()\n+\n+# get the training datasets\n+train_data = datasets.MNIST(root='data', train=True,\n+                                   download=True, transform=transform)\n+\n+# prepare data loader\n+train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n+                                           num_workers=num_workers)\n+\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+# Creating Generator and Discriminator for GAN\n+\n+class discriminator(nn.Module):",
    "comment": "class names should follow the [camelcase](https://en.wikipedia.org/wiki/camel_case) naming convention. please update the following name accordingly: discriminator",
    "line_number": 51,
    "enriched": "File: computer_vision/Generative_Adversarial_Network_MNIST.py\nCode: @@ -0,0 +1,269 @@\n+\"\"\"\n+Generative Adversarial Network\n+\n+Objective : To train a GAN model to generate handwritten digits that can be transferred to other domains.\n+\n+Resources GAN Theory :\n+    https://en.wikipedia.org/wiki/Generative_adversarial_network\n+Resources PyTorch: https://pytorch.org/\n+\n+Download dataset from :\n+PyTorch internal function\n+\n+1. Fetch the Dataset with PyTorch function.\n+2. Create Dataloader.\n+3. Create Discriminator and Generator.\n+4. Set the hyperparameters and models.\n+5. Set the loss functions.\n+6. Create the training loop.\n+7. Visualize the losses.\n+8. Visualize the result from GAN.\n+\n+\"\"\"\n+\n+import numpy as np\n+import torch\n+import matplotlib.pyplot as plt\n+from torchvision import datasets\n+import torchvision.transforms as transforms\n+\n+# number of subprocesses to use for data loading\n+num_workers = 0\n+# how many samples per batch to load\n+batch_size = 64\n+\n+# convert data to torch.FloatTensor\n+transform = transforms.ToTensor()\n+\n+# get the training datasets\n+train_data = datasets.MNIST(root='data', train=True,\n+                                   download=True, transform=transform)\n+\n+# prepare data loader\n+train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n+                                           num_workers=num_workers)\n+\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+# Creating Generator and Discriminator for GAN\n+\n+class discriminator(nn.Module):\nComment: Class names should follow the [`CamelCase`](https://en.wikipedia.org/wiki/Camel_case) naming convention. Please update the following name accordingly: `discriminator`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "computer_vision/Generative_Adversarial_Network_MNIST.py",
    "pr_number": 11961,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1796179045,
    "comment_created_at": "2024-10-10T22:47:58Z"
  },
  {
    "code": "@@ -0,0 +1,75 @@\n+\"\"\"\r\n+Sum of all nodes in a binary tree.\r\n+\r\n+Python implementation:\r\n+    O(n) time complexity - Recurses through :meth:`depth_first_search`\r\n+                            with each element.\r\n+    O(n) space complexity - At any point in time maximum number of stack\r\n+                            frames that could be in memory is `n`\r\n+\"\"\"\r\n+\r\n+\r\n+from __future__ import annotations\r\n+\r\n+\r\n+class Node:\r\n+    \"\"\"\r\n+    A Node has value variable and pointers\r\n+    to Nodes to its left and right.\r",
    "comment": "use the 88 chars.  ;-)",
    "line_number": 18,
    "enriched": "File: data_structures/binary_tree/binary_tree_node_sum.py\nCode: @@ -0,0 +1,75 @@\n+\"\"\"\r\n+Sum of all nodes in a binary tree.\r\n+\r\n+Python implementation:\r\n+    O(n) time complexity - Recurses through :meth:`depth_first_search`\r\n+                            with each element.\r\n+    O(n) space complexity - At any point in time maximum number of stack\r\n+                            frames that could be in memory is `n`\r\n+\"\"\"\r\n+\r\n+\r\n+from __future__ import annotations\r\n+\r\n+\r\n+class Node:\r\n+    \"\"\"\r\n+    A Node has value variable and pointers\r\n+    to Nodes to its left and right.\r\nComment: Use the 88 chars.  ;-)\r\n```suggestion\r\n    A Node has a value variable and pointers to Nodes to its left and right.\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "data_structures/binary_tree/binary_tree_node_sum.py",
    "pr_number": 7162,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 995912890,
    "comment_created_at": "2022-10-14T15:59:13Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+# https://leetcode.com/problems/monotonic-array/\n+def is_monotonic(nums: list[int]) -> bool:\n+    \"\"\"\n+    Check if a list is monotonic.\n+\n+    >>> is_monotonic([1, 2, 2, 3])\n+    True\n+    >>> is_monotonic([6, 5, 4, 4])\n+    True\n+    >>> is_monotonic([1, 3, 2])\n+    False\n+    \"\"\"\n+    return all(nums[i] <= nums[i + 1] for i in range(len(nums) - 1)) or all(\n+        nums[i] >= nums[i + 1] for i in range(len(nums) - 1)\n+    )\n+\n+\n+# Test the function with your examples\n+print(is_monotonic([1, 2, 2, 3]))  # Output: True\n+print(is_monotonic([6, 5, 4, 4]))  # Output: True\n+print(is_monotonic([1, 3, 2]))  # Output: False",
    "comment": "please encapsulate all driver code within an if  __name__ == \"__main__\" block",
    "line_number": 21,
    "enriched": "File: data_structures/arrays/monotonic_array.py\nCode: @@ -0,0 +1,21 @@\n+# https://leetcode.com/problems/monotonic-array/\n+def is_monotonic(nums: list[int]) -> bool:\n+    \"\"\"\n+    Check if a list is monotonic.\n+\n+    >>> is_monotonic([1, 2, 2, 3])\n+    True\n+    >>> is_monotonic([6, 5, 4, 4])\n+    True\n+    >>> is_monotonic([1, 3, 2])\n+    False\n+    \"\"\"\n+    return all(nums[i] <= nums[i + 1] for i in range(len(nums) - 1)) or all(\n+        nums[i] >= nums[i + 1] for i in range(len(nums) - 1)\n+    )\n+\n+\n+# Test the function with your examples\n+print(is_monotonic([1, 2, 2, 3]))  # Output: True\n+print(is_monotonic([6, 5, 4, 4]))  # Output: True\n+print(is_monotonic([1, 3, 2]))  # Output: False\nComment: Please encapsulate all driver code within an `if  __name__ == \"__main__\"` block",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "data_structures/arrays/monotonic_array.py",
    "pr_number": 11025,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1374696497,
    "comment_created_at": "2023-10-27T14:54:08Z"
  },
  {
    "code": "@@ -0,0 +1,71 @@\n+def rgb_to_cmyk(r_input, g_input, b_input: int) -> tuple[int, int, int, int]:\r",
    "comment": "please provide type hint for the parameter: r_input\n\nplease provide type hint for the parameter: g_input",
    "line_number": 1,
    "enriched": "File: conversions/rgb_cmyk_conversion.py\nCode: @@ -0,0 +1,71 @@\n+def rgb_to_cmyk(r_input, g_input, b_input: int) -> tuple[int, int, int, int]:\r\nComment: Please provide type hint for the parameter: `r_input`\n\nPlease provide type hint for the parameter: `g_input`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "conversions/rgb_cmyk_conversion.py",
    "pr_number": 10741,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367581099,
    "comment_created_at": "2023-10-21T01:09:02Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+\"\"\"\n+This is pure Python implementation of DateTime\n+\n+For doctests run following command:\n+\n+python3 validate_datetime.py\n+\n+\"\"\"\n+\n+\n+import datetime\n+import doctest\n+import os\n+\n+\n+def parseOptions():\n+\n+    import optparse\n+\n+    parser = optparse.OptionParser(usage=\"-h\")\n+    parser.add_option(\"-d\", \"--difference\", type=\"int\")\n+    (options, args) = parser.parse_args()\n+    return options\n+\n+\n+now = datetime.datetime.now()\n+\n+\n+def subtime(a, b):\n+    \"\"\"(datetime,int) -> datetime\n+    Subtract b hours from a datetime.datetime and return the new datetime object\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),10)\n+    datetime.datetime(2013, 11, 11, 1, 0)\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),24)\n+    datetime.datetime(2013, 11, 10, 11, 0)\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),0)\n+    datetime.datetime(2013, 11, 11, 11, 0)\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),-5)\n+    datetime.datetime(2013, 11, 11, 16, 0)\n+\n+    \"\"\"\n+    subtract = datetime.timedelta(hours=b)\n+    difference = a - subtract\n+    return difference\n+\n+\n+if __name__ == \"__main__\":\n+    doctest.testmod()\n+\n+print",
    "comment": "this does nothing",
    "line_number": 54,
    "enriched": "File: scripts/validate_datetime.py\nCode: @@ -0,0 +1,61 @@\n+\"\"\"\n+This is pure Python implementation of DateTime\n+\n+For doctests run following command:\n+\n+python3 validate_datetime.py\n+\n+\"\"\"\n+\n+\n+import datetime\n+import doctest\n+import os\n+\n+\n+def parseOptions():\n+\n+    import optparse\n+\n+    parser = optparse.OptionParser(usage=\"-h\")\n+    parser.add_option(\"-d\", \"--difference\", type=\"int\")\n+    (options, args) = parser.parse_args()\n+    return options\n+\n+\n+now = datetime.datetime.now()\n+\n+\n+def subtime(a, b):\n+    \"\"\"(datetime,int) -> datetime\n+    Subtract b hours from a datetime.datetime and return the new datetime object\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),10)\n+    datetime.datetime(2013, 11, 11, 1, 0)\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),24)\n+    datetime.datetime(2013, 11, 10, 11, 0)\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),0)\n+    datetime.datetime(2013, 11, 11, 11, 0)\n+\n+    >>> subtime(datetime.datetime(2013,11,11,11,0),-5)\n+    datetime.datetime(2013, 11, 11, 16, 0)\n+\n+    \"\"\"\n+    subtract = datetime.timedelta(hours=b)\n+    difference = a - subtract\n+    return difference\n+\n+\n+if __name__ == \"__main__\":\n+    doctest.testmod()\n+\n+print\nComment: ```suggestion\r\n```\r\nThis does nothing",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "scripts/validate_datetime.py",
    "pr_number": 7230,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996326726,
    "comment_created_at": "2022-10-15T16:27:39Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+import numpy as np\n+\n+class GeneticAlgorithm:\n+    def __init__(self, func, bounds, pop_size=20, generations=100, mutation_rate=0.1, crossover_rate=0.8, selection_method='tournament'):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: func\n\nplease provide type hint for the parameter: bounds\n\nplease provide type hint for the parameter: pop_size\n\nplease provide type hint for the parameter: generations\n\nplease provide type hint for the parameter: mutation_rate\n\nplease provide type hint for the parameter: crossover_rate\n\nplease provide type hint for the parameter: selection_method",
    "line_number": 4,
    "enriched": "File: genetic_algorithm/ga_optimisation.py\nCode: @@ -0,0 +1,94 @@\n+import numpy as np\n+\n+class GeneticAlgorithm:\n+    def __init__(self, func, bounds, pop_size=20, generations=100, mutation_rate=0.1, crossover_rate=0.8, selection_method='tournament'):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `func`\n\nPlease provide type hint for the parameter: `bounds`\n\nPlease provide type hint for the parameter: `pop_size`\n\nPlease provide type hint for the parameter: `generations`\n\nPlease provide type hint for the parameter: `mutation_rate`\n\nPlease provide type hint for the parameter: `crossover_rate`\n\nPlease provide type hint for the parameter: `selection_method`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "genetic_algorithm/ga_optimisation.py",
    "pr_number": 12039,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1798744224,
    "comment_created_at": "2024-10-14T04:50:08Z"
  },
  {
    "code": "@@ -0,0 +1,49 @@\n+\"\"\"\n+Financial ratios are quantitative metrics used to analyze and assess the relationships\n+between different financial elements in a company's financial statements, providing\n+insights into its performance, profitability, liquidity, and overall financial health.\n+\n+Reference: https://en.wikipedia.org/wiki/Current_ratio\n+The Current Ratio is a liquidity ratio that measures whether a firm has enough\n+resources to meet its short-term obligations.\n+\n+Reference: https://en.wikipedia.org/wiki/Quick_ratio\n+The quick ratio, also known as the acid-test ratio, is a financial ratio that measures\n+a company's ability to cover its short-term liabilities with its most liquid assets,\n+excluding inventory. It is calculated by dividing the sum of cash, marketable\n+securities, and accounts receivable by the total current liabilities. The quick ratio\n+provides a more conservative assessment of a company's liquidity compared to the\n+current ratio, as it excludes inventory, which may not be as readily convertible\n+to cash in the short term. A higher quick ratio indicates a stronger\n+ability to meet short-term obligations.\n+\"\"\"\n+\n+\n+def current_ratio(current_assets: int, current_liabilities) -> float:",
    "comment": "please provide type hint for the parameter: current_liabilities",
    "line_number": 22,
    "enriched": "File: financial/ratios.py\nCode: @@ -0,0 +1,49 @@\n+\"\"\"\n+Financial ratios are quantitative metrics used to analyze and assess the relationships\n+between different financial elements in a company's financial statements, providing\n+insights into its performance, profitability, liquidity, and overall financial health.\n+\n+Reference: https://en.wikipedia.org/wiki/Current_ratio\n+The Current Ratio is a liquidity ratio that measures whether a firm has enough\n+resources to meet its short-term obligations.\n+\n+Reference: https://en.wikipedia.org/wiki/Quick_ratio\n+The quick ratio, also known as the acid-test ratio, is a financial ratio that measures\n+a company's ability to cover its short-term liabilities with its most liquid assets,\n+excluding inventory. It is calculated by dividing the sum of cash, marketable\n+securities, and accounts receivable by the total current liabilities. The quick ratio\n+provides a more conservative assessment of a company's liquidity compared to the\n+current ratio, as it excludes inventory, which may not be as readily convertible\n+to cash in the short term. A higher quick ratio indicates a stronger\n+ability to meet short-term obligations.\n+\"\"\"\n+\n+\n+def current_ratio(current_assets: int, current_liabilities) -> float:\nComment: Please provide type hint for the parameter: `current_liabilities`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "financial/ratios.py",
    "pr_number": 8977,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1296626546,
    "comment_created_at": "2023-08-17T03:13:24Z"
  },
  {
    "code": "@@ -0,0 +1,26 @@\n+import hashlib\n+\n+class PeyxwBlock:\n+    \n+    def __init__(self, previous_block_hash, transaction_list):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: previous_block_hash\n\nplease provide type hint for the parameter: transaction_list",
    "line_number": 5,
    "enriched": "File: blockchain/simple_blockchain_algorithm.py\nCode: @@ -0,0 +1,26 @@\n+import hashlib\n+\n+class PeyxwBlock:\n+    \n+    def __init__(self, previous_block_hash, transaction_list):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `previous_block_hash`\n\nPlease provide type hint for the parameter: `transaction_list`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "blockchain/simple_blockchain_algorithm.py",
    "pr_number": 8671,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1171457619,
    "comment_created_at": "2023-04-19T14:47:08Z"
  },
  {
    "code": "@@ -0,0 +1,114 @@\n+# source - The ARRL Handbook for Radio Communications\n+\n+from math import exp  # value of exp = 2.718281828459\u2026\n+\n+\"\"\"\n+Description\n+-----------\n+when a capacitor is connected with a potential source(AC or DC). It is starts to charge\n+at a general speed but when a resistor is connected in the  circuit with in series to\n+a capacitor then the capacitor charges slowly means it will take more time than usual.\n+while the capacitor is being charged, the voltage is in exponential function with time.\n+\n+in the this function there is RC which is 'resistance(ohms)*capacitance(farads)'.\n+also represented as \u03c4 (tau).\n+\n+with the help of RC-timeconstant we can find the voltage at any time 't' from the\n+initiation of charging a capacitor with the help of the exponential function\n+containing RC.Both at charging and discharging of a capacitor.\n+\"\"\"\n+\n+\n+def charging_capacitor(",
    "comment": "please provide return type hint for the function: charging_capacitor. **if the function does not return a value, please provide the type hint as:** def function() -> none:",
    "line_number": 22,
    "enriched": "File: electronics/charging_capacitor.py\nCode: @@ -0,0 +1,114 @@\n+# source - The ARRL Handbook for Radio Communications\n+\n+from math import exp  # value of exp = 2.718281828459\u2026\n+\n+\"\"\"\n+Description\n+-----------\n+when a capacitor is connected with a potential source(AC or DC). It is starts to charge\n+at a general speed but when a resistor is connected in the  circuit with in series to\n+a capacitor then the capacitor charges slowly means it will take more time than usual.\n+while the capacitor is being charged, the voltage is in exponential function with time.\n+\n+in the this function there is RC which is 'resistance(ohms)*capacitance(farads)'.\n+also represented as \u03c4 (tau).\n+\n+with the help of RC-timeconstant we can find the voltage at any time 't' from the\n+initiation of charging a capacitor with the help of the exponential function\n+containing RC.Both at charging and discharging of a capacitor.\n+\"\"\"\n+\n+\n+def charging_capacitor(\nComment: Please provide return type hint for the function: `charging_capacitor`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "electronics/charging_capacitor.py",
    "pr_number": 10016,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349527018,
    "comment_created_at": "2023-10-07T14:18:07Z"
  },
  {
    "code": "@@ -0,0 +1,98 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval=None) -> None:",
    "comment": "please provide type hint for the parameter: initval",
    "line_number": 21,
    "enriched": "File: sorts/tree_sort_2.py\nCode: @@ -0,0 +1,98 @@\n+\"\"\"\n+\n+tree_sort_2\n+\n+- makes use of the data structure Binary Tree to sort the list in O(nlogn) time.\n+- Binary Search Tree [BST]\n+  - For each node with value v\n+    - All values in the left subtree are < v\n+    - All values in the right subtree are > v\n+  - No Duplicate values.\n+- Logic:\n+  - Build a Binary Search Tree\n+  - Traverse the tree in increasing order.\n+\n+\"\"\"\n+\n+\n+class Tree:\n+    # Binary Search Tree Data Structure\n+    # Constructor\n+    def __init__(self, initval=None) -> None:\nComment: Please provide type hint for the parameter: `initval`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "sorts/tree_sort_2.py",
    "pr_number": 7459,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000783884,
    "comment_created_at": "2022-10-20T15:23:06Z"
  },
  {
    "code": "@@ -0,0 +1,29 @@\n+rect1 = {\n+    \"x\": 10,\n+    \"y\": 10,\n+    \"height\": 30,\n+    \"width\": 50\n+}\n+\n+rect2 = {\n+    \"x\": 20,\n+    \"y\": 30,\n+    \"height\": 40,\n+    \"width\": 30\n+}\n+\n+def check_collision(rect1, rect2) -> bool:",
    "comment": "please provide type hint for the parameter: rect1\n\nplease provide type hint for the parameter: rect2",
    "line_number": 15,
    "enriched": "File: maths/collision_between_rectangles.py\nCode: @@ -0,0 +1,29 @@\n+rect1 = {\n+    \"x\": 10,\n+    \"y\": 10,\n+    \"height\": 30,\n+    \"width\": 50\n+}\n+\n+rect2 = {\n+    \"x\": 20,\n+    \"y\": 30,\n+    \"height\": 40,\n+    \"width\": 30\n+}\n+\n+def check_collision(rect1, rect2) -> bool:\nComment: Please provide type hint for the parameter: `rect1`\n\nPlease provide type hint for the parameter: `rect2`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "maths/collision_between_rectangles.py",
    "pr_number": 7831,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008653854,
    "comment_created_at": "2022-10-29T07:34:22Z"
  },
  {
    "code": "@@ -0,0 +1,51 @@\n+\"\"\"\n+This is a pure Python implementation of the greedy algorithm\n+reference: https://practice.geeksforgeeks.org/problems/minimum-platforms-1587115620/1#\n+\n+For doctests run following command:\n+python3 -m doctest -v minimum_platforms_required.py\n+\n+We will sort both arrays. When there is sorted events, it will be easy to maintain\n+the count of trains that have arrived but not departed. The total platforms needed\n+at one time can be found by taking the difference between arrivals and departures\n+minimum will be the final answer.\n+\"\"\"\n+\n+def minimum_platforms_required(arrival_times, departure_times):",
    "comment": "please provide return type hint for the function: minimum_platforms_required. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: arrival_times\n\nplease provide type hint for the parameter: departure_times",
    "line_number": 14,
    "enriched": "File: greedy_methods/minimum_platforms_required.py\nCode: @@ -0,0 +1,51 @@\n+\"\"\"\n+This is a pure Python implementation of the greedy algorithm\n+reference: https://practice.geeksforgeeks.org/problems/minimum-platforms-1587115620/1#\n+\n+For doctests run following command:\n+python3 -m doctest -v minimum_platforms_required.py\n+\n+We will sort both arrays. When there is sorted events, it will be easy to maintain\n+the count of trains that have arrived but not departed. The total platforms needed\n+at one time can be found by taking the difference between arrivals and departures\n+minimum will be the final answer.\n+\"\"\"\n+\n+def minimum_platforms_required(arrival_times, departure_times):\nComment: Please provide return type hint for the function: `minimum_platforms_required`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `arrival_times`\n\nPlease provide type hint for the parameter: `departure_times`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "greedy_methods/minimum_platforms_required.py",
    "pr_number": 9120,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1341198989,
    "comment_created_at": "2023-09-29T10:33:01Z"
  },
  {
    "code": "@@ -0,0 +1,88 @@\n+\"\"\"\n+ - - - - - -- - - - - - - - - - - - - - - - - - - - - - -\n+Name - - RBFNN - Radial Basis Function Neural Network\n+Goal - - Recognize Patterns in Data\n+Detail: Total 3 layers neural network\n+        * Input layer\n+        * Hidden layer with RBF activation\n+        * Output layer\n+Author: Your Name\n+Github: your_email@example.com\n+Date: 2024.10.31\n+- - - - - -- - - - - - - - - - - - - - - - - - - - - - -\n+\"\"\"\n+\n+import numpy as np  # For numerical operations\n+\n+class RBFNN:\n+    def __init__(self, input_size, hidden_size, output_size):",
    "comment": "please provide return type hint for the function: __init__. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: input_size\n\nplease provide type hint for the parameter: hidden_size\n\nplease provide type hint for the parameter: output_size",
    "line_number": 18,
    "enriched": "File: neural_network/radial_basis_function_neural_network.py\nCode: @@ -0,0 +1,88 @@\n+\"\"\"\n+ - - - - - -- - - - - - - - - - - - - - - - - - - - - - -\n+Name - - RBFNN - Radial Basis Function Neural Network\n+Goal - - Recognize Patterns in Data\n+Detail: Total 3 layers neural network\n+        * Input layer\n+        * Hidden layer with RBF activation\n+        * Output layer\n+Author: Your Name\n+Github: your_email@example.com\n+Date: 2024.10.31\n+- - - - - -- - - - - - - - - - - - - - - - - - - - - - -\n+\"\"\"\n+\n+import numpy as np  # For numerical operations\n+\n+class RBFNN:\n+    def __init__(self, input_size, hidden_size, output_size):\nComment: Please provide return type hint for the function: `__init__`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `input_size`\n\nPlease provide type hint for the parameter: `hidden_size`\n\nPlease provide type hint for the parameter: `output_size`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "neural_network/radial_basis_function_neural_network.py",
    "pr_number": 12342,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1824585149,
    "comment_created_at": "2024-10-31T14:34:07Z"
  },
  {
    "code": "@@ -0,0 +1,76 @@\n+import cv2\n+import mediapipe as mp\n+import time\n+\n+def resize_image(img, max_width=1000, max_height=800):",
    "comment": "please provide return type hint for the function: resize_image. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: img\n\nplease provide type hint for the parameter: max_width\n\nplease provide type hint for the parameter: max_height",
    "line_number": 5,
    "enriched": "File: web_programming/face_detection.py\nCode: @@ -0,0 +1,76 @@\n+import cv2\n+import mediapipe as mp\n+import time\n+\n+def resize_image(img, max_width=1000, max_height=800):\nComment: Please provide return type hint for the function: `resize_image`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `img`\n\nPlease provide type hint for the parameter: `max_width`\n\nPlease provide type hint for the parameter: `max_height`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "web_programming/face_detection.py",
    "pr_number": 12055,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1798747433,
    "comment_created_at": "2024-10-14T04:57:50Z"
  },
  {
    "code": "@@ -0,0 +1,116 @@\n+\"\"\"\n+Implementation of SimCLR.\n+Self-Supervised Learning (SSL) with SimCLR. SimCLR is a framework for learning visual representations without labels by maximizing the agreement between different augmented views of the same image.\n+\"\"\"\n+\n+import numpy as np\n+import tensorflow as tf\n+from tensorflow.keras import layers\n+from tensorflow.keras.models import Model\n+from tensorflow.keras.optimizers import Adam\n+from tensorflow.keras.losses import SparseCategoricalCrossentropy\n+from tensorflow.keras.applications import ResNet50\n+from sklearn.metrics import ConfusionMatrixDisplay\n+from sklearn.model_selection import train_test_split\n+from sklearn.preprocessing import LabelEncoder\n+import matplotlib.pyplot as plt\n+\n+\n+def data_handling(data: dict) -> tuple:\n+    \"\"\"\n+    Handles the data by splitting features and targets.\n+\n+    >>> data_handling({'data': np.array([[0.1, 0.2], [0.3, 0.4]]), 'target': np.array([0, 1])})\n+    (array([[0.1, 0.2], [0.3, 0.4]]), array([0, 1]))\n+    \"\"\"\n+    return (data[\"data\"], data[\"target\"])\n+\n+\n+def simclr_model(input_shape=(32, 32, 3), projection_dim=64) -> Model:",
    "comment": "please provide type hint for the parameter: input_shape\n\nplease provide type hint for the parameter: projection_dim",
    "line_number": 29,
    "enriched": "File: machine_learning/simclr.py\nCode: @@ -0,0 +1,116 @@\n+\"\"\"\n+Implementation of SimCLR.\n+Self-Supervised Learning (SSL) with SimCLR. SimCLR is a framework for learning visual representations without labels by maximizing the agreement between different augmented views of the same image.\n+\"\"\"\n+\n+import numpy as np\n+import tensorflow as tf\n+from tensorflow.keras import layers\n+from tensorflow.keras.models import Model\n+from tensorflow.keras.optimizers import Adam\n+from tensorflow.keras.losses import SparseCategoricalCrossentropy\n+from tensorflow.keras.applications import ResNet50\n+from sklearn.metrics import ConfusionMatrixDisplay\n+from sklearn.model_selection import train_test_split\n+from sklearn.preprocessing import LabelEncoder\n+import matplotlib.pyplot as plt\n+\n+\n+def data_handling(data: dict) -> tuple:\n+    \"\"\"\n+    Handles the data by splitting features and targets.\n+\n+    >>> data_handling({'data': np.array([[0.1, 0.2], [0.3, 0.4]]), 'target': np.array([0, 1])})\n+    (array([[0.1, 0.2], [0.3, 0.4]]), array([0, 1]))\n+    \"\"\"\n+    return (data[\"data\"], data[\"target\"])\n+\n+\n+def simclr_model(input_shape=(32, 32, 3), projection_dim=64) -> Model:\nComment: Please provide type hint for the parameter: `input_shape`\n\nPlease provide type hint for the parameter: `projection_dim`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "machine_learning/simclr.py",
    "pr_number": 11900,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1792393847,
    "comment_created_at": "2024-10-08T19:22:21Z"
  },
  {
    "code": "@@ -0,0 +1,60 @@\n+# Created by Pronay Debnath\n+# Date:- 1/10/2023\n+# Energy-Mass Equivalence in Python\n+\n+import doctest\n+\n+def mass_to_energy(mass_kg):",
    "comment": "please provide return type hint for the function: mass_to_energy. **if the function does not return a value, please provide the type hint as:** def function() -> none:\n\nplease provide type hint for the parameter: mass_kg",
    "line_number": 7,
    "enriched": "File: physics/energy_mass_equivalence.py\nCode: @@ -0,0 +1,60 @@\n+# Created by Pronay Debnath\n+# Date:- 1/10/2023\n+# Energy-Mass Equivalence in Python\n+\n+import doctest\n+\n+def mass_to_energy(mass_kg):\nComment: Please provide return type hint for the function: `mass_to_energy`. **If the function does not return a value, please provide the type hint as:** `def function() -> None:`\n\nPlease provide type hint for the parameter: `mass_kg`",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "physics/energy_mass_equivalence.py",
    "pr_number": 9236,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1342104477,
    "comment_created_at": "2023-10-01T09:11:32Z"
  },
  {
    "code": "@@ -11,29 +11,40 @@\n synchronization could be used.\n \"\"\"\n from multiprocessing import Lock, Pipe, Process\n+from traceback import format_exc\n \n # lock used to ensure that two processes do not access a pipe at the same time\n process_lock = Lock()\n \n \"\"\"\n The function run by the processes that sorts the list\n \n-position = the position in the list the process represents, used to know which\n-            neighbor we pass our value to\n-value = the initial value at list[position]\n+\"position\" ----> the position in the list the process represents, used to know which\n+neighbor we pass our value to\n+\"value\" ----> the initial value at list[position]\n LSend, RSend = the pipes we use to send to our left and right neighbors\n LRcv, RRcv = the pipes we use to receive from our left and right neighbors\n resultPipe = the pipe used to send results back to main\n+arr_len = length of the list to be sorted\n \"\"\"\n \n \n-def oe_process(position, value, l_send, r_send, lr_cv, rr_cv, result_pipe):\n+def oe_process(\n+    position,\n+    value,\n+    l_send,\n+    r_send,\n+    lr_cv,\n+    rr_cv,\n+    result_pipe,",
    "comment": "please add type hints.",
    "line_number": 39,
    "enriched": "File: sorts/odd_even_transposition_parallel.py\nCode: @@ -11,29 +11,40 @@\n synchronization could be used.\n \"\"\"\n from multiprocessing import Lock, Pipe, Process\n+from traceback import format_exc\n \n # lock used to ensure that two processes do not access a pipe at the same time\n process_lock = Lock()\n \n \"\"\"\n The function run by the processes that sorts the list\n \n-position = the position in the list the process represents, used to know which\n-            neighbor we pass our value to\n-value = the initial value at list[position]\n+\"position\" ----> the position in the list the process represents, used to know which\n+neighbor we pass our value to\n+\"value\" ----> the initial value at list[position]\n LSend, RSend = the pipes we use to send to our left and right neighbors\n LRcv, RRcv = the pipes we use to receive from our left and right neighbors\n resultPipe = the pipe used to send results back to main\n+arr_len = length of the list to be sorted\n \"\"\"\n \n \n-def oe_process(position, value, l_send, r_send, lr_cv, rr_cv, result_pipe):\n+def oe_process(\n+    position,\n+    value,\n+    l_send,\n+    r_send,\n+    lr_cv,\n+    rr_cv,\n+    result_pipe,\nComment: Please add type hints.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "sorts/odd_even_transposition_parallel.py",
    "pr_number": 10976,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1375062087,
    "comment_created_at": "2023-10-27T21:34:36Z"
  },
  {
    "code": "@@ -0,0 +1,150 @@\n+from selenium import webdriver\n+from selenium.webdriver.common.by import By\n+\n+\n+class BankDetails:\n+    def __init__(self, ifsc_code) -> None:",
    "comment": "please provide type hint for the parameter: ifsc_code",
    "line_number": 6,
    "enriched": "File: web_programming/indian_bank_info.py\nCode: @@ -0,0 +1,150 @@\n+from selenium import webdriver\n+from selenium.webdriver.common.by import By\n+\n+\n+class BankDetails:\n+    def __init__(self, ifsc_code) -> None:\nComment: Please provide type hint for the parameter: `ifsc_code`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "web_programming/indian_bank_info.py",
    "pr_number": 10454,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359479543,
    "comment_created_at": "2023-10-14T16:43:38Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+\"\"\"\n+Title : Calculating the speed of sound\n+\n+Description :\n+    The speed of sound (c) is the speed that a sound wave travels\n+    per unit time (m/s). During propagation, the sound wave propagates\n+    through an elastic medium. Its SI unit is meter per second (m/s).\n+\n+    Only longitudinal waves can propagate in liquids and gas other then\n+    solid where they also travel in  transverse wave. The following Algo-\n+    rithem calculates the speed of sound in fluid depanding on the bulk\n+    module and the density of the fluid.\n+\n+    Equation for calculating speed od sound in fluid:\n+    c_fluid = (K_s*p)**0.5\n+\n+    c_fluid: speed of sound in fluid\n+    K_s: isentropic bulk modulus\n+    p: density of fluid\n+\n+\n+\n+Source : https://en.wikipedia.org/wiki/Speed_of_sound\n+\"\"\"\n+\n+\n+class SpeedOfSound:\n+    @staticmethod\n+    def fluid(fluid_density: float, bulk_modulus: float) -> float:",
    "comment": "we don't really use the class so it seems like unnecessary overhead.",
    "line_number": 29,
    "enriched": "File: physics/speed_of_sound.py\nCode: @@ -0,0 +1,56 @@\n+\"\"\"\n+Title : Calculating the speed of sound\n+\n+Description :\n+    The speed of sound (c) is the speed that a sound wave travels\n+    per unit time (m/s). During propagation, the sound wave propagates\n+    through an elastic medium. Its SI unit is meter per second (m/s).\n+\n+    Only longitudinal waves can propagate in liquids and gas other then\n+    solid where they also travel in  transverse wave. The following Algo-\n+    rithem calculates the speed of sound in fluid depanding on the bulk\n+    module and the density of the fluid.\n+\n+    Equation for calculating speed od sound in fluid:\n+    c_fluid = (K_s*p)**0.5\n+\n+    c_fluid: speed of sound in fluid\n+    K_s: isentropic bulk modulus\n+    p: density of fluid\n+\n+\n+\n+Source : https://en.wikipedia.org/wiki/Speed_of_sound\n+\"\"\"\n+\n+\n+class SpeedOfSound:\n+    @staticmethod\n+    def fluid(fluid_density: float, bulk_modulus: float) -> float:\nComment: We don't really use the class so it seems like unnecessary overhead.\r\n```suggestion\r\ndef speed_of_sound_in_a_fluid(density: float, bulk_modulus: float) -> float:\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "physics/speed_of_sound.py",
    "pr_number": 8803,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1222041184,
    "comment_created_at": "2023-06-07T19:01:28Z"
  },
  {
    "code": "@@ -0,0 +1,26 @@\n+# Python program to draw\n+# Rainbow Benzene\n+# using Turtle Programming\n+import turtle\n+\n+pen = turtle.Turtle()\n+pen.speed(0)\n+# it will make a 3D look like circle\n+pen.fillcolor(\"white\")\n+pen.begin_fill()\n+\n+# Radius of circle\n+pen.circle(100)\n+pen.end_fill()\n+pen.hideturtle()\n+\n+# for the colour of rainbow\n+colors = [\"red\", \"purple\", \"blue\", \"green\", \"orange\", \"yellow\"]\n+# to generate the benzene\n+t = turtle.Pen()\n+turtle.bgcolor(\"black\")",
    "comment": "i don't think you need this line...",
    "line_number": 21,
    "enriched": "File: graphics/rainbow_benzene.py\nCode: @@ -0,0 +1,26 @@\n+# Python program to draw\n+# Rainbow Benzene\n+# using Turtle Programming\n+import turtle\n+\n+pen = turtle.Turtle()\n+pen.speed(0)\n+# it will make a 3D look like circle\n+pen.fillcolor(\"white\")\n+pen.begin_fill()\n+\n+# Radius of circle\n+pen.circle(100)\n+pen.end_fill()\n+pen.hideturtle()\n+\n+# for the colour of rainbow\n+colors = [\"red\", \"purple\", \"blue\", \"green\", \"orange\", \"yellow\"]\n+# to generate the benzene\n+t = turtle.Pen()\n+turtle.bgcolor(\"black\")\nComment: I don't think you need this line...",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "graphics/rainbow_benzene.py",
    "pr_number": 10286,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1357672919,
    "comment_created_at": "2023-10-13T02:08:27Z"
  },
  {
    "code": "@@ -1,48 +1,148 @@\n-\"\"\"Binary Exponentiation.\"\"\"\n+\"\"\"\n+Binary Exponentiation\n \n-# Author : Junth Basnet\n-# Time Complexity : O(logn)\n+This is a method to find a^b in O(log b) time complexity and is one of the most commonly\n+used methods of exponentiation. The method is also useful for modular exponentiation,\n+when the solution to (a^b) % c is required.\n \n+To calculate a^b:\n+- If b is even, then a b = (a * a)^(b / 2)\n+- If b is odd, then a^b = a * a^(b - 1)\n+Repeat until b = 1 or b = 0\n \n-def binary_exponentiation(a: int, n: int) -> int:\n+For modular exponentiation, we use the fact that (a * b) % c = ((a % c) * (b % c)) % c\n+\"\"\"\n+\n+\n+def binary_exp_recursive(base: int, exponent: int) -> int:\n     \"\"\"\n-    Compute a number raised by some quantity",
    "comment": "why remove this?  exp is a bit cryptic in the function name and a line of documentation here is useful.",
    "line_number": 9,
    "enriched": "File: maths/binary_exponentiation.py\nCode: @@ -1,48 +1,148 @@\n-\"\"\"Binary Exponentiation.\"\"\"\n+\"\"\"\n+Binary Exponentiation\n \n-# Author : Junth Basnet\n-# Time Complexity : O(logn)\n+This is a method to find a^b in O(log b) time complexity and is one of the most commonly\n+used methods of exponentiation. The method is also useful for modular exponentiation,\n+when the solution to (a^b) % c is required.\n \n+To calculate a^b:\n+- If b is even, then a b = (a * a)^(b / 2)\n+- If b is odd, then a^b = a * a^(b - 1)\n+Repeat until b = 1 or b = 0\n \n-def binary_exponentiation(a: int, n: int) -> int:\n+For modular exponentiation, we use the fact that (a * b) % c = ((a % c) * (b % c)) % c\n+\"\"\"\n+\n+\n+def binary_exp_recursive(base: int, exponent: int) -> int:\n     \"\"\"\n-    Compute a number raised by some quantity\nComment: Why remove this?  `exp` is a bit cryptic in the function name and a line of documentation here is useful.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "maths/binary_exponentiation.py",
    "pr_number": 10742,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367665971,
    "comment_created_at": "2023-10-21T05:17:19Z"
  },
  {
    "code": "@@ -4,7 +4,22 @@\n \r\n \r\n def sieve(n: int) -> list[int]:\r\n-    \"\"\"Segmented Sieve.\"\"\"\r\n+    \"\"\"\r\n+    Segmented Sieve.\r\n+\r\n+    Examples:\r\n+    >>> sieve(2**3)\r\n+    [2, 3, 5, 7]\r\n+\r\n+    >>> sieve(3**3)\r\n+    [2, 3, 5, 7, 11, 13, 17, 19, 23]\r\n+\r\n+    >>> sieve()\r\n+    Traceback (most recent call last):\r\n+        ...\r\n+    TypeError: sieve() missing 1 required positional argument: 'n'\r\n+\r",
    "comment": "i think it's fine to assume that the function will be called on the right number of inputs, so i don't think this test is necessary",
    "line_number": 21,
    "enriched": "File: maths/segmented_sieve.py\nCode: @@ -4,7 +4,22 @@\n \r\n \r\n def sieve(n: int) -> list[int]:\r\n-    \"\"\"Segmented Sieve.\"\"\"\r\n+    \"\"\"\r\n+    Segmented Sieve.\r\n+\r\n+    Examples:\r\n+    >>> sieve(2**3)\r\n+    [2, 3, 5, 7]\r\n+\r\n+    >>> sieve(3**3)\r\n+    [2, 3, 5, 7, 11, 13, 17, 19, 23]\r\n+\r\n+    >>> sieve()\r\n+    Traceback (most recent call last):\r\n+        ...\r\n+    TypeError: sieve() missing 1 required positional argument: 'n'\r\n+\r\nComment: ```suggestion\r\n```\r\nI think it's fine to assume that the function will be called on the right number of inputs, so I don't think this test is necessary",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/segmented_sieve.py",
    "pr_number": 9945,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349307798,
    "comment_created_at": "2023-10-06T20:45:47Z"
  },
  {
    "code": "@@ -4,7 +4,17 @@\n class Onepad:\n     @staticmethod\n     def encrypt(text: str) -> tuple[list[int], list[int]]:\n-        \"\"\"Function to encrypt text using pseudo-random numbers\"\"\"\n+        \"\"\"\n+        Function to encrypt text using pseudo-random numbers\n+        >>> Onepad().encrypt(\"\")\n+        ([], [])\n+        >>> random.seed(1)\n+        >>> Onepad().encrypt(\" \")\n+        ([6969], [69])\n+        >>> random.seed(1)\n+        >>> Onepad().encrypt(\"Hello\")\n+        ([9729, 114756, 4653, 31309, 10492], [69, 292, 33, 131, 61])",
    "comment": "let's make it raise some exceptions:",
    "line_number": 16,
    "enriched": "File: ciphers/onepad_cipher.py\nCode: @@ -4,7 +4,17 @@\n class Onepad:\n     @staticmethod\n     def encrypt(text: str) -> tuple[list[int], list[int]]:\n-        \"\"\"Function to encrypt text using pseudo-random numbers\"\"\"\n+        \"\"\"\n+        Function to encrypt text using pseudo-random numbers\n+        >>> Onepad().encrypt(\"\")\n+        ([], [])\n+        >>> random.seed(1)\n+        >>> Onepad().encrypt(\" \")\n+        ([6969], [69])\n+        >>> random.seed(1)\n+        >>> Onepad().encrypt(\"Hello\")\n+        ([9729, 114756, 4653, 31309, 10492], [69, 292, 33, 131, 61])\nComment: Let's make it raise some Exceptions:\r\n```suggestion\r\n        ([9729, 114756, 4653, 31309, 10492], [69, 292, 33, 131, 61])\r\n        >>> Onepad().encrypt(1)\r\n        >>> Onepad().encrypt(1.1)\r\n        >>> Onepad().encrypt([])\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "ciphers/onepad_cipher.py",
    "pr_number": 10740,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367667224,
    "comment_created_at": "2023-10-21T05:28:57Z"
  },
  {
    "code": "@@ -21,6 +21,29 @@ def __init__(\n         self._keys: dict = {}\n \n     def keys(self):",
    "comment": "i don't love this name .keys() because {0: 5, 4: 4, 3: 3, 2: 2, 1: 1}.keys() returns dict_keys([0, 4, 3, 2, 1]) which is a list, not a dict.  but let's leave those considerations for a separate pull request.\r\n* [https://docs.python.org/3/library/stdtypes.html#object.__dict__](https://docs.python.org/3/library/stdtypes.html#object.__dict__)",
    "line_number": 23,
    "enriched": "File: data_structures/hashing/hash_table.py\nCode: @@ -21,6 +21,29 @@ def __init__(\n         self._keys: dict = {}\n \n     def keys(self):\nComment: I don't love this name `.keys()` because `{0: 5, 4: 4, 3: 3, 2: 2, 1: 1}.keys()` returns `dict_keys([0, 4, 3, 2, 1])` which is a `list`, not a `dict`.  But let's leave those considerations for a separate pull request.\r\n* [`https://docs.python.org/3/library/stdtypes.html#object.__dict__`](https://docs.python.org/3/library/stdtypes.html#object.__dict__)",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "data_structures/hashing/hash_table.py",
    "pr_number": 10984,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1373057344,
    "comment_created_at": "2023-10-26T12:12:12Z"
  },
  {
    "code": "@@ -1,134 +1,36 @@\n-\"\"\"\n-https://en.wikipedia.org/wiki/Image_texture\n-https://en.wikipedia.org/wiki/Co-occurrence_matrix#Application_to_image_analysis\n-\"\"\"\n-",
    "comment": "why remove the references?",
    "line_number": 5,
    "enriched": "File: computer_vision/haralick_descriptors.py\nCode: @@ -1,134 +1,36 @@\n-\"\"\"\n-https://en.wikipedia.org/wiki/Image_texture\n-https://en.wikipedia.org/wiki/Co-occurrence_matrix#Application_to_image_analysis\n-\"\"\"\n-\nComment: Why remove the references?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "computer_vision/haralick_descriptors.py",
    "pr_number": 11597,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1782115189,
    "comment_created_at": "2024-10-01T05:05:04Z"
  },
  {
    "code": "@@ -1,19 +1,22 @@\n-def reverse_letters(input_str: str) -> str:\n+def reverse_letters(sentence: str, length: int) -> str:",
    "comment": "let's make the length an optional parameter so that we can still preserve the same functionality as the original reverse_letters\r\n\r\nplease also add a test case for length = 0",
    "line_number": 1,
    "enriched": "File: strings/reverse_letters.py\nCode: @@ -1,19 +1,22 @@\n-def reverse_letters(input_str: str) -> str:\n+def reverse_letters(sentence: str, length: int) -> str:\nComment: ```suggestion\r\ndef reverse_letters(sentence: str, length: int = 0) -> str:\r\n```\r\nLet's make the length an optional parameter so that we can still preserve the same functionality as the original `reverse_letters`\r\n\r\nPlease also add a test case for `length = 0`",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "strings/reverse_letters.py",
    "pr_number": 10107,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1349754894,
    "comment_created_at": "2023-10-08T18:08:17Z"
  },
  {
    "code": "@@ -28,15 +33,35 @@ def explicit_euler(\n     >>> y = explicit_euler(f, y0, 0.0, 0.01, 5)\n     >>> y[-1]\n     144.77277243257308\n+    >>> # the exact solution is [math.sin(x),math.cos(x)\n+    >>> def f(x,y):\n+    ...     return np.array([y[1],-y[0]])\n+    >>> y0 = np.array([0,1])\n+    >>> y = explicit_euler(f, y0, 0.0, 0.01, 7)\n+    >>> y[0,-1]\n+    0.6802048957744236\n+    >>> y[1,-1]\n+    0.7809133930833114\n     \"\"\"\n     n = int(np.ceil((x_end - x0) / step_size))\n-    y = np.zeros((n + 1,))\n-    y[0] = y0\n-    x = x0\n \n-    for k in range(n):\n-        y[k + 1] = y[k] + step_size * ode_func(x, y[k])\n-        x += step_size\n+    if type(y0) == np.ndarray or type(y0) == list:",
    "comment": "could be written as if isinstance(y0, (np.ndarray, list)): or if isinstance(y0, np.ndarray | list): in newer version(i think 3.10)",
    "line_number": 48,
    "enriched": "File: maths/euler_method.py\nCode: @@ -28,15 +33,35 @@ def explicit_euler(\n     >>> y = explicit_euler(f, y0, 0.0, 0.01, 5)\n     >>> y[-1]\n     144.77277243257308\n+    >>> # the exact solution is [math.sin(x),math.cos(x)\n+    >>> def f(x,y):\n+    ...     return np.array([y[1],-y[0]])\n+    >>> y0 = np.array([0,1])\n+    >>> y = explicit_euler(f, y0, 0.0, 0.01, 7)\n+    >>> y[0,-1]\n+    0.6802048957744236\n+    >>> y[1,-1]\n+    0.7809133930833114\n     \"\"\"\n     n = int(np.ceil((x_end - x0) / step_size))\n-    y = np.zeros((n + 1,))\n-    y[0] = y0\n-    x = x0\n \n-    for k in range(n):\n-        y[k + 1] = y[k] + step_size * ode_func(x, y[k])\n-        x += step_size\n+    if type(y0) == np.ndarray or type(y0) == list:\nComment: Could be written as `if isinstance(y0, (np.ndarray, list)):` or `if isinstance(y0, np.ndarray | list):` in newer version(I think 3.10)",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "maths/euler_method.py",
    "pr_number": 8783,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1208653532,
    "comment_created_at": "2023-05-28T20:59:53Z"
  },
  {
    "code": "@@ -0,0 +1,18 @@\n+# Created by NiyazUrazaev on 03/05/23\n+\n+import requests\n+\n+\n+def send_message_to_telegram(token: str, chat_id: str, message_text: str):\n+    url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n+    data = {\"chat_id\": chat_id, \"text\": message_text}\n+    response = requests.post(url, data=data)\n+    if not response.ok:\n+        print(\"Failed to send a message in Telegram:\", response.text)",
    "comment": "please raise an exception instead.  see contributing.md because algorithmic functions should not print().",
    "line_number": 11,
    "enriched": "File: web_programming/telegram_message.py\nCode: @@ -0,0 +1,18 @@\n+# Created by NiyazUrazaev on 03/05/23\n+\n+import requests\n+\n+\n+def send_message_to_telegram(token: str, chat_id: str, message_text: str):\n+    url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n+    data = {\"chat_id\": chat_id, \"text\": message_text}\n+    response = requests.post(url, data=data)\n+    if not response.ok:\n+        print(\"Failed to send a message in Telegram:\", response.text)\nComment: Please raise an Exception instead.  See CONTRIBUTING.md because algorithmic functions should not print().",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "web_programming/telegram_message.py",
    "pr_number": 8708,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1184160492,
    "comment_created_at": "2023-05-03T19:25:52Z"
  },
  {
    "code": "@@ -27,6 +27,12 @@ def points_to_polynomial(coordinates: list[list[int]]) -> str:\n     f(x)=x^2*-1.0+x^1*-0.0+x^0*-2.0\n     >>> print(points_to_polynomial([[1, 5], [2, 2], [3, 9]]))\n     f(x)=x^2*5.0+x^1*-18.0+x^0*18.0\n+    >>> print(points_to_polynomial([[1, 1], [1, 2], [1, 3]]))\n+    x=1\n+    >>> print(points_to_polynomial([[1, 1], [2, 2], [2, 2]]))",
    "comment": "please try removing the print()...",
    "line_number": 32,
    "enriched": "File: linear_algebra/src/polynom_for_points.py\nCode: @@ -27,6 +27,12 @@ def points_to_polynomial(coordinates: list[list[int]]) -> str:\n     f(x)=x^2*-1.0+x^1*-0.0+x^0*-2.0\n     >>> print(points_to_polynomial([[1, 5], [2, 2], [3, 9]]))\n     f(x)=x^2*5.0+x^1*-18.0+x^0*18.0\n+    >>> print(points_to_polynomial([[1, 1], [1, 2], [1, 3]]))\n+    x=1\n+    >>> print(points_to_polynomial([[1, 1], [2, 2], [2, 2]]))\nComment: Please try removing the `print()`...\r\n```suggestion\r\n    >>> points_to_polynomial([[1, 1], [1, 2], [1, 3]])\r\n    x=1\r\n    >>> points_to_polynomial([[1, 1], [2, 2], [2, 2]])\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "linear_algebra/src/polynom_for_points.py",
    "pr_number": 11811,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1788937849,
    "comment_created_at": "2024-10-06T07:15:55Z"
  },
  {
    "code": "@@ -0,0 +1,50 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Installation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interv_start: The start of your internal scale.\n+            - interv_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Examples:\n+    interv_start:0\n+    interv_end:100\n+    number:50\n+    output: 50.0\n+    interv_start:6\n+    interv_end:12\n+    number:9\n+    output:50.0\n+\n+    Link: https://en.wikipedia.org/wiki/Conversion_of_units\n+\"\"\"\n+\n+\n+def absolute_conversion(interv_start: float, interv_end: float, number: float) -> str:\n+    \"\"\"\n+    >>> absolute_conversion(0, 10, 4)\n+    '40.0'\n+    >>> absolute_conversion(120, 140, 125)\n+    '25.0'\n+    >>> absolute_conversion(0,20,21)\n+    '105.0'\n+    \"\"\"\n+    if interv_start > interv_end:\n+        message = \"Invalid arguments, start is higher than the end.\"\n+        return message",
    "comment": "you should raise an error instead of returning a message",
    "line_number": 37,
    "enriched": "File: conversions/absolute_conversion.py\nCode: @@ -0,0 +1,50 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Installation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interv_start: The start of your internal scale.\n+            - interv_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Examples:\n+    interv_start:0\n+    interv_end:100\n+    number:50\n+    output: 50.0\n+    interv_start:6\n+    interv_end:12\n+    number:9\n+    output:50.0\n+\n+    Link: https://en.wikipedia.org/wiki/Conversion_of_units\n+\"\"\"\n+\n+\n+def absolute_conversion(interv_start: float, interv_end: float, number: float) -> str:\n+    \"\"\"\n+    >>> absolute_conversion(0, 10, 4)\n+    '40.0'\n+    >>> absolute_conversion(120, 140, 125)\n+    '25.0'\n+    >>> absolute_conversion(0,20,21)\n+    '105.0'\n+    \"\"\"\n+    if interv_start > interv_end:\n+        message = \"Invalid arguments, start is higher than the end.\"\n+        return message\nComment: You should raise an error instead of returning a message",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "conversions/absolute_conversion.py",
    "pr_number": 7266,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996365966,
    "comment_created_at": "2022-10-15T23:32:32Z"
  },
  {
    "code": "@@ -1,5 +1,4 @@\n-# Setup for pytest\n [pytest]\n markers =\n     mat_ops: mark a test as utilizing matrix operations.\n-addopts = --durations=10\n+addopts = --durations=10 --doctest-modules --doctest-continue-on-failure --showlocals",
    "comment": "let's leave out continue on failure.",
    "line_number": 4,
    "enriched": "File: pytest.ini\nCode: @@ -1,5 +1,4 @@\n-# Setup for pytest\n [pytest]\n markers =\n     mat_ops: mark a test as utilizing matrix operations.\n-addopts = --durations=10\n+addopts = --durations=10 --doctest-modules --doctest-continue-on-failure --showlocals\nComment: Let's leave out continue on failure.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "pytest.ini",
    "pr_number": 7840,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1008679651,
    "comment_created_at": "2022-10-29T10:49:10Z"
  },
  {
    "code": "@@ -0,0 +1,50 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Installation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interv_start: The start of your internal scale.\n+            - interv_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Examples:\n+    interv_start:0\n+    interv_end:100\n+    number:50\n+    output: 50.0\n+    interv_start:6\n+    interv_end:12\n+    number:9\n+    output:50.0\n+\n+    Link: https://en.wikipedia.org/wiki/Conversion_of_units\n+\"\"\"\n+\n+\n+def absolute_conversion(interv_start: float, interv_end: float, number: float) -> str:\n+    \"\"\"\n+    >>> absolute_conversion(0, 10, 4)\n+    40.0\n+    >>> absolute_conversion(120, 140, 125)\n+    25.0\n+    >>> absolute_conversion(0,20,21)\n+    105.0\n+    \"\"\"\n+    if interv_start > interv_end:\n+        message = \"Invalid arguments, start is higher than the end.\"\n+        return message",
    "comment": "instead of returning message, why not raise an error?",
    "line_number": 37,
    "enriched": "File: conversions/absolute_conversion.py\nCode: @@ -0,0 +1,50 @@\n+\"\"\"\n+    Convert any interval into a 0 to 100 scale and see a respective number in between\n+    percentage of the whole.\n+    Installation:\n+        - Import this file\n+        - Call the absolute_conversion function\n+        - Parameters:\n+            - interv_start: The start of your internal scale.\n+            - interv_end: The end of your internal scale.\n+            - number: The number you want to know the percentage that it\n+            represents of the scale.\n+    Examples:\n+    interv_start:0\n+    interv_end:100\n+    number:50\n+    output: 50.0\n+    interv_start:6\n+    interv_end:12\n+    number:9\n+    output:50.0\n+\n+    Link: https://en.wikipedia.org/wiki/Conversion_of_units\n+\"\"\"\n+\n+\n+def absolute_conversion(interv_start: float, interv_end: float, number: float) -> str:\n+    \"\"\"\n+    >>> absolute_conversion(0, 10, 4)\n+    40.0\n+    >>> absolute_conversion(120, 140, 125)\n+    25.0\n+    >>> absolute_conversion(0,20,21)\n+    105.0\n+    \"\"\"\n+    if interv_start > interv_end:\n+        message = \"Invalid arguments, start is higher than the end.\"\n+        return message\nComment: Instead of returning `message`, why not raise an error?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "conversions/absolute_conversion.py",
    "pr_number": 7263,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996349733,
    "comment_created_at": "2022-10-15T20:11:05Z"
  },
  {
    "code": "@@ -161,14 +174,10 @@ def zigzag(root: Node | None) -> Sequence[Node | None] | list[Any]:\n \r\n \r\n def main() -> None:  # Main function for testing.\r\n-    \"\"\"\r\n-    Create binary tree.\r\n-    \"\"\"\r\n+    # Create binary tree.\r\n     root = make_tree()\r\n-    \"\"\"\r\n-    All Traversals of the binary are as follows:\r\n-    \"\"\"\r\n \r\n+    # All Traversals of the binary are as follows:\r\n     print(f\"In-order Traversal: {inorder(root)}\")\r",
    "comment": "add a print statement for the new traversal",
    "line_number": 181,
    "enriched": "File: data_structures/binary_tree/binary_tree_traversals.py\nCode: @@ -161,14 +174,10 @@ def zigzag(root: Node | None) -> Sequence[Node | None] | list[Any]:\n \r\n \r\n def main() -> None:  # Main function for testing.\r\n-    \"\"\"\r\n-    Create binary tree.\r\n-    \"\"\"\r\n+    # Create binary tree.\r\n     root = make_tree()\r\n-    \"\"\"\r\n-    All Traversals of the binary are as follows:\r\n-    \"\"\"\r\n \r\n+    # All Traversals of the binary are as follows:\r\n     print(f\"In-order Traversal: {inorder(root)}\")\r\nComment: ```suggestion\r\n    print(f\"In-order Traversal: {inorder(root)}\")\r\n    print(f\"Reverse In-order Traversal: {reverse_inorder(root)}\")\r\n```\r\n\r\nAdd a print statement for the new traversal",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "data_structures/binary_tree/binary_tree_traversals.py",
    "pr_number": 8726,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1194769865,
    "comment_created_at": "2023-05-16T08:00:09Z"
  },
  {
    "code": "@@ -10,38 +10,24 @@\n python3 binary_search.py\n \"\"\"\n from __future__ import annotations\n+from typing import List",
    "comment": "this is not needed in modern python.",
    "line_number": 13,
    "enriched": "File: searches/binary_search.py\nCode: @@ -10,38 +10,24 @@\n python3 binary_search.py\n \"\"\"\n from __future__ import annotations\n+from typing import List\nComment: This is not needed in modern Python.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "searches/binary_search.py",
    "pr_number": 10870,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1369850334,
    "comment_created_at": "2023-10-24T09:09:11Z"
  },
  {
    "code": "@@ -28,7 +28,6 @@\n Reference:\r\n     https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/smo-book.pdf\r\n     https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf\r\n-    https://web.cs.iastate.edu/~honavar/smo-svm.pdf\r",
    "comment": "this link does not exist; hence i removed it.",
    "line_number": 31,
    "enriched": "File: machine_learning/sequential_minimum_optimization.py\nCode: @@ -28,7 +28,6 @@\n Reference:\r\n     https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/smo-book.pdf\r\n     https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf\r\n-    https://web.cs.iastate.edu/~honavar/smo-svm.pdf\r\nComment: This link does not exist; hence I removed it.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "machine_learning/sequential_minimum_optimization.py",
    "pr_number": 7319,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 996443832,
    "comment_created_at": "2022-10-16T13:38:33Z"
  },
  {
    "code": "@@ -230,6 +235,86 @@ def fib_binet(n: int) -> list[int]:\n     return [round(phi**i / sqrt_5) for i in range(n + 1)]\n \n \n+def matrix_pow_np(m: ndarray, power: int) -> ndarray:\n+    \"\"\"\n+    Raises a matrix to the power of 'power' using binary exponentiation.\n+\n+    Args:\n+        m: Matrix as a numpy array.\n+        power: The power to which the matrix is to be raised.\n+\n+    Returns:\n+        The matrix raised to the power.\n+\n+    Raises:\n+        ValueError: If power is negative.\n+\n+    >>> m = np.array([[1, 1], [1, 0]], dtype=int)\n+    >>> matrix_pow_np(m, 0)  # Identity matrix when raised to the power of 0\n+    array([[1, 0],\n+           [0, 1]])\n+\n+    >>> matrix_pow_np(m, 1)  # Same matrix when raised to the power of 1\n+    array([[1, 1],\n+           [1, 0]])\n+\n+    >>> matrix_pow_np(m, 5)\n+    array([[8, 5],\n+           [5, 3]])\n+\n+    >>> matrix_pow_np(m, -1)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: power is negative",
    "comment": "your code never actually checks whether the power is negative. i think this is why your tests are never finishing: this test is just running in an infinite loop with power = -1, -2, -3, ...",
    "line_number": 268,
    "enriched": "File: maths/fibonacci.py\nCode: @@ -230,6 +235,86 @@ def fib_binet(n: int) -> list[int]:\n     return [round(phi**i / sqrt_5) for i in range(n + 1)]\n \n \n+def matrix_pow_np(m: ndarray, power: int) -> ndarray:\n+    \"\"\"\n+    Raises a matrix to the power of 'power' using binary exponentiation.\n+\n+    Args:\n+        m: Matrix as a numpy array.\n+        power: The power to which the matrix is to be raised.\n+\n+    Returns:\n+        The matrix raised to the power.\n+\n+    Raises:\n+        ValueError: If power is negative.\n+\n+    >>> m = np.array([[1, 1], [1, 0]], dtype=int)\n+    >>> matrix_pow_np(m, 0)  # Identity matrix when raised to the power of 0\n+    array([[1, 0],\n+           [0, 1]])\n+\n+    >>> matrix_pow_np(m, 1)  # Same matrix when raised to the power of 1\n+    array([[1, 1],\n+           [1, 0]])\n+\n+    >>> matrix_pow_np(m, 5)\n+    array([[8, 5],\n+           [5, 3]])\n+\n+    >>> matrix_pow_np(m, -1)\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: power is negative\nComment: Your code never actually checks whether the power is negative. I think this is why your tests are never finishing: this test is just running in an infinite loop with power = -1, -2, -3, ...",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/fibonacci.py",
    "pr_number": 11747,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1787909056,
    "comment_created_at": "2024-10-04T15:45:55Z"
  },
  {
    "code": "@@ -0,0 +1,90 @@\n+\"\"\"\n+Find the minimum number of multiplications needed to multiply chain of matrices.\n+Reference: https://www.geeksforgeeks.org/matrix-chain-multiplication-dp-8/\n+\n+The algorithm has interesting real-world applications. Example:\n+1. Image transformations in Computer Graphics as images are composed of matrix.\n+2. Solve complex polynomial equations in the field of algebra using least\n+processing power.\n+3. Calculate overall impact of macroeconomic decisions as economic\n+equations involve number of variables.\n+4. Self-driving car navigation can be made more accurate as matrix multiplication\n+can accurately determine position and orientation of obstacles in short time.\n+\n+Python doctests can be run with the following command:\n+python -m doctest -v matrix_chain_multiply.py\n+\n+Given a sequence arr[] that represents chain of 2D matrices such that\n+the dimension of ith matrix is arr[i-1]*arr[i].\n+So suppose arr = [40, 20, 30, 10, 30] means we have 4 matrices of\n+dimensions 40*20, 20*30, 30*10 and 10*30.\n+\n+matrix_chain_multiply() returns an integer denoting\n+minimum number of multiplications to multiply the chain.\n+\n+We do not need to perform actual multiplication here.\n+We only need to decide the order in which to perform the multiplication.\n+\n+Hints:\n+1. Number of multiplications (ie cost) to multiply 2 matrices\n+of size m*p and p*n is m*p*n.\n+2. Cost of matrix multiplication is neither associative ie (M1*M2)*M3 != M1*(M2*M3)",
    "comment": "this is just incorrect. if the individual multiplications are well-defined, then matrix multiplication _is_ associative.",
    "line_number": 31,
    "enriched": "File: dynamic_programming/matrix_chain_multiplication.py\nCode: @@ -0,0 +1,90 @@\n+\"\"\"\n+Find the minimum number of multiplications needed to multiply chain of matrices.\n+Reference: https://www.geeksforgeeks.org/matrix-chain-multiplication-dp-8/\n+\n+The algorithm has interesting real-world applications. Example:\n+1. Image transformations in Computer Graphics as images are composed of matrix.\n+2. Solve complex polynomial equations in the field of algebra using least\n+processing power.\n+3. Calculate overall impact of macroeconomic decisions as economic\n+equations involve number of variables.\n+4. Self-driving car navigation can be made more accurate as matrix multiplication\n+can accurately determine position and orientation of obstacles in short time.\n+\n+Python doctests can be run with the following command:\n+python -m doctest -v matrix_chain_multiply.py\n+\n+Given a sequence arr[] that represents chain of 2D matrices such that\n+the dimension of ith matrix is arr[i-1]*arr[i].\n+So suppose arr = [40, 20, 30, 10, 30] means we have 4 matrices of\n+dimensions 40*20, 20*30, 30*10 and 10*30.\n+\n+matrix_chain_multiply() returns an integer denoting\n+minimum number of multiplications to multiply the chain.\n+\n+We do not need to perform actual multiplication here.\n+We only need to decide the order in which to perform the multiplication.\n+\n+Hints:\n+1. Number of multiplications (ie cost) to multiply 2 matrices\n+of size m*p and p*n is m*p*n.\n+2. Cost of matrix multiplication is neither associative ie (M1*M2)*M3 != M1*(M2*M3)\nComment: This is just incorrect. If the individual multiplications are well-defined, then matrix multiplication _is_ associative.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "dynamic_programming/matrix_chain_multiplication.py",
    "pr_number": 10562,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1359965102,
    "comment_created_at": "2023-10-15T22:48:59Z"
  },
  {
    "code": "@@ -35,6 +35,33 @@ def __hash_double_function(self, key, data, increment):\n         return (increment * self.__hash_function_2(key, data)) % self.size_table\n \n     def _collision_resolution(self, key, data=None):\n+        \"\"\"\n+        Examples:",
    "comment": "cool",
    "line_number": 39,
    "enriched": "File: data_structures/hashing/double_hash.py\nCode: @@ -35,6 +35,33 @@ def __hash_double_function(self, key, data, increment):\n         return (increment * self.__hash_function_2(key, data)) % self.size_table\n \n     def _collision_resolution(self, key, data=None):\n+        \"\"\"\n+        Examples:\nComment: Cool",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "data_structures/hashing/double_hash.py",
    "pr_number": 11020,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1374281770,
    "comment_created_at": "2023-10-27T09:00:46Z"
  },
  {
    "code": "@@ -25,6 +25,18 @@ def get_english_count(message: str) -> float:\n \n \n def remove_non_letters(message: str) -> str:",
    "comment": "nice\u2705",
    "line_number": 27,
    "enriched": "File: strings/detecting_english_programmatically.py\nCode: @@ -25,6 +25,18 @@ def get_english_count(message: str) -> float:\n \n \n def remove_non_letters(message: str) -> str:\nComment: Nice\u2705",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "strings/detecting_english_programmatically.py",
    "pr_number": 11135,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1380414335,
    "comment_created_at": "2023-11-02T16:21:23Z"
  },
  {
    "code": "@@ -28,6 +28,12 @@ def to_simple_case(str_: str) -> str:\n     \"\"\"\n     >>> to_simple_case(\"one two 31235three4four\")\n     'OneTwo31235three4four'\n+    >>> to_simple_case(\"This should be combined\")\n+    'ThisShouldBeCombined'\n+    >>> to_simple_case(\"The first letters are capitalized, then string is merged\")\n+    'TheFirstLettersAreCapitalizedThenStringIsMerged'\n+    >>> to_simple_case(\"special characters :, ', %, ^, $, are ignored\")",
    "comment": "nice",
    "line_number": 35,
    "enriched": "File: strings/string_switch_case.py\nCode: @@ -28,6 +28,12 @@ def to_simple_case(str_: str) -> str:\n     \"\"\"\n     >>> to_simple_case(\"one two 31235three4four\")\n     'OneTwo31235three4four'\n+    >>> to_simple_case(\"This should be combined\")\n+    'ThisShouldBeCombined'\n+    >>> to_simple_case(\"The first letters are capitalized, then string is merged\")\n+    'TheFirstLettersAreCapitalizedThenStringIsMerged'\n+    >>> to_simple_case(\"special characters :, ', %, ^, $, are ignored\")\nComment:  Nice ",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "strings/string_switch_case.py",
    "pr_number": 11136,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1381581590,
    "comment_created_at": "2023-11-03T12:09:05Z"
  },
  {
    "code": "@@ -17,7 +17,7 @@ def diophantine(a: int, b: int, c: int) -> tuple[float, float]:\n     >>> diophantine(391,299,-69)\n     (9.0, -12.0)\n \n-    But above equation has one more solution i.e., x = -4, y = 5.\n+    But above equation has one more solution i.e. x = -4, y = 5.",
    "comment": "the comma is not strictly incorrect. \"i.e.\" is usually followed by a comma in american usage:\r\n\r\n[merriam-webster dictionary](https://www.merriam-webster.com/grammar/ie-vs-eg-abbreviation-meaning-usage-difference):\r\n> while i.e. is often set off by brackets or parentheses, it can also sometimes follow a comma or em dash. it is usually followed by a comma.\r\n\r\n[wiktionary](https://en.wiktionary.org/wiki/i.e.)\r\n> american english prefers a comma after i.e., but british english usually does not use a comma there and often does not use dots either.",
    "line_number": 20,
    "enriched": "File: blockchain/diophantine_equation.py\nCode: @@ -17,7 +17,7 @@ def diophantine(a: int, b: int, c: int) -> tuple[float, float]:\n     >>> diophantine(391,299,-69)\n     (9.0, -12.0)\n \n-    But above equation has one more solution i.e., x = -4, y = 5.\n+    But above equation has one more solution i.e. x = -4, y = 5.\nComment: The comma is not strictly incorrect. \"i.e.\" is usually followed by a comma in American usage:\r\n\r\n[Merriam-Webster Dictionary](https://www.merriam-webster.com/grammar/ie-vs-eg-abbreviation-meaning-usage-difference):\r\n> While i.e. is often set off by brackets or parentheses, it can also sometimes follow a comma or em dash. It is usually followed by a comma.\r\n\r\n[Wiktionary](https://en.wiktionary.org/wiki/i.e.)\r\n> American English prefers a comma after i.e., but British English usually does not use a comma there and often does not use dots either.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "blockchain/diophantine_equation.py",
    "pr_number": 10814,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1367939290,
    "comment_created_at": "2023-10-22T17:10:25Z"
  },
  {
    "code": "@@ -1,76 +1,85 @@\n-#!/usr/bin/python\n-\n-\"\"\"Author Anurag Kumar | anuragkumarak95@gmail.com | git/anuragkumarak95\n-\n-Simple example of Fractal generation using recursive function.\n-\n-What is Sierpinski Triangle?\n->>The Sierpinski triangle (also with the original orthography Sierpinski), also called\n-the Sierpinski gasket or the Sierpinski Sieve, is a fractal and attractive fixed set\n-with the overall shape of an equilateral triangle, subdivided recursively into smaller\n-equilateral triangles. Originally constructed as a curve, this is one of the basic\n-examples of self-similar sets, i.e., it is a mathematically generated pattern that can\n-be reproducible at any magnification or reduction. It is named after the Polish\n-mathematician Wac\u0142aw Sierpinski, but appeared as a decorative pattern many centuries\n-prior to the work of Sierpinski.\n+\"\"\"\n+Author Anurag Kumar | anuragkumarak95@gmail.com | git/anuragkumarak95\n \n-Requirements(pip):\n-  - turtle\n+Simple example of fractal generation using recursion.\n \n-Python:\n-  - 2.6\n+What is the Sierpi\u0144ski Triangle?\n+    The Sierpi\u0144ski triangle (sometimes spelled Sierpinski), also called the\n+Sierpi\u0144ski gasket or Sierpi\u0144ski sieve, is a fractal attractive fixed set with\n+the overall shape of an equilateral triangle, subdivided recursively into\n+smaller equilateral triangles. Originally constructed as a curve, this is one of\n+the basic examples of self-similar sets\u2014that is, it is a mathematically\n+generated pattern that is reproducible at any magnification or reduction. It is\n+named after the Polish mathematician Wac\u0142aw Sierpi\u0144ski, but appeared as a\n+decorative pattern many centuries before the work of Sierpi\u0144ski.\n \n-Usage:\n-  - $python sierpinski_triangle.py <int:depth_for_fractal>\n+Requirements (pip): turtle",
    "comment": "turtle builtin, not installed via pip... https://docs.python.org/3/library/turtle.html",
    "line_number": 16,
    "enriched": "File: fractals/sierpinski_triangle.py\nCode: @@ -1,76 +1,85 @@\n-#!/usr/bin/python\n-\n-\"\"\"Author Anurag Kumar | anuragkumarak95@gmail.com | git/anuragkumarak95\n-\n-Simple example of Fractal generation using recursive function.\n-\n-What is Sierpinski Triangle?\n->>The Sierpinski triangle (also with the original orthography Sierpinski), also called\n-the Sierpinski gasket or the Sierpinski Sieve, is a fractal and attractive fixed set\n-with the overall shape of an equilateral triangle, subdivided recursively into smaller\n-equilateral triangles. Originally constructed as a curve, this is one of the basic\n-examples of self-similar sets, i.e., it is a mathematically generated pattern that can\n-be reproducible at any magnification or reduction. It is named after the Polish\n-mathematician Wac\u0142aw Sierpinski, but appeared as a decorative pattern many centuries\n-prior to the work of Sierpinski.\n+\"\"\"\n+Author Anurag Kumar | anuragkumarak95@gmail.com | git/anuragkumarak95\n \n-Requirements(pip):\n-  - turtle\n+Simple example of fractal generation using recursion.\n \n-Python:\n-  - 2.6\n+What is the Sierpi\u0144ski Triangle?\n+    The Sierpi\u0144ski triangle (sometimes spelled Sierpinski), also called the\n+Sierpi\u0144ski gasket or Sierpi\u0144ski sieve, is a fractal attractive fixed set with\n+the overall shape of an equilateral triangle, subdivided recursively into\n+smaller equilateral triangles. Originally constructed as a curve, this is one of\n+the basic examples of self-similar sets\u2014that is, it is a mathematically\n+generated pattern that is reproducible at any magnification or reduction. It is\n+named after the Polish mathematician Wac\u0142aw Sierpi\u0144ski, but appeared as a\n+decorative pattern many centuries before the work of Sierpi\u0144ski.\n \n-Usage:\n-  - $python sierpinski_triangle.py <int:depth_for_fractal>\n+Requirements (pip): turtle\nComment: `turtle` builtin, not installed via pip... https://docs.python.org/3/library/turtle.html\r\n```suggestion\r\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "fractals/sierpinski_triangle.py",
    "pr_number": 8068,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1060023639,
    "comment_created_at": "2023-01-02T13:09:46Z"
  },
  {
    "code": "@@ -0,0 +1,134 @@\n+\"\"\"\n+VGG16 Model Implementation\n+\n+Paper: https://arxiv.org/abs/1409.1556\n+\"\"\"\n+\n+import torch\n+from torch import nn\n+\n+\n+class VGG16(nn.Module):\n+    def __init__(self, num_classes: int = 10) -> None:\n+        super().__init__()\n+\n+        # -------------------------- Feature Extraction Layers --------------------- #\n+        # The feature extraction layers consist of a series of convolutional and\n+        # max-pooling layers that learn hierarchical representations of input images.\n+        # The architecture follows a pattern of multiple convolutional layers with\n+        # ReLU activations, followed by max-pooling layers.\n+        # The number of convolutional layers per block increases as the network\n+        # goes deeper, and the number of output channels doubles after each\n+        # max-pooling layer.\n+        # -------------------------------------------------------------------------- #\n+\n+        self.features = nn.Sequential(\n+            # Layer 1\n+            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 2\n+            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 3\n+            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 4\n+            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 5\n+            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 6\n+            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 7\n+            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 8\n+            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 9\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 10\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 11\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 12\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 13\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+        )\n+\n+        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n+\n+        # -------------------------- Fully Connected Layers -------------------------- #\n+        # These layers receive the high-level features learned by the feature extraction\n+        # layers and use them to perform the final classification task.\n+        # The fully connected layers consist of three linear layers with ReLU\n+        # activations and dropout for regularization. The final layer has a\n+        # number of output units equal to the number of classes in the\n+        # classification problem.\n+        # ---------------------------------------------------------------------------- #\n+\n+        self.classifier = nn.Sequential(\n+            # Layer 14\n+            nn.Linear(512 * 7 * 7, 4096),\n+            nn.ReLU(inplace=True),\n+            nn.Dropout(),\n+            # Layer 15\n+            nn.Linear(4096, 4096),\n+            nn.ReLU(inplace=True),\n+            nn.Dropout(),\n+            # Layer 16\n+            nn.Linear(4096, num_classes),\n+        )\n+\n+    def forward(self, image: torch.tensor) -> torch.tensor:\n+        image = self.features(image)\n+        image = self.avgpool(image)\n+        image = torch.flatten(image, 1)\n+        image = self.classifier(image)\n+        return image\n+\n+\n+def test_model(image_tensor: torch.tensor) -> bool:\n+    \"\"\"\n+    Test the model for a batch of 64 images\n+\n+    Args:\n+        image_tensor (torch.tensor): Batch of 64 Images for the model.\n+\n+    Returns:\n+        bool: True if the model works, False otherwise\n+\n+    >>> test_model(torch.rand(64, 3, 224, 224))\n+    True\n+    \"\"\"\n+\n+    try:\n+        model = VGG16()\n+        output = model(image_tensor)\n+\n+    except Exception as e:",
    "comment": "https://realpython.com/the-most-diabolical-python-antipattern/",
    "line_number": 122,
    "enriched": "File: computer_vision/vgg_pytorch.py\nCode: @@ -0,0 +1,134 @@\n+\"\"\"\n+VGG16 Model Implementation\n+\n+Paper: https://arxiv.org/abs/1409.1556\n+\"\"\"\n+\n+import torch\n+from torch import nn\n+\n+\n+class VGG16(nn.Module):\n+    def __init__(self, num_classes: int = 10) -> None:\n+        super().__init__()\n+\n+        # -------------------------- Feature Extraction Layers --------------------- #\n+        # The feature extraction layers consist of a series of convolutional and\n+        # max-pooling layers that learn hierarchical representations of input images.\n+        # The architecture follows a pattern of multiple convolutional layers with\n+        # ReLU activations, followed by max-pooling layers.\n+        # The number of convolutional layers per block increases as the network\n+        # goes deeper, and the number of output channels doubles after each\n+        # max-pooling layer.\n+        # -------------------------------------------------------------------------- #\n+\n+        self.features = nn.Sequential(\n+            # Layer 1\n+            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 2\n+            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 3\n+            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 4\n+            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 5\n+            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 6\n+            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 7\n+            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 8\n+            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 9\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 10\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+            # Layer 11\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 12\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            # Layer 13\n+            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.MaxPool2d(kernel_size=2, stride=2),\n+        )\n+\n+        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n+\n+        # -------------------------- Fully Connected Layers -------------------------- #\n+        # These layers receive the high-level features learned by the feature extraction\n+        # layers and use them to perform the final classification task.\n+        # The fully connected layers consist of three linear layers with ReLU\n+        # activations and dropout for regularization. The final layer has a\n+        # number of output units equal to the number of classes in the\n+        # classification problem.\n+        # ---------------------------------------------------------------------------- #\n+\n+        self.classifier = nn.Sequential(\n+            # Layer 14\n+            nn.Linear(512 * 7 * 7, 4096),\n+            nn.ReLU(inplace=True),\n+            nn.Dropout(),\n+            # Layer 15\n+            nn.Linear(4096, 4096),\n+            nn.ReLU(inplace=True),\n+            nn.Dropout(),\n+            # Layer 16\n+            nn.Linear(4096, num_classes),\n+        )\n+\n+    def forward(self, image: torch.tensor) -> torch.tensor:\n+        image = self.features(image)\n+        image = self.avgpool(image)\n+        image = torch.flatten(image, 1)\n+        image = self.classifier(image)\n+        return image\n+\n+\n+def test_model(image_tensor: torch.tensor) -> bool:\n+    \"\"\"\n+    Test the model for a batch of 64 images\n+\n+    Args:\n+        image_tensor (torch.tensor): Batch of 64 Images for the model.\n+\n+    Returns:\n+        bool: True if the model works, False otherwise\n+\n+    >>> test_model(torch.rand(64, 3, 224, 224))\n+    True\n+    \"\"\"\n+\n+    try:\n+        model = VGG16()\n+        output = model(image_tensor)\n+\n+    except Exception as e:\nComment: https://realpython.com/the-most-diabolical-python-antipattern/",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "computer_vision/vgg_pytorch.py",
    "pr_number": 8620,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1160931758,
    "comment_created_at": "2023-04-07T19:58:35Z"
  },
  {
    "code": "@@ -0,0 +1,22 @@\n+# A Dynamic Programming based Python Program for 0-1 Knapsack problem",
    "comment": "add a link please",
    "line_number": 1,
    "enriched": "File: knapsack/assignment_problem.py\nCode: @@ -0,0 +1,22 @@\n+# A Dynamic Programming based Python Program for 0-1 Knapsack problem\nComment: Add a link please",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "knapsack/assignment_problem.py",
    "pr_number": 7414,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 998842080,
    "comment_created_at": "2022-10-19T00:58:24Z"
  },
  {
    "code": "@@ -1,7 +1,7 @@\n \"\"\"\r\n Find the area of various geometric shapes\r\n \"\"\"\r",
    "comment": "add a credible source as a reference.",
    "line_number": 3,
    "enriched": "File: maths/area.py\nCode: @@ -1,7 +1,7 @@\n \"\"\"\r\n Find the area of various geometric shapes\r\n \"\"\"\r\nComment: Add a credible source as a reference.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/area.py",
    "pr_number": 7438,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1000115134,
    "comment_created_at": "2022-10-20T03:47:19Z"
  },
  {
    "code": "@@ -0,0 +1,86 @@\n+import bs4\n+import requests\n+from fake_useragent import UserAgent\n+\n+MOVIES_URL = \"https://www.coolgenerator.com/random-movie-generator\"\n+\n+\n+def get_imdb_link(movie_title: str) -> str:\n+    \"\"\"\n+        Return the IMDB url of the movie title\n+    Args:\n+        movie_title (str): Movie title\n+\n+    Returns:\n+        str: Imdb Url of the movie\n+    \"\"\"\n+    url = f\"https://www.imdb.com/find?q={movie_title.replace(' ', '+')}\"\n+    try:\n+        for _ in range(2):\n+            response = requests.get(url, headers={\"User-Agent\": UserAgent().firefox})\n+            if response.ok:\n+                break\n+        soup = bs4.BeautifulSoup(response.content, \"html.parser\")\n+        imdb_link = soup.find(\"a\", {\"class\": \"ipc-metadata-list-summary-item__t\"})\n+        url_imdb = \"https://www.imdb.com\" + imdb_link.attrs[\"href\"]\n+\n+    except Exception:",
    "comment": "https://realpython.com/the-most-diabolical-python-antipattern",
    "line_number": 27,
    "enriched": "File: web_programming/random_movie.py\nCode: @@ -0,0 +1,86 @@\n+import bs4\n+import requests\n+from fake_useragent import UserAgent\n+\n+MOVIES_URL = \"https://www.coolgenerator.com/random-movie-generator\"\n+\n+\n+def get_imdb_link(movie_title: str) -> str:\n+    \"\"\"\n+        Return the IMDB url of the movie title\n+    Args:\n+        movie_title (str): Movie title\n+\n+    Returns:\n+        str: Imdb Url of the movie\n+    \"\"\"\n+    url = f\"https://www.imdb.com/find?q={movie_title.replace(' ', '+')}\"\n+    try:\n+        for _ in range(2):\n+            response = requests.get(url, headers={\"User-Agent\": UserAgent().firefox})\n+            if response.ok:\n+                break\n+        soup = bs4.BeautifulSoup(response.content, \"html.parser\")\n+        imdb_link = soup.find(\"a\", {\"class\": \"ipc-metadata-list-summary-item__t\"})\n+        url_imdb = \"https://www.imdb.com\" + imdb_link.attrs[\"href\"]\n+\n+    except Exception:\nComment: https://realpython.com/the-most-diabolical-python-antipattern\r\n",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "web_programming/random_movie.py",
    "pr_number": 8145,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1111120834,
    "comment_created_at": "2023-02-18T22:43:25Z"
  },
  {
    "code": "@@ -24,6 +50,35 @@ def b_expo(a: int, b: int) -> int:\n \n \n def b_expo_mod(a: int, b: int, c: int) -> int:\n+    \"\"\"\n+    Calculate the result of (a * b) % c using binary exponentiation and modular arithmetic.",
    "comment": "please fix the ruff error on this line. the line is too long (88 chars max)",
    "line_number": 54,
    "enriched": "File: maths/binary_multiplication.py\nCode: @@ -24,6 +50,35 @@ def b_expo(a: int, b: int) -> int:\n \n \n def b_expo_mod(a: int, b: int, c: int) -> int:\n+    \"\"\"\n+    Calculate the result of (a * b) % c using binary exponentiation and modular arithmetic.\nComment: Please fix the ruff error on this line. The line is too long (88 chars max)",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "maths/binary_multiplication.py",
    "pr_number": 9513,
    "repo": "Python",
    "owner": "TheAlgorithms",
    "comment_id": 1343148374,
    "comment_created_at": "2023-10-02T20:46:38Z"
  },
  {
    "code": "@@ -1332,7 +1332,10 @@ def decorator(f):\n \n     @staticmethod\n     def _get_exc_class_and_code(exc_class_or_code):\n-        \"\"\"Ensure that we register only exceptions as handler keys\"\"\"\n+        \"\"\"Ensure that we register only exceptions as handler keys\n+        \n+        :param exc_class_or_code: the class for the or exception code as integer",
    "comment": "Typo?",
    "line_number": 1337,
    "enriched": "File: src/flask/app.py\nCode: @@ -1332,7 +1332,10 @@ def decorator(f):\n \n     @staticmethod\n     def _get_exc_class_and_code(exc_class_or_code):\n-        \"\"\"Ensure that we register only exceptions as handler keys\"\"\"\n+        \"\"\"Ensure that we register only exceptions as handler keys\n+        \n+        :param exc_class_or_code: the class for the or exception code as integer\nComment: Typo?",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "src/flask/app.py",
    "pr_number": 3253,
    "repo": "flask",
    "owner": "pallets",
    "comment_id": 291809634,
    "comment_created_at": "2019-06-08T15:10:29Z"
  },
  {
    "code": "@@ -170,10 +170,12 @@ def _get_clickable(clickdata, form):\n     \"\"\"\n     clickables = [\n         el for el in form.xpath(\n-            'descendant::*[(self::input or self::button)'\n-            ' and re:test(@type, \"^submit$\", \"i\")]'\n-            '|descendant::button[not(@type)]',\n-            namespaces={\"re\": \"http://exslt.org/regular-expressions\"})",
    "comment": "is it me or these lines are changed from 4 spaces indentation to 5 spaces?",
    "line_number": 176,
    "enriched": "File: scrapy/http/request/form.py\nCode: @@ -170,10 +170,12 @@ def _get_clickable(clickdata, form):\n     \"\"\"\n     clickables = [\n         el for el in form.xpath(\n-            'descendant::*[(self::input or self::button)'\n-            ' and re:test(@type, \"^submit$\", \"i\")]'\n-            '|descendant::button[not(@type)]',\n-            namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\nComment: is it me or these lines are changed from 4 spaces indentation to 5 spaces?",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "scrapy/http/request/form.py",
    "pr_number": 3153,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 174508190,
    "comment_created_at": "2018-03-14T15:47:28Z"
  },
  {
    "code": "@@ -159,6 +159,24 @@ following methods:\n         :param spider: the spider to whom the start requests belong\n         :type spider: :class:`~scrapy.spiders.Spider` object\n \n+Accessing settings and other crawler attributes\n+-----------------------------------------------\n+\n+In order to access settings and other crawler attributes you have to use \n+``from_crawler`` factory method::\n+\n+  class CustomMiddleware(object):\n+\n+  def __init__(self, middleware_arg):\n+      self.middleware_arg = middleware_arg\n+      self.log('Middleware parameter: {}'.format(middleware_arg))\n+\n+  @classmethod\n+  def from_crawler(cls, crawler):\n+      settings = crawler.settings\n+      middleware_arg = settings.get('MIDDLEWARE_ARG')\n+    return cls(middleware_arg)\n+",
    "comment": "indentation does not look correct\n",
    "line_number": 179,
    "enriched": "File: docs/topics/spider-middleware.rst\nCode: @@ -159,6 +159,24 @@ following methods:\n         :param spider: the spider to whom the start requests belong\n         :type spider: :class:`~scrapy.spiders.Spider` object\n \n+Accessing settings and other crawler attributes\n+-----------------------------------------------\n+\n+In order to access settings and other crawler attributes you have to use \n+``from_crawler`` factory method::\n+\n+  class CustomMiddleware(object):\n+\n+  def __init__(self, middleware_arg):\n+      self.middleware_arg = middleware_arg\n+      self.log('Middleware parameter: {}'.format(middleware_arg))\n+\n+  @classmethod\n+  def from_crawler(cls, crawler):\n+      settings = crawler.settings\n+      middleware_arg = settings.get('MIDDLEWARE_ARG')\n+    return cls(middleware_arg)\n+\nComment: indentation does not look correct\n",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/topics/spider-middleware.rst",
    "pr_number": 2336,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 84040905,
    "comment_created_at": "2016-10-19T10:32:04Z"
  },
  {
    "code": "@@ -49,8 +49,14 @@ def robot_parser(self, request, spider):\n             )\n             dfd = self.crawler.engine.download(robotsreq, spider)\n             dfd.addCallback(self._parse_robots)\n+            dfd.addErrback(self._logerror, robotsreq, spider)\n         return self._parsers[netloc]\n \n+    def _logerror(self, failure, request, spider):\n+        if failure.type is not IgnoreRequest:\n+            log.msg(format=\"Error downloading %%(request)s: %s\" % failure.value,",
    "comment": "double percentage symbol is a typo @torymur ?\n",
    "line_number": 57,
    "enriched": "File: scrapy/contrib/downloadermiddleware/robotstxt.py\nCode: @@ -49,8 +49,14 @@ def robot_parser(self, request, spider):\n             )\n             dfd = self.crawler.engine.download(robotsreq, spider)\n             dfd.addCallback(self._parse_robots)\n+            dfd.addErrback(self._logerror, robotsreq, spider)\n         return self._parsers[netloc]\n \n+    def _logerror(self, failure, request, spider):\n+        if failure.type is not IgnoreRequest:\n+            log.msg(format=\"Error downloading %%(request)s: %s\" % failure.value,\nComment: double percentage symbol is a typo @torymur ?\n",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "scrapy/contrib/downloadermiddleware/robotstxt.py",
    "pr_number": 1131,
    "repo": "scrapy",
    "owner": "scrapy",
    "comment_id": 28391950,
    "comment_created_at": "2015-04-15T03:59:05Z"
  },
  {
    "code": "@@ -875,11 +875,12 @@ def test_form_encoded_post_query_multivalued_element(self, httpbin):\n         assert prep.body == 'test=foo&test=baz'\n \n     def test_different_encodings_dont_break_post(self, httpbin):\n-        r = requests.post(httpbin('post'),\n-            data={'stuff': json.dumps({'a': 123})},\n-            params={'blah': 'asdf1234'},\n-            files={'file': ('test_requests.py', open(__file__, 'rb'))})\n-        assert r.status_code == 200\n+        with open(__file__, 'rb') as f:\n+            r = requests.post(httpbin('post'),\n+                data={'stuff': json.dumps({'a': 123})},\n+                params={'blah': 'asdf1234'},\n+                files={'file': ('test_requests.py', f)})\n+            assert r.status_code == 200",
    "comment": "Let\u2019s pull the `assert` out to the same indentation level as `with`.",
    "line_number": 883,
    "enriched": "File: tests/test_requests.py\nCode: @@ -875,11 +875,12 @@ def test_form_encoded_post_query_multivalued_element(self, httpbin):\n         assert prep.body == 'test=foo&test=baz'\n \n     def test_different_encodings_dont_break_post(self, httpbin):\n-        r = requests.post(httpbin('post'),\n-            data={'stuff': json.dumps({'a': 123})},\n-            params={'blah': 'asdf1234'},\n-            files={'file': ('test_requests.py', open(__file__, 'rb'))})\n-        assert r.status_code == 200\n+        with open(__file__, 'rb') as f:\n+            r = requests.post(httpbin('post'),\n+                data={'stuff': json.dumps({'a': 123})},\n+                params={'blah': 'asdf1234'},\n+                files={'file': ('test_requests.py', f)})\n+            assert r.status_code == 200\nComment: Let\u2019s pull the `assert` out to the same indentation level as `with`.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "tests/test_requests.py",
    "pr_number": 4766,
    "repo": "requests",
    "owner": "psf",
    "comment_id": 209787773,
    "comment_created_at": "2018-08-13T23:14:55Z"
  },
  {
    "code": "@@ -1330,11 +1332,29 @@ string_replace_strided_loop(\n             goto fail;\n         }\n         else if (i1_isnull || i2_isnull || i3_isnull) {\n-            if (!has_string_na) {\n-                npy_gil_error(PyExc_ValueError,\n-                              \"Null values are not supported as replacement arguments \"\n-                              \"for replace\");\n-                goto fail;\n+            if (has_null && !has_string_na) {\n+                if (i2_isnull || i3_isnull) {\n+                    npy_gil_error(PyExc_ValueError,\n+                                  \"Null values are not supported as search \"\n+                                  \"patterns or replacement strings for \"\n+                                  \"replace\");\n+                    goto fail;\n+                }\n+                else if (i1_isnull) {\n+                    if (has_nan_na) {\n+                        if (NpyString_pack_null(oallocator, ops) < 0) {\n+                            npy_gil_error(PyExc_MemoryError,\n+                                          \"Failed to deallocate string in replace\");\n+                            goto fail;\n+                        }\n+                        goto next_step;\n+                    }\n+                    else {\n+                    npy_gil_error(PyExc_ValueError,",
    "comment": "Indentation off",
    "line_number": 1353,
    "enriched": "File: numpy/_core/src/umath/stringdtype_ufuncs.cpp\nCode: @@ -1330,11 +1332,29 @@ string_replace_strided_loop(\n             goto fail;\n         }\n         else if (i1_isnull || i2_isnull || i3_isnull) {\n-            if (!has_string_na) {\n-                npy_gil_error(PyExc_ValueError,\n-                              \"Null values are not supported as replacement arguments \"\n-                              \"for replace\");\n-                goto fail;\n+            if (has_null && !has_string_na) {\n+                if (i2_isnull || i3_isnull) {\n+                    npy_gil_error(PyExc_ValueError,\n+                                  \"Null values are not supported as search \"\n+                                  \"patterns or replacement strings for \"\n+                                  \"replace\");\n+                    goto fail;\n+                }\n+                else if (i1_isnull) {\n+                    if (has_nan_na) {\n+                        if (NpyString_pack_null(oallocator, ops) < 0) {\n+                            npy_gil_error(PyExc_MemoryError,\n+                                          \"Failed to deallocate string in replace\");\n+                            goto fail;\n+                        }\n+                        goto next_step;\n+                    }\n+                    else {\n+                    npy_gil_error(PyExc_ValueError,\nComment: Indentation off",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "numpy/_core/src/umath/stringdtype_ufuncs.cpp",
    "pr_number": 26355,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 1582157278,
    "comment_created_at": "2024-04-28T13:17:52Z"
  },
  {
    "code": "@@ -1165,6 +1165,10 @@ User-defined data types\n     *totype*. Any old casting function is over-written. A ``0`` is\n     returned on success or a ``-1`` on failure.\n \n+    .. c:type:: PyArray_VectorUnaryFunc",
    "comment": "Is the indentation correct?",
    "line_number": 1168,
    "enriched": "File: doc/source/reference/c-api/array.rst\nCode: @@ -1165,6 +1165,10 @@ User-defined data types\n     *totype*. Any old casting function is over-written. A ``0`` is\n     returned on success or a ``-1`` on failure.\n \n+    .. c:type:: PyArray_VectorUnaryFunc\nComment: Is the indentation correct?",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "doc/source/reference/c-api/array.rst",
    "pr_number": 25851,
    "repo": "numpy",
    "owner": "numpy",
    "comment_id": 1493870088,
    "comment_created_at": "2024-02-18T23:02:03Z"
  },
  {
    "code": "@@ -2092,4 +2092,4 @@ def test_even_thresholds_correctness_2(self, metric_cls):\n \n \n if __name__ == '__main__':\n-  tf.test.main()\n+    tf.test.main()",
    "comment": "Corrected Indentation from two spaces to ideal 4 spaces.",
    "line_number": 2095,
    "enriched": "File: keras/metrics/confusion_matrix_test.py\nCode: @@ -2092,4 +2092,4 @@ def test_even_thresholds_correctness_2(self, metric_cls):\n \n \n if __name__ == '__main__':\n-  tf.test.main()\n+    tf.test.main()\nComment: Corrected Indentation from two spaces to ideal 4 spaces.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "keras/metrics/confusion_matrix_test.py",
    "pr_number": 16695,
    "repo": "keras",
    "owner": "keras-team",
    "comment_id": 900691008,
    "comment_created_at": "2022-06-18T03:30:13Z"
  },
  {
    "code": "@@ -522,6 +510,15 @@ def create(cls, *children, **props) -> Component:\n             height=props.pop(\"height\", \"100%\"),\n         )\n \n+    def get_event_triggers(self) -> dict[str, Union[Var, Any]]:\n+        \"\"\"Get the event triggers that pass the component's value to the handler.\n+        Returns:\n+            A dict mapping the event trigger to the var that is passed to the handler.\n+        \"\"\"\n+        return {\n+            EventTriggers.ON_ANIMATION_START: lambda: [],\n+            EventTriggers.ON_ANIMATION_START: lambda: [],",
    "comment": "typo with copy/paste? should be END I guess",
    "line_number": 520,
    "enriched": "File: reflex/components/recharts/charts.py\nCode: @@ -522,6 +510,15 @@ def create(cls, *children, **props) -> Component:\n             height=props.pop(\"height\", \"100%\"),\n         )\n \n+    def get_event_triggers(self) -> dict[str, Union[Var, Any]]:\n+        \"\"\"Get the event triggers that pass the component's value to the handler.\n+        Returns:\n+            A dict mapping the event trigger to the var that is passed to the handler.\n+        \"\"\"\n+        return {\n+            EventTriggers.ON_ANIMATION_START: lambda: [],\n+            EventTriggers.ON_ANIMATION_START: lambda: [],\nComment: typo with copy/paste? should be END I guess",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "reflex/components/recharts/charts.py",
    "pr_number": 3430,
    "repo": "reflex",
    "owner": "reflex-dev",
    "comment_id": 1625156717,
    "comment_created_at": "2024-06-03T23:25:37Z"
  },
  {
    "code": "@@ -1,64 +1,183 @@\n+import importlib\n from functools import wraps\n+from typing import Protocol, runtime_checkable\n \n+import numpy as np\n from scipy.sparse import issparse\n \n from .._config import get_config\n-from . import check_pandas_support\n from ._available_if import available_if\n-from .validation import _is_pandas_df\n \n \n-def _wrap_in_pandas_container(\n-    data_to_wrap,\n-    *,\n-    columns,\n-    index=None,\n-):\n-    \"\"\"Create a Pandas DataFrame.\n+def check_library_installed(library):\n+    \"\"\"Check library is installed\"\"\"\n+    try:\n+        return importlib.import_module(library)\n+    except ImportError as e:\n+        raise ImportError(\n+            f\"Setting output container to '{library}' requires {library} to be\"\n+            \" installed\"\n+        ) from e\n \n-    If `data_to_wrap` is a DataFrame, then the `columns` and `index` will be changed\n-    inplace. If `data_to_wrap` is a ndarray, then a new DataFrame is created with\n-    `columns` and `index`.\n \n-    Parameters\n-    ----------\n-    data_to_wrap : {ndarray, dataframe}\n-        Data to be wrapped as pandas dataframe.\n+def get_columns(columns):\n+    if callable(columns):\n+        try:\n+            return columns()\n+        except Exception:\n+            return None\n+    return columns\n \n-    columns : callable, ndarray, or None\n-        The column names or a callable that returns the column names. The\n-        callable is useful if the column names require some computation.\n-        If `columns` is a callable that raises an error, `columns` will have\n-        the same semantics as `None`. If `None` and `data_to_wrap` is already a\n-        dataframe, then the column names are not changed. If `None` and\n-        `data_to_wrap` is **not** a dataframe, then columns are\n-        `range(n_features)`.\n \n-    index : array-like, default=None\n-        Index for data. `index` is ignored if `data_to_wrap` is already a DataFrame.\n+@runtime_checkable\n+class ContainerAdapaterProtocol(Protocol):",
    "comment": "Same typo in multiple places\r\n```suggestion\r\nclass ContainerAdapterProtocol(Protocol):\r\n```",
    "line_number": 33,
    "enriched": "File: sklearn/utils/_set_output.py\nCode: @@ -1,64 +1,183 @@\n+import importlib\n from functools import wraps\n+from typing import Protocol, runtime_checkable\n \n+import numpy as np\n from scipy.sparse import issparse\n \n from .._config import get_config\n-from . import check_pandas_support\n from ._available_if import available_if\n-from .validation import _is_pandas_df\n \n \n-def _wrap_in_pandas_container(\n-    data_to_wrap,\n-    *,\n-    columns,\n-    index=None,\n-):\n-    \"\"\"Create a Pandas DataFrame.\n+def check_library_installed(library):\n+    \"\"\"Check library is installed\"\"\"\n+    try:\n+        return importlib.import_module(library)\n+    except ImportError as e:\n+        raise ImportError(\n+            f\"Setting output container to '{library}' requires {library} to be\"\n+            \" installed\"\n+        ) from e\n \n-    If `data_to_wrap` is a DataFrame, then the `columns` and `index` will be changed\n-    inplace. If `data_to_wrap` is a ndarray, then a new DataFrame is created with\n-    `columns` and `index`.\n \n-    Parameters\n-    ----------\n-    data_to_wrap : {ndarray, dataframe}\n-        Data to be wrapped as pandas dataframe.\n+def get_columns(columns):\n+    if callable(columns):\n+        try:\n+            return columns()\n+        except Exception:\n+            return None\n+    return columns\n \n-    columns : callable, ndarray, or None\n-        The column names or a callable that returns the column names. The\n-        callable is useful if the column names require some computation.\n-        If `columns` is a callable that raises an error, `columns` will have\n-        the same semantics as `None`. If `None` and `data_to_wrap` is already a\n-        dataframe, then the column names are not changed. If `None` and\n-        `data_to_wrap` is **not** a dataframe, then columns are\n-        `range(n_features)`.\n \n-    index : array-like, default=None\n-        Index for data. `index` is ignored if `data_to_wrap` is already a DataFrame.\n+@runtime_checkable\n+class ContainerAdapaterProtocol(Protocol):\nComment: Same typo in multiple places\r\n```suggestion\r\nclass ContainerAdapterProtocol(Protocol):\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "sklearn/utils/_set_output.py",
    "pr_number": 27315,
    "repo": "scikit-learn",
    "owner": "scikit-learn",
    "comment_id": 1376456083,
    "comment_created_at": "2023-10-30T15:57:22Z"
  },
  {
    "code": "@@ -1142,23 +1142,22 @@ def _save_table(\n                         ),\n                     )[\"_order__max\"]\n                 )\n-            fields = [\n+            insert_fields = [",
    "comment": "I changed the variable name as it made it much easier for me to reason about the origin of the `f.generated and (pk_set or f is not meta.auto_field)` check.",
    "line_number": 1145,
    "enriched": "File: django/db/models/base.py\nCode: @@ -1142,23 +1142,22 @@ def _save_table(\n                         ),\n                     )[\"_order__max\"]\n                 )\n-            fields = [\n+            insert_fields = [\nComment: I changed the variable name as it made it much easier for me to reason about the origin of the `f.generated and (pk_set or f is not meta.auto_field)` check.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "django/db/models/base.py",
    "pr_number": 19871,
    "repo": "django",
    "owner": "django",
    "comment_id": 2353771065,
    "comment_created_at": "2025-09-16T22:14:50Z"
  },
  {
    "code": "@@ -1153,7 +1153,8 @@ def _save_table(\n                     getattr(self, field.attname) if raw else field.pre_save(self, False)\n                 )\n                 if hasattr(value, \"resolve_expression\"):\n-                    returning_fields.append(field)\n+                    if field not in returning_fields:",
    "comment": "In tandem with #19868 we can collapse this when the else is gone.",
    "line_number": 1156,
    "enriched": "File: django/db/models/base.py\nCode: @@ -1153,7 +1153,8 @@ def _save_table(\n                     getattr(self, field.attname) if raw else field.pre_save(self, False)\n                 )\n                 if hasattr(value, \"resolve_expression\"):\n-                    returning_fields.append(field)\n+                    if field not in returning_fields:\nComment: In tandem with #19868 we can collapse this when the else is gone.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "django/db/models/base.py",
    "pr_number": 19869,
    "repo": "django",
    "owner": "django",
    "comment_id": 2352984002,
    "comment_created_at": "2025-09-16T16:03:26Z"
  },
  {
    "code": "@@ -313,7 +315,7 @@ def no_available_apps(self):\n             \"Please define available_apps in TransactionTestCase and its subclasses.\"\n         )\n \n-    TransactionTestCase.available_apps = property(no_available_apps)\n+    TransactionTestCase.available_apps = classproperty(no_available_apps)",
    "comment": "Suggest we rename the argument of `no_available_apps` from `self` to `cls`.",
    "line_number": 317,
    "enriched": "File: tests/runtests.py\nCode: @@ -313,7 +315,7 @@ def no_available_apps(self):\n             \"Please define available_apps in TransactionTestCase and its subclasses.\"\n         )\n \n-    TransactionTestCase.available_apps = property(no_available_apps)\n+    TransactionTestCase.available_apps = classproperty(no_available_apps)\nComment: Suggest we rename the argument of `no_available_apps` from `self` to `cls`.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "tests/runtests.py",
    "pr_number": 18628,
    "repo": "django",
    "owner": "django",
    "comment_id": 1863556042,
    "comment_created_at": "2024-11-29T13:51:28Z"
  },
  {
    "code": "@@ -1169,7 +1169,7 @@ def now(parser, token):\n     return NowNode(format_string, asvar)\n \n \n-@register.simple_tag(takes_context=True)\n+@register.simple_tag(name=\"querystring\", takes_context=True)\n def query_string(context, query_dict=None, **kwargs):",
    "comment": "Since the tests were renamed to use `querystring`, is there a reason to not rename the function as well?",
    "line_number": 1173,
    "enriched": "File: django/template/defaulttags.py\nCode: @@ -1169,7 +1169,7 @@ def now(parser, token):\n     return NowNode(format_string, asvar)\n \n \n-@register.simple_tag(takes_context=True)\n+@register.simple_tag(name=\"querystring\", takes_context=True)\n def query_string(context, query_dict=None, **kwargs):\nComment: Since the tests were renamed to use `querystring`, is there a reason to not rename the function as well?",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "django/template/defaulttags.py",
    "pr_number": 18323,
    "repo": "django",
    "owner": "django",
    "comment_id": 1677857276,
    "comment_created_at": "2024-07-15T13:44:05Z"
  },
  {
    "code": "@@ -126,11 +126,31 @@ You could also write a decorator to encapsulate this logic::\n Just be aware that this logic will disable the signals whenever fixtures are\n deserialized, not just during ``loaddata``.\n \n-Note that the order in which fixture files are processed is undefined. However,\n-all fixture data is installed as a single transaction, so data in\n-one fixture can reference data in another fixture. If the database backend\n-supports row-level constraints, these constraints will be checked at the\n-end of the transaction.\n+Fixtures can be loaded in a specified order. For example:\n+\n+.. code-block:: shell\n+\n+    manage.py loaddata foo bar baz\n+\n+or (in a test case)\n+\n+.. code-block:: shell\n+\n+    Fixtures = ['foo', 'bar', 'baz']",
    "comment": "If I were writing this, I might use more real-sounding names, like `people`, `places`, `events` - I find that helps me imagine an example better.",
    "line_number": 139,
    "enriched": "File: docs/topics/db/fixtures.txt\nCode: @@ -126,11 +126,31 @@ You could also write a decorator to encapsulate this logic::\n Just be aware that this logic will disable the signals whenever fixtures are\n deserialized, not just during ``loaddata``.\n \n-Note that the order in which fixture files are processed is undefined. However,\n-all fixture data is installed as a single transaction, so data in\n-one fixture can reference data in another fixture. If the database backend\n-supports row-level constraints, these constraints will be checked at the\n-end of the transaction.\n+Fixtures can be loaded in a specified order. For example:\n+\n+.. code-block:: shell\n+\n+    manage.py loaddata foo bar baz\n+\n+or (in a test case)\n+\n+.. code-block:: shell\n+\n+    Fixtures = ['foo', 'bar', 'baz']\nComment: If I were writing this, I might use more real-sounding names, like `people`, `places`, `events` - I find that helps me imagine an example better.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/topics/db/fixtures.txt",
    "pr_number": 17384,
    "repo": "django",
    "owner": "django",
    "comment_id": 1366078761,
    "comment_created_at": "2023-10-19T20:30:23Z"
  },
  {
    "code": "@@ -549,8 +551,18 @@ def test_select_multiple(self):\n         with self.select2_ajax_wait():\n             # Select the result.\n             search.send_keys(Keys.RETURN)\n-            # Reopen the dropdown and add the first result to the selection.\n+        with self.disable_implicit_wait():\n+            with self.assertRaises(NoSuchElementException):\n+                self.selenium.find_element(By.CSS_SELECTOR, \".select2-results\")\n+        with self.select2_ajax_wait():\n+            # Reopen the dropdown.\n             elem.click()\n+        result_container = self.selenium.find_element(\n+            By.CSS_SELECTOR, \".select2-results\"\n+        )\n+        self.assertTrue(result_container.is_displayed())",
    "comment": "```suggestion\r\n        self.assertIs(result_container.is_displayed(), True)\r\n```",
    "line_number": 563,
    "enriched": "File: tests/admin_views/test_autocomplete_view.py\nCode: @@ -549,8 +551,18 @@ def test_select_multiple(self):\n         with self.select2_ajax_wait():\n             # Select the result.\n             search.send_keys(Keys.RETURN)\n-            # Reopen the dropdown and add the first result to the selection.\n+        with self.disable_implicit_wait():\n+            with self.assertRaises(NoSuchElementException):\n+                self.selenium.find_element(By.CSS_SELECTOR, \".select2-results\")\n+        with self.select2_ajax_wait():\n+            # Reopen the dropdown.\n             elem.click()\n+        result_container = self.selenium.find_element(\n+            By.CSS_SELECTOR, \".select2-results\"\n+        )\n+        self.assertTrue(result_container.is_displayed())\nComment: ```suggestion\r\n        self.assertIs(result_container.is_displayed(), True)\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "tests/admin_views/test_autocomplete_view.py",
    "pr_number": 17378,
    "repo": "django",
    "owner": "django",
    "comment_id": 1365256827,
    "comment_created_at": "2023-10-19T10:11:04Z"
  },
  {
    "code": "@@ -297,10 +297,15 @@ def lookup_field(name, obj, model_admin=None):\n             attr = getattr(model_admin, name)\n             value = attr(obj)\n         else:\n-            attr = getattr(obj, name)\n+            sentinel = object()\n+            attr = getattr(obj, name, sentinel)\n             if callable(attr):\n                 value = attr()\n             else:\n+                if attr is sentinel:\n+                    attr = obj\n+                    for part in name.split(LOOKUP_SEP):\n+                        attr = getattr(attr, part)",
    "comment": "There is a potentially un-handled `AttributeError` in this line, for example when `attr` is a model field with `null=True`.\r\n\r\nAn test that triggers this could be:\r\n\r\n```diff\r\ndiff --git a/tests/admin_changelist/tests.py b/tests/admin_changelist/tests.py\r\nindex 0db8cda4a0..9da3b4d8a2 100644\r\n--- a/tests/admin_changelist/tests.py\r\n+++ b/tests/admin_changelist/tests.py\r\n@@ -1629,6 +1629,21 @@ class ChangeListTests(TestCase):\r\n         self.assertContains(response, parent.name)\r\n         self.assertContains(response, child.name)\r\n \r\n+    def test_list_display_foreign_field_null(self):\r\n+        parent = Parent.objects.create(name=\"I am your father\")\r\n+        child = Child.objects.create(name=\"I am your child\", parent=parent)\r\n+        GrandChild.objects.create(name=\"I am parentless\", parent=None)\r\n+        request = self.factory.get(\"/grandchild/\")\r\n+        request.user = self.superuser\r\n+\r\n+        class GrandChildAdmin(admin.ModelAdmin):\r\n+            list_display = [\"parent__name\", \"parent__parent__name\"]\r\n+\r\n+        m = GrandChildAdmin(GrandChild, custom_site)\r\n+        response = m.changelist_view(request)\r\n+        self.assertContains(response, parent.name)\r\n+        self.assertContains(response, child.name)\r\n+\r\n \r\n class GetAdminLogTests(TestCase):\r\n     def test_custom_user_pk_not_named_id(self):\r\n```\r\n\r\nError is:\r\n```\r\nAttributeError: 'NoneType' object has no attribute 'name'\r\n```",
    "line_number": 308,
    "enriched": "File: django/contrib/admin/utils.py\nCode: @@ -297,10 +297,15 @@ def lookup_field(name, obj, model_admin=None):\n             attr = getattr(model_admin, name)\n             value = attr(obj)\n         else:\n-            attr = getattr(obj, name)\n+            sentinel = object()\n+            attr = getattr(obj, name, sentinel)\n             if callable(attr):\n                 value = attr()\n             else:\n+                if attr is sentinel:\n+                    attr = obj\n+                    for part in name.split(LOOKUP_SEP):\n+                        attr = getattr(attr, part)\nComment: There is a potentially un-handled `AttributeError` in this line, for example when `attr` is a model field with `null=True`.\r\n\r\nAn test that triggers this could be:\r\n\r\n```diff\r\ndiff --git a/tests/admin_changelist/tests.py b/tests/admin_changelist/tests.py\r\nindex 0db8cda4a0..9da3b4d8a2 100644\r\n--- a/tests/admin_changelist/tests.py\r\n+++ b/tests/admin_changelist/tests.py\r\n@@ -1629,6 +1629,21 @@ class ChangeListTests(TestCase):\r\n         self.assertContains(response, parent.name)\r\n         self.assertContains(response, child.name)\r\n \r\n+    def test_list_display_foreign_field_null(self):\r\n+        parent = Parent.objects.create(name=\"I am your father\")\r\n+        child = Child.objects.create(name=\"I am your child\", parent=parent)\r\n+        GrandChild.objects.create(name=\"I am parentless\", parent=None)\r\n+        request = self.factory.get(\"/grandchild/\")\r\n+        request.user = self.superuser\r\n+\r\n+        class GrandChildAdmin(admin.ModelAdmin):\r\n+            list_display = [\"parent__name\", \"parent__parent__name\"]\r\n+\r\n+        m = GrandChildAdmin(GrandChild, custom_site)\r\n+        response = m.changelist_view(request)\r\n+        self.assertContains(response, parent.name)\r\n+        self.assertContains(response, child.name)\r\n+\r\n \r\n class GetAdminLogTests(TestCase):\r\n     def test_custom_user_pk_not_named_id(self):\r\n```\r\n\r\nError is:\r\n```\r\nAttributeError: 'NoneType' object has no attribute 'name'\r\n```",
    "subcategory": "validation",
    "category": "functional",
    "file_path": "django/contrib/admin/utils.py",
    "pr_number": 17357,
    "repo": "django",
    "owner": "django",
    "comment_id": 1358421019,
    "comment_created_at": "2023-10-13T15:33:38Z"
  },
  {
    "code": "@@ -1554,6 +1554,19 @@ def test_validate_multiline_headers(self):\n                 \"Subject\\nMultiline\", \"Content\", \"from@example.com\", [\"to@example.com\"]\n             )\n \n+    def test_mutate_after_send(self) -> None:",
    "comment": "Please remove type annotations. We don't currently use them in Django.",
    "line_number": 1557,
    "enriched": "File: tests/mail/tests.py\nCode: @@ -1554,6 +1554,19 @@ def test_validate_multiline_headers(self):\n                 \"Subject\\nMultiline\", \"Content\", \"from@example.com\", [\"to@example.com\"]\n             )\n \n+    def test_mutate_after_send(self) -> None:\nComment: Please remove type annotations. We don't currently use them in Django.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/mail/tests.py",
    "pr_number": 17377,
    "repo": "django",
    "owner": "django",
    "comment_id": 1365092791,
    "comment_created_at": "2023-10-19T08:13:29Z"
  }
]